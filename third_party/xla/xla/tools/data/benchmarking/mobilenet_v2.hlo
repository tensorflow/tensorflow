HloModule IrToHlo.3979, entry_computation_layout={(f32[32]{0},f32[32]{0},f32[32]{0},f32[32]{0},f32[32,3,3,3]{3,2,1,0},f32[96,3,224,224]{3,2,1,0},s64[],f32[32]{0},f32[32]{0},f32[32]{0},f32[32]{0},f32[32,1,3,3]{3,2,1,0},f32[],s64[],f32[16]{0},f32[16]{0},f32[16]{0},f32[16]{0},f32[16,32,1,1]{3,2,1,0},s64[],f32[96]{0},f32[96]{0},f32[96]{0},f32[96]{0},f32[96,16,1,1]{3,2,1,0},s64[],f32[96]{0},f32[96]{0},f32[96]{0},f32[96]{0},f32[96,1,3,3]{3,2,1,0},s64[],f32[24]{0},f32[24]{0},f32[24]{0},f32[24]{0},f32[24,96,1,1]{3,2,1,0},s64[],f32[144]{0},f32[144]{0},f32[144]{0},f32[144]{0},f32[144,24,1,1]{3,2,1,0},s64[],f32[144]{0},f32[144]{0},f32[144]{0},f32[144]{0},f32[144,1,3,3]{3,2,1,0},s64[],f32[24]{0},f32[24]{0},f32[24]{0},f32[24]{0},f32[24,144,1,1]{3,2,1,0},s64[],f32[144]{0},f32[144]{0},f32[144]{0},f32[144]{0},f32[144,24,1,1]{3,2,1,0},s64[],f32[144]{0},f32[144]{0},f32[144]{0},f32[144]{0},f32[144,1,3,3]{3,2,1,0},s64[],f32[32]{0},f32[32]{0},f32[32]{0},f32[32]{0},f32[32,144,1,1]{3,2,1,0},s64[],f32[192]{0},f32[192]{0},f32[192]{0},f32[192]{0},f32[192,32,1,1]{3,2,1,0},s64[],f32[192]{0},f32[192]{0},f32[192]{0},f32[192]{0},f32[192,1,3,3]{3,2,1,0},s64[],f32[32]{0},f32[32]{0},f32[32]{0},f32[32]{0},f32[32,192,1,1]{3,2,1,0},s64[],f32[192]{0},f32[192]{0},f32[192]{0},f32[192]{0},f32[192,32,1,1]{3,2,1,0},s64[],f32[192]{0},f32[192]{0},f32[192]{0},f32[192]{0},f32[192,1,3,3]{3,2,1,0},s64[],f32[32]{0},f32[32]{0},f32[32]{0},f32[32]{0},f32[32,192,1,1]{3,2,1,0},s64[],f32[192]{0},f32[192]{0},f32[192]{0},f32[192]{0},f32[192,32,1,1]{3,2,1,0},s64[],f32[192]{0},f32[192]{0},f32[192]{0},f32[192]{0},f32[192,1,3,3]{3,2,1,0},s64[],f32[64]{0},f32[64]{0},f32[64]{0},f32[64]{0},f32[64,192,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,64,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,1,3,3]{3,2,1,0},s64[],f32[64]{0},f32[64]{0},f32[64]{0},f32[64]{0},f32[64,384,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,64,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,1,3,3]{3,2,1,0},s64[],f32[64]{0},f32[64]{0},f32[64]{0},f32[64]{0},f32[64,384,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,64,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,1,3,3]{3,2,1,0},s64[],f32[64]{0},f32[64]{0},f32[64]{0},f32[64]{0},f32[64,384,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,64,1,1]{3,2,1,0},s64[],f32[384]{0},f32[384]{0},f32[384]{0},f32[384]{0},f32[384,1,3,3]{3,2,1,0},s64[],f32[96]{0},f32[96]{0},f32[96]{0},f32[96]{0},f32[96,384,1,1]{3,2,1,0},s64[],f32[576]{0},f32[576]{0},f32[576]{0},f32[576]{0},f32[576,96,1,1]{3,2,1,0},s64[],f32[576]{0},f32[576]{0},f32[576]{0},f32[576]{0},f32[576,1,3,3]{3,2,1,0},s64[],f32[96]{0},f32[96]{0},f32[96]{0},f32[96]{0},f32[96,576,1,1]{3,2,1,0},s64[],f32[576]{0},f32[576]{0},f32[576]{0},f32[576]{0},f32[576,96,1,1]{3,2,1,0},s64[],f32[576]{0},f32[576]{0},f32[576]{0},f32[576]{0},f32[576,1,3,3]{3,2,1,0},s64[],f32[96]{0},f32[96]{0},f32[96]{0},f32[96]{0},f32[96,576,1,1]{3,2,1,0},s64[],f32[576]{0},f32[576]{0},f32[576]{0},f32[576]{0},f32[576,96,1,1]{3,2,1,0},s64[],f32[576]{0},f32[576]{0},f32[576]{0},f32[576]{0},f32[576,1,3,3]{3,2,1,0},s64[],f32[160]{0},f32[160]{0},f32[160]{0},f32[160]{0},f32[160,576,1,1]{3,2,1,0},s64[],f32[960]{0},f32[960]{0},f32[960]{0},f32[960]{0},f32[960,160,1,1]{3,2,1,0},s64[],f32[960]{0},f32[960]{0},f32[960]{0},f32[960]{0},f32[960,1,3,3]{3,2,1,0},s64[],f32[160]{0},f32[160]{0},f32[160]{0},f32[160]{0},f32[160,960,1,1]{3,2,1,0},s64[],f32[960]{0},f32[960]{0},f32[960]{0},f32[960]{0},f32[960,160,1,1]{3,2,1,0},s64[],f32[960]{0},f32[960]{0},f32[960]{0},f32[960]{0},f32[960,1,3,3]{3,2,1,0},s64[],f32[160]{0},f32[160]{0},f32[160]{0},f32[160]{0},f32[160,960,1,1]{3,2,1,0},s64[],f32[960]{0},f32[960]{0},f32[960]{0},f32[960]{0},f32[960,160,1,1]{3,2,1,0},s64[],f32[960]{0},f32[960]{0},f32[960]{0},f32[960]{0},f32[960,1,3,3]{3,2,1,0},s64[],f32[320]{0},f32[320]{0},f32[320]{0},f32[320]{0},f32[320,960,1,1]{3,2,1,0},s64[],f32[1280]{0},f32[1280]{0},f32[1280]{0},f32[1280]{0},f32[1280,320,1,1]{3,2,1,0},s64[],f32[],f32[],s64[],f32[],f32[1000,1280]{1,0})->(f32[32]{0}, f32[32]{0}, s64[], f32[32]{0}, f32[32]{0}, /*index=5*/s64[], f32[16]{0}, f32[16]{0}, s64[], f32[96]{0}, /*index=10*/f32[96]{0}, s64[], f32[96]{0}, f32[96]{0}, s64[], /*index=15*/f32[24]{0}, f32[24]{0}, s64[], f32[144]{0}, f32[144]{0}, /*index=20*/s64[], f32[144]{0}, f32[144]{0}, s64[], f32[24]{0}, /*index=25*/f32[24]{0}, s64[], f32[144]{0}, f32[144]{0}, s64[], /*index=30*/f32[144]{0}, f32[144]{0}, s64[], f32[32]{0}, f32[32]{0}, /*index=35*/s64[], f32[192]{0}, f32[192]{0}, s64[], f32[192]{0}, /*index=40*/f32[192]{0}, s64[], f32[32]{0}, f32[32]{0}, s64[], /*index=45*/f32[192]{0}, f32[192]{0}, s64[], f32[192]{0}, f32[192]{0}, /*index=50*/s64[], f32[32]{0}, f32[32]{0}, s64[], f32[192]{0}, /*index=55*/f32[192]{0}, s64[], f32[192]{0}, f32[192]{0}, s64[], /*index=60*/f32[64]{0}, f32[64]{0}, s64[], f32[384]{0}, f32[384]{0}, /*index=65*/s64[], f32[384]{0}, f32[384]{0}, s64[], f32[64]{0}, /*index=70*/f32[64]{0}, s64[], f32[384]{0}, f32[384]{0}, s64[], /*index=75*/f32[384]{0}, f32[384]{0}, s64[], f32[64]{0}, f32[64]{0}, /*index=80*/s64[], f32[384]{0}, f32[384]{0}, s64[], f32[384]{0}, /*index=85*/f32[384]{0}, s64[], f32[64]{0}, f32[64]{0}, s64[], /*index=90*/f32[384]{0}, f32[384]{0}, s64[], f32[384]{0}, f32[384]{0}, /*index=95*/s64[], f32[96]{0}, f32[96]{0}, s64[], f32[576]{0}, /*index=100*/f32[576]{0}, s64[], f32[576]{0}, f32[576]{0}, s64[], /*index=105*/f32[96]{0}, f32[96]{0}, s64[], f32[576]{0}, f32[576]{0}, /*index=110*/s64[], f32[576]{0}, f32[576]{0}, s64[], f32[96]{0}, /*index=115*/f32[96]{0}, s64[], f32[576]{0}, f32[576]{0}, s64[], /*index=120*/f32[576]{0}, f32[576]{0}, s64[], f32[160]{0}, f32[160]{0}, /*index=125*/s64[], f32[960]{0}, f32[960]{0}, s64[], f32[960]{0}, /*index=130*/f32[960]{0}, s64[], f32[160]{0}, f32[160]{0}, s64[], /*index=135*/f32[960]{0}, f32[960]{0}, s64[], f32[960]{0}, f32[960]{0}, /*index=140*/s64[], f32[160]{0}, f32[160]{0}, s64[], f32[960]{0}, /*index=145*/f32[960]{0}, s64[], f32[960]{0}, f32[960]{0}, s64[], /*index=150*/f32[320]{0}, f32[320]{0}, s64[], f32[1280]{0}, f32[1280]{0}, /*index=155*/s64[], f32[1000]{0}, f32[1000,1280]{0,1}, f32[1280]{0}, f32[1280]{0}, /*index=160*/f32[1280,320,1,1]{0,1,3,2}, f32[320]{0}, f32[320]{0}, f32[320,960,1,1]{0,1,3,2}, f32[960]{0}, /*index=165*/f32[960]{0}, f32[960,1,3,3]{0,1,3,2}, f32[960]{0}, f32[960]{0}, f32[960,160,1,1]{0,1,3,2}, /*index=170*/f32[160]{0}, f32[160]{0}, f32[160,960,1,1]{0,1,3,2}, f32[960]{0}, f32[960]{0}, /*index=175*/f32[960,1,3,3]{0,1,3,2}, f32[960]{0}, f32[960]{0}, f32[960,160,1,1]{0,1,3,2}, f32[160]{0}, /*index=180*/f32[160]{0}, f32[160,960,1,1]{0,1,3,2}, f32[960]{0}, f32[960]{0}, f32[960,1,3,3]{0,1,3,2}, /*index=185*/f32[960]{0}, f32[960]{0}, f32[960,160,1,1]{0,1,3,2}, f32[160]{0}, f32[160]{0}, /*index=190*/f32[160,576,1,1]{0,1,3,2}, f32[576]{0}, f32[576]{0}, f32[576,1,3,3]{0,1,3,2}, f32[576]{0}, /*index=195*/f32[576]{0}, f32[576,96,1,1]{0,1,3,2}, f32[96]{0}, f32[96]{0}, f32[96,576,1,1]{0,1,3,2}, /*index=200*/f32[576]{0}, f32[576]{0}, f32[576,1,3,3]{0,1,3,2}, f32[576]{0}, f32[576]{0}, /*index=205*/f32[576,96,1,1]{0,1,3,2}, f32[96]{0}, f32[96]{0}, f32[96,576,1,1]{0,1,3,2}, f32[576]{0}, /*index=210*/f32[576]{0}, f32[576,1,3,3]{0,1,3,2}, f32[576]{0}, f32[576]{0}, f32[576,96,1,1]{0,1,3,2}, /*index=215*/f32[96]{0}, f32[96]{0}, f32[96,384,1,1]{0,1,3,2}, f32[384]{0}, f32[384]{0}, /*index=220*/f32[384,1,3,3]{0,1,3,2}, f32[384]{0}, f32[384]{0}, f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, /*index=225*/f32[64]{0}, f32[64,384,1,1]{0,1,3,2}, f32[384]{0}, f32[384]{0}, f32[384,1,3,3]{0,1,3,2}, /*index=230*/f32[384]{0}, f32[384]{0}, f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, f32[64]{0}, /*index=235*/f32[64,384,1,1]{0,1,3,2}, f32[384]{0}, f32[384]{0}, f32[384,1,3,3]{0,1,3,2}, f32[384]{0}, /*index=240*/f32[384]{0}, f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, f32[64]{0}, f32[64,384,1,1]{0,1,3,2}, /*index=245*/f32[384]{0}, f32[384]{0}, f32[384,1,3,3]{0,1,3,2}, f32[384]{0}, f32[384]{0}, /*index=250*/f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, f32[64]{0}, f32[64,192,1,1]{0,1,3,2}, f32[192]{0}, /*index=255*/f32[192]{0}, f32[192,1,3,3]{0,1,3,2}, f32[192]{0}, f32[192]{0}, f32[192,32,1,1]{0,1,3,2}, /*index=260*/f32[32]{0}, f32[32]{0}, f32[32,192,1,1]{0,1,3,2}, f32[192]{0}, f32[192]{0}, /*index=265*/f32[192,1,3,3]{0,1,3,2}, f32[192]{0}, f32[192]{0}, f32[192,32,1,1]{0,1,3,2}, f32[32]{0}, /*index=270*/f32[32]{0}, f32[32,192,1,1]{0,1,3,2}, f32[192]{0}, f32[192]{0}, f32[192,1,3,3]{0,1,3,2}, /*index=275*/f32[192]{0}, f32[192]{0}, f32[192,32,1,1]{0,1,3,2}, f32[32]{0}, f32[32]{0}, /*index=280*/f32[32,144,1,1]{0,1,3,2}, f32[144]{0}, f32[144]{0}, f32[144,1,3,3]{0,1,3,2}, f32[144]{0}, /*index=285*/f32[144]{0}, f32[144,24,1,1]{0,1,3,2}, f32[24]{0}, f32[24]{0}, f32[24,144,1,1]{0,1,3,2}, /*index=290*/f32[144]{0}, f32[144]{0}, f32[144,1,3,3]{0,1,3,2}, f32[144]{0}, f32[144]{0}, /*index=295*/f32[144,24,1,1]{0,1,3,2}, f32[24]{0}, f32[24]{0}, f32[24,96,1,1]{0,1,3,2}, f32[96]{0}, /*index=300*/f32[96]{0}, f32[96,1,3,3]{0,1,3,2}, f32[96]{0}, f32[96]{0}, f32[96,16,1,1]{0,1,3,2}, /*index=305*/f32[16]{0}, f32[16]{0}, f32[16,32,1,1]{0,1,3,2}, f32[32]{0}, f32[32]{0}, /*index=310*/f32[32,1,3,3]{0,1,3,2}, f32[32]{0}, f32[32]{0}, f32[32,3,3,3]{0,1,3,2})}
 
%AddComputation.2038 (x.2039: f32[], y.2040: f32[]) -> f32[] {
  %x.2039 = f32[] parameter(0)
  %y.2040 = f32[] parameter(1)
  ROOT %add.2041 = f32[] add(f32[] %x.2039, f32[] %y.2040)
}
 
%AddComputation.2330 (x.2331: f32[], y.2332: f32[]) -> f32[] {
  %x.2331 = f32[] parameter(0)
  %y.2332 = f32[] parameter(1)
  ROOT %add.2333 = f32[] add(f32[] %x.2331, f32[] %y.2332)
}
 
%AddComputation.2390 (x.2391: f32[], y.2392: f32[]) -> f32[] {
  %x.2391 = f32[] parameter(0)
  %y.2392 = f32[] parameter(1)
  ROOT %add.2393 = f32[] add(f32[] %x.2391, f32[] %y.2392)
}
 
%AddComputation.2412 (x.2413: f32[], y.2414: f32[]) -> f32[] {
  %x.2413 = f32[] parameter(0)
  %y.2414 = f32[] parameter(1)
  ROOT %add.2415 = f32[] add(f32[] %x.2413, f32[] %y.2414)
}
 
%AddComputation.2447 (x.2448: f32[], y.2449: f32[]) -> f32[] {
  %x.2448 = f32[] parameter(0)
  %y.2449 = f32[] parameter(1)
  ROOT %add.2450 = f32[] add(f32[] %x.2448, f32[] %y.2449)
}
 
%AddComputation.2479 (x.2480: f32[], y.2481: f32[]) -> f32[] {
  %x.2480 = f32[] parameter(0)
  %y.2481 = f32[] parameter(1)
  ROOT %add.2482 = f32[] add(f32[] %x.2480, f32[] %y.2481)
}
 
%AddComputation.2501 (x.2502: f32[], y.2503: f32[]) -> f32[] {
  %x.2502 = f32[] parameter(0)
  %y.2503 = f32[] parameter(1)
  ROOT %add.2504 = f32[] add(f32[] %x.2502, f32[] %y.2503)
}
 
%AddComputation.2536 (x.2537: f32[], y.2538: f32[]) -> f32[] {
  %x.2537 = f32[] parameter(0)
  %y.2538 = f32[] parameter(1)
  ROOT %add.2539 = f32[] add(f32[] %x.2537, f32[] %y.2538)
}
 
%AddComputation.2568 (x.2569: f32[], y.2570: f32[]) -> f32[] {
  %x.2569 = f32[] parameter(0)
  %y.2570 = f32[] parameter(1)
  ROOT %add.2571 = f32[] add(f32[] %x.2569, f32[] %y.2570)
}
 
%AddComputation.2597 (x.2598: f32[], y.2599: f32[]) -> f32[] {
  %x.2598 = f32[] parameter(0)
  %y.2599 = f32[] parameter(1)
  ROOT %add.2600 = f32[] add(f32[] %x.2598, f32[] %y.2599)
}
 
%AddComputation.2632 (x.2633: f32[], y.2634: f32[]) -> f32[] {
  %x.2633 = f32[] parameter(0)
  %y.2634 = f32[] parameter(1)
  ROOT %add.2635 = f32[] add(f32[] %x.2633, f32[] %y.2634)
}
 
%AddComputation.2664 (x.2665: f32[], y.2666: f32[]) -> f32[] {
  %x.2665 = f32[] parameter(0)
  %y.2666 = f32[] parameter(1)
  ROOT %add.2667 = f32[] add(f32[] %x.2665, f32[] %y.2666)
}
 
%AddComputation.2693 (x.2694: f32[], y.2695: f32[]) -> f32[] {
  %x.2694 = f32[] parameter(0)
  %y.2695 = f32[] parameter(1)
  ROOT %add.2696 = f32[] add(f32[] %x.2694, f32[] %y.2695)
}
 
%AddComputation.2728 (x.2729: f32[], y.2730: f32[]) -> f32[] {
  %x.2729 = f32[] parameter(0)
  %y.2730 = f32[] parameter(1)
  ROOT %add.2731 = f32[] add(f32[] %x.2729, f32[] %y.2730)
}
 
%AddComputation.2760 (x.2761: f32[], y.2762: f32[]) -> f32[] {
  %x.2761 = f32[] parameter(0)
  %y.2762 = f32[] parameter(1)
  ROOT %add.2763 = f32[] add(f32[] %x.2761, f32[] %y.2762)
}
 
%AddComputation.2782 (x.2783: f32[], y.2784: f32[]) -> f32[] {
  %x.2783 = f32[] parameter(0)
  %y.2784 = f32[] parameter(1)
  ROOT %add.2785 = f32[] add(f32[] %x.2783, f32[] %y.2784)
}
 
%AddComputation.2817 (x.2818: f32[], y.2819: f32[]) -> f32[] {
  %x.2818 = f32[] parameter(0)
  %y.2819 = f32[] parameter(1)
  ROOT %add.2820 = f32[] add(f32[] %x.2818, f32[] %y.2819)
}
 
%AddComputation.2849 (x.2850: f32[], y.2851: f32[]) -> f32[] {
  %x.2850 = f32[] parameter(0)
  %y.2851 = f32[] parameter(1)
  ROOT %add.2852 = f32[] add(f32[] %x.2850, f32[] %y.2851)
}
 
%AddComputation.2878 (x.2879: f32[], y.2880: f32[]) -> f32[] {
  %x.2879 = f32[] parameter(0)
  %y.2880 = f32[] parameter(1)
  ROOT %add.2881 = f32[] add(f32[] %x.2879, f32[] %y.2880)
}
 
%AddComputation.2913 (x.2914: f32[], y.2915: f32[]) -> f32[] {
  %x.2914 = f32[] parameter(0)
  %y.2915 = f32[] parameter(1)
  ROOT %add.2916 = f32[] add(f32[] %x.2914, f32[] %y.2915)
}
 
%AddComputation.2945 (x.2946: f32[], y.2947: f32[]) -> f32[] {
  %x.2946 = f32[] parameter(0)
  %y.2947 = f32[] parameter(1)
  ROOT %add.2948 = f32[] add(f32[] %x.2946, f32[] %y.2947)
}
 
%AddComputation.2974 (x.2975: f32[], y.2976: f32[]) -> f32[] {
  %x.2975 = f32[] parameter(0)
  %y.2976 = f32[] parameter(1)
  ROOT %add.2977 = f32[] add(f32[] %x.2975, f32[] %y.2976)
}
 
%AddComputation.3009 (x.3010: f32[], y.3011: f32[]) -> f32[] {
  %x.3010 = f32[] parameter(0)
  %y.3011 = f32[] parameter(1)
  ROOT %add.3012 = f32[] add(f32[] %x.3010, f32[] %y.3011)
}
 
%AddComputation.3041 (x.3042: f32[], y.3043: f32[]) -> f32[] {
  %x.3042 = f32[] parameter(0)
  %y.3043 = f32[] parameter(1)
  ROOT %add.3044 = f32[] add(f32[] %x.3042, f32[] %y.3043)
}
 
%AddComputation.3063 (x.3064: f32[], y.3065: f32[]) -> f32[] {
  %x.3064 = f32[] parameter(0)
  %y.3065 = f32[] parameter(1)
  ROOT %add.3066 = f32[] add(f32[] %x.3064, f32[] %y.3065)
}
 
%AddComputation.3098 (x.3099: f32[], y.3100: f32[]) -> f32[] {
  %x.3099 = f32[] parameter(0)
  %y.3100 = f32[] parameter(1)
  ROOT %add.3101 = f32[] add(f32[] %x.3099, f32[] %y.3100)
}
 
%AddComputation.3130 (x.3131: f32[], y.3132: f32[]) -> f32[] {
  %x.3131 = f32[] parameter(0)
  %y.3132 = f32[] parameter(1)
  ROOT %add.3133 = f32[] add(f32[] %x.3131, f32[] %y.3132)
}
 
%AddComputation.3159 (x.3160: f32[], y.3161: f32[]) -> f32[] {
  %x.3160 = f32[] parameter(0)
  %y.3161 = f32[] parameter(1)
  ROOT %add.3162 = f32[] add(f32[] %x.3160, f32[] %y.3161)
}
 
%AddComputation.3194 (x.3195: f32[], y.3196: f32[]) -> f32[] {
  %x.3195 = f32[] parameter(0)
  %y.3196 = f32[] parameter(1)
  ROOT %add.3197 = f32[] add(f32[] %x.3195, f32[] %y.3196)
}
 
%AddComputation.3226 (x.3227: f32[], y.3228: f32[]) -> f32[] {
  %x.3227 = f32[] parameter(0)
  %y.3228 = f32[] parameter(1)
  ROOT %add.3229 = f32[] add(f32[] %x.3227, f32[] %y.3228)
}
 
%AddComputation.3255 (x.3256: f32[], y.3257: f32[]) -> f32[] {
  %x.3256 = f32[] parameter(0)
  %y.3257 = f32[] parameter(1)
  ROOT %add.3258 = f32[] add(f32[] %x.3256, f32[] %y.3257)
}
 
%AddComputation.3290 (x.3291: f32[], y.3292: f32[]) -> f32[] {
  %x.3291 = f32[] parameter(0)
  %y.3292 = f32[] parameter(1)
  ROOT %add.3293 = f32[] add(f32[] %x.3291, f32[] %y.3292)
}
 
%AddComputation.3322 (x.3323: f32[], y.3324: f32[]) -> f32[] {
  %x.3323 = f32[] parameter(0)
  %y.3324 = f32[] parameter(1)
  ROOT %add.3325 = f32[] add(f32[] %x.3323, f32[] %y.3324)
}
 
%AddComputation.3351 (x.3352: f32[], y.3353: f32[]) -> f32[] {
  %x.3352 = f32[] parameter(0)
  %y.3353 = f32[] parameter(1)
  ROOT %add.3354 = f32[] add(f32[] %x.3352, f32[] %y.3353)
}
 
%AddComputation.3386 (x.3387: f32[], y.3388: f32[]) -> f32[] {
  %x.3387 = f32[] parameter(0)
  %y.3388 = f32[] parameter(1)
  ROOT %add.3389 = f32[] add(f32[] %x.3387, f32[] %y.3388)
}
 
%AddComputation.3418 (x.3419: f32[], y.3420: f32[]) -> f32[] {
  %x.3419 = f32[] parameter(0)
  %y.3420 = f32[] parameter(1)
  ROOT %add.3421 = f32[] add(f32[] %x.3419, f32[] %y.3420)
}
 
%AddComputation.3440 (x.3441: f32[], y.3442: f32[]) -> f32[] {
  %x.3441 = f32[] parameter(0)
  %y.3442 = f32[] parameter(1)
  ROOT %add.3443 = f32[] add(f32[] %x.3441, f32[] %y.3442)
}
 
%AddComputation.3475 (x.3476: f32[], y.3477: f32[]) -> f32[] {
  %x.3476 = f32[] parameter(0)
  %y.3477 = f32[] parameter(1)
  ROOT %add.3478 = f32[] add(f32[] %x.3476, f32[] %y.3477)
}
 
%AddComputation.3507 (x.3508: f32[], y.3509: f32[]) -> f32[] {
  %x.3508 = f32[] parameter(0)
  %y.3509 = f32[] parameter(1)
  ROOT %add.3510 = f32[] add(f32[] %x.3508, f32[] %y.3509)
}
 
%AddComputation.3536 (x.3537: f32[], y.3538: f32[]) -> f32[] {
  %x.3537 = f32[] parameter(0)
  %y.3538 = f32[] parameter(1)
  ROOT %add.3539 = f32[] add(f32[] %x.3537, f32[] %y.3538)
}
 
%AddComputation.3571 (x.3572: f32[], y.3573: f32[]) -> f32[] {
  %x.3572 = f32[] parameter(0)
  %y.3573 = f32[] parameter(1)
  ROOT %add.3574 = f32[] add(f32[] %x.3572, f32[] %y.3573)
}
 
%AddComputation.3603 (x.3604: f32[], y.3605: f32[]) -> f32[] {
  %x.3604 = f32[] parameter(0)
  %y.3605 = f32[] parameter(1)
  ROOT %add.3606 = f32[] add(f32[] %x.3604, f32[] %y.3605)
}
 
%AddComputation.3632 (x.3633: f32[], y.3634: f32[]) -> f32[] {
  %x.3633 = f32[] parameter(0)
  %y.3634 = f32[] parameter(1)
  ROOT %add.3635 = f32[] add(f32[] %x.3633, f32[] %y.3634)
}
 
%AddComputation.3667 (x.3668: f32[], y.3669: f32[]) -> f32[] {
  %x.3668 = f32[] parameter(0)
  %y.3669 = f32[] parameter(1)
  ROOT %add.3670 = f32[] add(f32[] %x.3668, f32[] %y.3669)
}
 
%AddComputation.3699 (x.3700: f32[], y.3701: f32[]) -> f32[] {
  %x.3700 = f32[] parameter(0)
  %y.3701 = f32[] parameter(1)
  ROOT %add.3702 = f32[] add(f32[] %x.3700, f32[] %y.3701)
}
 
%AddComputation.3721 (x.3722: f32[], y.3723: f32[]) -> f32[] {
  %x.3722 = f32[] parameter(0)
  %y.3723 = f32[] parameter(1)
  ROOT %add.3724 = f32[] add(f32[] %x.3722, f32[] %y.3723)
}
 
%AddComputation.3756 (x.3757: f32[], y.3758: f32[]) -> f32[] {
  %x.3757 = f32[] parameter(0)
  %y.3758 = f32[] parameter(1)
  ROOT %add.3759 = f32[] add(f32[] %x.3757, f32[] %y.3758)
}
 
%AddComputation.3788 (x.3789: f32[], y.3790: f32[]) -> f32[] {
  %x.3789 = f32[] parameter(0)
  %y.3790 = f32[] parameter(1)
  ROOT %add.3791 = f32[] add(f32[] %x.3789, f32[] %y.3790)
}
 
%AddComputation.3817 (x.3818: f32[], y.3819: f32[]) -> f32[] {
  %x.3818 = f32[] parameter(0)
  %y.3819 = f32[] parameter(1)
  ROOT %add.3820 = f32[] add(f32[] %x.3818, f32[] %y.3819)
}
 
%AddComputation.3852 (x.3853: f32[], y.3854: f32[]) -> f32[] {
  %x.3853 = f32[] parameter(0)
  %y.3854 = f32[] parameter(1)
  ROOT %add.3855 = f32[] add(f32[] %x.3853, f32[] %y.3854)
}
 
%AddComputation.3884 (x.3885: f32[], y.3886: f32[]) -> f32[] {
  %x.3885 = f32[] parameter(0)
  %y.3886 = f32[] parameter(1)
  ROOT %add.3887 = f32[] add(f32[] %x.3885, f32[] %y.3886)
}
 
%AddComputation.3906 (x.3907: f32[], y.3908: f32[]) -> f32[] {
  %x.3907 = f32[] parameter(0)
  %y.3908 = f32[] parameter(1)
  ROOT %add.3909 = f32[] add(f32[] %x.3907, f32[] %y.3908)
}
 
%AddComputation.3941 (x.3942: f32[], y.3943: f32[]) -> f32[] {
  %x.3942 = f32[] parameter(0)
  %y.3943 = f32[] parameter(1)
  ROOT %add.3944 = f32[] add(f32[] %x.3942, f32[] %y.3943)
}
 
%AddComputation.3973 (x.3974: f32[], y.3975: f32[]) -> f32[] {
  %x.3974 = f32[] parameter(0)
  %y.3975 = f32[] parameter(1)
  ROOT %add.3976 = f32[] add(f32[] %x.3974, f32[] %y.3975)
}
 
ENTRY %IrToHlo.3979 (p0.1: f32[32], p1.2: f32[32], p2.3: f32[32], p3.4: f32[32], p4.5: f32[32,3,3,3], p5.6: f32[96,3,224,224], p6.35: s64[], p7.37: f32[32], p8.38: f32[32], p9.39: f32[32], p10.40: f32[32], p11.41: f32[32,1,3,3], p12.42: f32[], p13.75: s64[], p14.77: f32[16], p15.78: f32[16], p16.79: f32[16], p17.80: f32[16], p18.81: f32[16,32,1,1], p19.114: s64[], p20.116: f32[96], p21.117: f32[96], p22.118: f32[96], p23.119: f32[96], p24.120: f32[96,16,1,1], p25.149: s64[], p26.151: f32[96], p27.152: f32[96], p28.153: f32[96], p29.154: f32[96], p30.155: f32[96,1,3,3], p31.188: s64[], p32.190: f32[24], p33.191: f32[24], p34.192: f32[24], p35.193: f32[24], p36.194: f32[24,96,1,1], p37.227: s64[], p38.229: f32[144], p39.230: f32[144], p40.231: f32[144], p41.232: f32[144], p42.233: f32[144,24,1,1], p43.262: s64[], p44.264: f32[144], p45.265: f32[144], p46.266: f32[144], p47.267: f32[144], p48.268: f32[144,1,3,3], p49.301: s64[], p50.303: f32[24], p51.304: f32[24], p52.305: f32[24], p53.306: f32[24], p54.307: f32[24,144,1,1], p55.340: s64[], p56.342: f32[144], p57.343: f32[144], p58.344: f32[144], p59.345: f32[144], p60.346: f32[144,24,1,1], p61.382: s64[], p62.384: f32[144], p63.385: f32[144], p64.386: f32[144], p65.387: f32[144], p66.388: f32[144,1,3,3], p67.421: s64[], p68.423: f32[32], p69.424: f32[32], p70.425: f32[32], p71.426: f32[32], p72.427: f32[32,144,1,1], p73.460: s64[], p74.462: f32[192], p75.463: f32[192], p76.464: f32[192], p77.465: f32[192], p78.466: f32[192,32,1,1], p79.495: s64[], p80.497: f32[192], p81.498: f32[192], p82.499: f32[192], p83.500: f32[192], p84.501: f32[192,1,3,3], p85.534: s64[], p86.536: f32[32], p87.537: f32[32], p88.538: f32[32], p89.539: f32[32], p90.540: f32[32,192,1,1], p91.573: s64[], p92.575: f32[192], p93.576: f32[192], p94.577: f32[192], p95.578: f32[192], p96.579: f32[192,32,1,1], p97.615: s64[], p98.617: f32[192], p99.618: f32[192], p100.619: f32[192], p101.620: f32[192], p102.621: f32[192,1,3,3], p103.654: s64[], p104.656: f32[32], p105.657: f32[32], p106.658: f32[32], p107.659: f32[32], p108.660: f32[32,192,1,1], p109.693: s64[], p110.695: f32[192], p111.696: f32[192], p112.697: f32[192], p113.698: f32[192], p114.699: f32[192,32,1,1], p115.735: s64[], p116.737: f32[192], p117.738: f32[192], p118.739: f32[192], p119.740: f32[192], p120.741: f32[192,1,3,3], p121.774: s64[], p122.776: f32[64], p123.777: f32[64], p124.778: f32[64], p125.779: f32[64], p126.780: f32[64,192,1,1], p127.813: s64[], p128.815: f32[384], p129.816: f32[384], p130.817: f32[384], p131.818: f32[384], p132.819: f32[384,64,1,1], p133.848: s64[], p134.850: f32[384], p135.851: f32[384], p136.852: f32[384], p137.853: f32[384], p138.854: f32[384,1,3,3], p139.887: s64[], p140.889: f32[64], p141.890: f32[64], p142.891: f32[64], p143.892: f32[64], p144.893: f32[64,384,1,1], p145.926: s64[], p146.928: f32[384], p147.929: f32[384], p148.930: f32[384], p149.931: f32[384], p150.932: f32[384,64,1,1], p151.968: s64[], p152.970: f32[384], p153.971: f32[384], p154.972: f32[384], p155.973: f32[384], p156.974: f32[384,1,3,3], p157.1007: s64[], p158.1009: f32[64], p159.1010: f32[64], p160.1011: f32[64], p161.1012: f32[64], p162.1013: f32[64,384,1,1], p163.1046: s64[], p164.1048: f32[384], p165.1049: f32[384], p166.1050: f32[384], p167.1051: f32[384], p168.1052: f32[384,64,1,1], p169.1088: s64[], p170.1090: f32[384], p171.1091: f32[384], p172.1092: f32[384], p173.1093: f32[384], p174.1094: f32[384,1,3,3], p175.1127: s64[], p176.1129: f32[64], p177.1130: f32[64], p178.1131: f32[64], p179.1132: f32[64], p180.1133: f32[64,384,1,1], p181.1166: s64[], p182.1168: f32[384], p183.1169: f32[384], p184.1170: f32[384], p185.1171: f32[384], p186.1172: f32[384,64,1,1], p187.1208: s64[], p188.1210: f32[384], p189.1211: f32[384], p190.1212: f32[384], p191.1213: f32[384], p192.1214: f32[384,1,3,3], p193.1247: s64[], p194.1249: f32[96], p195.1250: f32[96], p196.1251: f32[96], p197.1252: f32[96], p198.1253: f32[96,384,1,1], p199.1286: s64[], p200.1288: f32[576], p201.1289: f32[576], p202.1290: f32[576], p203.1291: f32[576], p204.1292: f32[576,96,1,1], p205.1321: s64[], p206.1323: f32[576], p207.1324: f32[576], p208.1325: f32[576], p209.1326: f32[576], p210.1327: f32[576,1,3,3], p211.1360: s64[], p212.1362: f32[96], p213.1363: f32[96], p214.1364: f32[96], p215.1365: f32[96], p216.1366: f32[96,576,1,1], p217.1399: s64[], p218.1401: f32[576], p219.1402: f32[576], p220.1403: f32[576], p221.1404: f32[576], p222.1405: f32[576,96,1,1], p223.1441: s64[], p224.1443: f32[576], p225.1444: f32[576], p226.1445: f32[576], p227.1446: f32[576], p228.1447: f32[576,1,3,3], p229.1480: s64[], p230.1482: f32[96], p231.1483: f32[96], p232.1484: f32[96], p233.1485: f32[96], p234.1486: f32[96,576,1,1], p235.1519: s64[], p236.1521: f32[576], p237.1522: f32[576], p238.1523: f32[576], p239.1524: f32[576], p240.1525: f32[576,96,1,1], p241.1561: s64[], p242.1563: f32[576], p243.1564: f32[576], p244.1565: f32[576], p245.1566: f32[576], p246.1567: f32[576,1,3,3], p247.1600: s64[], p248.1602: f32[160], p249.1603: f32[160], p250.1604: f32[160], p251.1605: f32[160], p252.1606: f32[160,576,1,1], p253.1639: s64[], p254.1641: f32[960], p255.1642: f32[960], p256.1643: f32[960], p257.1644: f32[960], p258.1645: f32[960,160,1,1], p259.1674: s64[], p260.1676: f32[960], p261.1677: f32[960], p262.1678: f32[960], p263.1679: f32[960], p264.1680: f32[960,1,3,3], p265.1713: s64[], p266.1715: f32[160], p267.1716: f32[160], p268.1717: f32[160], p269.1718: f32[160], p270.1719: f32[160,960,1,1], p271.1752: s64[], p272.1754: f32[960], p273.1755: f32[960], p274.1756: f32[960], p275.1757: f32[960], p276.1758: f32[960,160,1,1], p277.1794: s64[], p278.1796: f32[960], p279.1797: f32[960], p280.1798: f32[960], p281.1799: f32[960], p282.1800: f32[960,1,3,3], p283.1833: s64[], p284.1835: f32[160], p285.1836: f32[160], p286.1837: f32[160], p287.1838: f32[160], p288.1839: f32[160,960,1,1], p289.1872: s64[], p290.1874: f32[960], p291.1875: f32[960], p292.1876: f32[960], p293.1877: f32[960], p294.1878: f32[960,160,1,1], p295.1914: s64[], p296.1916: f32[960], p297.1917: f32[960], p298.1918: f32[960], p299.1919: f32[960], p300.1920: f32[960,1,3,3], p301.1953: s64[], p302.1955: f32[320], p303.1956: f32[320], p304.1957: f32[320], p305.1958: f32[320], p306.1959: f32[320,960,1,1], p307.1992: s64[], p308.1994: f32[1280], p309.1995: f32[1280], p310.1996: f32[1280], p311.1997: f32[1280], p312.1998: f32[1280,320,1,1], p313.2027: s64[], p314.2029: f32[], p315.2045: f32[], p316.2046: s64[], p317.2351: f32[], p318.2352: f32[1000,1280]) -> (f32[32], f32[32], s64[], f32[32], f32[32], /*index=5*/s64[], f32[16], f32[16], s64[], f32[96], /*index=10*/f32[96], s64[], f32[96], f32[96], s64[], /*index=15*/f32[24], f32[24], s64[], f32[144], f32[144], /*index=20*/s64[], f32[144], f32[144], s64[], f32[24], /*index=25*/f32[24], s64[], f32[144], f32[144], s64[], /*index=30*/f32[144], f32[144], s64[], f32[32], f32[32], /*index=35*/s64[], f32[192], f32[192], s64[], f32[192], /*index=40*/f32[192], s64[], f32[32], f32[32], s64[], /*index=45*/f32[192], f32[192], s64[], f32[192], f32[192], /*index=50*/s64[], f32[32], f32[32], s64[], f32[192], /*index=55*/f32[192], s64[], f32[192], f32[192], s64[], /*index=60*/f32[64], f32[64], s64[], f32[384], f32[384], /*index=65*/s64[], f32[384], f32[384], s64[], f32[64], /*index=70*/f32[64], s64[], f32[384], f32[384], s64[], /*index=75*/f32[384], f32[384], s64[], f32[64], f32[64], /*index=80*/s64[], f32[384], f32[384], s64[], f32[384], /*index=85*/f32[384], s64[], f32[64], f32[64], s64[], /*index=90*/f32[384], f32[384], s64[], f32[384], f32[384], /*index=95*/s64[], f32[96], f32[96], s64[], f32[576], /*index=100*/f32[576], s64[], f32[576], f32[576], s64[], /*index=105*/f32[96], f32[96], s64[], f32[576], f32[576], /*index=110*/s64[], f32[576], f32[576], s64[], f32[96], /*index=115*/f32[96], s64[], f32[576], f32[576], s64[], /*index=120*/f32[576], f32[576], s64[], f32[160], f32[160], /*index=125*/s64[], f32[960], f32[960], s64[], f32[960], /*index=130*/f32[960], s64[], f32[160], f32[160], s64[], /*index=135*/f32[960], f32[960], s64[], f32[960], f32[960], /*index=140*/s64[], f32[160], f32[160], s64[], f32[960], /*index=145*/f32[960], s64[], f32[960], f32[960], s64[], /*index=150*/f32[320], f32[320], s64[], f32[1280], f32[1280], /*index=155*/s64[], f32[1000], f32[1000,1280], f32[1280], f32[1280], /*index=160*/f32[1280,320,1,1], f32[320], f32[320], f32[320,960,1,1], f32[960], /*index=165*/f32[960], f32[960,1,3,3], f32[960], f32[960], f32[960,160,1,1], /*index=170*/f32[160], f32[160], f32[160,960,1,1], f32[960], f32[960], /*index=175*/f32[960,1,3,3], f32[960], f32[960], f32[960,160,1,1], f32[160], /*index=180*/f32[160], f32[160,960,1,1], f32[960], f32[960], f32[960,1,3,3], /*index=185*/f32[960], f32[960], f32[960,160,1,1], f32[160], f32[160], /*index=190*/f32[160,576,1,1], f32[576], f32[576], f32[576,1,3,3], f32[576], /*index=195*/f32[576], f32[576,96,1,1], f32[96], f32[96], f32[96,576,1,1], /*index=200*/f32[576], f32[576], f32[576,1,3,3], f32[576], f32[576], /*index=205*/f32[576,96,1,1], f32[96], f32[96], f32[96,576,1,1], f32[576], /*index=210*/f32[576], f32[576,1,3,3], f32[576], f32[576], f32[576,96,1,1], /*index=215*/f32[96], f32[96], f32[96,384,1,1], f32[384], f32[384], /*index=220*/f32[384,1,3,3], f32[384], f32[384], f32[384,64,1,1], f32[64], /*index=225*/f32[64], f32[64,384,1,1], f32[384], f32[384], f32[384,1,3,3], /*index=230*/f32[384], f32[384], f32[384,64,1,1], f32[64], f32[64], /*index=235*/f32[64,384,1,1], f32[384], f32[384], f32[384,1,3,3], f32[384], /*index=240*/f32[384], f32[384,64,1,1], f32[64], f32[64], f32[64,384,1,1], /*index=245*/f32[384], f32[384], f32[384,1,3,3], f32[384], f32[384], /*index=250*/f32[384,64,1,1], f32[64], f32[64], f32[64,192,1,1], f32[192], /*index=255*/f32[192], f32[192,1,3,3], f32[192], f32[192], f32[192,32,1,1], /*index=260*/f32[32], f32[32], f32[32,192,1,1], f32[192], f32[192], /*index=265*/f32[192,1,3,3], f32[192], f32[192], f32[192,32,1,1], f32[32], /*index=270*/f32[32], f32[32,192,1,1], f32[192], f32[192], f32[192,1,3,3], /*index=275*/f32[192], f32[192], f32[192,32,1,1], f32[32], f32[32], /*index=280*/f32[32,144,1,1], f32[144], f32[144], f32[144,1,3,3], f32[144], /*index=285*/f32[144], f32[144,24,1,1], f32[24], f32[24], f32[24,144,1,1], /*index=290*/f32[144], f32[144], f32[144,1,3,3], f32[144], f32[144], /*index=295*/f32[144,24,1,1], f32[24], f32[24], f32[24,96,1,1], f32[96], /*index=300*/f32[96], f32[96,1,3,3], f32[96], f32[96], f32[96,16,1,1], /*index=305*/f32[16], f32[16], f32[16,32,1,1], f32[32], f32[32], /*index=310*/f32[32,1,3,3], f32[32], f32[32], f32[32,3,3,3]) {
  %constant.2037 = s32[] constant(96), metadata={op_type="aten__sum" op_name="aten__sum"}
  %constant.2058 = u64[] constant(0), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2071 = u64[] constant(61440), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2072 = u64[] add(u64[] %constant.2058, u64[] %constant.2071), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.1960 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1961 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.1960), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1921 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1922 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.1921), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1607 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1608 = f32[96,576,7,7]{3,2,1,0} broadcast(f32[] %constant.1607), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1568 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1569 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.1568), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1254 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1255 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.1254), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1215 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1216 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.1215), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.781 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.782 = f32[96,192,14,14]{3,2,1,0} broadcast(f32[] %constant.781), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.742 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.743 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.742), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.428 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.429 = f32[96,144,28,28]{3,2,1,0} broadcast(f32[] %constant.428), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.389 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.390 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.389), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.195 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.196 = f32[96,96,56,56]{3,2,1,0} broadcast(f32[] %constant.195), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.156 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.157 = f32[96,96,112,112]{3,2,1,0} broadcast(f32[] %constant.156), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.82 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.83 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.82), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.43 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.44 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.43), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p5.6 = f32[96,3,224,224]{3,2,1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %p4.5 = f32[32,3,3,3]{3,2,1,0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.7 = f32[96,32,112,112]{3,2,1,0} convolution(f32[96,3,224,224]{3,2,1,0} %p5.6, f32[32,3,3,3]{3,2,1,0} %p4.5), window={size=3x3 stride=2x2 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p3.4 = f32[32]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p2.3 = f32[32]{0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.8 = (f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-training(f32[96,32,112,112]{3,2,1,0} %convolution.7, f32[32]{0} %p3.4, f32[32]{0} %p2.3), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.9 = f32[96,32,112,112]{3,2,1,0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.8), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %p12.42 = f32[] parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.45 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.46 = f32[96,32,112,112]{3,2,1,0} clamp(f32[96,32,112,112]{3,2,1,0} %broadcast.44, f32[96,32,112,112]{3,2,1,0} %get-tuple-element.9, f32[96,32,112,112]{3,2,1,0} %broadcast.45), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p11.41 = f32[32,1,3,3]{3,2,1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.47 = f32[96,32,112,112]{3,2,1,0} convolution(f32[96,32,112,112]{3,2,1,0} %clamp.46, f32[32,1,3,3]{3,2,1,0} %p11.41), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=32, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p10.40 = f32[32]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p9.39 = f32[32]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.48 = (f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-training(f32[96,32,112,112]{3,2,1,0} %convolution.47, f32[32]{0} %p10.40, f32[32]{0} %p9.39), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.49 = f32[96,32,112,112]{3,2,1,0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.48), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.84 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.85 = f32[96,32,112,112]{3,2,1,0} clamp(f32[96,32,112,112]{3,2,1,0} %broadcast.83, f32[96,32,112,112]{3,2,1,0} %get-tuple-element.49, f32[96,32,112,112]{3,2,1,0} %broadcast.84), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p18.81 = f32[16,32,1,1]{3,2,1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.86 = f32[96,16,112,112]{3,2,1,0} convolution(f32[96,32,112,112]{3,2,1,0} %clamp.85, f32[16,32,1,1]{3,2,1,0} %p18.81), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p17.80 = f32[16]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p16.79 = f32[16]{0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.87 = (f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) batch-norm-training(f32[96,16,112,112]{3,2,1,0} %convolution.86, f32[16]{0} %p17.80, f32[16]{0} %p16.79), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.88 = f32[96,16,112,112]{3,2,1,0} get-tuple-element((f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) %batch-norm-training.87), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %p24.120 = f32[96,16,1,1]{3,2,1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.121 = f32[96,96,112,112]{3,2,1,0} convolution(f32[96,16,112,112]{3,2,1,0} %get-tuple-element.88, f32[96,16,1,1]{3,2,1,0} %p24.120), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p23.119 = f32[96]{0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p22.118 = f32[96]{0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.122 = (f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-training(f32[96,96,112,112]{3,2,1,0} %convolution.121, f32[96]{0} %p23.119, f32[96]{0} %p22.118), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.123 = f32[96,96,112,112]{3,2,1,0} get-tuple-element((f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.122), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.158 = f32[96,96,112,112]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.159 = f32[96,96,112,112]{3,2,1,0} clamp(f32[96,96,112,112]{3,2,1,0} %broadcast.157, f32[96,96,112,112]{3,2,1,0} %get-tuple-element.123, f32[96,96,112,112]{3,2,1,0} %broadcast.158), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p30.155 = f32[96,1,3,3]{3,2,1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.160 = f32[96,96,56,56]{3,2,1,0} convolution(f32[96,96,112,112]{3,2,1,0} %clamp.159, f32[96,1,3,3]{3,2,1,0} %p30.155), window={size=3x3 stride=2x2 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=96, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p29.154 = f32[96]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p28.153 = f32[96]{0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.161 = (f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-training(f32[96,96,56,56]{3,2,1,0} %convolution.160, f32[96]{0} %p29.154, f32[96]{0} %p28.153), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.162 = f32[96,96,56,56]{3,2,1,0} get-tuple-element((f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.161), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.197 = f32[96,96,56,56]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.198 = f32[96,96,56,56]{3,2,1,0} clamp(f32[96,96,56,56]{3,2,1,0} %broadcast.196, f32[96,96,56,56]{3,2,1,0} %get-tuple-element.162, f32[96,96,56,56]{3,2,1,0} %broadcast.197), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p36.194 = f32[24,96,1,1]{3,2,1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.199 = f32[96,24,56,56]{3,2,1,0} convolution(f32[96,96,56,56]{3,2,1,0} %clamp.198, f32[24,96,1,1]{3,2,1,0} %p36.194), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p35.193 = f32[24]{0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p34.192 = f32[24]{0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.200 = (f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) batch-norm-training(f32[96,24,56,56]{3,2,1,0} %convolution.199, f32[24]{0} %p35.193, f32[24]{0} %p34.192), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.201 = f32[96,24,56,56]{3,2,1,0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-training.200), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.308 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.309 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.308), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.269 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.270 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.269), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p42.233 = f32[144,24,1,1]{3,2,1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.234 = f32[96,144,56,56]{3,2,1,0} convolution(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.201, f32[144,24,1,1]{3,2,1,0} %p42.233), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p41.232 = f32[144]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p40.231 = f32[144]{0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.235 = (f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-training(f32[96,144,56,56]{3,2,1,0} %convolution.234, f32[144]{0} %p41.232, f32[144]{0} %p40.231), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.236 = f32[96,144,56,56]{3,2,1,0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.235), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.271 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.272 = f32[96,144,56,56]{3,2,1,0} clamp(f32[96,144,56,56]{3,2,1,0} %broadcast.270, f32[96,144,56,56]{3,2,1,0} %get-tuple-element.236, f32[96,144,56,56]{3,2,1,0} %broadcast.271), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p48.268 = f32[144,1,3,3]{3,2,1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.273 = f32[96,144,56,56]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %clamp.272, f32[144,1,3,3]{3,2,1,0} %p48.268), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=144, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p47.267 = f32[144]{0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p46.266 = f32[144]{0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.274 = (f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-training(f32[96,144,56,56]{3,2,1,0} %convolution.273, f32[144]{0} %p47.267, f32[144]{0} %p46.266), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.275 = f32[96,144,56,56]{3,2,1,0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.274), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.310 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.311 = f32[96,144,56,56]{3,2,1,0} clamp(f32[96,144,56,56]{3,2,1,0} %broadcast.309, f32[96,144,56,56]{3,2,1,0} %get-tuple-element.275, f32[96,144,56,56]{3,2,1,0} %broadcast.310), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p54.307 = f32[24,144,1,1]{3,2,1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.312 = f32[96,24,56,56]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %clamp.311, f32[24,144,1,1]{3,2,1,0} %p54.307), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p53.306 = f32[24]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p52.305 = f32[24]{0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.313 = (f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) batch-norm-training(f32[96,24,56,56]{3,2,1,0} %convolution.312, f32[24]{0} %p53.306, f32[24]{0} %p52.305), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.314 = f32[96,24,56,56]{3,2,1,0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-training.313), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.347 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.348 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.347), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.349 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.348), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.350 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.349), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.351 = f32[96,24,56,56]{3,2,1,0} broadcast(f32[] %reshape.350), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.352 = f32[96,24,56,56]{3,2,1,0} multiply(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.314, f32[96,24,56,56]{3,2,1,0} %broadcast.351), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.353 = f32[96,24,56,56]{3,2,1,0} add(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.201, f32[96,24,56,56]{3,2,1,0} %multiply.352), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %p60.346 = f32[144,24,1,1]{3,2,1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.354 = f32[96,144,56,56]{3,2,1,0} convolution(f32[96,24,56,56]{3,2,1,0} %add.353, f32[144,24,1,1]{3,2,1,0} %p60.346), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p59.345 = f32[144]{0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p58.344 = f32[144]{0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.355 = (f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-training(f32[96,144,56,56]{3,2,1,0} %convolution.354, f32[144]{0} %p59.345, f32[144]{0} %p58.344), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.356 = f32[96,144,56,56]{3,2,1,0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.355), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.391 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.392 = f32[96,144,56,56]{3,2,1,0} clamp(f32[96,144,56,56]{3,2,1,0} %broadcast.390, f32[96,144,56,56]{3,2,1,0} %get-tuple-element.356, f32[96,144,56,56]{3,2,1,0} %broadcast.391), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p66.388 = f32[144,1,3,3]{3,2,1,0} parameter(66), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.393 = f32[96,144,28,28]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %clamp.392, f32[144,1,3,3]{3,2,1,0} %p66.388), window={size=3x3 stride=2x2 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=144, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p65.387 = f32[144]{0} parameter(65), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p64.386 = f32[144]{0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.394 = (f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-training(f32[96,144,28,28]{3,2,1,0} %convolution.393, f32[144]{0} %p65.387, f32[144]{0} %p64.386), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.395 = f32[96,144,28,28]{3,2,1,0} get-tuple-element((f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.394), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.430 = f32[96,144,28,28]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.431 = f32[96,144,28,28]{3,2,1,0} clamp(f32[96,144,28,28]{3,2,1,0} %broadcast.429, f32[96,144,28,28]{3,2,1,0} %get-tuple-element.395, f32[96,144,28,28]{3,2,1,0} %broadcast.430), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p72.427 = f32[32,144,1,1]{3,2,1,0} parameter(72), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.432 = f32[96,32,28,28]{3,2,1,0} convolution(f32[96,144,28,28]{3,2,1,0} %clamp.431, f32[32,144,1,1]{3,2,1,0} %p72.427), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p71.426 = f32[32]{0} parameter(71), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p70.425 = f32[32]{0} parameter(70), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.433 = (f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-training(f32[96,32,28,28]{3,2,1,0} %convolution.432, f32[32]{0} %p71.426, f32[32]{0} %p70.425), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.434 = f32[96,32,28,28]{3,2,1,0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.433), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.541 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.542 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.541), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.502 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.503 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.502), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p78.466 = f32[192,32,1,1]{3,2,1,0} parameter(78), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.467 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.434, f32[192,32,1,1]{3,2,1,0} %p78.466), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p77.465 = f32[192]{0} parameter(77), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p76.464 = f32[192]{0} parameter(76), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.468 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-training(f32[96,192,28,28]{3,2,1,0} %convolution.467, f32[192]{0} %p77.465, f32[192]{0} %p76.464), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.469 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.468), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.504 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.505 = f32[96,192,28,28]{3,2,1,0} clamp(f32[96,192,28,28]{3,2,1,0} %broadcast.503, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.469, f32[96,192,28,28]{3,2,1,0} %broadcast.504), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p84.501 = f32[192,1,3,3]{3,2,1,0} parameter(84), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.506 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.505, f32[192,1,3,3]{3,2,1,0} %p84.501), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=192, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p83.500 = f32[192]{0} parameter(83), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p82.499 = f32[192]{0} parameter(82), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.507 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-training(f32[96,192,28,28]{3,2,1,0} %convolution.506, f32[192]{0} %p83.500, f32[192]{0} %p82.499), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.508 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.507), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.543 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.544 = f32[96,192,28,28]{3,2,1,0} clamp(f32[96,192,28,28]{3,2,1,0} %broadcast.542, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.508, f32[96,192,28,28]{3,2,1,0} %broadcast.543), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p90.540 = f32[32,192,1,1]{3,2,1,0} parameter(90), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.545 = f32[96,32,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.544, f32[32,192,1,1]{3,2,1,0} %p90.540), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p89.539 = f32[32]{0} parameter(89), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p88.538 = f32[32]{0} parameter(88), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.546 = (f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-training(f32[96,32,28,28]{3,2,1,0} %convolution.545, f32[32]{0} %p89.539, f32[32]{0} %p88.538), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.547 = f32[96,32,28,28]{3,2,1,0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.546), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.580 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.581 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.580), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.582 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.581), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.583 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.582), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.584 = f32[96,32,28,28]{3,2,1,0} broadcast(f32[] %reshape.583), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.585 = f32[96,32,28,28]{3,2,1,0} multiply(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.547, f32[96,32,28,28]{3,2,1,0} %broadcast.584), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.586 = f32[96,32,28,28]{3,2,1,0} add(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.434, f32[96,32,28,28]{3,2,1,0} %multiply.585), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %constant.661 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.662 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.661), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.622 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.623 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.622), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p96.579 = f32[192,32,1,1]{3,2,1,0} parameter(96), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.587 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %add.586, f32[192,32,1,1]{3,2,1,0} %p96.579), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p95.578 = f32[192]{0} parameter(95), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p94.577 = f32[192]{0} parameter(94), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.588 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-training(f32[96,192,28,28]{3,2,1,0} %convolution.587, f32[192]{0} %p95.578, f32[192]{0} %p94.577), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.589 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.588), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.624 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.625 = f32[96,192,28,28]{3,2,1,0} clamp(f32[96,192,28,28]{3,2,1,0} %broadcast.623, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.589, f32[96,192,28,28]{3,2,1,0} %broadcast.624), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p102.621 = f32[192,1,3,3]{3,2,1,0} parameter(102), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.626 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.625, f32[192,1,3,3]{3,2,1,0} %p102.621), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=192, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p101.620 = f32[192]{0} parameter(101), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p100.619 = f32[192]{0} parameter(100), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.627 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-training(f32[96,192,28,28]{3,2,1,0} %convolution.626, f32[192]{0} %p101.620, f32[192]{0} %p100.619), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.628 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.627), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.663 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.664 = f32[96,192,28,28]{3,2,1,0} clamp(f32[96,192,28,28]{3,2,1,0} %broadcast.662, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.628, f32[96,192,28,28]{3,2,1,0} %broadcast.663), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p108.660 = f32[32,192,1,1]{3,2,1,0} parameter(108), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.665 = f32[96,32,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.664, f32[32,192,1,1]{3,2,1,0} %p108.660), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p107.659 = f32[32]{0} parameter(107), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p106.658 = f32[32]{0} parameter(106), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.666 = (f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-training(f32[96,32,28,28]{3,2,1,0} %convolution.665, f32[32]{0} %p107.659, f32[32]{0} %p106.658), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.667 = f32[96,32,28,28]{3,2,1,0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.666), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.700 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.701 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.700), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.702 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.701), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.703 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.702), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.704 = f32[96,32,28,28]{3,2,1,0} broadcast(f32[] %reshape.703), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.705 = f32[96,32,28,28]{3,2,1,0} multiply(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.667, f32[96,32,28,28]{3,2,1,0} %broadcast.704), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.706 = f32[96,32,28,28]{3,2,1,0} add(f32[96,32,28,28]{3,2,1,0} %add.586, f32[96,32,28,28]{3,2,1,0} %multiply.705), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %p114.699 = f32[192,32,1,1]{3,2,1,0} parameter(114), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.707 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %add.706, f32[192,32,1,1]{3,2,1,0} %p114.699), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p113.698 = f32[192]{0} parameter(113), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p112.697 = f32[192]{0} parameter(112), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.708 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-training(f32[96,192,28,28]{3,2,1,0} %convolution.707, f32[192]{0} %p113.698, f32[192]{0} %p112.697), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.709 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.708), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.744 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.745 = f32[96,192,28,28]{3,2,1,0} clamp(f32[96,192,28,28]{3,2,1,0} %broadcast.743, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.709, f32[96,192,28,28]{3,2,1,0} %broadcast.744), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p120.741 = f32[192,1,3,3]{3,2,1,0} parameter(120), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.746 = f32[96,192,14,14]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.745, f32[192,1,3,3]{3,2,1,0} %p120.741), window={size=3x3 stride=2x2 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=192, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p119.740 = f32[192]{0} parameter(119), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p118.739 = f32[192]{0} parameter(118), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.747 = (f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-training(f32[96,192,14,14]{3,2,1,0} %convolution.746, f32[192]{0} %p119.740, f32[192]{0} %p118.739), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.748 = f32[96,192,14,14]{3,2,1,0} get-tuple-element((f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.747), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.783 = f32[96,192,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.784 = f32[96,192,14,14]{3,2,1,0} clamp(f32[96,192,14,14]{3,2,1,0} %broadcast.782, f32[96,192,14,14]{3,2,1,0} %get-tuple-element.748, f32[96,192,14,14]{3,2,1,0} %broadcast.783), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p126.780 = f32[64,192,1,1]{3,2,1,0} parameter(126), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.785 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,192,14,14]{3,2,1,0} %clamp.784, f32[64,192,1,1]{3,2,1,0} %p126.780), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p125.779 = f32[64]{0} parameter(125), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p124.778 = f32[64]{0} parameter(124), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.786 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-training(f32[96,64,14,14]{3,2,1,0} %convolution.785, f32[64]{0} %p125.779, f32[64]{0} %p124.778), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.787 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.786), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.894 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.895 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.894), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.855 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.856 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.855), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p132.819 = f32[384,64,1,1]{3,2,1,0} parameter(132), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.820 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.787, f32[384,64,1,1]{3,2,1,0} %p132.819), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p131.818 = f32[384]{0} parameter(131), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p130.817 = f32[384]{0} parameter(130), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.821 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.820, f32[384]{0} %p131.818, f32[384]{0} %p130.817), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.822 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.821), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.857 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.858 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.856, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.822, f32[96,384,14,14]{3,2,1,0} %broadcast.857), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p138.854 = f32[384,1,3,3]{3,2,1,0} parameter(138), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.859 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.858, f32[384,1,3,3]{3,2,1,0} %p138.854), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=384, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p137.853 = f32[384]{0} parameter(137), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p136.852 = f32[384]{0} parameter(136), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.860 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.859, f32[384]{0} %p137.853, f32[384]{0} %p136.852), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.861 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.860), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.896 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.897 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.895, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.861, f32[96,384,14,14]{3,2,1,0} %broadcast.896), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p144.893 = f32[64,384,1,1]{3,2,1,0} parameter(144), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.898 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.897, f32[64,384,1,1]{3,2,1,0} %p144.893), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p143.892 = f32[64]{0} parameter(143), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p142.891 = f32[64]{0} parameter(142), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.899 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-training(f32[96,64,14,14]{3,2,1,0} %convolution.898, f32[64]{0} %p143.892, f32[64]{0} %p142.891), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.900 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.899), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.933 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.934 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.933), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.935 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.934), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.936 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.935), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.937 = f32[96,64,14,14]{3,2,1,0} broadcast(f32[] %reshape.936), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.938 = f32[96,64,14,14]{3,2,1,0} multiply(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.900, f32[96,64,14,14]{3,2,1,0} %broadcast.937), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.939 = f32[96,64,14,14]{3,2,1,0} add(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.787, f32[96,64,14,14]{3,2,1,0} %multiply.938), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %constant.1014 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1015 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.1014), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.975 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.976 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.975), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p150.932 = f32[384,64,1,1]{3,2,1,0} parameter(150), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.940 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %add.939, f32[384,64,1,1]{3,2,1,0} %p150.932), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p149.931 = f32[384]{0} parameter(149), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p148.930 = f32[384]{0} parameter(148), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.941 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.940, f32[384]{0} %p149.931, f32[384]{0} %p148.930), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.942 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.941), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.977 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.978 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.976, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.942, f32[96,384,14,14]{3,2,1,0} %broadcast.977), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p156.974 = f32[384,1,3,3]{3,2,1,0} parameter(156), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.979 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.978, f32[384,1,3,3]{3,2,1,0} %p156.974), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=384, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p155.973 = f32[384]{0} parameter(155), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p154.972 = f32[384]{0} parameter(154), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.980 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.979, f32[384]{0} %p155.973, f32[384]{0} %p154.972), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.981 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.980), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1016 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1017 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.1015, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.981, f32[96,384,14,14]{3,2,1,0} %broadcast.1016), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p162.1013 = f32[64,384,1,1]{3,2,1,0} parameter(162), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1018 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1017, f32[64,384,1,1]{3,2,1,0} %p162.1013), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p161.1012 = f32[64]{0} parameter(161), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p160.1011 = f32[64]{0} parameter(160), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1019 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-training(f32[96,64,14,14]{3,2,1,0} %convolution.1018, f32[64]{0} %p161.1012, f32[64]{0} %p160.1011), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1020 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.1019), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1053 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1054 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.1053), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1055 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.1054), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1056 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.1055), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1057 = f32[96,64,14,14]{3,2,1,0} broadcast(f32[] %reshape.1056), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.1058 = f32[96,64,14,14]{3,2,1,0} multiply(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.1020, f32[96,64,14,14]{3,2,1,0} %broadcast.1057), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.1059 = f32[96,64,14,14]{3,2,1,0} add(f32[96,64,14,14]{3,2,1,0} %add.939, f32[96,64,14,14]{3,2,1,0} %multiply.1058), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %constant.1134 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1135 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.1134), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1095 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1096 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.1095), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p168.1052 = f32[384,64,1,1]{3,2,1,0} parameter(168), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1060 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %add.1059, f32[384,64,1,1]{3,2,1,0} %p168.1052), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p167.1051 = f32[384]{0} parameter(167), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p166.1050 = f32[384]{0} parameter(166), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1061 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.1060, f32[384]{0} %p167.1051, f32[384]{0} %p166.1050), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1062 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1061), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1097 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1098 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.1096, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1062, f32[96,384,14,14]{3,2,1,0} %broadcast.1097), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p174.1094 = f32[384,1,3,3]{3,2,1,0} parameter(174), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1099 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1098, f32[384,1,3,3]{3,2,1,0} %p174.1094), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=384, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p173.1093 = f32[384]{0} parameter(173), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p172.1092 = f32[384]{0} parameter(172), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1100 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.1099, f32[384]{0} %p173.1093, f32[384]{0} %p172.1092), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1101 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1100), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1136 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1137 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.1135, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1101, f32[96,384,14,14]{3,2,1,0} %broadcast.1136), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p180.1133 = f32[64,384,1,1]{3,2,1,0} parameter(180), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1138 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1137, f32[64,384,1,1]{3,2,1,0} %p180.1133), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p179.1132 = f32[64]{0} parameter(179), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p178.1131 = f32[64]{0} parameter(178), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1139 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-training(f32[96,64,14,14]{3,2,1,0} %convolution.1138, f32[64]{0} %p179.1132, f32[64]{0} %p178.1131), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1140 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.1139), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1173 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1174 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.1173), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1175 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.1174), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1176 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.1175), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1177 = f32[96,64,14,14]{3,2,1,0} broadcast(f32[] %reshape.1176), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.1178 = f32[96,64,14,14]{3,2,1,0} multiply(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.1140, f32[96,64,14,14]{3,2,1,0} %broadcast.1177), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.1179 = f32[96,64,14,14]{3,2,1,0} add(f32[96,64,14,14]{3,2,1,0} %add.1059, f32[96,64,14,14]{3,2,1,0} %multiply.1178), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %p186.1172 = f32[384,64,1,1]{3,2,1,0} parameter(186), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1180 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %add.1179, f32[384,64,1,1]{3,2,1,0} %p186.1172), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p185.1171 = f32[384]{0} parameter(185), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p184.1170 = f32[384]{0} parameter(184), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1181 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.1180, f32[384]{0} %p185.1171, f32[384]{0} %p184.1170), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1182 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1181), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1217 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1218 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.1216, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1182, f32[96,384,14,14]{3,2,1,0} %broadcast.1217), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p192.1214 = f32[384,1,3,3]{3,2,1,0} parameter(192), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1219 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1218, f32[384,1,3,3]{3,2,1,0} %p192.1214), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=384, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p191.1213 = f32[384]{0} parameter(191), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p190.1212 = f32[384]{0} parameter(190), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1220 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-training(f32[96,384,14,14]{3,2,1,0} %convolution.1219, f32[384]{0} %p191.1213, f32[384]{0} %p190.1212), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1221 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1220), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1256 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1257 = f32[96,384,14,14]{3,2,1,0} clamp(f32[96,384,14,14]{3,2,1,0} %broadcast.1255, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1221, f32[96,384,14,14]{3,2,1,0} %broadcast.1256), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p198.1253 = f32[96,384,1,1]{3,2,1,0} parameter(198), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1258 = f32[96,96,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1257, f32[96,384,1,1]{3,2,1,0} %p198.1253), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p197.1252 = f32[96]{0} parameter(197), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p196.1251 = f32[96]{0} parameter(196), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1259 = (f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-training(f32[96,96,14,14]{3,2,1,0} %convolution.1258, f32[96]{0} %p197.1252, f32[96]{0} %p196.1251), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1260 = f32[96,96,14,14]{3,2,1,0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1259), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1367 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1368 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.1367), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1328 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1329 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.1328), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p204.1292 = f32[576,96,1,1]{3,2,1,0} parameter(204), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1293 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.1260, f32[576,96,1,1]{3,2,1,0} %p204.1292), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p203.1291 = f32[576]{0} parameter(203), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p202.1290 = f32[576]{0} parameter(202), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1294 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-training(f32[96,576,14,14]{3,2,1,0} %convolution.1293, f32[576]{0} %p203.1291, f32[576]{0} %p202.1290), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1295 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1294), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1330 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1331 = f32[96,576,14,14]{3,2,1,0} clamp(f32[96,576,14,14]{3,2,1,0} %broadcast.1329, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1295, f32[96,576,14,14]{3,2,1,0} %broadcast.1330), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p210.1327 = f32[576,1,3,3]{3,2,1,0} parameter(210), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1332 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1331, f32[576,1,3,3]{3,2,1,0} %p210.1327), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=576, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p209.1326 = f32[576]{0} parameter(209), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p208.1325 = f32[576]{0} parameter(208), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1333 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-training(f32[96,576,14,14]{3,2,1,0} %convolution.1332, f32[576]{0} %p209.1326, f32[576]{0} %p208.1325), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1334 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1333), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1369 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1370 = f32[96,576,14,14]{3,2,1,0} clamp(f32[96,576,14,14]{3,2,1,0} %broadcast.1368, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1334, f32[96,576,14,14]{3,2,1,0} %broadcast.1369), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p216.1366 = f32[96,576,1,1]{3,2,1,0} parameter(216), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1371 = f32[96,96,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1370, f32[96,576,1,1]{3,2,1,0} %p216.1366), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p215.1365 = f32[96]{0} parameter(215), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p214.1364 = f32[96]{0} parameter(214), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1372 = (f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-training(f32[96,96,14,14]{3,2,1,0} %convolution.1371, f32[96]{0} %p215.1365, f32[96]{0} %p214.1364), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1373 = f32[96,96,14,14]{3,2,1,0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1372), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1406 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1407 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.1406), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1408 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.1407), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1409 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.1408), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1410 = f32[96,96,14,14]{3,2,1,0} broadcast(f32[] %reshape.1409), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.1411 = f32[96,96,14,14]{3,2,1,0} multiply(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.1373, f32[96,96,14,14]{3,2,1,0} %broadcast.1410), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.1412 = f32[96,96,14,14]{3,2,1,0} add(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.1260, f32[96,96,14,14]{3,2,1,0} %multiply.1411), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %constant.1487 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1488 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.1487), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1448 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1449 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.1448), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p222.1405 = f32[576,96,1,1]{3,2,1,0} parameter(222), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1413 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %add.1412, f32[576,96,1,1]{3,2,1,0} %p222.1405), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p221.1404 = f32[576]{0} parameter(221), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p220.1403 = f32[576]{0} parameter(220), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1414 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-training(f32[96,576,14,14]{3,2,1,0} %convolution.1413, f32[576]{0} %p221.1404, f32[576]{0} %p220.1403), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1415 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1414), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1450 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1451 = f32[96,576,14,14]{3,2,1,0} clamp(f32[96,576,14,14]{3,2,1,0} %broadcast.1449, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1415, f32[96,576,14,14]{3,2,1,0} %broadcast.1450), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p228.1447 = f32[576,1,3,3]{3,2,1,0} parameter(228), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1452 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1451, f32[576,1,3,3]{3,2,1,0} %p228.1447), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=576, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p227.1446 = f32[576]{0} parameter(227), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p226.1445 = f32[576]{0} parameter(226), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1453 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-training(f32[96,576,14,14]{3,2,1,0} %convolution.1452, f32[576]{0} %p227.1446, f32[576]{0} %p226.1445), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1454 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1453), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1489 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1490 = f32[96,576,14,14]{3,2,1,0} clamp(f32[96,576,14,14]{3,2,1,0} %broadcast.1488, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1454, f32[96,576,14,14]{3,2,1,0} %broadcast.1489), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p234.1486 = f32[96,576,1,1]{3,2,1,0} parameter(234), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1491 = f32[96,96,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1490, f32[96,576,1,1]{3,2,1,0} %p234.1486), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p233.1485 = f32[96]{0} parameter(233), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p232.1484 = f32[96]{0} parameter(232), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1492 = (f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-training(f32[96,96,14,14]{3,2,1,0} %convolution.1491, f32[96]{0} %p233.1485, f32[96]{0} %p232.1484), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1493 = f32[96,96,14,14]{3,2,1,0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1492), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1526 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1527 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.1526), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1528 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.1527), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1529 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.1528), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1530 = f32[96,96,14,14]{3,2,1,0} broadcast(f32[] %reshape.1529), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.1531 = f32[96,96,14,14]{3,2,1,0} multiply(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.1493, f32[96,96,14,14]{3,2,1,0} %broadcast.1530), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.1532 = f32[96,96,14,14]{3,2,1,0} add(f32[96,96,14,14]{3,2,1,0} %add.1412, f32[96,96,14,14]{3,2,1,0} %multiply.1531), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %p240.1525 = f32[576,96,1,1]{3,2,1,0} parameter(240), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1533 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %add.1532, f32[576,96,1,1]{3,2,1,0} %p240.1525), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p239.1524 = f32[576]{0} parameter(239), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p238.1523 = f32[576]{0} parameter(238), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1534 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-training(f32[96,576,14,14]{3,2,1,0} %convolution.1533, f32[576]{0} %p239.1524, f32[576]{0} %p238.1523), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1535 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1534), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1570 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1571 = f32[96,576,14,14]{3,2,1,0} clamp(f32[96,576,14,14]{3,2,1,0} %broadcast.1569, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1535, f32[96,576,14,14]{3,2,1,0} %broadcast.1570), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p246.1567 = f32[576,1,3,3]{3,2,1,0} parameter(246), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1572 = f32[96,576,7,7]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1571, f32[576,1,3,3]{3,2,1,0} %p246.1567), window={size=3x3 stride=2x2 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=576, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p245.1566 = f32[576]{0} parameter(245), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p244.1565 = f32[576]{0} parameter(244), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1573 = (f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-training(f32[96,576,7,7]{3,2,1,0} %convolution.1572, f32[576]{0} %p245.1566, f32[576]{0} %p244.1565), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1574 = f32[96,576,7,7]{3,2,1,0} get-tuple-element((f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1573), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1609 = f32[96,576,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1610 = f32[96,576,7,7]{3,2,1,0} clamp(f32[96,576,7,7]{3,2,1,0} %broadcast.1608, f32[96,576,7,7]{3,2,1,0} %get-tuple-element.1574, f32[96,576,7,7]{3,2,1,0} %broadcast.1609), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p252.1606 = f32[160,576,1,1]{3,2,1,0} parameter(252), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1611 = f32[96,160,7,7]{3,2,1,0} convolution(f32[96,576,7,7]{3,2,1,0} %clamp.1610, f32[160,576,1,1]{3,2,1,0} %p252.1606), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p251.1605 = f32[160]{0} parameter(251), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p250.1604 = f32[160]{0} parameter(250), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1612 = (f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) batch-norm-training(f32[96,160,7,7]{3,2,1,0} %convolution.1611, f32[160]{0} %p251.1605, f32[160]{0} %p250.1604), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1613 = f32[96,160,7,7]{3,2,1,0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1612), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1720 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1721 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.1720), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1681 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1682 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.1681), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p258.1645 = f32[960,160,1,1]{3,2,1,0} parameter(258), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1646 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.1613, f32[960,160,1,1]{3,2,1,0} %p258.1645), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p257.1644 = f32[960]{0} parameter(257), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p256.1643 = f32[960]{0} parameter(256), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1647 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-training(f32[96,960,7,7]{3,2,1,0} %convolution.1646, f32[960]{0} %p257.1644, f32[960]{0} %p256.1643), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1648 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1647), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1683 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1684 = f32[96,960,7,7]{3,2,1,0} clamp(f32[96,960,7,7]{3,2,1,0} %broadcast.1682, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1648, f32[96,960,7,7]{3,2,1,0} %broadcast.1683), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p264.1680 = f32[960,1,3,3]{3,2,1,0} parameter(264), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1685 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1684, f32[960,1,3,3]{3,2,1,0} %p264.1680), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=960, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p263.1679 = f32[960]{0} parameter(263), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p262.1678 = f32[960]{0} parameter(262), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1686 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-training(f32[96,960,7,7]{3,2,1,0} %convolution.1685, f32[960]{0} %p263.1679, f32[960]{0} %p262.1678), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1687 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1686), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1722 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1723 = f32[96,960,7,7]{3,2,1,0} clamp(f32[96,960,7,7]{3,2,1,0} %broadcast.1721, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1687, f32[96,960,7,7]{3,2,1,0} %broadcast.1722), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p270.1719 = f32[160,960,1,1]{3,2,1,0} parameter(270), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1724 = f32[96,160,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1723, f32[160,960,1,1]{3,2,1,0} %p270.1719), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p269.1718 = f32[160]{0} parameter(269), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p268.1717 = f32[160]{0} parameter(268), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1725 = (f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) batch-norm-training(f32[96,160,7,7]{3,2,1,0} %convolution.1724, f32[160]{0} %p269.1718, f32[160]{0} %p268.1717), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1726 = f32[96,160,7,7]{3,2,1,0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1725), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1759 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1760 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.1759), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1761 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.1760), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1762 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.1761), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1763 = f32[96,160,7,7]{3,2,1,0} broadcast(f32[] %reshape.1762), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.1764 = f32[96,160,7,7]{3,2,1,0} multiply(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.1726, f32[96,160,7,7]{3,2,1,0} %broadcast.1763), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.1765 = f32[96,160,7,7]{3,2,1,0} add(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.1613, f32[96,160,7,7]{3,2,1,0} %multiply.1764), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %constant.1840 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1841 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.1840), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.1801 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.1802 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.1801), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p276.1758 = f32[960,160,1,1]{3,2,1,0} parameter(276), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1766 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %add.1765, f32[960,160,1,1]{3,2,1,0} %p276.1758), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p275.1757 = f32[960]{0} parameter(275), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p274.1756 = f32[960]{0} parameter(274), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1767 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-training(f32[96,960,7,7]{3,2,1,0} %convolution.1766, f32[960]{0} %p275.1757, f32[960]{0} %p274.1756), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1768 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1767), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1803 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1804 = f32[96,960,7,7]{3,2,1,0} clamp(f32[96,960,7,7]{3,2,1,0} %broadcast.1802, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1768, f32[96,960,7,7]{3,2,1,0} %broadcast.1803), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p282.1800 = f32[960,1,3,3]{3,2,1,0} parameter(282), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1805 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1804, f32[960,1,3,3]{3,2,1,0} %p282.1800), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=960, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p281.1799 = f32[960]{0} parameter(281), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p280.1798 = f32[960]{0} parameter(280), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1806 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-training(f32[96,960,7,7]{3,2,1,0} %convolution.1805, f32[960]{0} %p281.1799, f32[960]{0} %p280.1798), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1807 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1806), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1842 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1843 = f32[96,960,7,7]{3,2,1,0} clamp(f32[96,960,7,7]{3,2,1,0} %broadcast.1841, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1807, f32[96,960,7,7]{3,2,1,0} %broadcast.1842), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p288.1839 = f32[160,960,1,1]{3,2,1,0} parameter(288), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1844 = f32[96,160,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1843, f32[160,960,1,1]{3,2,1,0} %p288.1839), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p287.1838 = f32[160]{0} parameter(287), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p286.1837 = f32[160]{0} parameter(286), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1845 = (f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) batch-norm-training(f32[96,160,7,7]{3,2,1,0} %convolution.1844, f32[160]{0} %p287.1838, f32[160]{0} %p286.1837), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1846 = f32[96,160,7,7]{3,2,1,0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1845), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1879 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1880 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.1879), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1881 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.1880), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %reshape.1882 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.1881), metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %broadcast.1883 = f32[96,160,7,7]{3,2,1,0} broadcast(f32[] %reshape.1882), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="forward@mobilenetv2.py" source_line=62}
  %multiply.1884 = f32[96,160,7,7]{3,2,1,0} multiply(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.1846, f32[96,160,7,7]{3,2,1,0} %broadcast.1883), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@mobilenetv2.py" source_line=62}
  %add.1885 = f32[96,160,7,7]{3,2,1,0} add(f32[96,160,7,7]{3,2,1,0} %add.1765, f32[96,160,7,7]{3,2,1,0} %multiply.1884), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@mobilenetv2.py" source_line=62}
  %p294.1878 = f32[960,160,1,1]{3,2,1,0} parameter(294), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1886 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %add.1885, f32[960,160,1,1]{3,2,1,0} %p294.1878), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p293.1877 = f32[960]{0} parameter(293), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p292.1876 = f32[960]{0} parameter(292), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1887 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-training(f32[96,960,7,7]{3,2,1,0} %convolution.1886, f32[960]{0} %p293.1877, f32[960]{0} %p292.1876), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1888 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1887), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1923 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1924 = f32[96,960,7,7]{3,2,1,0} clamp(f32[96,960,7,7]{3,2,1,0} %broadcast.1922, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1888, f32[96,960,7,7]{3,2,1,0} %broadcast.1923), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p300.1920 = f32[960,1,3,3]{3,2,1,0} parameter(300), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1925 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1924, f32[960,1,3,3]{3,2,1,0} %p300.1920), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, feature_group_count=960, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p299.1919 = f32[960]{0} parameter(299), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p298.1918 = f32[960]{0} parameter(298), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1926 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-training(f32[96,960,7,7]{3,2,1,0} %convolution.1925, f32[960]{0} %p299.1919, f32[960]{0} %p298.1918), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1927 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1926), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1962 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.1963 = f32[96,960,7,7]{3,2,1,0} clamp(f32[96,960,7,7]{3,2,1,0} %broadcast.1961, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1927, f32[96,960,7,7]{3,2,1,0} %broadcast.1962), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %p306.1959 = f32[320,960,1,1]{3,2,1,0} parameter(306), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1964 = f32[96,320,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1963, f32[320,960,1,1]{3,2,1,0} %p306.1959), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p305.1958 = f32[320]{0} parameter(305), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p304.1957 = f32[320]{0} parameter(304), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.1965 = (f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) batch-norm-training(f32[96,320,7,7]{3,2,1,0} %convolution.1964, f32[320]{0} %p305.1958, f32[320]{0} %p304.1957), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.1966 = f32[96,320,7,7]{3,2,1,0} get-tuple-element((f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) %batch-norm-training.1965), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %p312.1998 = f32[1280,320,1,1]{3,2,1,0} parameter(312), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="_conv_forward@conv.py" source_line=460}
  %convolution.1999 = f32[96,1280,7,7]{3,2,1,0} convolution(f32[96,320,7,7]{3,2,1,0} %get-tuple-element.1966, f32[1280,320,1,1]{3,2,1,0} %p312.1998), window={size=1x1}, dim_labels=bf01_oi01->bf01, metadata={op_type="aten__convolution_overrideable" op_name="aten__convolution_overrideable" source_file="_conv_forward@conv.py" source_line=460}
  %p311.1997 = f32[1280]{0} parameter(311), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %p310.1996 = f32[1280]{0} parameter(310), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %batch-norm-training.2000 = (f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) batch-norm-training(f32[96,1280,7,7]{3,2,1,0} %convolution.1999, f32[1280]{0} %p311.1997, f32[1280]{0} %p310.1996), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %get-tuple-element.2002 = f32[1280]{0} get-tuple-element((f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) %batch-norm-training.2000), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2374 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2375 = f32[1280]{0} broadcast(f32[] %constant.2374), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2003 = f32[1280]{0} get-tuple-element((f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) %batch-norm-training.2000), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2004 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.2005 = f32[1280]{0} broadcast(f32[] %constant.2004), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.2006 = f32[1280]{0} add(f32[1280]{0} %get-tuple-element.2003, f32[1280]{0} %broadcast.2005), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.2007 = f32[1280]{0} rsqrt(f32[1280]{0} %add.2006), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2376 = f32[1280]{0} divide(f32[1280]{0} %broadcast.2375, f32[1280]{0} %rsqrt.2007), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2377 = f32[1280]{0} multiply(f32[1280]{0} %divide.2376, f32[1280]{0} %divide.2376), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2373 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2378 = f32[1280]{0} broadcast(f32[] %constant.2373), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2379 = f32[1280]{0} subtract(f32[1280]{0} %multiply.2377, f32[1280]{0} %broadcast.2378), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2001 = f32[96,1280,7,7]{3,2,1,0} get-tuple-element((f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) %batch-norm-training.2000), index=0, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2364 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2365 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[] %constant.2364), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2366 = pred[96,1280,7,7]{3,2,1,0} compare(f32[96,1280,7,7]{3,2,1,0} %get-tuple-element.2001, f32[96,1280,7,7]{3,2,1,0} %broadcast.2365), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2367 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2368 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[] %constant.2367), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2369 = pred[96,1280,7,7]{3,2,1,0} compare(f32[96,1280,7,7]{3,2,1,0} %get-tuple-element.2001, f32[96,1280,7,7]{3,2,1,0} %broadcast.2368), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2370 = pred[96,1280,7,7]{3,2,1,0} and(pred[96,1280,7,7]{3,2,1,0} %compare.2366, pred[96,1280,7,7]{3,2,1,0} %compare.2369), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2030 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="_make_grads@__init__.py" source_line=86}
  %p314.2029 = f32[] parameter(314), metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %divide.2031 = f32[] divide(f32[] %constant.2030, f32[] %p314.2029), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.2032 = f32[1,1]{1,0} reshape(f32[] %divide.2031), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2033 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.2032), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.2034 = f32[] reshape(f32[1,1]{1,0} %broadcast.2033), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2035 = f32[96,1000]{1,0} broadcast(f32[] %reshape.2034), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %p318.2352 = f32[1000,1280]{1,0} parameter(318), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@linear.py" source_line=114}
  %transpose.2353 = f32[1280,1000]{0,1} transpose(f32[1000,1280]{1,0} %p318.2352), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %transpose.2354 = f32[1000,1280]{1,0} transpose(f32[1280,1000]{0,1} %transpose.2353), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.2355 = f32[96,1280]{1,0} dot(f32[96,1000]{1,0} %broadcast.2035, f32[1000,1280]{1,0} %transpose.2354), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %reshape.2059 = u64[] reshape(u64[] %constant.2058), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2060 = u64[48,1,1280]{2,1,0} broadcast(u64[] %reshape.2059), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %iota.2062 = u64[48,1,1280]{2,1,0} iota(), iota_dimension=2, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2061 = u64[] constant(1), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2063 = u64[48,1,1280]{2,1,0} broadcast(u64[] %constant.2061), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %multiply.2064 = u64[48,1,1280]{2,1,0} multiply(u64[48,1,1280]{2,1,0} %iota.2062, u64[48,1,1280]{2,1,0} %broadcast.2063), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2065 = u64[48,1,1280]{2,1,0} add(u64[48,1,1280]{2,1,0} %broadcast.2060, u64[48,1,1280]{2,1,0} %multiply.2064), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %iota.2067 = u64[48,1,1280]{2,1,0} iota(), iota_dimension=0, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2066 = u64[] constant(1280), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2068 = u64[48,1,1280]{2,1,0} broadcast(u64[] %constant.2066), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %multiply.2069 = u64[48,1,1280]{2,1,0} multiply(u64[48,1,1280]{2,1,0} %iota.2067, u64[48,1,1280]{2,1,0} %broadcast.2068), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2070 = u64[48,1,1280]{2,1,0} add(u64[48,1,1280]{2,1,0} %add.2065, u64[48,1,1280]{2,1,0} %multiply.2069), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %convert.2074 = u32[48,1,1280]{2,1,0} convert(u64[48,1,1280]{2,1,0} %add.2070), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2049 = s64[] constant(2531011), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="dropout@functional.py" source_line=1252}
  %constant.2047 = s64[] constant(214013), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="dropout@functional.py" source_line=1252}
  %p316.2046 = s64[] parameter(316), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="dropout@functional.py" source_line=1252}
  %multiply.2048 = s64[] multiply(s64[] %constant.2047, s64[] %p316.2046), metadata={op_type="aten__mul" op_name="aten__mul" source_file="dropout@functional.py" source_line=1252}
  %add.2050 = s64[] add(s64[] %constant.2049, s64[] %multiply.2048), metadata={op_type="aten__add" op_name="aten__add" source_file="dropout@functional.py" source_line=1252}
  %convert.2057 = u64[] convert(s64[] %add.2050), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %convert.2079 = u32[] convert(u64[] %convert.2057), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %bitcast-convert.2082 = u32[] bitcast-convert(u32[] %convert.2079), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2087 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2082), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2088 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %convert.2074, u32[48,1,1280]{2,1,0} %broadcast.2087), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2073 = u64[] constant(32), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2075 = u64[48,1,1280]{2,1,0} broadcast(u64[] %constant.2073), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2076 = u64[48,1,1280]{2,1,0} shift-right-logical(u64[48,1,1280]{2,1,0} %add.2070, u64[48,1,1280]{2,1,0} %broadcast.2075), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %convert.2077 = u32[48,1,1280]{2,1,0} convert(u64[48,1,1280]{2,1,0} %shift-right-logical.2076), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2078 = u64[] constant(32), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2080 = u64[] shift-right-logical(u64[] %convert.2057, u64[] %constant.2078), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %convert.2081 = u32[] convert(u64[] %shift-right-logical.2080), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %bitcast-convert.2083 = u32[] bitcast-convert(u32[] %convert.2081), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2089 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2083), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2090 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %convert.2077, u32[48,1,1280]{2,1,0} %broadcast.2089), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2091 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2088, u32[48,1,1280]{2,1,0} %add.2090), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2095 = u32[] constant(13), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2096 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2095), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2097 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %add.2090, u32[48,1,1280]{2,1,0} %broadcast.2096), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2092 = u32[] constant(19), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2093 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2092), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2094 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %add.2090, u32[48,1,1280]{2,1,0} %broadcast.2093), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2098 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2097, u32[48,1,1280]{2,1,0} %shift-right-logical.2094), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2099 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2091, u32[48,1,1280]{2,1,0} %or.2098), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2100 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2091, u32[48,1,1280]{2,1,0} %xor.2099), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2104 = u32[] constant(15), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2105 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2104), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2106 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2099, u32[48,1,1280]{2,1,0} %broadcast.2105), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2101 = u32[] constant(17), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2102 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2101), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2103 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2099, u32[48,1,1280]{2,1,0} %broadcast.2102), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2107 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2106, u32[48,1,1280]{2,1,0} %shift-right-logical.2103), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2108 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2100, u32[48,1,1280]{2,1,0} %or.2107), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2109 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2100, u32[48,1,1280]{2,1,0} %xor.2108), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2113 = u32[] constant(26), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2114 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2113), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2115 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2108, u32[48,1,1280]{2,1,0} %broadcast.2114), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2110 = u32[] constant(6), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2111 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2110), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2112 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2108, u32[48,1,1280]{2,1,0} %broadcast.2111), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2116 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2115, u32[48,1,1280]{2,1,0} %shift-right-logical.2112), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2117 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2109, u32[48,1,1280]{2,1,0} %or.2116), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2118 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2109, u32[48,1,1280]{2,1,0} %xor.2117), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2127 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2083), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2128 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2118, u32[48,1,1280]{2,1,0} %broadcast.2127), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2122 = u32[] constant(6), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2123 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2122), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2124 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2117, u32[48,1,1280]{2,1,0} %broadcast.2123), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2119 = u32[] constant(26), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2120 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2119), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2121 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2117, u32[48,1,1280]{2,1,0} %broadcast.2120), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2125 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2124, u32[48,1,1280]{2,1,0} %shift-right-logical.2121), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2126 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2118, u32[48,1,1280]{2,1,0} %or.2125), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2084 = u32[] constant(466688986), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2085 = u32[] xor(u32[] %constant.2084, u32[] %bitcast-convert.2082), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2086 = u32[] xor(u32[] %xor.2085, u32[] %bitcast-convert.2083), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2130 = u32[48,1,1280]{2,1,0} broadcast(u32[] %xor.2086), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2131 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %xor.2126, u32[48,1,1280]{2,1,0} %broadcast.2130), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2129 = u32[] constant(1), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2132 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2129), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2133 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2131, u32[48,1,1280]{2,1,0} %broadcast.2132), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2134 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2128, u32[48,1,1280]{2,1,0} %add.2133), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2138 = u32[] constant(17), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2139 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2138), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2140 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %add.2133, u32[48,1,1280]{2,1,0} %broadcast.2139), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2135 = u32[] constant(15), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2136 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2135), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2137 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %add.2133, u32[48,1,1280]{2,1,0} %broadcast.2136), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2141 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2140, u32[48,1,1280]{2,1,0} %shift-right-logical.2137), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2142 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2134, u32[48,1,1280]{2,1,0} %or.2141), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2143 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2134, u32[48,1,1280]{2,1,0} %xor.2142), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2147 = u32[] constant(29), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2148 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2147), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2149 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2142, u32[48,1,1280]{2,1,0} %broadcast.2148), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2144 = u32[] constant(3), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2145 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2144), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2146 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2142, u32[48,1,1280]{2,1,0} %broadcast.2145), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2150 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2149, u32[48,1,1280]{2,1,0} %shift-right-logical.2146), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2151 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2143, u32[48,1,1280]{2,1,0} %or.2150), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2152 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2143, u32[48,1,1280]{2,1,0} %xor.2151), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2156 = u32[] constant(16), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2157 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2156), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2158 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2151, u32[48,1,1280]{2,1,0} %broadcast.2157), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2153 = u32[] constant(16), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2154 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2153), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2155 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2151, u32[48,1,1280]{2,1,0} %broadcast.2154), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2159 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2158, u32[48,1,1280]{2,1,0} %shift-right-logical.2155), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2160 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2152, u32[48,1,1280]{2,1,0} %or.2159), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2161 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2152, u32[48,1,1280]{2,1,0} %xor.2160), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2170 = u32[48,1,1280]{2,1,0} broadcast(u32[] %xor.2086), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2171 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2161, u32[48,1,1280]{2,1,0} %broadcast.2170), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2165 = u32[] constant(24), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2166 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2165), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2167 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2160, u32[48,1,1280]{2,1,0} %broadcast.2166), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2162 = u32[] constant(8), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2163 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2162), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2164 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2160, u32[48,1,1280]{2,1,0} %broadcast.2163), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2168 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2167, u32[48,1,1280]{2,1,0} %shift-right-logical.2164), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2169 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2161, u32[48,1,1280]{2,1,0} %or.2168), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2173 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2082), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2174 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %xor.2169, u32[48,1,1280]{2,1,0} %broadcast.2173), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2172 = u32[] constant(2), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2175 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2172), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2176 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2174, u32[48,1,1280]{2,1,0} %broadcast.2175), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2177 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2171, u32[48,1,1280]{2,1,0} %add.2176), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2181 = u32[] constant(13), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2182 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2181), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2183 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %add.2176, u32[48,1,1280]{2,1,0} %broadcast.2182), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2178 = u32[] constant(19), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2179 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2178), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2180 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %add.2176, u32[48,1,1280]{2,1,0} %broadcast.2179), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2184 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2183, u32[48,1,1280]{2,1,0} %shift-right-logical.2180), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2185 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2177, u32[48,1,1280]{2,1,0} %or.2184), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2186 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2177, u32[48,1,1280]{2,1,0} %xor.2185), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2190 = u32[] constant(15), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2191 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2190), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2192 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2185, u32[48,1,1280]{2,1,0} %broadcast.2191), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2187 = u32[] constant(17), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2188 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2187), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2189 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2185, u32[48,1,1280]{2,1,0} %broadcast.2188), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2193 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2192, u32[48,1,1280]{2,1,0} %shift-right-logical.2189), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2194 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2186, u32[48,1,1280]{2,1,0} %or.2193), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2195 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2186, u32[48,1,1280]{2,1,0} %xor.2194), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2199 = u32[] constant(26), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2200 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2199), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2201 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2194, u32[48,1,1280]{2,1,0} %broadcast.2200), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2196 = u32[] constant(6), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2197 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2196), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2198 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2194, u32[48,1,1280]{2,1,0} %broadcast.2197), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2202 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2201, u32[48,1,1280]{2,1,0} %shift-right-logical.2198), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2203 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2195, u32[48,1,1280]{2,1,0} %or.2202), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2204 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2195, u32[48,1,1280]{2,1,0} %xor.2203), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2213 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2082), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2214 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2204, u32[48,1,1280]{2,1,0} %broadcast.2213), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2208 = u32[] constant(6), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2209 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2208), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2210 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2203, u32[48,1,1280]{2,1,0} %broadcast.2209), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2205 = u32[] constant(26), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2206 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2205), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2207 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2203, u32[48,1,1280]{2,1,0} %broadcast.2206), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2211 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2210, u32[48,1,1280]{2,1,0} %shift-right-logical.2207), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2212 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2204, u32[48,1,1280]{2,1,0} %or.2211), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2216 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2083), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2217 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %xor.2212, u32[48,1,1280]{2,1,0} %broadcast.2216), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2215 = u32[] constant(3), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2218 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2215), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2219 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2217, u32[48,1,1280]{2,1,0} %broadcast.2218), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2220 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2214, u32[48,1,1280]{2,1,0} %add.2219), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2224 = u32[] constant(17), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2225 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2224), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2226 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %add.2219, u32[48,1,1280]{2,1,0} %broadcast.2225), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2221 = u32[] constant(15), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2222 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2221), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2223 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %add.2219, u32[48,1,1280]{2,1,0} %broadcast.2222), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2227 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2226, u32[48,1,1280]{2,1,0} %shift-right-logical.2223), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2228 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2220, u32[48,1,1280]{2,1,0} %or.2227), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2229 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2220, u32[48,1,1280]{2,1,0} %xor.2228), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2233 = u32[] constant(29), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2234 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2233), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2235 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2228, u32[48,1,1280]{2,1,0} %broadcast.2234), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2230 = u32[] constant(3), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2231 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2230), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2232 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2228, u32[48,1,1280]{2,1,0} %broadcast.2231), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2236 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2235, u32[48,1,1280]{2,1,0} %shift-right-logical.2232), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2237 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2229, u32[48,1,1280]{2,1,0} %or.2236), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2238 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2229, u32[48,1,1280]{2,1,0} %xor.2237), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2242 = u32[] constant(16), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2243 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2242), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2244 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2237, u32[48,1,1280]{2,1,0} %broadcast.2243), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2239 = u32[] constant(16), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2240 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2239), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2241 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2237, u32[48,1,1280]{2,1,0} %broadcast.2240), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2245 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2244, u32[48,1,1280]{2,1,0} %shift-right-logical.2241), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2246 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2238, u32[48,1,1280]{2,1,0} %or.2245), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2247 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2238, u32[48,1,1280]{2,1,0} %xor.2246), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2256 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2083), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2257 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2247, u32[48,1,1280]{2,1,0} %broadcast.2256), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2251 = u32[] constant(24), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2252 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2251), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2253 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2246, u32[48,1,1280]{2,1,0} %broadcast.2252), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2248 = u32[] constant(8), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2249 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2248), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2250 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2246, u32[48,1,1280]{2,1,0} %broadcast.2249), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2254 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2253, u32[48,1,1280]{2,1,0} %shift-right-logical.2250), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2255 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2247, u32[48,1,1280]{2,1,0} %or.2254), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2259 = u32[48,1,1280]{2,1,0} broadcast(u32[] %xor.2086), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2260 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %xor.2255, u32[48,1,1280]{2,1,0} %broadcast.2259), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2258 = u32[] constant(4), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2261 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2258), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2262 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2260, u32[48,1,1280]{2,1,0} %broadcast.2261), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2263 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2257, u32[48,1,1280]{2,1,0} %add.2262), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2267 = u32[] constant(13), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2268 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2267), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2269 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %add.2262, u32[48,1,1280]{2,1,0} %broadcast.2268), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2264 = u32[] constant(19), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2265 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2264), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2266 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %add.2262, u32[48,1,1280]{2,1,0} %broadcast.2265), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2270 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2269, u32[48,1,1280]{2,1,0} %shift-right-logical.2266), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2271 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2263, u32[48,1,1280]{2,1,0} %or.2270), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2272 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2263, u32[48,1,1280]{2,1,0} %xor.2271), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2276 = u32[] constant(15), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2277 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2276), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2278 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2271, u32[48,1,1280]{2,1,0} %broadcast.2277), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2273 = u32[] constant(17), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2274 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2273), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2275 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2271, u32[48,1,1280]{2,1,0} %broadcast.2274), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2279 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2278, u32[48,1,1280]{2,1,0} %shift-right-logical.2275), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2280 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2272, u32[48,1,1280]{2,1,0} %or.2279), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2281 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2272, u32[48,1,1280]{2,1,0} %xor.2280), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2285 = u32[] constant(26), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2286 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2285), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2287 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2280, u32[48,1,1280]{2,1,0} %broadcast.2286), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2282 = u32[] constant(6), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2283 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2282), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2284 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2280, u32[48,1,1280]{2,1,0} %broadcast.2283), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2288 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2287, u32[48,1,1280]{2,1,0} %shift-right-logical.2284), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2289 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2281, u32[48,1,1280]{2,1,0} %or.2288), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2290 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2281, u32[48,1,1280]{2,1,0} %xor.2289), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2299 = u32[48,1,1280]{2,1,0} broadcast(u32[] %xor.2086), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2300 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2290, u32[48,1,1280]{2,1,0} %broadcast.2299), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2294 = u32[] constant(6), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2295 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2294), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-left.2296 = u32[48,1,1280]{2,1,0} shift-left(u32[48,1,1280]{2,1,0} %xor.2289, u32[48,1,1280]{2,1,0} %broadcast.2295), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2291 = u32[] constant(26), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2292 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2291), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2293 = u32[48,1,1280]{2,1,0} shift-right-logical(u32[48,1,1280]{2,1,0} %xor.2289, u32[48,1,1280]{2,1,0} %broadcast.2292), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %or.2297 = u32[48,1,1280]{2,1,0} or(u32[48,1,1280]{2,1,0} %shift-left.2296, u32[48,1,1280]{2,1,0} %shift-right-logical.2293), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %xor.2298 = u32[48,1,1280]{2,1,0} xor(u32[48,1,1280]{2,1,0} %add.2290, u32[48,1,1280]{2,1,0} %or.2297), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2302 = u32[48,1,1280]{2,1,0} broadcast(u32[] %bitcast-convert.2082), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2303 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %xor.2298, u32[48,1,1280]{2,1,0} %broadcast.2302), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2301 = u32[] constant(5), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2304 = u32[48,1,1280]{2,1,0} broadcast(u32[] %constant.2301), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2305 = u32[48,1,1280]{2,1,0} add(u32[48,1,1280]{2,1,0} %add.2303, u32[48,1,1280]{2,1,0} %broadcast.2304), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %concatenate.2306 = u32[48,2,1280]{2,1,0} concatenate(u32[48,1,1280]{2,1,0} %add.2300, u32[48,1,1280]{2,1,0} %add.2305), dimensions={1}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %reshape.2307 = u32[96,1280]{1,0} reshape(u32[48,2,1280]{2,1,0} %concatenate.2306), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2308 = u32[] constant(9), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2309 = u32[96,1280]{1,0} broadcast(u32[] %constant.2308), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %shift-right-logical.2310 = u32[96,1280]{1,0} shift-right-logical(u32[96,1280]{1,0} %reshape.2307, u32[96,1280]{1,0} %broadcast.2309), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %convert.2311 = f32[96,1280]{1,0} convert(u32[96,1280]{1,0} %shift-right-logical.2310), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2312 = f32[] constant(1.1920929e-07), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2313 = f32[96,1280]{1,0} broadcast(f32[] %constant.2312), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %multiply.2314 = f32[96,1280]{1,0} multiply(f32[96,1280]{1,0} %convert.2311, f32[96,1280]{1,0} %broadcast.2313), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2056 = f32[] constant(1), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %constant.2055 = f32[] constant(0), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %subtract.2315 = f32[] subtract(f32[] %constant.2056, f32[] %constant.2055), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2316 = f32[96,1280]{1,0} broadcast(f32[] %subtract.2315), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %multiply.2317 = f32[96,1280]{1,0} multiply(f32[96,1280]{1,0} %multiply.2314, f32[96,1280]{1,0} %broadcast.2316), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2318 = f32[96,1280]{1,0} broadcast(f32[] %constant.2055), dimensions={}, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %add.2319 = f32[96,1280]{1,0} add(f32[96,1280]{1,0} %multiply.2317, f32[96,1280]{1,0} %broadcast.2318), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %p315.2045 = f32[] parameter(315), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="dropout@functional.py" source_line=1252}
  %reshape.2051 = f32[1,1]{1,0} reshape(f32[] %p315.2045), metadata={op_type="aten__expand" op_name="aten__expand" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2052 = f32[1,1]{1,0} broadcast(f32[1,1]{1,0} %reshape.2051), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="dropout@functional.py" source_line=1252}
  %reshape.2053 = f32[] reshape(f32[1,1]{1,0} %broadcast.2052), metadata={op_type="aten__expand" op_name="aten__expand" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2054 = f32[96,1280]{1,0} broadcast(f32[] %reshape.2053), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="dropout@functional.py" source_line=1252}
  %compare.2320 = pred[96,1280]{1,0} compare(f32[96,1280]{1,0} %add.2319, f32[96,1280]{1,0} %broadcast.2054), direction=LT, metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %convert.2321 = f32[96,1280]{1,0} convert(pred[96,1280]{1,0} %compare.2320), metadata={op_type="aten__bernoulli" op_name="aten__bernoulli" source_file="dropout@functional.py" source_line=1252}
  %broadcast.2322 = f32[96,1280]{1,0} broadcast(f32[] %p315.2045), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="dropout@functional.py" source_line=1252}
  %divide.2323 = f32[96,1280]{1,0} divide(f32[96,1280]{1,0} %convert.2321, f32[96,1280]{1,0} %broadcast.2322), metadata={op_type="aten__div" op_name="aten__div" source_file="dropout@functional.py" source_line=1252}
  %multiply.2356 = f32[96,1280]{1,0} multiply(f32[96,1280]{1,0} %dot.2355, f32[96,1280]{1,0} %divide.2323), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2357 = f32[96,1280,1,1]{3,2,1,0} reshape(f32[96,1280]{1,0} %multiply.2356), metadata={op_type="aten__view" op_name="aten__view"}
  %broadcast.2358 = f32[96,1280,1,1]{3,2,1,0} broadcast(f32[96,1280,1,1]{3,2,1,0} %reshape.2357), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.2359 = f32[96,1280]{1,0} reshape(f32[96,1280,1,1]{3,2,1,0} %broadcast.2358), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2360 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[96,1280]{1,0} %reshape.2359), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %p317.2351 = f32[] parameter(317), metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %broadcast.2361 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[] %p317.2351), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.2362 = f32[96,1280,7,7]{3,2,1,0} divide(f32[96,1280,7,7]{3,2,1,0} %broadcast.2360, f32[96,1280,7,7]{3,2,1,0} %broadcast.2361), metadata={op_type="aten__div" op_name="aten__div"}
  %constant.2363 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2371 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[] %constant.2363), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2372 = f32[96,1280,7,7]{3,2,1,0} select(pred[96,1280,7,7]{3,2,1,0} %and.2370, f32[96,1280,7,7]{3,2,1,0} %divide.2362, f32[96,1280,7,7]{3,2,1,0} %broadcast.2371), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2380 = (f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) batch-norm-grad(f32[96,1280,7,7]{3,2,1,0} %convolution.1999, f32[1280]{0} %p311.1997, f32[1280]{0} %get-tuple-element.2002, f32[1280]{0} %subtract.2379, f32[96,1280,7,7]{3,2,1,0} %select.2372), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2381 = f32[96,1280,7,7]{3,2,1,0} get-tuple-element((f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) %batch-norm-grad.2380), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2389 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2394 = f32[1280]{0} reduce(f32[96,1280,7,7]{3,2,1,0} %get-tuple-element.2381, f32[] %constant.2389), dimensions={0,2,3}, to_apply=%AddComputation.2390, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1967 = f32[320]{0} get-tuple-element((f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) %batch-norm-training.1965), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2396 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2397 = f32[320]{0} broadcast(f32[] %constant.2396), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1968 = f32[320]{0} get-tuple-element((f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) %batch-norm-training.1965), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1969 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1970 = f32[320]{0} broadcast(f32[] %constant.1969), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1971 = f32[320]{0} add(f32[320]{0} %get-tuple-element.1968, f32[320]{0} %broadcast.1970), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1972 = f32[320]{0} rsqrt(f32[320]{0} %add.1971), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2398 = f32[320]{0} divide(f32[320]{0} %broadcast.2397, f32[320]{0} %rsqrt.1972), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2399 = f32[320]{0} multiply(f32[320]{0} %divide.2398, f32[320]{0} %divide.2398), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2395 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2400 = f32[320]{0} broadcast(f32[] %constant.2395), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2401 = f32[320]{0} subtract(f32[320]{0} %multiply.2399, f32[320]{0} %broadcast.2400), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2384 = f32[1,1,320,1280]{1,0,2,3} transpose(f32[1280,320,1,1]{3,2,1,0} %p312.1998), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2385 = f32[1,1,320,1280]{1,0,2,3} reverse(f32[1,1,320,1280]{1,0,2,3} %transpose.2384), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2386 = f32[96,320,7,7]{3,2,1,0} convolution(f32[96,1280,7,7]{3,2,1,0} %get-tuple-element.2381, f32[1,1,320,1280]{1,0,2,3} %reverse.2385), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.2402 = (f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) batch-norm-grad(f32[96,320,7,7]{3,2,1,0} %convolution.1964, f32[320]{0} %p305.1958, f32[320]{0} %get-tuple-element.1967, f32[320]{0} %subtract.2401, f32[96,320,7,7]{3,2,1,0} %convolution.2386), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2403 = f32[96,320,7,7]{3,2,1,0} get-tuple-element((f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) %batch-norm-grad.2402), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2411 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2416 = f32[320]{0} reduce(f32[96,320,7,7]{3,2,1,0} %get-tuple-element.2403, f32[] %constant.2411), dimensions={0,2,3}, to_apply=%AddComputation.2412, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1928 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1926), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2428 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2429 = f32[960]{0} broadcast(f32[] %constant.2428), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1929 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1926), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1930 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1931 = f32[960]{0} broadcast(f32[] %constant.1930), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1932 = f32[960]{0} add(f32[960]{0} %get-tuple-element.1929, f32[960]{0} %broadcast.1931), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1933 = f32[960]{0} rsqrt(f32[960]{0} %add.1932), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2430 = f32[960]{0} divide(f32[960]{0} %broadcast.2429, f32[960]{0} %rsqrt.1933), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2431 = f32[960]{0} multiply(f32[960]{0} %divide.2430, f32[960]{0} %divide.2430), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2427 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2432 = f32[960]{0} broadcast(f32[] %constant.2427), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2433 = f32[960]{0} subtract(f32[960]{0} %multiply.2431, f32[960]{0} %broadcast.2432), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2418 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2419 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2418), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2420 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1927, f32[96,960,7,7]{3,2,1,0} %broadcast.2419), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2421 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2422 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2421), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2423 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1927, f32[96,960,7,7]{3,2,1,0} %broadcast.2422), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2424 = pred[96,960,7,7]{3,2,1,0} and(pred[96,960,7,7]{3,2,1,0} %compare.2420, pred[96,960,7,7]{3,2,1,0} %compare.2423), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2406 = f32[1,1,960,320]{1,0,2,3} transpose(f32[320,960,1,1]{3,2,1,0} %p306.1959), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2407 = f32[1,1,960,320]{1,0,2,3} reverse(f32[1,1,960,320]{1,0,2,3} %transpose.2406), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2408 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,320,7,7]{3,2,1,0} %get-tuple-element.2403, f32[1,1,960,320]{1,0,2,3} %reverse.2407), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2417 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2425 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2417), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2426 = f32[96,960,7,7]{3,2,1,0} select(pred[96,960,7,7]{3,2,1,0} %and.2424, f32[96,960,7,7]{3,2,1,0} %convolution.2408, f32[96,960,7,7]{3,2,1,0} %broadcast.2425), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2434 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-grad(f32[96,960,7,7]{3,2,1,0} %convolution.1925, f32[960]{0} %p299.1919, f32[960]{0} %get-tuple-element.1928, f32[960]{0} %subtract.2433, f32[96,960,7,7]{3,2,1,0} %select.2426), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2435 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2434), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2446 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2451 = f32[960]{0} reduce(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2435, f32[] %constant.2446), dimensions={0,2,3}, to_apply=%AddComputation.2447, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1889 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1887), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2463 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2464 = f32[960]{0} broadcast(f32[] %constant.2463), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1890 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1887), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1891 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1892 = f32[960]{0} broadcast(f32[] %constant.1891), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1893 = f32[960]{0} add(f32[960]{0} %get-tuple-element.1890, f32[960]{0} %broadcast.1892), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1894 = f32[960]{0} rsqrt(f32[960]{0} %add.1893), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2465 = f32[960]{0} divide(f32[960]{0} %broadcast.2464, f32[960]{0} %rsqrt.1894), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2466 = f32[960]{0} multiply(f32[960]{0} %divide.2465, f32[960]{0} %divide.2465), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2462 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2467 = f32[960]{0} broadcast(f32[] %constant.2462), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2468 = f32[960]{0} subtract(f32[960]{0} %multiply.2466, f32[960]{0} %broadcast.2467), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2453 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2454 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2453), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2455 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1888, f32[96,960,7,7]{3,2,1,0} %broadcast.2454), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2456 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2457 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2456), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2458 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1888, f32[96,960,7,7]{3,2,1,0} %broadcast.2457), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2459 = pred[96,960,7,7]{3,2,1,0} and(pred[96,960,7,7]{3,2,1,0} %compare.2455, pred[96,960,7,7]{3,2,1,0} %compare.2458), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2438 = f32[3,3,1,960]{1,0,2,3} transpose(f32[960,1,3,3]{3,2,1,0} %p300.1920), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2439 = f32[3,3,1,960,1]{4,3,2,1,0} reshape(f32[3,3,1,960]{1,0,2,3} %transpose.2438), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2440 = f32[3,3,960,1,1]{4,2,3,1,0} transpose(f32[3,3,1,960,1]{4,3,2,1,0} %reshape.2439), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2441 = f32[3,3,960,1]{3,2,1,0} reshape(f32[3,3,960,1,1]{4,2,3,1,0} %transpose.2440), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2442 = f32[3,3,960,1]{3,2,1,0} reverse(f32[3,3,960,1]{3,2,1,0} %reshape.2441), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2443 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2435, f32[3,3,960,1]{3,2,1,0} %reverse.2442), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=960, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2452 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2460 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2452), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2461 = f32[96,960,7,7]{3,2,1,0} select(pred[96,960,7,7]{3,2,1,0} %and.2459, f32[96,960,7,7]{3,2,1,0} %convolution.2443, f32[96,960,7,7]{3,2,1,0} %broadcast.2460), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2469 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-grad(f32[96,960,7,7]{3,2,1,0} %convolution.1886, f32[960]{0} %p293.1877, f32[960]{0} %get-tuple-element.1889, f32[960]{0} %subtract.2468, f32[96,960,7,7]{3,2,1,0} %select.2461), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2470 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2469), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2478 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2483 = f32[960]{0} reduce(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2470, f32[] %constant.2478), dimensions={0,2,3}, to_apply=%AddComputation.2479, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1847 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1845), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2485 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2486 = f32[160]{0} broadcast(f32[] %constant.2485), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1848 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1845), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1849 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1850 = f32[160]{0} broadcast(f32[] %constant.1849), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1851 = f32[160]{0} add(f32[160]{0} %get-tuple-element.1848, f32[160]{0} %broadcast.1850), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1852 = f32[160]{0} rsqrt(f32[160]{0} %add.1851), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2487 = f32[160]{0} divide(f32[160]{0} %broadcast.2486, f32[160]{0} %rsqrt.1852), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2488 = f32[160]{0} multiply(f32[160]{0} %divide.2487, f32[160]{0} %divide.2487), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2484 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2489 = f32[160]{0} broadcast(f32[] %constant.2484), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2490 = f32[160]{0} subtract(f32[160]{0} %multiply.2488, f32[160]{0} %broadcast.2489), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2473 = f32[1,1,160,960]{1,0,2,3} transpose(f32[960,160,1,1]{3,2,1,0} %p294.1878), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2474 = f32[1,1,160,960]{1,0,2,3} reverse(f32[1,1,160,960]{1,0,2,3} %transpose.2473), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2475 = f32[96,160,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2470, f32[1,1,160,960]{1,0,2,3} %reverse.2474), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.2491 = (f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) batch-norm-grad(f32[96,160,7,7]{3,2,1,0} %convolution.1844, f32[160]{0} %p287.1838, f32[160]{0} %get-tuple-element.1847, f32[160]{0} %subtract.2490, f32[96,160,7,7]{3,2,1,0} %convolution.2475), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2492 = f32[96,160,7,7]{3,2,1,0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2491), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2500 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2505 = f32[160]{0} reduce(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2492, f32[] %constant.2500), dimensions={0,2,3}, to_apply=%AddComputation.2501, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1808 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1806), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2517 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2518 = f32[960]{0} broadcast(f32[] %constant.2517), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1809 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1806), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1810 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1811 = f32[960]{0} broadcast(f32[] %constant.1810), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1812 = f32[960]{0} add(f32[960]{0} %get-tuple-element.1809, f32[960]{0} %broadcast.1811), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1813 = f32[960]{0} rsqrt(f32[960]{0} %add.1812), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2519 = f32[960]{0} divide(f32[960]{0} %broadcast.2518, f32[960]{0} %rsqrt.1813), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2520 = f32[960]{0} multiply(f32[960]{0} %divide.2519, f32[960]{0} %divide.2519), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2516 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2521 = f32[960]{0} broadcast(f32[] %constant.2516), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2522 = f32[960]{0} subtract(f32[960]{0} %multiply.2520, f32[960]{0} %broadcast.2521), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2507 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2508 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2507), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2509 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1807, f32[96,960,7,7]{3,2,1,0} %broadcast.2508), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2510 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2511 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2510), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2512 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1807, f32[96,960,7,7]{3,2,1,0} %broadcast.2511), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2513 = pred[96,960,7,7]{3,2,1,0} and(pred[96,960,7,7]{3,2,1,0} %compare.2509, pred[96,960,7,7]{3,2,1,0} %compare.2512), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2495 = f32[1,1,960,160]{1,0,2,3} transpose(f32[160,960,1,1]{3,2,1,0} %p288.1839), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2496 = f32[1,1,960,160]{1,0,2,3} reverse(f32[1,1,960,160]{1,0,2,3} %transpose.2495), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2497 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2492, f32[1,1,960,160]{1,0,2,3} %reverse.2496), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2506 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2514 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2506), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2515 = f32[96,960,7,7]{3,2,1,0} select(pred[96,960,7,7]{3,2,1,0} %and.2513, f32[96,960,7,7]{3,2,1,0} %convolution.2497, f32[96,960,7,7]{3,2,1,0} %broadcast.2514), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2523 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-grad(f32[96,960,7,7]{3,2,1,0} %convolution.1805, f32[960]{0} %p281.1799, f32[960]{0} %get-tuple-element.1808, f32[960]{0} %subtract.2522, f32[96,960,7,7]{3,2,1,0} %select.2515), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2524 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2523), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2535 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2540 = f32[960]{0} reduce(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2524, f32[] %constant.2535), dimensions={0,2,3}, to_apply=%AddComputation.2536, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1769 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1767), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2552 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2553 = f32[960]{0} broadcast(f32[] %constant.2552), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1770 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1767), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1771 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1772 = f32[960]{0} broadcast(f32[] %constant.1771), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1773 = f32[960]{0} add(f32[960]{0} %get-tuple-element.1770, f32[960]{0} %broadcast.1772), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1774 = f32[960]{0} rsqrt(f32[960]{0} %add.1773), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2554 = f32[960]{0} divide(f32[960]{0} %broadcast.2553, f32[960]{0} %rsqrt.1774), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2555 = f32[960]{0} multiply(f32[960]{0} %divide.2554, f32[960]{0} %divide.2554), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2551 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2556 = f32[960]{0} broadcast(f32[] %constant.2551), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2557 = f32[960]{0} subtract(f32[960]{0} %multiply.2555, f32[960]{0} %broadcast.2556), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2542 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2543 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2542), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2544 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1768, f32[96,960,7,7]{3,2,1,0} %broadcast.2543), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2545 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2546 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2545), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2547 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1768, f32[96,960,7,7]{3,2,1,0} %broadcast.2546), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2548 = pred[96,960,7,7]{3,2,1,0} and(pred[96,960,7,7]{3,2,1,0} %compare.2544, pred[96,960,7,7]{3,2,1,0} %compare.2547), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2527 = f32[3,3,1,960]{1,0,2,3} transpose(f32[960,1,3,3]{3,2,1,0} %p282.1800), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2528 = f32[3,3,1,960,1]{4,3,2,1,0} reshape(f32[3,3,1,960]{1,0,2,3} %transpose.2527), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2529 = f32[3,3,960,1,1]{4,2,3,1,0} transpose(f32[3,3,1,960,1]{4,3,2,1,0} %reshape.2528), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2530 = f32[3,3,960,1]{3,2,1,0} reshape(f32[3,3,960,1,1]{4,2,3,1,0} %transpose.2529), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2531 = f32[3,3,960,1]{3,2,1,0} reverse(f32[3,3,960,1]{3,2,1,0} %reshape.2530), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2532 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2524, f32[3,3,960,1]{3,2,1,0} %reverse.2531), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=960, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2541 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2549 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2541), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2550 = f32[96,960,7,7]{3,2,1,0} select(pred[96,960,7,7]{3,2,1,0} %and.2548, f32[96,960,7,7]{3,2,1,0} %convolution.2532, f32[96,960,7,7]{3,2,1,0} %broadcast.2549), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2558 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-grad(f32[96,960,7,7]{3,2,1,0} %convolution.1766, f32[960]{0} %p275.1757, f32[960]{0} %get-tuple-element.1769, f32[960]{0} %subtract.2557, f32[96,960,7,7]{3,2,1,0} %select.2550), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2559 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2558), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2567 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2572 = f32[960]{0} reduce(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2559, f32[] %constant.2567), dimensions={0,2,3}, to_apply=%AddComputation.2568, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1727 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1725), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2581 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2582 = f32[160]{0} broadcast(f32[] %constant.2581), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1728 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1725), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1729 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1730 = f32[160]{0} broadcast(f32[] %constant.1729), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1731 = f32[160]{0} add(f32[160]{0} %get-tuple-element.1728, f32[160]{0} %broadcast.1730), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1732 = f32[160]{0} rsqrt(f32[160]{0} %add.1731), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2583 = f32[160]{0} divide(f32[160]{0} %broadcast.2582, f32[160]{0} %rsqrt.1732), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2584 = f32[160]{0} multiply(f32[160]{0} %divide.2583, f32[160]{0} %divide.2583), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2580 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2585 = f32[160]{0} broadcast(f32[] %constant.2580), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2586 = f32[160]{0} subtract(f32[160]{0} %multiply.2584, f32[160]{0} %broadcast.2585), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2562 = f32[1,1,160,960]{1,0,2,3} transpose(f32[960,160,1,1]{3,2,1,0} %p276.1758), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2563 = f32[1,1,160,960]{1,0,2,3} reverse(f32[1,1,160,960]{1,0,2,3} %transpose.2562), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2564 = f32[96,160,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2559, f32[1,1,160,960]{1,0,2,3} %reverse.2563), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2573 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.2574 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.2573), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2575 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.2574), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.2576 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.2575), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2577 = f32[96,160,7,7]{3,2,1,0} broadcast(f32[] %reshape.2576), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2578 = f32[96,160,7,7]{3,2,1,0} multiply(f32[96,160,7,7]{3,2,1,0} %convolution.2564, f32[96,160,7,7]{3,2,1,0} %broadcast.2577), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2579 = f32[96,160,7,7]{3,2,1,0} add(f32[96,160,7,7]{3,2,1,0} %convolution.2475, f32[96,160,7,7]{3,2,1,0} %multiply.2578), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.2587 = (f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) batch-norm-grad(f32[96,160,7,7]{3,2,1,0} %convolution.1724, f32[160]{0} %p269.1718, f32[160]{0} %get-tuple-element.1727, f32[160]{0} %subtract.2586, f32[96,160,7,7]{3,2,1,0} %add.2579), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2588 = f32[96,160,7,7]{3,2,1,0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2587), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2596 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2601 = f32[160]{0} reduce(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2588, f32[] %constant.2596), dimensions={0,2,3}, to_apply=%AddComputation.2597, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1688 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1686), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2613 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2614 = f32[960]{0} broadcast(f32[] %constant.2613), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1689 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1686), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1690 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1691 = f32[960]{0} broadcast(f32[] %constant.1690), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1692 = f32[960]{0} add(f32[960]{0} %get-tuple-element.1689, f32[960]{0} %broadcast.1691), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1693 = f32[960]{0} rsqrt(f32[960]{0} %add.1692), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2615 = f32[960]{0} divide(f32[960]{0} %broadcast.2614, f32[960]{0} %rsqrt.1693), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2616 = f32[960]{0} multiply(f32[960]{0} %divide.2615, f32[960]{0} %divide.2615), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2612 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2617 = f32[960]{0} broadcast(f32[] %constant.2612), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2618 = f32[960]{0} subtract(f32[960]{0} %multiply.2616, f32[960]{0} %broadcast.2617), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2603 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2604 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2603), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2605 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1687, f32[96,960,7,7]{3,2,1,0} %broadcast.2604), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2606 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2607 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2606), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2608 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1687, f32[96,960,7,7]{3,2,1,0} %broadcast.2607), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2609 = pred[96,960,7,7]{3,2,1,0} and(pred[96,960,7,7]{3,2,1,0} %compare.2605, pred[96,960,7,7]{3,2,1,0} %compare.2608), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2591 = f32[1,1,960,160]{1,0,2,3} transpose(f32[160,960,1,1]{3,2,1,0} %p270.1719), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2592 = f32[1,1,960,160]{1,0,2,3} reverse(f32[1,1,960,160]{1,0,2,3} %transpose.2591), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2593 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2588, f32[1,1,960,160]{1,0,2,3} %reverse.2592), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2602 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2610 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2602), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2611 = f32[96,960,7,7]{3,2,1,0} select(pred[96,960,7,7]{3,2,1,0} %and.2609, f32[96,960,7,7]{3,2,1,0} %convolution.2593, f32[96,960,7,7]{3,2,1,0} %broadcast.2610), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2619 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-grad(f32[96,960,7,7]{3,2,1,0} %convolution.1685, f32[960]{0} %p263.1679, f32[960]{0} %get-tuple-element.1688, f32[960]{0} %subtract.2618, f32[96,960,7,7]{3,2,1,0} %select.2611), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2620 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2619), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2631 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2636 = f32[960]{0} reduce(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2620, f32[] %constant.2631), dimensions={0,2,3}, to_apply=%AddComputation.2632, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1649 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1647), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2648 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2649 = f32[960]{0} broadcast(f32[] %constant.2648), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1650 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-training.1647), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1651 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1652 = f32[960]{0} broadcast(f32[] %constant.1651), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1653 = f32[960]{0} add(f32[960]{0} %get-tuple-element.1650, f32[960]{0} %broadcast.1652), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1654 = f32[960]{0} rsqrt(f32[960]{0} %add.1653), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2650 = f32[960]{0} divide(f32[960]{0} %broadcast.2649, f32[960]{0} %rsqrt.1654), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2651 = f32[960]{0} multiply(f32[960]{0} %divide.2650, f32[960]{0} %divide.2650), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2647 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2652 = f32[960]{0} broadcast(f32[] %constant.2647), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2653 = f32[960]{0} subtract(f32[960]{0} %multiply.2651, f32[960]{0} %broadcast.2652), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2638 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2639 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2638), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2640 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1648, f32[96,960,7,7]{3,2,1,0} %broadcast.2639), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2641 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2642 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2641), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2643 = pred[96,960,7,7]{3,2,1,0} compare(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.1648, f32[96,960,7,7]{3,2,1,0} %broadcast.2642), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2644 = pred[96,960,7,7]{3,2,1,0} and(pred[96,960,7,7]{3,2,1,0} %compare.2640, pred[96,960,7,7]{3,2,1,0} %compare.2643), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2623 = f32[3,3,1,960]{1,0,2,3} transpose(f32[960,1,3,3]{3,2,1,0} %p264.1680), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2624 = f32[3,3,1,960,1]{4,3,2,1,0} reshape(f32[3,3,1,960]{1,0,2,3} %transpose.2623), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2625 = f32[3,3,960,1,1]{4,2,3,1,0} transpose(f32[3,3,1,960,1]{4,3,2,1,0} %reshape.2624), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2626 = f32[3,3,960,1]{3,2,1,0} reshape(f32[3,3,960,1,1]{4,2,3,1,0} %transpose.2625), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2627 = f32[3,3,960,1]{3,2,1,0} reverse(f32[3,3,960,1]{3,2,1,0} %reshape.2626), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2628 = f32[96,960,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2620, f32[3,3,960,1]{3,2,1,0} %reverse.2627), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=960, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2637 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2645 = f32[96,960,7,7]{3,2,1,0} broadcast(f32[] %constant.2637), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2646 = f32[96,960,7,7]{3,2,1,0} select(pred[96,960,7,7]{3,2,1,0} %and.2644, f32[96,960,7,7]{3,2,1,0} %convolution.2628, f32[96,960,7,7]{3,2,1,0} %broadcast.2645), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2654 = (f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) batch-norm-grad(f32[96,960,7,7]{3,2,1,0} %convolution.1646, f32[960]{0} %p257.1644, f32[960]{0} %get-tuple-element.1649, f32[960]{0} %subtract.2653, f32[96,960,7,7]{3,2,1,0} %select.2646), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2655 = f32[96,960,7,7]{3,2,1,0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2654), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2663 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2668 = f32[960]{0} reduce(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2655, f32[] %constant.2663), dimensions={0,2,3}, to_apply=%AddComputation.2664, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1614 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1612), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2677 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2678 = f32[160]{0} broadcast(f32[] %constant.2677), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1615 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-training.1612), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1616 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1617 = f32[160]{0} broadcast(f32[] %constant.1616), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1618 = f32[160]{0} add(f32[160]{0} %get-tuple-element.1615, f32[160]{0} %broadcast.1617), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1619 = f32[160]{0} rsqrt(f32[160]{0} %add.1618), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2679 = f32[160]{0} divide(f32[160]{0} %broadcast.2678, f32[160]{0} %rsqrt.1619), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2680 = f32[160]{0} multiply(f32[160]{0} %divide.2679, f32[160]{0} %divide.2679), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2676 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2681 = f32[160]{0} broadcast(f32[] %constant.2676), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2682 = f32[160]{0} subtract(f32[160]{0} %multiply.2680, f32[160]{0} %broadcast.2681), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2658 = f32[1,1,160,960]{1,0,2,3} transpose(f32[960,160,1,1]{3,2,1,0} %p258.1645), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2659 = f32[1,1,160,960]{1,0,2,3} reverse(f32[1,1,160,960]{1,0,2,3} %transpose.2658), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2660 = f32[96,160,7,7]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2655, f32[1,1,160,960]{1,0,2,3} %reverse.2659), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2669 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.2670 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.2669), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2671 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.2670), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.2672 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.2671), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2673 = f32[96,160,7,7]{3,2,1,0} broadcast(f32[] %reshape.2672), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2674 = f32[96,160,7,7]{3,2,1,0} multiply(f32[96,160,7,7]{3,2,1,0} %convolution.2660, f32[96,160,7,7]{3,2,1,0} %broadcast.2673), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2675 = f32[96,160,7,7]{3,2,1,0} add(f32[96,160,7,7]{3,2,1,0} %add.2579, f32[96,160,7,7]{3,2,1,0} %multiply.2674), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.2683 = (f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) batch-norm-grad(f32[96,160,7,7]{3,2,1,0} %convolution.1611, f32[160]{0} %p251.1605, f32[160]{0} %get-tuple-element.1614, f32[160]{0} %subtract.2682, f32[96,160,7,7]{3,2,1,0} %add.2675), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2684 = f32[96,160,7,7]{3,2,1,0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2683), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2692 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2697 = f32[160]{0} reduce(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2684, f32[] %constant.2692), dimensions={0,2,3}, to_apply=%AddComputation.2693, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1575 = f32[576]{0} get-tuple-element((f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1573), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2709 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2710 = f32[576]{0} broadcast(f32[] %constant.2709), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1576 = f32[576]{0} get-tuple-element((f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1573), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1577 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1578 = f32[576]{0} broadcast(f32[] %constant.1577), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1579 = f32[576]{0} add(f32[576]{0} %get-tuple-element.1576, f32[576]{0} %broadcast.1578), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1580 = f32[576]{0} rsqrt(f32[576]{0} %add.1579), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2711 = f32[576]{0} divide(f32[576]{0} %broadcast.2710, f32[576]{0} %rsqrt.1580), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2712 = f32[576]{0} multiply(f32[576]{0} %divide.2711, f32[576]{0} %divide.2711), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2708 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2713 = f32[576]{0} broadcast(f32[] %constant.2708), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2714 = f32[576]{0} subtract(f32[576]{0} %multiply.2712, f32[576]{0} %broadcast.2713), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2699 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2700 = f32[96,576,7,7]{3,2,1,0} broadcast(f32[] %constant.2699), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2701 = pred[96,576,7,7]{3,2,1,0} compare(f32[96,576,7,7]{3,2,1,0} %get-tuple-element.1574, f32[96,576,7,7]{3,2,1,0} %broadcast.2700), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2702 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2703 = f32[96,576,7,7]{3,2,1,0} broadcast(f32[] %constant.2702), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2704 = pred[96,576,7,7]{3,2,1,0} compare(f32[96,576,7,7]{3,2,1,0} %get-tuple-element.1574, f32[96,576,7,7]{3,2,1,0} %broadcast.2703), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2705 = pred[96,576,7,7]{3,2,1,0} and(pred[96,576,7,7]{3,2,1,0} %compare.2701, pred[96,576,7,7]{3,2,1,0} %compare.2704), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2687 = f32[1,1,576,160]{1,0,2,3} transpose(f32[160,576,1,1]{3,2,1,0} %p252.1606), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2688 = f32[1,1,576,160]{1,0,2,3} reverse(f32[1,1,576,160]{1,0,2,3} %transpose.2687), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2689 = f32[96,576,7,7]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2684, f32[1,1,576,160]{1,0,2,3} %reverse.2688), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2698 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2706 = f32[96,576,7,7]{3,2,1,0} broadcast(f32[] %constant.2698), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2707 = f32[96,576,7,7]{3,2,1,0} select(pred[96,576,7,7]{3,2,1,0} %and.2705, f32[96,576,7,7]{3,2,1,0} %convolution.2689, f32[96,576,7,7]{3,2,1,0} %broadcast.2706), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2715 = (f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-grad(f32[96,576,7,7]{3,2,1,0} %convolution.1572, f32[576]{0} %p245.1566, f32[576]{0} %get-tuple-element.1575, f32[576]{0} %subtract.2714, f32[96,576,7,7]{3,2,1,0} %select.2707), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2716 = f32[96,576,7,7]{3,2,1,0} get-tuple-element((f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2715), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2727 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2732 = f32[576]{0} reduce(f32[96,576,7,7]{3,2,1,0} %get-tuple-element.2716, f32[] %constant.2727), dimensions={0,2,3}, to_apply=%AddComputation.2728, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1536 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1534), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2744 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2745 = f32[576]{0} broadcast(f32[] %constant.2744), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1537 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1534), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1538 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1539 = f32[576]{0} broadcast(f32[] %constant.1538), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1540 = f32[576]{0} add(f32[576]{0} %get-tuple-element.1537, f32[576]{0} %broadcast.1539), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1541 = f32[576]{0} rsqrt(f32[576]{0} %add.1540), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2746 = f32[576]{0} divide(f32[576]{0} %broadcast.2745, f32[576]{0} %rsqrt.1541), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2747 = f32[576]{0} multiply(f32[576]{0} %divide.2746, f32[576]{0} %divide.2746), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2743 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2748 = f32[576]{0} broadcast(f32[] %constant.2743), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2749 = f32[576]{0} subtract(f32[576]{0} %multiply.2747, f32[576]{0} %broadcast.2748), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2734 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2735 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2734), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2736 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1535, f32[96,576,14,14]{3,2,1,0} %broadcast.2735), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2737 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2738 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2737), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2739 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1535, f32[96,576,14,14]{3,2,1,0} %broadcast.2738), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2740 = pred[96,576,14,14]{3,2,1,0} and(pred[96,576,14,14]{3,2,1,0} %compare.2736, pred[96,576,14,14]{3,2,1,0} %compare.2739), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2719 = f32[3,3,1,576]{1,0,2,3} transpose(f32[576,1,3,3]{3,2,1,0} %p246.1567), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2720 = f32[3,3,1,576,1]{4,3,2,1,0} reshape(f32[3,3,1,576]{1,0,2,3} %transpose.2719), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2721 = f32[3,3,576,1,1]{4,2,3,1,0} transpose(f32[3,3,1,576,1]{4,3,2,1,0} %reshape.2720), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2722 = f32[3,3,576,1]{3,2,1,0} reshape(f32[3,3,576,1,1]{4,2,3,1,0} %transpose.2721), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2723 = f32[3,3,576,1]{3,2,1,0} reverse(f32[3,3,576,1]{3,2,1,0} %reshape.2722), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2724 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,576,7,7]{3,2,1,0} %get-tuple-element.2716, f32[3,3,576,1]{3,2,1,0} %reverse.2723), window={size=3x3 pad=1_2x1_2 lhs_dilate=2x2}, dim_labels=bf01_01oi->bf01, feature_group_count=576, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2733 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2741 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2733), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2742 = f32[96,576,14,14]{3,2,1,0} select(pred[96,576,14,14]{3,2,1,0} %and.2740, f32[96,576,14,14]{3,2,1,0} %convolution.2724, f32[96,576,14,14]{3,2,1,0} %broadcast.2741), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2750 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-grad(f32[96,576,14,14]{3,2,1,0} %convolution.1533, f32[576]{0} %p239.1524, f32[576]{0} %get-tuple-element.1536, f32[576]{0} %subtract.2749, f32[96,576,14,14]{3,2,1,0} %select.2742), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2751 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2750), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2759 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2764 = f32[576]{0} reduce(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2751, f32[] %constant.2759), dimensions={0,2,3}, to_apply=%AddComputation.2760, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1494 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1492), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2766 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2767 = f32[96]{0} broadcast(f32[] %constant.2766), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1495 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1492), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1496 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1497 = f32[96]{0} broadcast(f32[] %constant.1496), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1498 = f32[96]{0} add(f32[96]{0} %get-tuple-element.1495, f32[96]{0} %broadcast.1497), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1499 = f32[96]{0} rsqrt(f32[96]{0} %add.1498), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2768 = f32[96]{0} divide(f32[96]{0} %broadcast.2767, f32[96]{0} %rsqrt.1499), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2769 = f32[96]{0} multiply(f32[96]{0} %divide.2768, f32[96]{0} %divide.2768), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2765 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2770 = f32[96]{0} broadcast(f32[] %constant.2765), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2771 = f32[96]{0} subtract(f32[96]{0} %multiply.2769, f32[96]{0} %broadcast.2770), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2754 = f32[1,1,96,576]{1,0,2,3} transpose(f32[576,96,1,1]{3,2,1,0} %p240.1525), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2755 = f32[1,1,96,576]{1,0,2,3} reverse(f32[1,1,96,576]{1,0,2,3} %transpose.2754), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2756 = f32[96,96,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2751, f32[1,1,96,576]{1,0,2,3} %reverse.2755), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.2772 = (f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-grad(f32[96,96,14,14]{3,2,1,0} %convolution.1491, f32[96]{0} %p233.1485, f32[96]{0} %get-tuple-element.1494, f32[96]{0} %subtract.2771, f32[96,96,14,14]{3,2,1,0} %convolution.2756), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2773 = f32[96,96,14,14]{3,2,1,0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2772), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2781 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2786 = f32[96]{0} reduce(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2773, f32[] %constant.2781), dimensions={0,2,3}, to_apply=%AddComputation.2782, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1455 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1453), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2798 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2799 = f32[576]{0} broadcast(f32[] %constant.2798), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1456 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1453), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1457 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1458 = f32[576]{0} broadcast(f32[] %constant.1457), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1459 = f32[576]{0} add(f32[576]{0} %get-tuple-element.1456, f32[576]{0} %broadcast.1458), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1460 = f32[576]{0} rsqrt(f32[576]{0} %add.1459), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2800 = f32[576]{0} divide(f32[576]{0} %broadcast.2799, f32[576]{0} %rsqrt.1460), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2801 = f32[576]{0} multiply(f32[576]{0} %divide.2800, f32[576]{0} %divide.2800), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2797 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2802 = f32[576]{0} broadcast(f32[] %constant.2797), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2803 = f32[576]{0} subtract(f32[576]{0} %multiply.2801, f32[576]{0} %broadcast.2802), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2788 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2789 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2788), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2790 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1454, f32[96,576,14,14]{3,2,1,0} %broadcast.2789), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2791 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2792 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2791), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2793 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1454, f32[96,576,14,14]{3,2,1,0} %broadcast.2792), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2794 = pred[96,576,14,14]{3,2,1,0} and(pred[96,576,14,14]{3,2,1,0} %compare.2790, pred[96,576,14,14]{3,2,1,0} %compare.2793), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2776 = f32[1,1,576,96]{1,0,2,3} transpose(f32[96,576,1,1]{3,2,1,0} %p234.1486), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2777 = f32[1,1,576,96]{1,0,2,3} reverse(f32[1,1,576,96]{1,0,2,3} %transpose.2776), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2778 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2773, f32[1,1,576,96]{1,0,2,3} %reverse.2777), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2787 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2795 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2787), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2796 = f32[96,576,14,14]{3,2,1,0} select(pred[96,576,14,14]{3,2,1,0} %and.2794, f32[96,576,14,14]{3,2,1,0} %convolution.2778, f32[96,576,14,14]{3,2,1,0} %broadcast.2795), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2804 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-grad(f32[96,576,14,14]{3,2,1,0} %convolution.1452, f32[576]{0} %p227.1446, f32[576]{0} %get-tuple-element.1455, f32[576]{0} %subtract.2803, f32[96,576,14,14]{3,2,1,0} %select.2796), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2805 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2804), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2816 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2821 = f32[576]{0} reduce(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2805, f32[] %constant.2816), dimensions={0,2,3}, to_apply=%AddComputation.2817, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1416 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1414), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2833 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2834 = f32[576]{0} broadcast(f32[] %constant.2833), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1417 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1414), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1418 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1419 = f32[576]{0} broadcast(f32[] %constant.1418), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1420 = f32[576]{0} add(f32[576]{0} %get-tuple-element.1417, f32[576]{0} %broadcast.1419), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1421 = f32[576]{0} rsqrt(f32[576]{0} %add.1420), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2835 = f32[576]{0} divide(f32[576]{0} %broadcast.2834, f32[576]{0} %rsqrt.1421), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2836 = f32[576]{0} multiply(f32[576]{0} %divide.2835, f32[576]{0} %divide.2835), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2832 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2837 = f32[576]{0} broadcast(f32[] %constant.2832), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2838 = f32[576]{0} subtract(f32[576]{0} %multiply.2836, f32[576]{0} %broadcast.2837), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2823 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2824 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2823), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2825 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1415, f32[96,576,14,14]{3,2,1,0} %broadcast.2824), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2826 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2827 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2826), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2828 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1415, f32[96,576,14,14]{3,2,1,0} %broadcast.2827), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2829 = pred[96,576,14,14]{3,2,1,0} and(pred[96,576,14,14]{3,2,1,0} %compare.2825, pred[96,576,14,14]{3,2,1,0} %compare.2828), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2808 = f32[3,3,1,576]{1,0,2,3} transpose(f32[576,1,3,3]{3,2,1,0} %p228.1447), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2809 = f32[3,3,1,576,1]{4,3,2,1,0} reshape(f32[3,3,1,576]{1,0,2,3} %transpose.2808), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2810 = f32[3,3,576,1,1]{4,2,3,1,0} transpose(f32[3,3,1,576,1]{4,3,2,1,0} %reshape.2809), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2811 = f32[3,3,576,1]{3,2,1,0} reshape(f32[3,3,576,1,1]{4,2,3,1,0} %transpose.2810), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2812 = f32[3,3,576,1]{3,2,1,0} reverse(f32[3,3,576,1]{3,2,1,0} %reshape.2811), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2813 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2805, f32[3,3,576,1]{3,2,1,0} %reverse.2812), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=576, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2822 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2830 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2822), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2831 = f32[96,576,14,14]{3,2,1,0} select(pred[96,576,14,14]{3,2,1,0} %and.2829, f32[96,576,14,14]{3,2,1,0} %convolution.2813, f32[96,576,14,14]{3,2,1,0} %broadcast.2830), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2839 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-grad(f32[96,576,14,14]{3,2,1,0} %convolution.1413, f32[576]{0} %p221.1404, f32[576]{0} %get-tuple-element.1416, f32[576]{0} %subtract.2838, f32[96,576,14,14]{3,2,1,0} %select.2831), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2840 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2839), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2848 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2853 = f32[576]{0} reduce(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2840, f32[] %constant.2848), dimensions={0,2,3}, to_apply=%AddComputation.2849, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1374 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1372), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2862 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2863 = f32[96]{0} broadcast(f32[] %constant.2862), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1375 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1372), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1376 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1377 = f32[96]{0} broadcast(f32[] %constant.1376), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1378 = f32[96]{0} add(f32[96]{0} %get-tuple-element.1375, f32[96]{0} %broadcast.1377), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1379 = f32[96]{0} rsqrt(f32[96]{0} %add.1378), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2864 = f32[96]{0} divide(f32[96]{0} %broadcast.2863, f32[96]{0} %rsqrt.1379), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2865 = f32[96]{0} multiply(f32[96]{0} %divide.2864, f32[96]{0} %divide.2864), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2861 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2866 = f32[96]{0} broadcast(f32[] %constant.2861), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2867 = f32[96]{0} subtract(f32[96]{0} %multiply.2865, f32[96]{0} %broadcast.2866), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2843 = f32[1,1,96,576]{1,0,2,3} transpose(f32[576,96,1,1]{3,2,1,0} %p222.1405), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2844 = f32[1,1,96,576]{1,0,2,3} reverse(f32[1,1,96,576]{1,0,2,3} %transpose.2843), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2845 = f32[96,96,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2840, f32[1,1,96,576]{1,0,2,3} %reverse.2844), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2854 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.2855 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.2854), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2856 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.2855), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.2857 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.2856), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2858 = f32[96,96,14,14]{3,2,1,0} broadcast(f32[] %reshape.2857), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2859 = f32[96,96,14,14]{3,2,1,0} multiply(f32[96,96,14,14]{3,2,1,0} %convolution.2845, f32[96,96,14,14]{3,2,1,0} %broadcast.2858), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2860 = f32[96,96,14,14]{3,2,1,0} add(f32[96,96,14,14]{3,2,1,0} %convolution.2756, f32[96,96,14,14]{3,2,1,0} %multiply.2859), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.2868 = (f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-grad(f32[96,96,14,14]{3,2,1,0} %convolution.1371, f32[96]{0} %p215.1365, f32[96]{0} %get-tuple-element.1374, f32[96]{0} %subtract.2867, f32[96,96,14,14]{3,2,1,0} %add.2860), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2869 = f32[96,96,14,14]{3,2,1,0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2868), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2877 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2882 = f32[96]{0} reduce(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2869, f32[] %constant.2877), dimensions={0,2,3}, to_apply=%AddComputation.2878, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1335 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1333), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2894 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2895 = f32[576]{0} broadcast(f32[] %constant.2894), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1336 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1333), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1337 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1338 = f32[576]{0} broadcast(f32[] %constant.1337), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1339 = f32[576]{0} add(f32[576]{0} %get-tuple-element.1336, f32[576]{0} %broadcast.1338), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1340 = f32[576]{0} rsqrt(f32[576]{0} %add.1339), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2896 = f32[576]{0} divide(f32[576]{0} %broadcast.2895, f32[576]{0} %rsqrt.1340), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2897 = f32[576]{0} multiply(f32[576]{0} %divide.2896, f32[576]{0} %divide.2896), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2893 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2898 = f32[576]{0} broadcast(f32[] %constant.2893), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2899 = f32[576]{0} subtract(f32[576]{0} %multiply.2897, f32[576]{0} %broadcast.2898), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2884 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2885 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2884), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2886 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1334, f32[96,576,14,14]{3,2,1,0} %broadcast.2885), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2887 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2888 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2887), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2889 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1334, f32[96,576,14,14]{3,2,1,0} %broadcast.2888), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2890 = pred[96,576,14,14]{3,2,1,0} and(pred[96,576,14,14]{3,2,1,0} %compare.2886, pred[96,576,14,14]{3,2,1,0} %compare.2889), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2872 = f32[1,1,576,96]{1,0,2,3} transpose(f32[96,576,1,1]{3,2,1,0} %p216.1366), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2873 = f32[1,1,576,96]{1,0,2,3} reverse(f32[1,1,576,96]{1,0,2,3} %transpose.2872), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2874 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2869, f32[1,1,576,96]{1,0,2,3} %reverse.2873), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2883 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2891 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2883), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2892 = f32[96,576,14,14]{3,2,1,0} select(pred[96,576,14,14]{3,2,1,0} %and.2890, f32[96,576,14,14]{3,2,1,0} %convolution.2874, f32[96,576,14,14]{3,2,1,0} %broadcast.2891), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2900 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-grad(f32[96,576,14,14]{3,2,1,0} %convolution.1332, f32[576]{0} %p209.1326, f32[576]{0} %get-tuple-element.1335, f32[576]{0} %subtract.2899, f32[96,576,14,14]{3,2,1,0} %select.2892), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2901 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2900), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2912 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2917 = f32[576]{0} reduce(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2901, f32[] %constant.2912), dimensions={0,2,3}, to_apply=%AddComputation.2913, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1296 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1294), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2929 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2930 = f32[576]{0} broadcast(f32[] %constant.2929), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1297 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-training.1294), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1298 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1299 = f32[576]{0} broadcast(f32[] %constant.1298), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1300 = f32[576]{0} add(f32[576]{0} %get-tuple-element.1297, f32[576]{0} %broadcast.1299), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1301 = f32[576]{0} rsqrt(f32[576]{0} %add.1300), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2931 = f32[576]{0} divide(f32[576]{0} %broadcast.2930, f32[576]{0} %rsqrt.1301), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2932 = f32[576]{0} multiply(f32[576]{0} %divide.2931, f32[576]{0} %divide.2931), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2928 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2933 = f32[576]{0} broadcast(f32[] %constant.2928), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2934 = f32[576]{0} subtract(f32[576]{0} %multiply.2932, f32[576]{0} %broadcast.2933), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2919 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2920 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2919), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2921 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1295, f32[96,576,14,14]{3,2,1,0} %broadcast.2920), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2922 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2923 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2922), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2924 = pred[96,576,14,14]{3,2,1,0} compare(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.1295, f32[96,576,14,14]{3,2,1,0} %broadcast.2923), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2925 = pred[96,576,14,14]{3,2,1,0} and(pred[96,576,14,14]{3,2,1,0} %compare.2921, pred[96,576,14,14]{3,2,1,0} %compare.2924), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2904 = f32[3,3,1,576]{1,0,2,3} transpose(f32[576,1,3,3]{3,2,1,0} %p210.1327), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2905 = f32[3,3,1,576,1]{4,3,2,1,0} reshape(f32[3,3,1,576]{1,0,2,3} %transpose.2904), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2906 = f32[3,3,576,1,1]{4,2,3,1,0} transpose(f32[3,3,1,576,1]{4,3,2,1,0} %reshape.2905), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.2907 = f32[3,3,576,1]{3,2,1,0} reshape(f32[3,3,576,1,1]{4,2,3,1,0} %transpose.2906), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2908 = f32[3,3,576,1]{3,2,1,0} reverse(f32[3,3,576,1]{3,2,1,0} %reshape.2907), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2909 = f32[96,576,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2901, f32[3,3,576,1]{3,2,1,0} %reverse.2908), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=576, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2918 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2926 = f32[96,576,14,14]{3,2,1,0} broadcast(f32[] %constant.2918), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2927 = f32[96,576,14,14]{3,2,1,0} select(pred[96,576,14,14]{3,2,1,0} %and.2925, f32[96,576,14,14]{3,2,1,0} %convolution.2909, f32[96,576,14,14]{3,2,1,0} %broadcast.2926), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2935 = (f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) batch-norm-grad(f32[96,576,14,14]{3,2,1,0} %convolution.1293, f32[576]{0} %p203.1291, f32[576]{0} %get-tuple-element.1296, f32[576]{0} %subtract.2934, f32[96,576,14,14]{3,2,1,0} %select.2927), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2936 = f32[96,576,14,14]{3,2,1,0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2935), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2944 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2949 = f32[576]{0} reduce(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2936, f32[] %constant.2944), dimensions={0,2,3}, to_apply=%AddComputation.2945, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1261 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1259), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2958 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2959 = f32[96]{0} broadcast(f32[] %constant.2958), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1262 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.1259), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1263 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1264 = f32[96]{0} broadcast(f32[] %constant.1263), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1265 = f32[96]{0} add(f32[96]{0} %get-tuple-element.1262, f32[96]{0} %broadcast.1264), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1266 = f32[96]{0} rsqrt(f32[96]{0} %add.1265), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2960 = f32[96]{0} divide(f32[96]{0} %broadcast.2959, f32[96]{0} %rsqrt.1266), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2961 = f32[96]{0} multiply(f32[96]{0} %divide.2960, f32[96]{0} %divide.2960), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2957 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2962 = f32[96]{0} broadcast(f32[] %constant.2957), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2963 = f32[96]{0} subtract(f32[96]{0} %multiply.2961, f32[96]{0} %broadcast.2962), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.2939 = f32[1,1,96,576]{1,0,2,3} transpose(f32[576,96,1,1]{3,2,1,0} %p204.1292), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2940 = f32[1,1,96,576]{1,0,2,3} reverse(f32[1,1,96,576]{1,0,2,3} %transpose.2939), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2941 = f32[96,96,14,14]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2936, f32[1,1,96,576]{1,0,2,3} %reverse.2940), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2950 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.2951 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.2950), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2952 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.2951), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.2953 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.2952), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2954 = f32[96,96,14,14]{3,2,1,0} broadcast(f32[] %reshape.2953), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2955 = f32[96,96,14,14]{3,2,1,0} multiply(f32[96,96,14,14]{3,2,1,0} %convolution.2941, f32[96,96,14,14]{3,2,1,0} %broadcast.2954), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2956 = f32[96,96,14,14]{3,2,1,0} add(f32[96,96,14,14]{3,2,1,0} %add.2860, f32[96,96,14,14]{3,2,1,0} %multiply.2955), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.2964 = (f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-grad(f32[96,96,14,14]{3,2,1,0} %convolution.1258, f32[96]{0} %p197.1252, f32[96]{0} %get-tuple-element.1261, f32[96]{0} %subtract.2963, f32[96,96,14,14]{3,2,1,0} %add.2956), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2965 = f32[96,96,14,14]{3,2,1,0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2964), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2973 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.2978 = f32[96]{0} reduce(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2965, f32[] %constant.2973), dimensions={0,2,3}, to_apply=%AddComputation.2974, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1222 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1220), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2990 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2991 = f32[384]{0} broadcast(f32[] %constant.2990), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1223 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1220), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1224 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1225 = f32[384]{0} broadcast(f32[] %constant.1224), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1226 = f32[384]{0} add(f32[384]{0} %get-tuple-element.1223, f32[384]{0} %broadcast.1225), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1227 = f32[384]{0} rsqrt(f32[384]{0} %add.1226), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.2992 = f32[384]{0} divide(f32[384]{0} %broadcast.2991, f32[384]{0} %rsqrt.1227), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.2993 = f32[384]{0} multiply(f32[384]{0} %divide.2992, f32[384]{0} %divide.2992), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2989 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.2994 = f32[384]{0} broadcast(f32[] %constant.2989), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.2995 = f32[384]{0} subtract(f32[384]{0} %multiply.2993, f32[384]{0} %broadcast.2994), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.2980 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2981 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.2980), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2982 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1221, f32[96,384,14,14]{3,2,1,0} %broadcast.2981), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.2983 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2984 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.2983), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.2985 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1221, f32[96,384,14,14]{3,2,1,0} %broadcast.2984), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.2986 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.2982, pred[96,384,14,14]{3,2,1,0} %compare.2985), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.2968 = f32[1,1,384,96]{1,0,2,3} transpose(f32[96,384,1,1]{3,2,1,0} %p198.1253), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.2969 = f32[1,1,384,96]{1,0,2,3} reverse(f32[1,1,384,96]{1,0,2,3} %transpose.2968), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.2970 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2965, f32[1,1,384,96]{1,0,2,3} %reverse.2969), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.2979 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.2987 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.2979), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.2988 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.2986, f32[96,384,14,14]{3,2,1,0} %convolution.2970, f32[96,384,14,14]{3,2,1,0} %broadcast.2987), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.2996 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.1219, f32[384]{0} %p191.1213, f32[384]{0} %get-tuple-element.1222, f32[384]{0} %subtract.2995, f32[96,384,14,14]{3,2,1,0} %select.2988), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2997 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.2996), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3008 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3013 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.2997, f32[] %constant.3008), dimensions={0,2,3}, to_apply=%AddComputation.3009, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1183 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1181), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3025 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3026 = f32[384]{0} broadcast(f32[] %constant.3025), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1184 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1181), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1185 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1186 = f32[384]{0} broadcast(f32[] %constant.1185), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1187 = f32[384]{0} add(f32[384]{0} %get-tuple-element.1184, f32[384]{0} %broadcast.1186), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1188 = f32[384]{0} rsqrt(f32[384]{0} %add.1187), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3027 = f32[384]{0} divide(f32[384]{0} %broadcast.3026, f32[384]{0} %rsqrt.1188), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3028 = f32[384]{0} multiply(f32[384]{0} %divide.3027, f32[384]{0} %divide.3027), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3024 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3029 = f32[384]{0} broadcast(f32[] %constant.3024), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3030 = f32[384]{0} subtract(f32[384]{0} %multiply.3028, f32[384]{0} %broadcast.3029), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3015 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3016 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3015), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3017 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1182, f32[96,384,14,14]{3,2,1,0} %broadcast.3016), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3018 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3019 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3018), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3020 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1182, f32[96,384,14,14]{3,2,1,0} %broadcast.3019), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3021 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3017, pred[96,384,14,14]{3,2,1,0} %compare.3020), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3000 = f32[3,3,1,384]{1,0,2,3} transpose(f32[384,1,3,3]{3,2,1,0} %p192.1214), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3001 = f32[3,3,1,384,1]{4,3,2,1,0} reshape(f32[3,3,1,384]{1,0,2,3} %transpose.3000), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3002 = f32[3,3,384,1,1]{4,2,3,1,0} transpose(f32[3,3,1,384,1]{4,3,2,1,0} %reshape.3001), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3003 = f32[3,3,384,1]{3,2,1,0} reshape(f32[3,3,384,1,1]{4,2,3,1,0} %transpose.3002), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3004 = f32[3,3,384,1]{3,2,1,0} reverse(f32[3,3,384,1]{3,2,1,0} %reshape.3003), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3005 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.2997, f32[3,3,384,1]{3,2,1,0} %reverse.3004), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3014 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3022 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3014), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3023 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3021, f32[96,384,14,14]{3,2,1,0} %convolution.3005, f32[96,384,14,14]{3,2,1,0} %broadcast.3022), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3031 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.1180, f32[384]{0} %p185.1171, f32[384]{0} %get-tuple-element.1183, f32[384]{0} %subtract.3030, f32[96,384,14,14]{3,2,1,0} %select.3023), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3032 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3031), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3040 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3045 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3032, f32[] %constant.3040), dimensions={0,2,3}, to_apply=%AddComputation.3041, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1141 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.1139), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3047 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3048 = f32[64]{0} broadcast(f32[] %constant.3047), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1142 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.1139), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1143 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1144 = f32[64]{0} broadcast(f32[] %constant.1143), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1145 = f32[64]{0} add(f32[64]{0} %get-tuple-element.1142, f32[64]{0} %broadcast.1144), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1146 = f32[64]{0} rsqrt(f32[64]{0} %add.1145), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3049 = f32[64]{0} divide(f32[64]{0} %broadcast.3048, f32[64]{0} %rsqrt.1146), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3050 = f32[64]{0} multiply(f32[64]{0} %divide.3049, f32[64]{0} %divide.3049), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3046 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3051 = f32[64]{0} broadcast(f32[] %constant.3046), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3052 = f32[64]{0} subtract(f32[64]{0} %multiply.3050, f32[64]{0} %broadcast.3051), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3035 = f32[1,1,64,384]{1,0,2,3} transpose(f32[384,64,1,1]{3,2,1,0} %p186.1172), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3036 = f32[1,1,64,384]{1,0,2,3} reverse(f32[1,1,64,384]{1,0,2,3} %transpose.3035), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3037 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3032, f32[1,1,64,384]{1,0,2,3} %reverse.3036), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.3053 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-grad(f32[96,64,14,14]{3,2,1,0} %convolution.1138, f32[64]{0} %p179.1132, f32[64]{0} %get-tuple-element.1141, f32[64]{0} %subtract.3052, f32[96,64,14,14]{3,2,1,0} %convolution.3037), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3054 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3053), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3062 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3067 = f32[64]{0} reduce(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3054, f32[] %constant.3062), dimensions={0,2,3}, to_apply=%AddComputation.3063, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1102 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1100), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3079 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3080 = f32[384]{0} broadcast(f32[] %constant.3079), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1103 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1100), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1104 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1105 = f32[384]{0} broadcast(f32[] %constant.1104), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1106 = f32[384]{0} add(f32[384]{0} %get-tuple-element.1103, f32[384]{0} %broadcast.1105), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1107 = f32[384]{0} rsqrt(f32[384]{0} %add.1106), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3081 = f32[384]{0} divide(f32[384]{0} %broadcast.3080, f32[384]{0} %rsqrt.1107), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3082 = f32[384]{0} multiply(f32[384]{0} %divide.3081, f32[384]{0} %divide.3081), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3078 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3083 = f32[384]{0} broadcast(f32[] %constant.3078), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3084 = f32[384]{0} subtract(f32[384]{0} %multiply.3082, f32[384]{0} %broadcast.3083), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3069 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3070 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3069), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3071 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1101, f32[96,384,14,14]{3,2,1,0} %broadcast.3070), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3072 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3073 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3072), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3074 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1101, f32[96,384,14,14]{3,2,1,0} %broadcast.3073), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3075 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3071, pred[96,384,14,14]{3,2,1,0} %compare.3074), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3057 = f32[1,1,384,64]{1,0,2,3} transpose(f32[64,384,1,1]{3,2,1,0} %p180.1133), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3058 = f32[1,1,384,64]{1,0,2,3} reverse(f32[1,1,384,64]{1,0,2,3} %transpose.3057), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3059 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3054, f32[1,1,384,64]{1,0,2,3} %reverse.3058), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3068 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3076 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3068), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3077 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3075, f32[96,384,14,14]{3,2,1,0} %convolution.3059, f32[96,384,14,14]{3,2,1,0} %broadcast.3076), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3085 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.1099, f32[384]{0} %p173.1093, f32[384]{0} %get-tuple-element.1102, f32[384]{0} %subtract.3084, f32[96,384,14,14]{3,2,1,0} %select.3077), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3086 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3085), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3097 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3102 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3086, f32[] %constant.3097), dimensions={0,2,3}, to_apply=%AddComputation.3098, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1063 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1061), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3114 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3115 = f32[384]{0} broadcast(f32[] %constant.3114), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1064 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.1061), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1065 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1066 = f32[384]{0} broadcast(f32[] %constant.1065), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1067 = f32[384]{0} add(f32[384]{0} %get-tuple-element.1064, f32[384]{0} %broadcast.1066), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1068 = f32[384]{0} rsqrt(f32[384]{0} %add.1067), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3116 = f32[384]{0} divide(f32[384]{0} %broadcast.3115, f32[384]{0} %rsqrt.1068), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3117 = f32[384]{0} multiply(f32[384]{0} %divide.3116, f32[384]{0} %divide.3116), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3113 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3118 = f32[384]{0} broadcast(f32[] %constant.3113), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3119 = f32[384]{0} subtract(f32[384]{0} %multiply.3117, f32[384]{0} %broadcast.3118), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3104 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3105 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3104), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3106 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1062, f32[96,384,14,14]{3,2,1,0} %broadcast.3105), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3107 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3108 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3107), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3109 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.1062, f32[96,384,14,14]{3,2,1,0} %broadcast.3108), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3110 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3106, pred[96,384,14,14]{3,2,1,0} %compare.3109), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3089 = f32[3,3,1,384]{1,0,2,3} transpose(f32[384,1,3,3]{3,2,1,0} %p174.1094), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3090 = f32[3,3,1,384,1]{4,3,2,1,0} reshape(f32[3,3,1,384]{1,0,2,3} %transpose.3089), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3091 = f32[3,3,384,1,1]{4,2,3,1,0} transpose(f32[3,3,1,384,1]{4,3,2,1,0} %reshape.3090), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3092 = f32[3,3,384,1]{3,2,1,0} reshape(f32[3,3,384,1,1]{4,2,3,1,0} %transpose.3091), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3093 = f32[3,3,384,1]{3,2,1,0} reverse(f32[3,3,384,1]{3,2,1,0} %reshape.3092), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3094 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3086, f32[3,3,384,1]{3,2,1,0} %reverse.3093), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3103 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3111 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3103), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3112 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3110, f32[96,384,14,14]{3,2,1,0} %convolution.3094, f32[96,384,14,14]{3,2,1,0} %broadcast.3111), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3120 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.1060, f32[384]{0} %p167.1051, f32[384]{0} %get-tuple-element.1063, f32[384]{0} %subtract.3119, f32[96,384,14,14]{3,2,1,0} %select.3112), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3121 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3120), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3129 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3134 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3121, f32[] %constant.3129), dimensions={0,2,3}, to_apply=%AddComputation.3130, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.1021 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.1019), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3143 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3144 = f32[64]{0} broadcast(f32[] %constant.3143), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.1022 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.1019), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1023 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1024 = f32[64]{0} broadcast(f32[] %constant.1023), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.1025 = f32[64]{0} add(f32[64]{0} %get-tuple-element.1022, f32[64]{0} %broadcast.1024), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.1026 = f32[64]{0} rsqrt(f32[64]{0} %add.1025), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3145 = f32[64]{0} divide(f32[64]{0} %broadcast.3144, f32[64]{0} %rsqrt.1026), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3146 = f32[64]{0} multiply(f32[64]{0} %divide.3145, f32[64]{0} %divide.3145), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3142 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3147 = f32[64]{0} broadcast(f32[] %constant.3142), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3148 = f32[64]{0} subtract(f32[64]{0} %multiply.3146, f32[64]{0} %broadcast.3147), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3124 = f32[1,1,64,384]{1,0,2,3} transpose(f32[384,64,1,1]{3,2,1,0} %p168.1052), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3125 = f32[1,1,64,384]{1,0,2,3} reverse(f32[1,1,64,384]{1,0,2,3} %transpose.3124), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3126 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3121, f32[1,1,64,384]{1,0,2,3} %reverse.3125), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3135 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.3136 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.3135), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3137 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.3136), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.3138 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.3137), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3139 = f32[96,64,14,14]{3,2,1,0} broadcast(f32[] %reshape.3138), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3140 = f32[96,64,14,14]{3,2,1,0} multiply(f32[96,64,14,14]{3,2,1,0} %convolution.3126, f32[96,64,14,14]{3,2,1,0} %broadcast.3139), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3141 = f32[96,64,14,14]{3,2,1,0} add(f32[96,64,14,14]{3,2,1,0} %convolution.3037, f32[96,64,14,14]{3,2,1,0} %multiply.3140), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.3149 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-grad(f32[96,64,14,14]{3,2,1,0} %convolution.1018, f32[64]{0} %p161.1012, f32[64]{0} %get-tuple-element.1021, f32[64]{0} %subtract.3148, f32[96,64,14,14]{3,2,1,0} %add.3141), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3150 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3149), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3158 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3163 = f32[64]{0} reduce(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3150, f32[] %constant.3158), dimensions={0,2,3}, to_apply=%AddComputation.3159, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.982 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.980), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3175 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3176 = f32[384]{0} broadcast(f32[] %constant.3175), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.983 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.980), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.984 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.985 = f32[384]{0} broadcast(f32[] %constant.984), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.986 = f32[384]{0} add(f32[384]{0} %get-tuple-element.983, f32[384]{0} %broadcast.985), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.987 = f32[384]{0} rsqrt(f32[384]{0} %add.986), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3177 = f32[384]{0} divide(f32[384]{0} %broadcast.3176, f32[384]{0} %rsqrt.987), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3178 = f32[384]{0} multiply(f32[384]{0} %divide.3177, f32[384]{0} %divide.3177), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3174 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3179 = f32[384]{0} broadcast(f32[] %constant.3174), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3180 = f32[384]{0} subtract(f32[384]{0} %multiply.3178, f32[384]{0} %broadcast.3179), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3165 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3166 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3165), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3167 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.981, f32[96,384,14,14]{3,2,1,0} %broadcast.3166), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3168 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3169 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3168), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3170 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.981, f32[96,384,14,14]{3,2,1,0} %broadcast.3169), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3171 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3167, pred[96,384,14,14]{3,2,1,0} %compare.3170), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3153 = f32[1,1,384,64]{1,0,2,3} transpose(f32[64,384,1,1]{3,2,1,0} %p162.1013), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3154 = f32[1,1,384,64]{1,0,2,3} reverse(f32[1,1,384,64]{1,0,2,3} %transpose.3153), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3155 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3150, f32[1,1,384,64]{1,0,2,3} %reverse.3154), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3164 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3172 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3164), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3173 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3171, f32[96,384,14,14]{3,2,1,0} %convolution.3155, f32[96,384,14,14]{3,2,1,0} %broadcast.3172), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3181 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.979, f32[384]{0} %p155.973, f32[384]{0} %get-tuple-element.982, f32[384]{0} %subtract.3180, f32[96,384,14,14]{3,2,1,0} %select.3173), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3182 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3181), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3193 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3198 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3182, f32[] %constant.3193), dimensions={0,2,3}, to_apply=%AddComputation.3194, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.943 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.941), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3210 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3211 = f32[384]{0} broadcast(f32[] %constant.3210), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.944 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.941), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.945 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.946 = f32[384]{0} broadcast(f32[] %constant.945), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.947 = f32[384]{0} add(f32[384]{0} %get-tuple-element.944, f32[384]{0} %broadcast.946), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.948 = f32[384]{0} rsqrt(f32[384]{0} %add.947), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3212 = f32[384]{0} divide(f32[384]{0} %broadcast.3211, f32[384]{0} %rsqrt.948), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3213 = f32[384]{0} multiply(f32[384]{0} %divide.3212, f32[384]{0} %divide.3212), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3209 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3214 = f32[384]{0} broadcast(f32[] %constant.3209), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3215 = f32[384]{0} subtract(f32[384]{0} %multiply.3213, f32[384]{0} %broadcast.3214), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3200 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3201 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3200), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3202 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.942, f32[96,384,14,14]{3,2,1,0} %broadcast.3201), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3203 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3204 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3203), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3205 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.942, f32[96,384,14,14]{3,2,1,0} %broadcast.3204), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3206 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3202, pred[96,384,14,14]{3,2,1,0} %compare.3205), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3185 = f32[3,3,1,384]{1,0,2,3} transpose(f32[384,1,3,3]{3,2,1,0} %p156.974), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3186 = f32[3,3,1,384,1]{4,3,2,1,0} reshape(f32[3,3,1,384]{1,0,2,3} %transpose.3185), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3187 = f32[3,3,384,1,1]{4,2,3,1,0} transpose(f32[3,3,1,384,1]{4,3,2,1,0} %reshape.3186), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3188 = f32[3,3,384,1]{3,2,1,0} reshape(f32[3,3,384,1,1]{4,2,3,1,0} %transpose.3187), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3189 = f32[3,3,384,1]{3,2,1,0} reverse(f32[3,3,384,1]{3,2,1,0} %reshape.3188), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3190 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3182, f32[3,3,384,1]{3,2,1,0} %reverse.3189), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3199 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3207 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3199), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3208 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3206, f32[96,384,14,14]{3,2,1,0} %convolution.3190, f32[96,384,14,14]{3,2,1,0} %broadcast.3207), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3216 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.940, f32[384]{0} %p149.931, f32[384]{0} %get-tuple-element.943, f32[384]{0} %subtract.3215, f32[96,384,14,14]{3,2,1,0} %select.3208), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3217 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3216), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3225 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3230 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3217, f32[] %constant.3225), dimensions={0,2,3}, to_apply=%AddComputation.3226, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.901 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.899), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3239 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3240 = f32[64]{0} broadcast(f32[] %constant.3239), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.902 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.899), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.903 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.904 = f32[64]{0} broadcast(f32[] %constant.903), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.905 = f32[64]{0} add(f32[64]{0} %get-tuple-element.902, f32[64]{0} %broadcast.904), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.906 = f32[64]{0} rsqrt(f32[64]{0} %add.905), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3241 = f32[64]{0} divide(f32[64]{0} %broadcast.3240, f32[64]{0} %rsqrt.906), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3242 = f32[64]{0} multiply(f32[64]{0} %divide.3241, f32[64]{0} %divide.3241), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3238 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3243 = f32[64]{0} broadcast(f32[] %constant.3238), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3244 = f32[64]{0} subtract(f32[64]{0} %multiply.3242, f32[64]{0} %broadcast.3243), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3220 = f32[1,1,64,384]{1,0,2,3} transpose(f32[384,64,1,1]{3,2,1,0} %p150.932), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3221 = f32[1,1,64,384]{1,0,2,3} reverse(f32[1,1,64,384]{1,0,2,3} %transpose.3220), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3222 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3217, f32[1,1,64,384]{1,0,2,3} %reverse.3221), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3231 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.3232 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.3231), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3233 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.3232), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.3234 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.3233), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3235 = f32[96,64,14,14]{3,2,1,0} broadcast(f32[] %reshape.3234), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3236 = f32[96,64,14,14]{3,2,1,0} multiply(f32[96,64,14,14]{3,2,1,0} %convolution.3222, f32[96,64,14,14]{3,2,1,0} %broadcast.3235), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3237 = f32[96,64,14,14]{3,2,1,0} add(f32[96,64,14,14]{3,2,1,0} %add.3141, f32[96,64,14,14]{3,2,1,0} %multiply.3236), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.3245 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-grad(f32[96,64,14,14]{3,2,1,0} %convolution.898, f32[64]{0} %p143.892, f32[64]{0} %get-tuple-element.901, f32[64]{0} %subtract.3244, f32[96,64,14,14]{3,2,1,0} %add.3237), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3246 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3245), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3254 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3259 = f32[64]{0} reduce(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3246, f32[] %constant.3254), dimensions={0,2,3}, to_apply=%AddComputation.3255, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.862 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.860), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3271 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3272 = f32[384]{0} broadcast(f32[] %constant.3271), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.863 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.860), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.864 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.865 = f32[384]{0} broadcast(f32[] %constant.864), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.866 = f32[384]{0} add(f32[384]{0} %get-tuple-element.863, f32[384]{0} %broadcast.865), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.867 = f32[384]{0} rsqrt(f32[384]{0} %add.866), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3273 = f32[384]{0} divide(f32[384]{0} %broadcast.3272, f32[384]{0} %rsqrt.867), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3274 = f32[384]{0} multiply(f32[384]{0} %divide.3273, f32[384]{0} %divide.3273), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3270 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3275 = f32[384]{0} broadcast(f32[] %constant.3270), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3276 = f32[384]{0} subtract(f32[384]{0} %multiply.3274, f32[384]{0} %broadcast.3275), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3261 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3262 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3261), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3263 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.861, f32[96,384,14,14]{3,2,1,0} %broadcast.3262), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3264 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3265 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3264), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3266 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.861, f32[96,384,14,14]{3,2,1,0} %broadcast.3265), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3267 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3263, pred[96,384,14,14]{3,2,1,0} %compare.3266), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3249 = f32[1,1,384,64]{1,0,2,3} transpose(f32[64,384,1,1]{3,2,1,0} %p144.893), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3250 = f32[1,1,384,64]{1,0,2,3} reverse(f32[1,1,384,64]{1,0,2,3} %transpose.3249), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3251 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3246, f32[1,1,384,64]{1,0,2,3} %reverse.3250), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3260 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3268 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3260), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3269 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3267, f32[96,384,14,14]{3,2,1,0} %convolution.3251, f32[96,384,14,14]{3,2,1,0} %broadcast.3268), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3277 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.859, f32[384]{0} %p137.853, f32[384]{0} %get-tuple-element.862, f32[384]{0} %subtract.3276, f32[96,384,14,14]{3,2,1,0} %select.3269), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3278 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3277), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3289 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3294 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3278, f32[] %constant.3289), dimensions={0,2,3}, to_apply=%AddComputation.3290, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.823 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.821), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3306 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3307 = f32[384]{0} broadcast(f32[] %constant.3306), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.824 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-training.821), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.825 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.826 = f32[384]{0} broadcast(f32[] %constant.825), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.827 = f32[384]{0} add(f32[384]{0} %get-tuple-element.824, f32[384]{0} %broadcast.826), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.828 = f32[384]{0} rsqrt(f32[384]{0} %add.827), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3308 = f32[384]{0} divide(f32[384]{0} %broadcast.3307, f32[384]{0} %rsqrt.828), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3309 = f32[384]{0} multiply(f32[384]{0} %divide.3308, f32[384]{0} %divide.3308), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3305 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3310 = f32[384]{0} broadcast(f32[] %constant.3305), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3311 = f32[384]{0} subtract(f32[384]{0} %multiply.3309, f32[384]{0} %broadcast.3310), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3296 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3297 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3296), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3298 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.822, f32[96,384,14,14]{3,2,1,0} %broadcast.3297), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3299 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3300 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3299), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3301 = pred[96,384,14,14]{3,2,1,0} compare(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.822, f32[96,384,14,14]{3,2,1,0} %broadcast.3300), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3302 = pred[96,384,14,14]{3,2,1,0} and(pred[96,384,14,14]{3,2,1,0} %compare.3298, pred[96,384,14,14]{3,2,1,0} %compare.3301), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3281 = f32[3,3,1,384]{1,0,2,3} transpose(f32[384,1,3,3]{3,2,1,0} %p138.854), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3282 = f32[3,3,1,384,1]{4,3,2,1,0} reshape(f32[3,3,1,384]{1,0,2,3} %transpose.3281), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3283 = f32[3,3,384,1,1]{4,2,3,1,0} transpose(f32[3,3,1,384,1]{4,3,2,1,0} %reshape.3282), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3284 = f32[3,3,384,1]{3,2,1,0} reshape(f32[3,3,384,1,1]{4,2,3,1,0} %transpose.3283), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3285 = f32[3,3,384,1]{3,2,1,0} reverse(f32[3,3,384,1]{3,2,1,0} %reshape.3284), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3286 = f32[96,384,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3278, f32[3,3,384,1]{3,2,1,0} %reverse.3285), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3295 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3303 = f32[96,384,14,14]{3,2,1,0} broadcast(f32[] %constant.3295), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3304 = f32[96,384,14,14]{3,2,1,0} select(pred[96,384,14,14]{3,2,1,0} %and.3302, f32[96,384,14,14]{3,2,1,0} %convolution.3286, f32[96,384,14,14]{3,2,1,0} %broadcast.3303), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3312 = (f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) batch-norm-grad(f32[96,384,14,14]{3,2,1,0} %convolution.820, f32[384]{0} %p131.818, f32[384]{0} %get-tuple-element.823, f32[384]{0} %subtract.3311, f32[96,384,14,14]{3,2,1,0} %select.3304), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3313 = f32[96,384,14,14]{3,2,1,0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3312), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3321 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3326 = f32[384]{0} reduce(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3313, f32[] %constant.3321), dimensions={0,2,3}, to_apply=%AddComputation.3322, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.788 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.786), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3335 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3336 = f32[64]{0} broadcast(f32[] %constant.3335), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.789 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-training.786), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.790 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.791 = f32[64]{0} broadcast(f32[] %constant.790), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.792 = f32[64]{0} add(f32[64]{0} %get-tuple-element.789, f32[64]{0} %broadcast.791), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.793 = f32[64]{0} rsqrt(f32[64]{0} %add.792), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3337 = f32[64]{0} divide(f32[64]{0} %broadcast.3336, f32[64]{0} %rsqrt.793), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3338 = f32[64]{0} multiply(f32[64]{0} %divide.3337, f32[64]{0} %divide.3337), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3334 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3339 = f32[64]{0} broadcast(f32[] %constant.3334), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3340 = f32[64]{0} subtract(f32[64]{0} %multiply.3338, f32[64]{0} %broadcast.3339), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3316 = f32[1,1,64,384]{1,0,2,3} transpose(f32[384,64,1,1]{3,2,1,0} %p132.819), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3317 = f32[1,1,64,384]{1,0,2,3} reverse(f32[1,1,64,384]{1,0,2,3} %transpose.3316), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3318 = f32[96,64,14,14]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3313, f32[1,1,64,384]{1,0,2,3} %reverse.3317), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3327 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.3328 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.3327), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3329 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.3328), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.3330 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.3329), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3331 = f32[96,64,14,14]{3,2,1,0} broadcast(f32[] %reshape.3330), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3332 = f32[96,64,14,14]{3,2,1,0} multiply(f32[96,64,14,14]{3,2,1,0} %convolution.3318, f32[96,64,14,14]{3,2,1,0} %broadcast.3331), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3333 = f32[96,64,14,14]{3,2,1,0} add(f32[96,64,14,14]{3,2,1,0} %add.3237, f32[96,64,14,14]{3,2,1,0} %multiply.3332), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.3341 = (f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) batch-norm-grad(f32[96,64,14,14]{3,2,1,0} %convolution.785, f32[64]{0} %p125.779, f32[64]{0} %get-tuple-element.788, f32[64]{0} %subtract.3340, f32[96,64,14,14]{3,2,1,0} %add.3333), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3342 = f32[96,64,14,14]{3,2,1,0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3341), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3350 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3355 = f32[64]{0} reduce(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3342, f32[] %constant.3350), dimensions={0,2,3}, to_apply=%AddComputation.3351, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.749 = f32[192]{0} get-tuple-element((f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.747), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3367 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3368 = f32[192]{0} broadcast(f32[] %constant.3367), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.750 = f32[192]{0} get-tuple-element((f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.747), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.751 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.752 = f32[192]{0} broadcast(f32[] %constant.751), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.753 = f32[192]{0} add(f32[192]{0} %get-tuple-element.750, f32[192]{0} %broadcast.752), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.754 = f32[192]{0} rsqrt(f32[192]{0} %add.753), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3369 = f32[192]{0} divide(f32[192]{0} %broadcast.3368, f32[192]{0} %rsqrt.754), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3370 = f32[192]{0} multiply(f32[192]{0} %divide.3369, f32[192]{0} %divide.3369), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3366 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3371 = f32[192]{0} broadcast(f32[] %constant.3366), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3372 = f32[192]{0} subtract(f32[192]{0} %multiply.3370, f32[192]{0} %broadcast.3371), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3357 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3358 = f32[96,192,14,14]{3,2,1,0} broadcast(f32[] %constant.3357), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3359 = pred[96,192,14,14]{3,2,1,0} compare(f32[96,192,14,14]{3,2,1,0} %get-tuple-element.748, f32[96,192,14,14]{3,2,1,0} %broadcast.3358), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3360 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3361 = f32[96,192,14,14]{3,2,1,0} broadcast(f32[] %constant.3360), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3362 = pred[96,192,14,14]{3,2,1,0} compare(f32[96,192,14,14]{3,2,1,0} %get-tuple-element.748, f32[96,192,14,14]{3,2,1,0} %broadcast.3361), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3363 = pred[96,192,14,14]{3,2,1,0} and(pred[96,192,14,14]{3,2,1,0} %compare.3359, pred[96,192,14,14]{3,2,1,0} %compare.3362), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3345 = f32[1,1,192,64]{1,0,2,3} transpose(f32[64,192,1,1]{3,2,1,0} %p126.780), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3346 = f32[1,1,192,64]{1,0,2,3} reverse(f32[1,1,192,64]{1,0,2,3} %transpose.3345), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3347 = f32[96,192,14,14]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3342, f32[1,1,192,64]{1,0,2,3} %reverse.3346), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3356 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3364 = f32[96,192,14,14]{3,2,1,0} broadcast(f32[] %constant.3356), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3365 = f32[96,192,14,14]{3,2,1,0} select(pred[96,192,14,14]{3,2,1,0} %and.3363, f32[96,192,14,14]{3,2,1,0} %convolution.3347, f32[96,192,14,14]{3,2,1,0} %broadcast.3364), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3373 = (f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-grad(f32[96,192,14,14]{3,2,1,0} %convolution.746, f32[192]{0} %p119.740, f32[192]{0} %get-tuple-element.749, f32[192]{0} %subtract.3372, f32[96,192,14,14]{3,2,1,0} %select.3365), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3374 = f32[96,192,14,14]{3,2,1,0} get-tuple-element((f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3373), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3385 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3390 = f32[192]{0} reduce(f32[96,192,14,14]{3,2,1,0} %get-tuple-element.3374, f32[] %constant.3385), dimensions={0,2,3}, to_apply=%AddComputation.3386, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.710 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.708), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3402 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3403 = f32[192]{0} broadcast(f32[] %constant.3402), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.711 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.708), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.712 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.713 = f32[192]{0} broadcast(f32[] %constant.712), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.714 = f32[192]{0} add(f32[192]{0} %get-tuple-element.711, f32[192]{0} %broadcast.713), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.715 = f32[192]{0} rsqrt(f32[192]{0} %add.714), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3404 = f32[192]{0} divide(f32[192]{0} %broadcast.3403, f32[192]{0} %rsqrt.715), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3405 = f32[192]{0} multiply(f32[192]{0} %divide.3404, f32[192]{0} %divide.3404), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3401 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3406 = f32[192]{0} broadcast(f32[] %constant.3401), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3407 = f32[192]{0} subtract(f32[192]{0} %multiply.3405, f32[192]{0} %broadcast.3406), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3392 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3393 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3392), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3394 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.709, f32[96,192,28,28]{3,2,1,0} %broadcast.3393), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3395 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3396 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3395), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3397 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.709, f32[96,192,28,28]{3,2,1,0} %broadcast.3396), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3398 = pred[96,192,28,28]{3,2,1,0} and(pred[96,192,28,28]{3,2,1,0} %compare.3394, pred[96,192,28,28]{3,2,1,0} %compare.3397), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3377 = f32[3,3,1,192]{1,0,2,3} transpose(f32[192,1,3,3]{3,2,1,0} %p120.741), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3378 = f32[3,3,1,192,1]{4,3,2,1,0} reshape(f32[3,3,1,192]{1,0,2,3} %transpose.3377), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3379 = f32[3,3,192,1,1]{4,2,3,1,0} transpose(f32[3,3,1,192,1]{4,3,2,1,0} %reshape.3378), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3380 = f32[3,3,192,1]{3,2,1,0} reshape(f32[3,3,192,1,1]{4,2,3,1,0} %transpose.3379), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3381 = f32[3,3,192,1]{3,2,1,0} reverse(f32[3,3,192,1]{3,2,1,0} %reshape.3380), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3382 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,192,14,14]{3,2,1,0} %get-tuple-element.3374, f32[3,3,192,1]{3,2,1,0} %reverse.3381), window={size=3x3 pad=1_2x1_2 lhs_dilate=2x2}, dim_labels=bf01_01oi->bf01, feature_group_count=192, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3391 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3399 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3391), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3400 = f32[96,192,28,28]{3,2,1,0} select(pred[96,192,28,28]{3,2,1,0} %and.3398, f32[96,192,28,28]{3,2,1,0} %convolution.3382, f32[96,192,28,28]{3,2,1,0} %broadcast.3399), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3408 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-grad(f32[96,192,28,28]{3,2,1,0} %convolution.707, f32[192]{0} %p113.698, f32[192]{0} %get-tuple-element.710, f32[192]{0} %subtract.3407, f32[96,192,28,28]{3,2,1,0} %select.3400), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3409 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3408), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3417 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3422 = f32[192]{0} reduce(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3409, f32[] %constant.3417), dimensions={0,2,3}, to_apply=%AddComputation.3418, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.668 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.666), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3424 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3425 = f32[32]{0} broadcast(f32[] %constant.3424), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.669 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.666), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.670 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.671 = f32[32]{0} broadcast(f32[] %constant.670), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.672 = f32[32]{0} add(f32[32]{0} %get-tuple-element.669, f32[32]{0} %broadcast.671), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.673 = f32[32]{0} rsqrt(f32[32]{0} %add.672), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3426 = f32[32]{0} divide(f32[32]{0} %broadcast.3425, f32[32]{0} %rsqrt.673), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3427 = f32[32]{0} multiply(f32[32]{0} %divide.3426, f32[32]{0} %divide.3426), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3423 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3428 = f32[32]{0} broadcast(f32[] %constant.3423), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3429 = f32[32]{0} subtract(f32[32]{0} %multiply.3427, f32[32]{0} %broadcast.3428), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3412 = f32[1,1,32,192]{1,0,2,3} transpose(f32[192,32,1,1]{3,2,1,0} %p114.699), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3413 = f32[1,1,32,192]{1,0,2,3} reverse(f32[1,1,32,192]{1,0,2,3} %transpose.3412), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3414 = f32[96,32,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3409, f32[1,1,32,192]{1,0,2,3} %reverse.3413), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.3430 = (f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-grad(f32[96,32,28,28]{3,2,1,0} %convolution.665, f32[32]{0} %p107.659, f32[32]{0} %get-tuple-element.668, f32[32]{0} %subtract.3429, f32[96,32,28,28]{3,2,1,0} %convolution.3414), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3431 = f32[96,32,28,28]{3,2,1,0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3430), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3439 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3444 = f32[32]{0} reduce(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3431, f32[] %constant.3439), dimensions={0,2,3}, to_apply=%AddComputation.3440, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.629 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.627), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3456 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3457 = f32[192]{0} broadcast(f32[] %constant.3456), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.630 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.627), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.631 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.632 = f32[192]{0} broadcast(f32[] %constant.631), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.633 = f32[192]{0} add(f32[192]{0} %get-tuple-element.630, f32[192]{0} %broadcast.632), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.634 = f32[192]{0} rsqrt(f32[192]{0} %add.633), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3458 = f32[192]{0} divide(f32[192]{0} %broadcast.3457, f32[192]{0} %rsqrt.634), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3459 = f32[192]{0} multiply(f32[192]{0} %divide.3458, f32[192]{0} %divide.3458), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3455 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3460 = f32[192]{0} broadcast(f32[] %constant.3455), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3461 = f32[192]{0} subtract(f32[192]{0} %multiply.3459, f32[192]{0} %broadcast.3460), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3446 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3447 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3446), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3448 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.628, f32[96,192,28,28]{3,2,1,0} %broadcast.3447), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3449 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3450 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3449), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3451 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.628, f32[96,192,28,28]{3,2,1,0} %broadcast.3450), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3452 = pred[96,192,28,28]{3,2,1,0} and(pred[96,192,28,28]{3,2,1,0} %compare.3448, pred[96,192,28,28]{3,2,1,0} %compare.3451), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3434 = f32[1,1,192,32]{1,0,2,3} transpose(f32[32,192,1,1]{3,2,1,0} %p108.660), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3435 = f32[1,1,192,32]{1,0,2,3} reverse(f32[1,1,192,32]{1,0,2,3} %transpose.3434), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3436 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3431, f32[1,1,192,32]{1,0,2,3} %reverse.3435), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3445 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3453 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3445), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3454 = f32[96,192,28,28]{3,2,1,0} select(pred[96,192,28,28]{3,2,1,0} %and.3452, f32[96,192,28,28]{3,2,1,0} %convolution.3436, f32[96,192,28,28]{3,2,1,0} %broadcast.3453), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3462 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-grad(f32[96,192,28,28]{3,2,1,0} %convolution.626, f32[192]{0} %p101.620, f32[192]{0} %get-tuple-element.629, f32[192]{0} %subtract.3461, f32[96,192,28,28]{3,2,1,0} %select.3454), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3463 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3462), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3474 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3479 = f32[192]{0} reduce(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3463, f32[] %constant.3474), dimensions={0,2,3}, to_apply=%AddComputation.3475, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.590 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.588), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3491 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3492 = f32[192]{0} broadcast(f32[] %constant.3491), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.591 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.588), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.592 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.593 = f32[192]{0} broadcast(f32[] %constant.592), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.594 = f32[192]{0} add(f32[192]{0} %get-tuple-element.591, f32[192]{0} %broadcast.593), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.595 = f32[192]{0} rsqrt(f32[192]{0} %add.594), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3493 = f32[192]{0} divide(f32[192]{0} %broadcast.3492, f32[192]{0} %rsqrt.595), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3494 = f32[192]{0} multiply(f32[192]{0} %divide.3493, f32[192]{0} %divide.3493), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3490 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3495 = f32[192]{0} broadcast(f32[] %constant.3490), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3496 = f32[192]{0} subtract(f32[192]{0} %multiply.3494, f32[192]{0} %broadcast.3495), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3481 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3482 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3481), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3483 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.589, f32[96,192,28,28]{3,2,1,0} %broadcast.3482), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3484 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3485 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3484), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3486 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.589, f32[96,192,28,28]{3,2,1,0} %broadcast.3485), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3487 = pred[96,192,28,28]{3,2,1,0} and(pred[96,192,28,28]{3,2,1,0} %compare.3483, pred[96,192,28,28]{3,2,1,0} %compare.3486), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3466 = f32[3,3,1,192]{1,0,2,3} transpose(f32[192,1,3,3]{3,2,1,0} %p102.621), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3467 = f32[3,3,1,192,1]{4,3,2,1,0} reshape(f32[3,3,1,192]{1,0,2,3} %transpose.3466), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3468 = f32[3,3,192,1,1]{4,2,3,1,0} transpose(f32[3,3,1,192,1]{4,3,2,1,0} %reshape.3467), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3469 = f32[3,3,192,1]{3,2,1,0} reshape(f32[3,3,192,1,1]{4,2,3,1,0} %transpose.3468), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3470 = f32[3,3,192,1]{3,2,1,0} reverse(f32[3,3,192,1]{3,2,1,0} %reshape.3469), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3471 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3463, f32[3,3,192,1]{3,2,1,0} %reverse.3470), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=192, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3480 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3488 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3480), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3489 = f32[96,192,28,28]{3,2,1,0} select(pred[96,192,28,28]{3,2,1,0} %and.3487, f32[96,192,28,28]{3,2,1,0} %convolution.3471, f32[96,192,28,28]{3,2,1,0} %broadcast.3488), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3497 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-grad(f32[96,192,28,28]{3,2,1,0} %convolution.587, f32[192]{0} %p95.578, f32[192]{0} %get-tuple-element.590, f32[192]{0} %subtract.3496, f32[96,192,28,28]{3,2,1,0} %select.3489), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3498 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3497), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3506 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3511 = f32[192]{0} reduce(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3498, f32[] %constant.3506), dimensions={0,2,3}, to_apply=%AddComputation.3507, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.548 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.546), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3520 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3521 = f32[32]{0} broadcast(f32[] %constant.3520), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.549 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.546), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.550 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.551 = f32[32]{0} broadcast(f32[] %constant.550), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.552 = f32[32]{0} add(f32[32]{0} %get-tuple-element.549, f32[32]{0} %broadcast.551), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.553 = f32[32]{0} rsqrt(f32[32]{0} %add.552), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3522 = f32[32]{0} divide(f32[32]{0} %broadcast.3521, f32[32]{0} %rsqrt.553), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3523 = f32[32]{0} multiply(f32[32]{0} %divide.3522, f32[32]{0} %divide.3522), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3519 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3524 = f32[32]{0} broadcast(f32[] %constant.3519), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3525 = f32[32]{0} subtract(f32[32]{0} %multiply.3523, f32[32]{0} %broadcast.3524), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3501 = f32[1,1,32,192]{1,0,2,3} transpose(f32[192,32,1,1]{3,2,1,0} %p96.579), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3502 = f32[1,1,32,192]{1,0,2,3} reverse(f32[1,1,32,192]{1,0,2,3} %transpose.3501), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3503 = f32[96,32,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3498, f32[1,1,32,192]{1,0,2,3} %reverse.3502), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3512 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.3513 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.3512), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3514 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.3513), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.3515 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.3514), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3516 = f32[96,32,28,28]{3,2,1,0} broadcast(f32[] %reshape.3515), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3517 = f32[96,32,28,28]{3,2,1,0} multiply(f32[96,32,28,28]{3,2,1,0} %convolution.3503, f32[96,32,28,28]{3,2,1,0} %broadcast.3516), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3518 = f32[96,32,28,28]{3,2,1,0} add(f32[96,32,28,28]{3,2,1,0} %convolution.3414, f32[96,32,28,28]{3,2,1,0} %multiply.3517), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.3526 = (f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-grad(f32[96,32,28,28]{3,2,1,0} %convolution.545, f32[32]{0} %p89.539, f32[32]{0} %get-tuple-element.548, f32[32]{0} %subtract.3525, f32[96,32,28,28]{3,2,1,0} %add.3518), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3527 = f32[96,32,28,28]{3,2,1,0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3526), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3535 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3540 = f32[32]{0} reduce(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3527, f32[] %constant.3535), dimensions={0,2,3}, to_apply=%AddComputation.3536, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.509 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.507), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3552 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3553 = f32[192]{0} broadcast(f32[] %constant.3552), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.510 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.507), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.511 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.512 = f32[192]{0} broadcast(f32[] %constant.511), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.513 = f32[192]{0} add(f32[192]{0} %get-tuple-element.510, f32[192]{0} %broadcast.512), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.514 = f32[192]{0} rsqrt(f32[192]{0} %add.513), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3554 = f32[192]{0} divide(f32[192]{0} %broadcast.3553, f32[192]{0} %rsqrt.514), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3555 = f32[192]{0} multiply(f32[192]{0} %divide.3554, f32[192]{0} %divide.3554), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3551 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3556 = f32[192]{0} broadcast(f32[] %constant.3551), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3557 = f32[192]{0} subtract(f32[192]{0} %multiply.3555, f32[192]{0} %broadcast.3556), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3542 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3543 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3542), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3544 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.508, f32[96,192,28,28]{3,2,1,0} %broadcast.3543), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3545 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3546 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3545), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3547 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.508, f32[96,192,28,28]{3,2,1,0} %broadcast.3546), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3548 = pred[96,192,28,28]{3,2,1,0} and(pred[96,192,28,28]{3,2,1,0} %compare.3544, pred[96,192,28,28]{3,2,1,0} %compare.3547), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3530 = f32[1,1,192,32]{1,0,2,3} transpose(f32[32,192,1,1]{3,2,1,0} %p90.540), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3531 = f32[1,1,192,32]{1,0,2,3} reverse(f32[1,1,192,32]{1,0,2,3} %transpose.3530), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3532 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3527, f32[1,1,192,32]{1,0,2,3} %reverse.3531), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3541 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3549 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3541), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3550 = f32[96,192,28,28]{3,2,1,0} select(pred[96,192,28,28]{3,2,1,0} %and.3548, f32[96,192,28,28]{3,2,1,0} %convolution.3532, f32[96,192,28,28]{3,2,1,0} %broadcast.3549), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3558 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-grad(f32[96,192,28,28]{3,2,1,0} %convolution.506, f32[192]{0} %p83.500, f32[192]{0} %get-tuple-element.509, f32[192]{0} %subtract.3557, f32[96,192,28,28]{3,2,1,0} %select.3550), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3559 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3558), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3570 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3575 = f32[192]{0} reduce(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3559, f32[] %constant.3570), dimensions={0,2,3}, to_apply=%AddComputation.3571, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.470 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.468), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3587 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3588 = f32[192]{0} broadcast(f32[] %constant.3587), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.471 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-training.468), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.472 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.473 = f32[192]{0} broadcast(f32[] %constant.472), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.474 = f32[192]{0} add(f32[192]{0} %get-tuple-element.471, f32[192]{0} %broadcast.473), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.475 = f32[192]{0} rsqrt(f32[192]{0} %add.474), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3589 = f32[192]{0} divide(f32[192]{0} %broadcast.3588, f32[192]{0} %rsqrt.475), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3590 = f32[192]{0} multiply(f32[192]{0} %divide.3589, f32[192]{0} %divide.3589), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3586 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3591 = f32[192]{0} broadcast(f32[] %constant.3586), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3592 = f32[192]{0} subtract(f32[192]{0} %multiply.3590, f32[192]{0} %broadcast.3591), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3577 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3578 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3577), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3579 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.469, f32[96,192,28,28]{3,2,1,0} %broadcast.3578), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3580 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3581 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3580), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3582 = pred[96,192,28,28]{3,2,1,0} compare(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.469, f32[96,192,28,28]{3,2,1,0} %broadcast.3581), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3583 = pred[96,192,28,28]{3,2,1,0} and(pred[96,192,28,28]{3,2,1,0} %compare.3579, pred[96,192,28,28]{3,2,1,0} %compare.3582), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3562 = f32[3,3,1,192]{1,0,2,3} transpose(f32[192,1,3,3]{3,2,1,0} %p84.501), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3563 = f32[3,3,1,192,1]{4,3,2,1,0} reshape(f32[3,3,1,192]{1,0,2,3} %transpose.3562), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3564 = f32[3,3,192,1,1]{4,2,3,1,0} transpose(f32[3,3,1,192,1]{4,3,2,1,0} %reshape.3563), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3565 = f32[3,3,192,1]{3,2,1,0} reshape(f32[3,3,192,1,1]{4,2,3,1,0} %transpose.3564), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3566 = f32[3,3,192,1]{3,2,1,0} reverse(f32[3,3,192,1]{3,2,1,0} %reshape.3565), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3567 = f32[96,192,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3559, f32[3,3,192,1]{3,2,1,0} %reverse.3566), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=192, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3576 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3584 = f32[96,192,28,28]{3,2,1,0} broadcast(f32[] %constant.3576), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3585 = f32[96,192,28,28]{3,2,1,0} select(pred[96,192,28,28]{3,2,1,0} %and.3583, f32[96,192,28,28]{3,2,1,0} %convolution.3567, f32[96,192,28,28]{3,2,1,0} %broadcast.3584), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3593 = (f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) batch-norm-grad(f32[96,192,28,28]{3,2,1,0} %convolution.467, f32[192]{0} %p77.465, f32[192]{0} %get-tuple-element.470, f32[192]{0} %subtract.3592, f32[96,192,28,28]{3,2,1,0} %select.3585), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3594 = f32[96,192,28,28]{3,2,1,0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3593), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3602 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3607 = f32[192]{0} reduce(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3594, f32[] %constant.3602), dimensions={0,2,3}, to_apply=%AddComputation.3603, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.435 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.433), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3616 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3617 = f32[32]{0} broadcast(f32[] %constant.3616), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.436 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.433), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.437 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.438 = f32[32]{0} broadcast(f32[] %constant.437), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.439 = f32[32]{0} add(f32[32]{0} %get-tuple-element.436, f32[32]{0} %broadcast.438), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.440 = f32[32]{0} rsqrt(f32[32]{0} %add.439), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3618 = f32[32]{0} divide(f32[32]{0} %broadcast.3617, f32[32]{0} %rsqrt.440), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3619 = f32[32]{0} multiply(f32[32]{0} %divide.3618, f32[32]{0} %divide.3618), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3615 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3620 = f32[32]{0} broadcast(f32[] %constant.3615), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3621 = f32[32]{0} subtract(f32[32]{0} %multiply.3619, f32[32]{0} %broadcast.3620), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3597 = f32[1,1,32,192]{1,0,2,3} transpose(f32[192,32,1,1]{3,2,1,0} %p78.466), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3598 = f32[1,1,32,192]{1,0,2,3} reverse(f32[1,1,32,192]{1,0,2,3} %transpose.3597), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3599 = f32[96,32,28,28]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3594, f32[1,1,32,192]{1,0,2,3} %reverse.3598), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3608 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.3609 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.3608), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3610 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.3609), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.3611 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.3610), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3612 = f32[96,32,28,28]{3,2,1,0} broadcast(f32[] %reshape.3611), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3613 = f32[96,32,28,28]{3,2,1,0} multiply(f32[96,32,28,28]{3,2,1,0} %convolution.3599, f32[96,32,28,28]{3,2,1,0} %broadcast.3612), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3614 = f32[96,32,28,28]{3,2,1,0} add(f32[96,32,28,28]{3,2,1,0} %add.3518, f32[96,32,28,28]{3,2,1,0} %multiply.3613), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.3622 = (f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-grad(f32[96,32,28,28]{3,2,1,0} %convolution.432, f32[32]{0} %p71.426, f32[32]{0} %get-tuple-element.435, f32[32]{0} %subtract.3621, f32[96,32,28,28]{3,2,1,0} %add.3614), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3623 = f32[96,32,28,28]{3,2,1,0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3622), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3631 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3636 = f32[32]{0} reduce(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3623, f32[] %constant.3631), dimensions={0,2,3}, to_apply=%AddComputation.3632, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.396 = f32[144]{0} get-tuple-element((f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.394), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3648 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3649 = f32[144]{0} broadcast(f32[] %constant.3648), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.397 = f32[144]{0} get-tuple-element((f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.394), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.398 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.399 = f32[144]{0} broadcast(f32[] %constant.398), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.400 = f32[144]{0} add(f32[144]{0} %get-tuple-element.397, f32[144]{0} %broadcast.399), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.401 = f32[144]{0} rsqrt(f32[144]{0} %add.400), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3650 = f32[144]{0} divide(f32[144]{0} %broadcast.3649, f32[144]{0} %rsqrt.401), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3651 = f32[144]{0} multiply(f32[144]{0} %divide.3650, f32[144]{0} %divide.3650), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3647 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3652 = f32[144]{0} broadcast(f32[] %constant.3647), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3653 = f32[144]{0} subtract(f32[144]{0} %multiply.3651, f32[144]{0} %broadcast.3652), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3638 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3639 = f32[96,144,28,28]{3,2,1,0} broadcast(f32[] %constant.3638), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3640 = pred[96,144,28,28]{3,2,1,0} compare(f32[96,144,28,28]{3,2,1,0} %get-tuple-element.395, f32[96,144,28,28]{3,2,1,0} %broadcast.3639), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3641 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3642 = f32[96,144,28,28]{3,2,1,0} broadcast(f32[] %constant.3641), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3643 = pred[96,144,28,28]{3,2,1,0} compare(f32[96,144,28,28]{3,2,1,0} %get-tuple-element.395, f32[96,144,28,28]{3,2,1,0} %broadcast.3642), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3644 = pred[96,144,28,28]{3,2,1,0} and(pred[96,144,28,28]{3,2,1,0} %compare.3640, pred[96,144,28,28]{3,2,1,0} %compare.3643), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3626 = f32[1,1,144,32]{1,0,2,3} transpose(f32[32,144,1,1]{3,2,1,0} %p72.427), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3627 = f32[1,1,144,32]{1,0,2,3} reverse(f32[1,1,144,32]{1,0,2,3} %transpose.3626), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3628 = f32[96,144,28,28]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3623, f32[1,1,144,32]{1,0,2,3} %reverse.3627), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3637 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3645 = f32[96,144,28,28]{3,2,1,0} broadcast(f32[] %constant.3637), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3646 = f32[96,144,28,28]{3,2,1,0} select(pred[96,144,28,28]{3,2,1,0} %and.3644, f32[96,144,28,28]{3,2,1,0} %convolution.3628, f32[96,144,28,28]{3,2,1,0} %broadcast.3645), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3654 = (f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-grad(f32[96,144,28,28]{3,2,1,0} %convolution.393, f32[144]{0} %p65.387, f32[144]{0} %get-tuple-element.396, f32[144]{0} %subtract.3653, f32[96,144,28,28]{3,2,1,0} %select.3646), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3655 = f32[96,144,28,28]{3,2,1,0} get-tuple-element((f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3654), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3666 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3671 = f32[144]{0} reduce(f32[96,144,28,28]{3,2,1,0} %get-tuple-element.3655, f32[] %constant.3666), dimensions={0,2,3}, to_apply=%AddComputation.3667, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.357 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.355), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3683 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3684 = f32[144]{0} broadcast(f32[] %constant.3683), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.358 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.355), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.359 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.360 = f32[144]{0} broadcast(f32[] %constant.359), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.361 = f32[144]{0} add(f32[144]{0} %get-tuple-element.358, f32[144]{0} %broadcast.360), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.362 = f32[144]{0} rsqrt(f32[144]{0} %add.361), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3685 = f32[144]{0} divide(f32[144]{0} %broadcast.3684, f32[144]{0} %rsqrt.362), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3686 = f32[144]{0} multiply(f32[144]{0} %divide.3685, f32[144]{0} %divide.3685), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3682 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3687 = f32[144]{0} broadcast(f32[] %constant.3682), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3688 = f32[144]{0} subtract(f32[144]{0} %multiply.3686, f32[144]{0} %broadcast.3687), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3673 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3674 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3673), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3675 = pred[96,144,56,56]{3,2,1,0} compare(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.356, f32[96,144,56,56]{3,2,1,0} %broadcast.3674), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3676 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3677 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3676), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3678 = pred[96,144,56,56]{3,2,1,0} compare(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.356, f32[96,144,56,56]{3,2,1,0} %broadcast.3677), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3679 = pred[96,144,56,56]{3,2,1,0} and(pred[96,144,56,56]{3,2,1,0} %compare.3675, pred[96,144,56,56]{3,2,1,0} %compare.3678), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3658 = f32[3,3,1,144]{1,0,2,3} transpose(f32[144,1,3,3]{3,2,1,0} %p66.388), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3659 = f32[3,3,1,144,1]{4,3,2,1,0} reshape(f32[3,3,1,144]{1,0,2,3} %transpose.3658), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3660 = f32[3,3,144,1,1]{4,2,3,1,0} transpose(f32[3,3,1,144,1]{4,3,2,1,0} %reshape.3659), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3661 = f32[3,3,144,1]{3,2,1,0} reshape(f32[3,3,144,1,1]{4,2,3,1,0} %transpose.3660), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3662 = f32[3,3,144,1]{3,2,1,0} reverse(f32[3,3,144,1]{3,2,1,0} %reshape.3661), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3663 = f32[96,144,56,56]{3,2,1,0} convolution(f32[96,144,28,28]{3,2,1,0} %get-tuple-element.3655, f32[3,3,144,1]{3,2,1,0} %reverse.3662), window={size=3x3 pad=1_2x1_2 lhs_dilate=2x2}, dim_labels=bf01_01oi->bf01, feature_group_count=144, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3672 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3680 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3672), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3681 = f32[96,144,56,56]{3,2,1,0} select(pred[96,144,56,56]{3,2,1,0} %and.3679, f32[96,144,56,56]{3,2,1,0} %convolution.3663, f32[96,144,56,56]{3,2,1,0} %broadcast.3680), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3689 = (f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-grad(f32[96,144,56,56]{3,2,1,0} %convolution.354, f32[144]{0} %p59.345, f32[144]{0} %get-tuple-element.357, f32[144]{0} %subtract.3688, f32[96,144,56,56]{3,2,1,0} %select.3681), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3690 = f32[96,144,56,56]{3,2,1,0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3689), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3698 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3703 = f32[144]{0} reduce(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3690, f32[] %constant.3698), dimensions={0,2,3}, to_apply=%AddComputation.3699, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.315 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-training.313), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3705 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3706 = f32[24]{0} broadcast(f32[] %constant.3705), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.316 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-training.313), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.317 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.318 = f32[24]{0} broadcast(f32[] %constant.317), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.319 = f32[24]{0} add(f32[24]{0} %get-tuple-element.316, f32[24]{0} %broadcast.318), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.320 = f32[24]{0} rsqrt(f32[24]{0} %add.319), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3707 = f32[24]{0} divide(f32[24]{0} %broadcast.3706, f32[24]{0} %rsqrt.320), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3708 = f32[24]{0} multiply(f32[24]{0} %divide.3707, f32[24]{0} %divide.3707), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3704 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3709 = f32[24]{0} broadcast(f32[] %constant.3704), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3710 = f32[24]{0} subtract(f32[24]{0} %multiply.3708, f32[24]{0} %broadcast.3709), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3693 = f32[1,1,24,144]{1,0,2,3} transpose(f32[144,24,1,1]{3,2,1,0} %p60.346), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3694 = f32[1,1,24,144]{1,0,2,3} reverse(f32[1,1,24,144]{1,0,2,3} %transpose.3693), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3695 = f32[96,24,56,56]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3690, f32[1,1,24,144]{1,0,2,3} %reverse.3694), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.3711 = (f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) batch-norm-grad(f32[96,24,56,56]{3,2,1,0} %convolution.312, f32[24]{0} %p53.306, f32[24]{0} %get-tuple-element.315, f32[24]{0} %subtract.3710, f32[96,24,56,56]{3,2,1,0} %convolution.3695), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3712 = f32[96,24,56,56]{3,2,1,0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-grad.3711), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3720 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3725 = f32[24]{0} reduce(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.3712, f32[] %constant.3720), dimensions={0,2,3}, to_apply=%AddComputation.3721, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.276 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.274), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3737 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3738 = f32[144]{0} broadcast(f32[] %constant.3737), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.277 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.274), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.278 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.279 = f32[144]{0} broadcast(f32[] %constant.278), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.280 = f32[144]{0} add(f32[144]{0} %get-tuple-element.277, f32[144]{0} %broadcast.279), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.281 = f32[144]{0} rsqrt(f32[144]{0} %add.280), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3739 = f32[144]{0} divide(f32[144]{0} %broadcast.3738, f32[144]{0} %rsqrt.281), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3740 = f32[144]{0} multiply(f32[144]{0} %divide.3739, f32[144]{0} %divide.3739), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3736 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3741 = f32[144]{0} broadcast(f32[] %constant.3736), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3742 = f32[144]{0} subtract(f32[144]{0} %multiply.3740, f32[144]{0} %broadcast.3741), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3727 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3728 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3727), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3729 = pred[96,144,56,56]{3,2,1,0} compare(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.275, f32[96,144,56,56]{3,2,1,0} %broadcast.3728), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3730 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3731 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3730), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3732 = pred[96,144,56,56]{3,2,1,0} compare(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.275, f32[96,144,56,56]{3,2,1,0} %broadcast.3731), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3733 = pred[96,144,56,56]{3,2,1,0} and(pred[96,144,56,56]{3,2,1,0} %compare.3729, pred[96,144,56,56]{3,2,1,0} %compare.3732), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3715 = f32[1,1,144,24]{1,0,2,3} transpose(f32[24,144,1,1]{3,2,1,0} %p54.307), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3716 = f32[1,1,144,24]{1,0,2,3} reverse(f32[1,1,144,24]{1,0,2,3} %transpose.3715), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3717 = f32[96,144,56,56]{3,2,1,0} convolution(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.3712, f32[1,1,144,24]{1,0,2,3} %reverse.3716), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3726 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3734 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3726), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3735 = f32[96,144,56,56]{3,2,1,0} select(pred[96,144,56,56]{3,2,1,0} %and.3733, f32[96,144,56,56]{3,2,1,0} %convolution.3717, f32[96,144,56,56]{3,2,1,0} %broadcast.3734), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3743 = (f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-grad(f32[96,144,56,56]{3,2,1,0} %convolution.273, f32[144]{0} %p47.267, f32[144]{0} %get-tuple-element.276, f32[144]{0} %subtract.3742, f32[96,144,56,56]{3,2,1,0} %select.3735), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3744 = f32[96,144,56,56]{3,2,1,0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3743), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3755 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3760 = f32[144]{0} reduce(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3744, f32[] %constant.3755), dimensions={0,2,3}, to_apply=%AddComputation.3756, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.237 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.235), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3772 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3773 = f32[144]{0} broadcast(f32[] %constant.3772), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.238 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-training.235), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.239 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.240 = f32[144]{0} broadcast(f32[] %constant.239), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.241 = f32[144]{0} add(f32[144]{0} %get-tuple-element.238, f32[144]{0} %broadcast.240), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.242 = f32[144]{0} rsqrt(f32[144]{0} %add.241), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3774 = f32[144]{0} divide(f32[144]{0} %broadcast.3773, f32[144]{0} %rsqrt.242), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3775 = f32[144]{0} multiply(f32[144]{0} %divide.3774, f32[144]{0} %divide.3774), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3771 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3776 = f32[144]{0} broadcast(f32[] %constant.3771), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3777 = f32[144]{0} subtract(f32[144]{0} %multiply.3775, f32[144]{0} %broadcast.3776), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3762 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3763 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3762), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3764 = pred[96,144,56,56]{3,2,1,0} compare(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.236, f32[96,144,56,56]{3,2,1,0} %broadcast.3763), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3765 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3766 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3765), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3767 = pred[96,144,56,56]{3,2,1,0} compare(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.236, f32[96,144,56,56]{3,2,1,0} %broadcast.3766), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3768 = pred[96,144,56,56]{3,2,1,0} and(pred[96,144,56,56]{3,2,1,0} %compare.3764, pred[96,144,56,56]{3,2,1,0} %compare.3767), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3747 = f32[3,3,1,144]{1,0,2,3} transpose(f32[144,1,3,3]{3,2,1,0} %p48.268), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3748 = f32[3,3,1,144,1]{4,3,2,1,0} reshape(f32[3,3,1,144]{1,0,2,3} %transpose.3747), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3749 = f32[3,3,144,1,1]{4,2,3,1,0} transpose(f32[3,3,1,144,1]{4,3,2,1,0} %reshape.3748), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3750 = f32[3,3,144,1]{3,2,1,0} reshape(f32[3,3,144,1,1]{4,2,3,1,0} %transpose.3749), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3751 = f32[3,3,144,1]{3,2,1,0} reverse(f32[3,3,144,1]{3,2,1,0} %reshape.3750), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3752 = f32[96,144,56,56]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3744, f32[3,3,144,1]{3,2,1,0} %reverse.3751), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=144, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3761 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3769 = f32[96,144,56,56]{3,2,1,0} broadcast(f32[] %constant.3761), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3770 = f32[96,144,56,56]{3,2,1,0} select(pred[96,144,56,56]{3,2,1,0} %and.3768, f32[96,144,56,56]{3,2,1,0} %convolution.3752, f32[96,144,56,56]{3,2,1,0} %broadcast.3769), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3778 = (f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) batch-norm-grad(f32[96,144,56,56]{3,2,1,0} %convolution.234, f32[144]{0} %p41.232, f32[144]{0} %get-tuple-element.237, f32[144]{0} %subtract.3777, f32[96,144,56,56]{3,2,1,0} %select.3770), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3779 = f32[96,144,56,56]{3,2,1,0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3778), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3787 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3792 = f32[144]{0} reduce(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3779, f32[] %constant.3787), dimensions={0,2,3}, to_apply=%AddComputation.3788, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.202 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-training.200), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3801 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3802 = f32[24]{0} broadcast(f32[] %constant.3801), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.203 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-training.200), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.204 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.205 = f32[24]{0} broadcast(f32[] %constant.204), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.206 = f32[24]{0} add(f32[24]{0} %get-tuple-element.203, f32[24]{0} %broadcast.205), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.207 = f32[24]{0} rsqrt(f32[24]{0} %add.206), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3803 = f32[24]{0} divide(f32[24]{0} %broadcast.3802, f32[24]{0} %rsqrt.207), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3804 = f32[24]{0} multiply(f32[24]{0} %divide.3803, f32[24]{0} %divide.3803), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3800 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3805 = f32[24]{0} broadcast(f32[] %constant.3800), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3806 = f32[24]{0} subtract(f32[24]{0} %multiply.3804, f32[24]{0} %broadcast.3805), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3782 = f32[1,1,24,144]{1,0,2,3} transpose(f32[144,24,1,1]{3,2,1,0} %p42.233), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3783 = f32[1,1,24,144]{1,0,2,3} reverse(f32[1,1,24,144]{1,0,2,3} %transpose.3782), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3784 = f32[96,24,56,56]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3779, f32[1,1,24,144]{1,0,2,3} %reverse.3783), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3793 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %reshape.3794 = f32[1,1,1,1]{3,2,1,0} reshape(f32[] %constant.3793), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3795 = f32[1,1,1,1]{3,2,1,0} broadcast(f32[1,1,1,1]{3,2,1,0} %reshape.3794), dimensions={0,1,2,3}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %reshape.3796 = f32[] reshape(f32[1,1,1,1]{3,2,1,0} %broadcast.3795), metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3797 = f32[96,24,56,56]{3,2,1,0} broadcast(f32[] %reshape.3796), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3798 = f32[96,24,56,56]{3,2,1,0} multiply(f32[96,24,56,56]{3,2,1,0} %convolution.3784, f32[96,24,56,56]{3,2,1,0} %broadcast.3797), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3799 = f32[96,24,56,56]{3,2,1,0} add(f32[96,24,56,56]{3,2,1,0} %convolution.3695, f32[96,24,56,56]{3,2,1,0} %multiply.3798), metadata={op_type="aten__add" op_name="aten__add"}
  %batch-norm-grad.3807 = (f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) batch-norm-grad(f32[96,24,56,56]{3,2,1,0} %convolution.199, f32[24]{0} %p35.193, f32[24]{0} %get-tuple-element.202, f32[24]{0} %subtract.3806, f32[96,24,56,56]{3,2,1,0} %add.3799), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3808 = f32[96,24,56,56]{3,2,1,0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-grad.3807), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3816 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3821 = f32[24]{0} reduce(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.3808, f32[] %constant.3816), dimensions={0,2,3}, to_apply=%AddComputation.3817, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.163 = f32[96]{0} get-tuple-element((f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.161), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3833 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3834 = f32[96]{0} broadcast(f32[] %constant.3833), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.164 = f32[96]{0} get-tuple-element((f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.161), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.165 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.166 = f32[96]{0} broadcast(f32[] %constant.165), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.167 = f32[96]{0} add(f32[96]{0} %get-tuple-element.164, f32[96]{0} %broadcast.166), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.168 = f32[96]{0} rsqrt(f32[96]{0} %add.167), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3835 = f32[96]{0} divide(f32[96]{0} %broadcast.3834, f32[96]{0} %rsqrt.168), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3836 = f32[96]{0} multiply(f32[96]{0} %divide.3835, f32[96]{0} %divide.3835), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3832 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3837 = f32[96]{0} broadcast(f32[] %constant.3832), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3838 = f32[96]{0} subtract(f32[96]{0} %multiply.3836, f32[96]{0} %broadcast.3837), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3823 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3824 = f32[96,96,56,56]{3,2,1,0} broadcast(f32[] %constant.3823), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3825 = pred[96,96,56,56]{3,2,1,0} compare(f32[96,96,56,56]{3,2,1,0} %get-tuple-element.162, f32[96,96,56,56]{3,2,1,0} %broadcast.3824), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3826 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3827 = f32[96,96,56,56]{3,2,1,0} broadcast(f32[] %constant.3826), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3828 = pred[96,96,56,56]{3,2,1,0} compare(f32[96,96,56,56]{3,2,1,0} %get-tuple-element.162, f32[96,96,56,56]{3,2,1,0} %broadcast.3827), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3829 = pred[96,96,56,56]{3,2,1,0} and(pred[96,96,56,56]{3,2,1,0} %compare.3825, pred[96,96,56,56]{3,2,1,0} %compare.3828), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3811 = f32[1,1,96,24]{1,0,2,3} transpose(f32[24,96,1,1]{3,2,1,0} %p36.194), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3812 = f32[1,1,96,24]{1,0,2,3} reverse(f32[1,1,96,24]{1,0,2,3} %transpose.3811), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3813 = f32[96,96,56,56]{3,2,1,0} convolution(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.3808, f32[1,1,96,24]{1,0,2,3} %reverse.3812), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3822 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3830 = f32[96,96,56,56]{3,2,1,0} broadcast(f32[] %constant.3822), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3831 = f32[96,96,56,56]{3,2,1,0} select(pred[96,96,56,56]{3,2,1,0} %and.3829, f32[96,96,56,56]{3,2,1,0} %convolution.3813, f32[96,96,56,56]{3,2,1,0} %broadcast.3830), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3839 = (f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-grad(f32[96,96,56,56]{3,2,1,0} %convolution.160, f32[96]{0} %p29.154, f32[96]{0} %get-tuple-element.163, f32[96]{0} %subtract.3838, f32[96,96,56,56]{3,2,1,0} %select.3831), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3840 = f32[96,96,56,56]{3,2,1,0} get-tuple-element((f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.3839), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3851 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3856 = f32[96]{0} reduce(f32[96,96,56,56]{3,2,1,0} %get-tuple-element.3840, f32[] %constant.3851), dimensions={0,2,3}, to_apply=%AddComputation.3852, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.124 = f32[96]{0} get-tuple-element((f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.122), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3868 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3869 = f32[96]{0} broadcast(f32[] %constant.3868), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.125 = f32[96]{0} get-tuple-element((f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-training.122), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.126 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.127 = f32[96]{0} broadcast(f32[] %constant.126), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.128 = f32[96]{0} add(f32[96]{0} %get-tuple-element.125, f32[96]{0} %broadcast.127), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.129 = f32[96]{0} rsqrt(f32[96]{0} %add.128), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3870 = f32[96]{0} divide(f32[96]{0} %broadcast.3869, f32[96]{0} %rsqrt.129), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3871 = f32[96]{0} multiply(f32[96]{0} %divide.3870, f32[96]{0} %divide.3870), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3867 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3872 = f32[96]{0} broadcast(f32[] %constant.3867), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3873 = f32[96]{0} subtract(f32[96]{0} %multiply.3871, f32[96]{0} %broadcast.3872), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3858 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3859 = f32[96,96,112,112]{3,2,1,0} broadcast(f32[] %constant.3858), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3860 = pred[96,96,112,112]{3,2,1,0} compare(f32[96,96,112,112]{3,2,1,0} %get-tuple-element.123, f32[96,96,112,112]{3,2,1,0} %broadcast.3859), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3861 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3862 = f32[96,96,112,112]{3,2,1,0} broadcast(f32[] %constant.3861), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3863 = pred[96,96,112,112]{3,2,1,0} compare(f32[96,96,112,112]{3,2,1,0} %get-tuple-element.123, f32[96,96,112,112]{3,2,1,0} %broadcast.3862), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3864 = pred[96,96,112,112]{3,2,1,0} and(pred[96,96,112,112]{3,2,1,0} %compare.3860, pred[96,96,112,112]{3,2,1,0} %compare.3863), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3843 = f32[3,3,1,96]{1,0,2,3} transpose(f32[96,1,3,3]{3,2,1,0} %p30.155), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3844 = f32[3,3,1,96,1]{4,3,2,1,0} reshape(f32[3,3,1,96]{1,0,2,3} %transpose.3843), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3845 = f32[3,3,96,1,1]{4,2,3,1,0} transpose(f32[3,3,1,96,1]{4,3,2,1,0} %reshape.3844), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3846 = f32[3,3,96,1]{3,2,1,0} reshape(f32[3,3,96,1,1]{4,2,3,1,0} %transpose.3845), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3847 = f32[3,3,96,1]{3,2,1,0} reverse(f32[3,3,96,1]{3,2,1,0} %reshape.3846), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3848 = f32[96,96,112,112]{3,2,1,0} convolution(f32[96,96,56,56]{3,2,1,0} %get-tuple-element.3840, f32[3,3,96,1]{3,2,1,0} %reverse.3847), window={size=3x3 pad=1_2x1_2 lhs_dilate=2x2}, dim_labels=bf01_01oi->bf01, feature_group_count=96, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3857 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3865 = f32[96,96,112,112]{3,2,1,0} broadcast(f32[] %constant.3857), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3866 = f32[96,96,112,112]{3,2,1,0} select(pred[96,96,112,112]{3,2,1,0} %and.3864, f32[96,96,112,112]{3,2,1,0} %convolution.3848, f32[96,96,112,112]{3,2,1,0} %broadcast.3865), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3874 = (f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) batch-norm-grad(f32[96,96,112,112]{3,2,1,0} %convolution.121, f32[96]{0} %p23.119, f32[96]{0} %get-tuple-element.124, f32[96]{0} %subtract.3873, f32[96,96,112,112]{3,2,1,0} %select.3866), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3875 = f32[96,96,112,112]{3,2,1,0} get-tuple-element((f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.3874), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3883 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3888 = f32[96]{0} reduce(f32[96,96,112,112]{3,2,1,0} %get-tuple-element.3875, f32[] %constant.3883), dimensions={0,2,3}, to_apply=%AddComputation.3884, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.89 = f32[16]{0} get-tuple-element((f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) %batch-norm-training.87), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3890 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3891 = f32[16]{0} broadcast(f32[] %constant.3890), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.90 = f32[16]{0} get-tuple-element((f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) %batch-norm-training.87), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.91 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.92 = f32[16]{0} broadcast(f32[] %constant.91), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.93 = f32[16]{0} add(f32[16]{0} %get-tuple-element.90, f32[16]{0} %broadcast.92), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.94 = f32[16]{0} rsqrt(f32[16]{0} %add.93), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3892 = f32[16]{0} divide(f32[16]{0} %broadcast.3891, f32[16]{0} %rsqrt.94), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3893 = f32[16]{0} multiply(f32[16]{0} %divide.3892, f32[16]{0} %divide.3892), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3889 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3894 = f32[16]{0} broadcast(f32[] %constant.3889), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3895 = f32[16]{0} subtract(f32[16]{0} %multiply.3893, f32[16]{0} %broadcast.3894), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3878 = f32[1,1,16,96]{1,0,2,3} transpose(f32[96,16,1,1]{3,2,1,0} %p24.120), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3879 = f32[1,1,16,96]{1,0,2,3} reverse(f32[1,1,16,96]{1,0,2,3} %transpose.3878), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3880 = f32[96,16,112,112]{3,2,1,0} convolution(f32[96,96,112,112]{3,2,1,0} %get-tuple-element.3875, f32[1,1,16,96]{1,0,2,3} %reverse.3879), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %batch-norm-grad.3896 = (f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) batch-norm-grad(f32[96,16,112,112]{3,2,1,0} %convolution.86, f32[16]{0} %p17.80, f32[16]{0} %get-tuple-element.89, f32[16]{0} %subtract.3895, f32[96,16,112,112]{3,2,1,0} %convolution.3880), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3897 = f32[96,16,112,112]{3,2,1,0} get-tuple-element((f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) %batch-norm-grad.3896), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3905 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3910 = f32[16]{0} reduce(f32[96,16,112,112]{3,2,1,0} %get-tuple-element.3897, f32[] %constant.3905), dimensions={0,2,3}, to_apply=%AddComputation.3906, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.50 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.48), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3922 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3923 = f32[32]{0} broadcast(f32[] %constant.3922), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.51 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.48), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.52 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.53 = f32[32]{0} broadcast(f32[] %constant.52), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.54 = f32[32]{0} add(f32[32]{0} %get-tuple-element.51, f32[32]{0} %broadcast.53), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.55 = f32[32]{0} rsqrt(f32[32]{0} %add.54), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3924 = f32[32]{0} divide(f32[32]{0} %broadcast.3923, f32[32]{0} %rsqrt.55), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3925 = f32[32]{0} multiply(f32[32]{0} %divide.3924, f32[32]{0} %divide.3924), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3921 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3926 = f32[32]{0} broadcast(f32[] %constant.3921), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3927 = f32[32]{0} subtract(f32[32]{0} %multiply.3925, f32[32]{0} %broadcast.3926), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3912 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3913 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.3912), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3914 = pred[96,32,112,112]{3,2,1,0} compare(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.49, f32[96,32,112,112]{3,2,1,0} %broadcast.3913), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3915 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3916 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.3915), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3917 = pred[96,32,112,112]{3,2,1,0} compare(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.49, f32[96,32,112,112]{3,2,1,0} %broadcast.3916), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3918 = pred[96,32,112,112]{3,2,1,0} and(pred[96,32,112,112]{3,2,1,0} %compare.3914, pred[96,32,112,112]{3,2,1,0} %compare.3917), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3900 = f32[1,1,32,16]{1,0,2,3} transpose(f32[16,32,1,1]{3,2,1,0} %p18.81), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3901 = f32[1,1,32,16]{1,0,2,3} reverse(f32[1,1,32,16]{1,0,2,3} %transpose.3900), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3902 = f32[96,32,112,112]{3,2,1,0} convolution(f32[96,16,112,112]{3,2,1,0} %get-tuple-element.3897, f32[1,1,32,16]{1,0,2,3} %reverse.3901), window={size=1x1}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3911 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3919 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.3911), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3920 = f32[96,32,112,112]{3,2,1,0} select(pred[96,32,112,112]{3,2,1,0} %and.3918, f32[96,32,112,112]{3,2,1,0} %convolution.3902, f32[96,32,112,112]{3,2,1,0} %broadcast.3919), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3928 = (f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-grad(f32[96,32,112,112]{3,2,1,0} %convolution.47, f32[32]{0} %p10.40, f32[32]{0} %get-tuple-element.50, f32[32]{0} %subtract.3927, f32[96,32,112,112]{3,2,1,0} %select.3920), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3929 = f32[96,32,112,112]{3,2,1,0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3928), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3940 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3945 = f32[32]{0} reduce(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.3929, f32[] %constant.3940), dimensions={0,2,3}, to_apply=%AddComputation.3941, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.10 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.8), index=1, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.3957 = f32[] constant(1), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3958 = f32[32]{0} broadcast(f32[] %constant.3957), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.11 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-training.8), index=2, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %constant.12 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.13 = f32[32]{0} broadcast(f32[] %constant.12), dimensions={}, metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %add.14 = f32[32]{0} add(f32[32]{0} %get-tuple-element.11, f32[32]{0} %broadcast.13), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %rsqrt.15 = f32[32]{0} rsqrt(f32[32]{0} %add.14), metadata={op_type="aten__native_batch_norm" op_name="aten__native_batch_norm" source_file="batch_norm@functional.py" source_line=2451}
  %divide.3959 = f32[32]{0} divide(f32[32]{0} %broadcast.3958, f32[32]{0} %rsqrt.15), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %multiply.3960 = f32[32]{0} multiply(f32[32]{0} %divide.3959, f32[32]{0} %divide.3959), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3956 = f32[] constant(1e-05), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %broadcast.3961 = f32[32]{0} broadcast(f32[] %constant.3956), dimensions={}, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %subtract.3962 = f32[32]{0} subtract(f32[32]{0} %multiply.3960, f32[32]{0} %broadcast.3961), metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %constant.3947 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3948 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.3947), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3949 = pred[96,32,112,112]{3,2,1,0} compare(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.9, f32[96,32,112,112]{3,2,1,0} %broadcast.3948), direction=GE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %constant.3950 = f32[] constant(6), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3951 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.3950), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %compare.3952 = pred[96,32,112,112]{3,2,1,0} compare(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.9, f32[96,32,112,112]{3,2,1,0} %broadcast.3951), direction=LE, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %and.3953 = pred[96,32,112,112]{3,2,1,0} and(pred[96,32,112,112]{3,2,1,0} %compare.3949, pred[96,32,112,112]{3,2,1,0} %compare.3952), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %transpose.3932 = f32[3,3,1,32]{1,0,2,3} transpose(f32[32,1,3,3]{3,2,1,0} %p11.41), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3933 = f32[3,3,1,32,1]{4,3,2,1,0} reshape(f32[3,3,1,32]{1,0,2,3} %transpose.3932), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3934 = f32[3,3,32,1,1]{4,2,3,1,0} transpose(f32[3,3,1,32,1]{4,3,2,1,0} %reshape.3933), dimensions={0,1,3,2,4}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reshape.3935 = f32[3,3,32,1]{3,2,1,0} reshape(f32[3,3,32,1,1]{4,2,3,1,0} %transpose.3934), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3936 = f32[3,3,32,1]{3,2,1,0} reverse(f32[3,3,32,1]{3,2,1,0} %reshape.3935), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3937 = f32[96,32,112,112]{3,2,1,0} convolution(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.3929, f32[3,3,32,1]{3,2,1,0} %reverse.3936), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_01oi->bf01, feature_group_count=32, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3946 = f32[] constant(0), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %broadcast.3954 = f32[96,32,112,112]{3,2,1,0} broadcast(f32[] %constant.3946), dimensions={}, metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %select.3955 = f32[96,32,112,112]{3,2,1,0} select(pred[96,32,112,112]{3,2,1,0} %and.3953, f32[96,32,112,112]{3,2,1,0} %convolution.3937, f32[96,32,112,112]{3,2,1,0} %broadcast.3954), metadata={op_type="aten__hardtanh_backward" op_name="aten__hardtanh_backward"}
  %batch-norm-grad.3963 = (f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) batch-norm-grad(f32[96,32,112,112]{3,2,1,0} %convolution.7, f32[32]{0} %p3.4, f32[32]{0} %get-tuple-element.10, f32[32]{0} %subtract.3962, f32[96,32,112,112]{3,2,1,0} %select.3955), epsilon=1e-05, feature_index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3964 = f32[96,32,112,112]{3,2,1,0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3963), index=0, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %transpose.3967 = f32[3,3,3,32]{1,0,2,3} transpose(f32[32,3,3,3]{3,2,1,0} %p4.5), dimensions={2,3,1,0}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reverse.3968 = f32[3,3,3,32]{1,0,2,3} reverse(f32[3,3,3,32]{1,0,2,3} %transpose.3967), dimensions={0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %convolution.3969 = f32[96,3,224,224]{3,2,1,0} convolution(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.3964, f32[3,3,3,32]{1,0,2,3} %reverse.3968), window={size=3x3 pad=1_2x1_2 lhs_dilate=2x2}, dim_labels=bf01_01oi->bf01, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.3972 = f32[] constant(0), metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %reduce.3977 = f32[32]{0} reduce(f32[96,32,112,112]{3,2,1,0} %get-tuple-element.3964, f32[] %constant.3972), dimensions={0,2,3}, to_apply=%AddComputation.3973, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %constant.17 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.18 = f32[32]{0} broadcast(f32[] %constant.17), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.19 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.10, f32[32]{0} %broadcast.18), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p0.1 = f32[32]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.16 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.20 = f32[] subtract(f32[] %constant.16, f32[] %constant.17), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.21 = f32[32]{0} broadcast(f32[] %subtract.20), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.22 = f32[32]{0} multiply(f32[32]{0} %p0.1, f32[32]{0} %broadcast.21), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.23 = f32[32]{0} add(f32[32]{0} %multiply.19, f32[32]{0} %multiply.22), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.25 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.26 = f32[32]{0} broadcast(f32[] %constant.25), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.27 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.11, f32[32]{0} %broadcast.26), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p1.2 = f32[32]{0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.24 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.28 = f32[] subtract(f32[] %constant.24, f32[] %constant.25), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.29 = f32[32]{0} broadcast(f32[] %subtract.28), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.30 = f32[32]{0} multiply(f32[32]{0} %p1.2, f32[32]{0} %broadcast.29), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.31 = f32[32]{0} add(f32[32]{0} %multiply.27, f32[32]{0} %multiply.30), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p6.35 = s64[] parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.33 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.32 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.34 = s64[] multiply(s64[] %constant.33, s64[] %constant.32), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.36 = s64[] add(s64[] %p6.35, s64[] %multiply.34), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.57 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.58 = f32[32]{0} broadcast(f32[] %constant.57), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.59 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.50, f32[32]{0} %broadcast.58), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p7.37 = f32[32]{0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.56 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.60 = f32[] subtract(f32[] %constant.56, f32[] %constant.57), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.61 = f32[32]{0} broadcast(f32[] %subtract.60), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.62 = f32[32]{0} multiply(f32[32]{0} %p7.37, f32[32]{0} %broadcast.61), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.63 = f32[32]{0} add(f32[32]{0} %multiply.59, f32[32]{0} %multiply.62), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.65 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.66 = f32[32]{0} broadcast(f32[] %constant.65), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.67 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.51, f32[32]{0} %broadcast.66), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p8.38 = f32[32]{0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.64 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.68 = f32[] subtract(f32[] %constant.64, f32[] %constant.65), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.69 = f32[32]{0} broadcast(f32[] %subtract.68), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.70 = f32[32]{0} multiply(f32[32]{0} %p8.38, f32[32]{0} %broadcast.69), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.71 = f32[32]{0} add(f32[32]{0} %multiply.67, f32[32]{0} %multiply.70), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p13.75 = s64[] parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.73 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.72 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.74 = s64[] multiply(s64[] %constant.73, s64[] %constant.72), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.76 = s64[] add(s64[] %p13.75, s64[] %multiply.74), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.96 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.97 = f32[16]{0} broadcast(f32[] %constant.96), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.98 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.89, f32[16]{0} %broadcast.97), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p14.77 = f32[16]{0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.95 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.99 = f32[] subtract(f32[] %constant.95, f32[] %constant.96), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.100 = f32[16]{0} broadcast(f32[] %subtract.99), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.101 = f32[16]{0} multiply(f32[16]{0} %p14.77, f32[16]{0} %broadcast.100), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.102 = f32[16]{0} add(f32[16]{0} %multiply.98, f32[16]{0} %multiply.101), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.104 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.105 = f32[16]{0} broadcast(f32[] %constant.104), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.106 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.90, f32[16]{0} %broadcast.105), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p15.78 = f32[16]{0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.103 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.107 = f32[] subtract(f32[] %constant.103, f32[] %constant.104), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.108 = f32[16]{0} broadcast(f32[] %subtract.107), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.109 = f32[16]{0} multiply(f32[16]{0} %p15.78, f32[16]{0} %broadcast.108), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.110 = f32[16]{0} add(f32[16]{0} %multiply.106, f32[16]{0} %multiply.109), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p19.114 = s64[] parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.112 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.111 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.113 = s64[] multiply(s64[] %constant.112, s64[] %constant.111), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.115 = s64[] add(s64[] %p19.114, s64[] %multiply.113), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.131 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.132 = f32[96]{0} broadcast(f32[] %constant.131), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.133 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.124, f32[96]{0} %broadcast.132), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p20.116 = f32[96]{0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.130 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.134 = f32[] subtract(f32[] %constant.130, f32[] %constant.131), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.135 = f32[96]{0} broadcast(f32[] %subtract.134), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.136 = f32[96]{0} multiply(f32[96]{0} %p20.116, f32[96]{0} %broadcast.135), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.137 = f32[96]{0} add(f32[96]{0} %multiply.133, f32[96]{0} %multiply.136), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.139 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.140 = f32[96]{0} broadcast(f32[] %constant.139), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.141 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.125, f32[96]{0} %broadcast.140), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p21.117 = f32[96]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.138 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.142 = f32[] subtract(f32[] %constant.138, f32[] %constant.139), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.143 = f32[96]{0} broadcast(f32[] %subtract.142), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.144 = f32[96]{0} multiply(f32[96]{0} %p21.117, f32[96]{0} %broadcast.143), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.145 = f32[96]{0} add(f32[96]{0} %multiply.141, f32[96]{0} %multiply.144), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p25.149 = s64[] parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.147 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.146 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.148 = s64[] multiply(s64[] %constant.147, s64[] %constant.146), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.150 = s64[] add(s64[] %p25.149, s64[] %multiply.148), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.170 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.171 = f32[96]{0} broadcast(f32[] %constant.170), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.172 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.163, f32[96]{0} %broadcast.171), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p26.151 = f32[96]{0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.169 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.173 = f32[] subtract(f32[] %constant.169, f32[] %constant.170), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.174 = f32[96]{0} broadcast(f32[] %subtract.173), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.175 = f32[96]{0} multiply(f32[96]{0} %p26.151, f32[96]{0} %broadcast.174), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.176 = f32[96]{0} add(f32[96]{0} %multiply.172, f32[96]{0} %multiply.175), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.178 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.179 = f32[96]{0} broadcast(f32[] %constant.178), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.180 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.164, f32[96]{0} %broadcast.179), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p27.152 = f32[96]{0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.177 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.181 = f32[] subtract(f32[] %constant.177, f32[] %constant.178), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.182 = f32[96]{0} broadcast(f32[] %subtract.181), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.183 = f32[96]{0} multiply(f32[96]{0} %p27.152, f32[96]{0} %broadcast.182), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.184 = f32[96]{0} add(f32[96]{0} %multiply.180, f32[96]{0} %multiply.183), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p31.188 = s64[] parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.186 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.185 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.187 = s64[] multiply(s64[] %constant.186, s64[] %constant.185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.189 = s64[] add(s64[] %p31.188, s64[] %multiply.187), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.209 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.210 = f32[24]{0} broadcast(f32[] %constant.209), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.211 = f32[24]{0} multiply(f32[24]{0} %get-tuple-element.202, f32[24]{0} %broadcast.210), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p32.190 = f32[24]{0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.208 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.212 = f32[] subtract(f32[] %constant.208, f32[] %constant.209), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.213 = f32[24]{0} broadcast(f32[] %subtract.212), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.214 = f32[24]{0} multiply(f32[24]{0} %p32.190, f32[24]{0} %broadcast.213), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.215 = f32[24]{0} add(f32[24]{0} %multiply.211, f32[24]{0} %multiply.214), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.217 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.218 = f32[24]{0} broadcast(f32[] %constant.217), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.219 = f32[24]{0} multiply(f32[24]{0} %get-tuple-element.203, f32[24]{0} %broadcast.218), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p33.191 = f32[24]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.216 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.220 = f32[] subtract(f32[] %constant.216, f32[] %constant.217), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.221 = f32[24]{0} broadcast(f32[] %subtract.220), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.222 = f32[24]{0} multiply(f32[24]{0} %p33.191, f32[24]{0} %broadcast.221), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.223 = f32[24]{0} add(f32[24]{0} %multiply.219, f32[24]{0} %multiply.222), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p37.227 = s64[] parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.225 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.224 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.226 = s64[] multiply(s64[] %constant.225, s64[] %constant.224), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.228 = s64[] add(s64[] %p37.227, s64[] %multiply.226), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.244 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.245 = f32[144]{0} broadcast(f32[] %constant.244), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.246 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.237, f32[144]{0} %broadcast.245), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p38.229 = f32[144]{0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.243 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.247 = f32[] subtract(f32[] %constant.243, f32[] %constant.244), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.248 = f32[144]{0} broadcast(f32[] %subtract.247), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.249 = f32[144]{0} multiply(f32[144]{0} %p38.229, f32[144]{0} %broadcast.248), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.250 = f32[144]{0} add(f32[144]{0} %multiply.246, f32[144]{0} %multiply.249), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.252 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.253 = f32[144]{0} broadcast(f32[] %constant.252), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.254 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.238, f32[144]{0} %broadcast.253), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p39.230 = f32[144]{0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.251 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.255 = f32[] subtract(f32[] %constant.251, f32[] %constant.252), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.256 = f32[144]{0} broadcast(f32[] %subtract.255), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.257 = f32[144]{0} multiply(f32[144]{0} %p39.230, f32[144]{0} %broadcast.256), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.258 = f32[144]{0} add(f32[144]{0} %multiply.254, f32[144]{0} %multiply.257), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p43.262 = s64[] parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.260 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.259 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.261 = s64[] multiply(s64[] %constant.260, s64[] %constant.259), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.263 = s64[] add(s64[] %p43.262, s64[] %multiply.261), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.283 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.284 = f32[144]{0} broadcast(f32[] %constant.283), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.285 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.276, f32[144]{0} %broadcast.284), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p44.264 = f32[144]{0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.282 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.286 = f32[] subtract(f32[] %constant.282, f32[] %constant.283), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.287 = f32[144]{0} broadcast(f32[] %subtract.286), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.288 = f32[144]{0} multiply(f32[144]{0} %p44.264, f32[144]{0} %broadcast.287), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.289 = f32[144]{0} add(f32[144]{0} %multiply.285, f32[144]{0} %multiply.288), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.291 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.292 = f32[144]{0} broadcast(f32[] %constant.291), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.293 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.277, f32[144]{0} %broadcast.292), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p45.265 = f32[144]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.290 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.294 = f32[] subtract(f32[] %constant.290, f32[] %constant.291), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.295 = f32[144]{0} broadcast(f32[] %subtract.294), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.296 = f32[144]{0} multiply(f32[144]{0} %p45.265, f32[144]{0} %broadcast.295), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.297 = f32[144]{0} add(f32[144]{0} %multiply.293, f32[144]{0} %multiply.296), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p49.301 = s64[] parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.299 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.298 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.300 = s64[] multiply(s64[] %constant.299, s64[] %constant.298), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.302 = s64[] add(s64[] %p49.301, s64[] %multiply.300), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.322 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.323 = f32[24]{0} broadcast(f32[] %constant.322), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.324 = f32[24]{0} multiply(f32[24]{0} %get-tuple-element.315, f32[24]{0} %broadcast.323), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p50.303 = f32[24]{0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.321 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.325 = f32[] subtract(f32[] %constant.321, f32[] %constant.322), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.326 = f32[24]{0} broadcast(f32[] %subtract.325), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.327 = f32[24]{0} multiply(f32[24]{0} %p50.303, f32[24]{0} %broadcast.326), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.328 = f32[24]{0} add(f32[24]{0} %multiply.324, f32[24]{0} %multiply.327), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.330 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.331 = f32[24]{0} broadcast(f32[] %constant.330), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.332 = f32[24]{0} multiply(f32[24]{0} %get-tuple-element.316, f32[24]{0} %broadcast.331), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p51.304 = f32[24]{0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.329 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.333 = f32[] subtract(f32[] %constant.329, f32[] %constant.330), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.334 = f32[24]{0} broadcast(f32[] %subtract.333), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.335 = f32[24]{0} multiply(f32[24]{0} %p51.304, f32[24]{0} %broadcast.334), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.336 = f32[24]{0} add(f32[24]{0} %multiply.332, f32[24]{0} %multiply.335), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p55.340 = s64[] parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.338 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.337 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.339 = s64[] multiply(s64[] %constant.338, s64[] %constant.337), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.341 = s64[] add(s64[] %p55.340, s64[] %multiply.339), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.364 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.365 = f32[144]{0} broadcast(f32[] %constant.364), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.366 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.357, f32[144]{0} %broadcast.365), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p56.342 = f32[144]{0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.363 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.367 = f32[] subtract(f32[] %constant.363, f32[] %constant.364), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.368 = f32[144]{0} broadcast(f32[] %subtract.367), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.369 = f32[144]{0} multiply(f32[144]{0} %p56.342, f32[144]{0} %broadcast.368), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.370 = f32[144]{0} add(f32[144]{0} %multiply.366, f32[144]{0} %multiply.369), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.372 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.373 = f32[144]{0} broadcast(f32[] %constant.372), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.374 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.358, f32[144]{0} %broadcast.373), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p57.343 = f32[144]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.371 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.375 = f32[] subtract(f32[] %constant.371, f32[] %constant.372), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.376 = f32[144]{0} broadcast(f32[] %subtract.375), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.377 = f32[144]{0} multiply(f32[144]{0} %p57.343, f32[144]{0} %broadcast.376), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.378 = f32[144]{0} add(f32[144]{0} %multiply.374, f32[144]{0} %multiply.377), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p61.382 = s64[] parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.380 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.379 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.381 = s64[] multiply(s64[] %constant.380, s64[] %constant.379), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.383 = s64[] add(s64[] %p61.382, s64[] %multiply.381), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.403 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.404 = f32[144]{0} broadcast(f32[] %constant.403), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.405 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.396, f32[144]{0} %broadcast.404), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p62.384 = f32[144]{0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.402 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.406 = f32[] subtract(f32[] %constant.402, f32[] %constant.403), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.407 = f32[144]{0} broadcast(f32[] %subtract.406), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.408 = f32[144]{0} multiply(f32[144]{0} %p62.384, f32[144]{0} %broadcast.407), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.409 = f32[144]{0} add(f32[144]{0} %multiply.405, f32[144]{0} %multiply.408), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.411 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.412 = f32[144]{0} broadcast(f32[] %constant.411), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.413 = f32[144]{0} multiply(f32[144]{0} %get-tuple-element.397, f32[144]{0} %broadcast.412), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p63.385 = f32[144]{0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.410 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.414 = f32[] subtract(f32[] %constant.410, f32[] %constant.411), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.415 = f32[144]{0} broadcast(f32[] %subtract.414), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.416 = f32[144]{0} multiply(f32[144]{0} %p63.385, f32[144]{0} %broadcast.415), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.417 = f32[144]{0} add(f32[144]{0} %multiply.413, f32[144]{0} %multiply.416), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p67.421 = s64[] parameter(67), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.419 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.418 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.420 = s64[] multiply(s64[] %constant.419, s64[] %constant.418), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.422 = s64[] add(s64[] %p67.421, s64[] %multiply.420), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.442 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.443 = f32[32]{0} broadcast(f32[] %constant.442), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.444 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.435, f32[32]{0} %broadcast.443), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p68.423 = f32[32]{0} parameter(68), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.441 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.445 = f32[] subtract(f32[] %constant.441, f32[] %constant.442), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.446 = f32[32]{0} broadcast(f32[] %subtract.445), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.447 = f32[32]{0} multiply(f32[32]{0} %p68.423, f32[32]{0} %broadcast.446), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.448 = f32[32]{0} add(f32[32]{0} %multiply.444, f32[32]{0} %multiply.447), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.450 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.451 = f32[32]{0} broadcast(f32[] %constant.450), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.452 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.436, f32[32]{0} %broadcast.451), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p69.424 = f32[32]{0} parameter(69), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.449 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.453 = f32[] subtract(f32[] %constant.449, f32[] %constant.450), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.454 = f32[32]{0} broadcast(f32[] %subtract.453), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.455 = f32[32]{0} multiply(f32[32]{0} %p69.424, f32[32]{0} %broadcast.454), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.456 = f32[32]{0} add(f32[32]{0} %multiply.452, f32[32]{0} %multiply.455), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p73.460 = s64[] parameter(73), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.458 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.457 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.459 = s64[] multiply(s64[] %constant.458, s64[] %constant.457), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.461 = s64[] add(s64[] %p73.460, s64[] %multiply.459), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.477 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.478 = f32[192]{0} broadcast(f32[] %constant.477), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.479 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.470, f32[192]{0} %broadcast.478), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p74.462 = f32[192]{0} parameter(74), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.476 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.480 = f32[] subtract(f32[] %constant.476, f32[] %constant.477), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.481 = f32[192]{0} broadcast(f32[] %subtract.480), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.482 = f32[192]{0} multiply(f32[192]{0} %p74.462, f32[192]{0} %broadcast.481), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.483 = f32[192]{0} add(f32[192]{0} %multiply.479, f32[192]{0} %multiply.482), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.485 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.486 = f32[192]{0} broadcast(f32[] %constant.485), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.487 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.471, f32[192]{0} %broadcast.486), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p75.463 = f32[192]{0} parameter(75), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.484 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.488 = f32[] subtract(f32[] %constant.484, f32[] %constant.485), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.489 = f32[192]{0} broadcast(f32[] %subtract.488), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.490 = f32[192]{0} multiply(f32[192]{0} %p75.463, f32[192]{0} %broadcast.489), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.491 = f32[192]{0} add(f32[192]{0} %multiply.487, f32[192]{0} %multiply.490), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p79.495 = s64[] parameter(79), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.493 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.492 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.494 = s64[] multiply(s64[] %constant.493, s64[] %constant.492), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.496 = s64[] add(s64[] %p79.495, s64[] %multiply.494), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.516 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.517 = f32[192]{0} broadcast(f32[] %constant.516), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.518 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.509, f32[192]{0} %broadcast.517), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p80.497 = f32[192]{0} parameter(80), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.515 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.519 = f32[] subtract(f32[] %constant.515, f32[] %constant.516), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.520 = f32[192]{0} broadcast(f32[] %subtract.519), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.521 = f32[192]{0} multiply(f32[192]{0} %p80.497, f32[192]{0} %broadcast.520), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.522 = f32[192]{0} add(f32[192]{0} %multiply.518, f32[192]{0} %multiply.521), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.524 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.525 = f32[192]{0} broadcast(f32[] %constant.524), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.526 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.510, f32[192]{0} %broadcast.525), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p81.498 = f32[192]{0} parameter(81), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.523 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.527 = f32[] subtract(f32[] %constant.523, f32[] %constant.524), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.528 = f32[192]{0} broadcast(f32[] %subtract.527), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.529 = f32[192]{0} multiply(f32[192]{0} %p81.498, f32[192]{0} %broadcast.528), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.530 = f32[192]{0} add(f32[192]{0} %multiply.526, f32[192]{0} %multiply.529), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p85.534 = s64[] parameter(85), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.532 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.531 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.533 = s64[] multiply(s64[] %constant.532, s64[] %constant.531), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.535 = s64[] add(s64[] %p85.534, s64[] %multiply.533), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.555 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.556 = f32[32]{0} broadcast(f32[] %constant.555), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.557 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.548, f32[32]{0} %broadcast.556), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p86.536 = f32[32]{0} parameter(86), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.554 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.558 = f32[] subtract(f32[] %constant.554, f32[] %constant.555), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.559 = f32[32]{0} broadcast(f32[] %subtract.558), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.560 = f32[32]{0} multiply(f32[32]{0} %p86.536, f32[32]{0} %broadcast.559), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.561 = f32[32]{0} add(f32[32]{0} %multiply.557, f32[32]{0} %multiply.560), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.563 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.564 = f32[32]{0} broadcast(f32[] %constant.563), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.565 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.549, f32[32]{0} %broadcast.564), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p87.537 = f32[32]{0} parameter(87), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.562 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.566 = f32[] subtract(f32[] %constant.562, f32[] %constant.563), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.567 = f32[32]{0} broadcast(f32[] %subtract.566), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.568 = f32[32]{0} multiply(f32[32]{0} %p87.537, f32[32]{0} %broadcast.567), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.569 = f32[32]{0} add(f32[32]{0} %multiply.565, f32[32]{0} %multiply.568), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p91.573 = s64[] parameter(91), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.571 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.570 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.572 = s64[] multiply(s64[] %constant.571, s64[] %constant.570), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.574 = s64[] add(s64[] %p91.573, s64[] %multiply.572), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.597 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.598 = f32[192]{0} broadcast(f32[] %constant.597), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.599 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.590, f32[192]{0} %broadcast.598), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p92.575 = f32[192]{0} parameter(92), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.596 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.600 = f32[] subtract(f32[] %constant.596, f32[] %constant.597), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.601 = f32[192]{0} broadcast(f32[] %subtract.600), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.602 = f32[192]{0} multiply(f32[192]{0} %p92.575, f32[192]{0} %broadcast.601), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.603 = f32[192]{0} add(f32[192]{0} %multiply.599, f32[192]{0} %multiply.602), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.605 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.606 = f32[192]{0} broadcast(f32[] %constant.605), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.607 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.591, f32[192]{0} %broadcast.606), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p93.576 = f32[192]{0} parameter(93), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.604 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.608 = f32[] subtract(f32[] %constant.604, f32[] %constant.605), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.609 = f32[192]{0} broadcast(f32[] %subtract.608), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.610 = f32[192]{0} multiply(f32[192]{0} %p93.576, f32[192]{0} %broadcast.609), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.611 = f32[192]{0} add(f32[192]{0} %multiply.607, f32[192]{0} %multiply.610), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p97.615 = s64[] parameter(97), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.613 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.612 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.614 = s64[] multiply(s64[] %constant.613, s64[] %constant.612), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.616 = s64[] add(s64[] %p97.615, s64[] %multiply.614), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.636 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.637 = f32[192]{0} broadcast(f32[] %constant.636), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.638 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.629, f32[192]{0} %broadcast.637), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p98.617 = f32[192]{0} parameter(98), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.635 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.639 = f32[] subtract(f32[] %constant.635, f32[] %constant.636), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.640 = f32[192]{0} broadcast(f32[] %subtract.639), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.641 = f32[192]{0} multiply(f32[192]{0} %p98.617, f32[192]{0} %broadcast.640), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.642 = f32[192]{0} add(f32[192]{0} %multiply.638, f32[192]{0} %multiply.641), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.644 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.645 = f32[192]{0} broadcast(f32[] %constant.644), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.646 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.630, f32[192]{0} %broadcast.645), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p99.618 = f32[192]{0} parameter(99), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.643 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.647 = f32[] subtract(f32[] %constant.643, f32[] %constant.644), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.648 = f32[192]{0} broadcast(f32[] %subtract.647), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.649 = f32[192]{0} multiply(f32[192]{0} %p99.618, f32[192]{0} %broadcast.648), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.650 = f32[192]{0} add(f32[192]{0} %multiply.646, f32[192]{0} %multiply.649), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p103.654 = s64[] parameter(103), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.652 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.651 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.653 = s64[] multiply(s64[] %constant.652, s64[] %constant.651), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.655 = s64[] add(s64[] %p103.654, s64[] %multiply.653), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.675 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.676 = f32[32]{0} broadcast(f32[] %constant.675), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.677 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.668, f32[32]{0} %broadcast.676), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p104.656 = f32[32]{0} parameter(104), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.674 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.678 = f32[] subtract(f32[] %constant.674, f32[] %constant.675), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.679 = f32[32]{0} broadcast(f32[] %subtract.678), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.680 = f32[32]{0} multiply(f32[32]{0} %p104.656, f32[32]{0} %broadcast.679), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.681 = f32[32]{0} add(f32[32]{0} %multiply.677, f32[32]{0} %multiply.680), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.683 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.684 = f32[32]{0} broadcast(f32[] %constant.683), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.685 = f32[32]{0} multiply(f32[32]{0} %get-tuple-element.669, f32[32]{0} %broadcast.684), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p105.657 = f32[32]{0} parameter(105), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.682 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.686 = f32[] subtract(f32[] %constant.682, f32[] %constant.683), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.687 = f32[32]{0} broadcast(f32[] %subtract.686), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.688 = f32[32]{0} multiply(f32[32]{0} %p105.657, f32[32]{0} %broadcast.687), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.689 = f32[32]{0} add(f32[32]{0} %multiply.685, f32[32]{0} %multiply.688), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p109.693 = s64[] parameter(109), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.691 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.690 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.692 = s64[] multiply(s64[] %constant.691, s64[] %constant.690), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.694 = s64[] add(s64[] %p109.693, s64[] %multiply.692), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.717 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.718 = f32[192]{0} broadcast(f32[] %constant.717), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.719 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.710, f32[192]{0} %broadcast.718), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p110.695 = f32[192]{0} parameter(110), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.716 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.720 = f32[] subtract(f32[] %constant.716, f32[] %constant.717), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.721 = f32[192]{0} broadcast(f32[] %subtract.720), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.722 = f32[192]{0} multiply(f32[192]{0} %p110.695, f32[192]{0} %broadcast.721), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.723 = f32[192]{0} add(f32[192]{0} %multiply.719, f32[192]{0} %multiply.722), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.725 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.726 = f32[192]{0} broadcast(f32[] %constant.725), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.727 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.711, f32[192]{0} %broadcast.726), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p111.696 = f32[192]{0} parameter(111), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.724 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.728 = f32[] subtract(f32[] %constant.724, f32[] %constant.725), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.729 = f32[192]{0} broadcast(f32[] %subtract.728), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.730 = f32[192]{0} multiply(f32[192]{0} %p111.696, f32[192]{0} %broadcast.729), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.731 = f32[192]{0} add(f32[192]{0} %multiply.727, f32[192]{0} %multiply.730), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p115.735 = s64[] parameter(115), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.733 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.732 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.734 = s64[] multiply(s64[] %constant.733, s64[] %constant.732), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.736 = s64[] add(s64[] %p115.735, s64[] %multiply.734), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.756 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.757 = f32[192]{0} broadcast(f32[] %constant.756), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.758 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.749, f32[192]{0} %broadcast.757), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p116.737 = f32[192]{0} parameter(116), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.755 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.759 = f32[] subtract(f32[] %constant.755, f32[] %constant.756), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.760 = f32[192]{0} broadcast(f32[] %subtract.759), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.761 = f32[192]{0} multiply(f32[192]{0} %p116.737, f32[192]{0} %broadcast.760), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.762 = f32[192]{0} add(f32[192]{0} %multiply.758, f32[192]{0} %multiply.761), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.764 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.765 = f32[192]{0} broadcast(f32[] %constant.764), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.766 = f32[192]{0} multiply(f32[192]{0} %get-tuple-element.750, f32[192]{0} %broadcast.765), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p117.738 = f32[192]{0} parameter(117), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.763 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.767 = f32[] subtract(f32[] %constant.763, f32[] %constant.764), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.768 = f32[192]{0} broadcast(f32[] %subtract.767), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.769 = f32[192]{0} multiply(f32[192]{0} %p117.738, f32[192]{0} %broadcast.768), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.770 = f32[192]{0} add(f32[192]{0} %multiply.766, f32[192]{0} %multiply.769), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p121.774 = s64[] parameter(121), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.772 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.771 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.773 = s64[] multiply(s64[] %constant.772, s64[] %constant.771), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.775 = s64[] add(s64[] %p121.774, s64[] %multiply.773), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.795 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.796 = f32[64]{0} broadcast(f32[] %constant.795), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.797 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.788, f32[64]{0} %broadcast.796), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p122.776 = f32[64]{0} parameter(122), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.794 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.798 = f32[] subtract(f32[] %constant.794, f32[] %constant.795), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.799 = f32[64]{0} broadcast(f32[] %subtract.798), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.800 = f32[64]{0} multiply(f32[64]{0} %p122.776, f32[64]{0} %broadcast.799), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.801 = f32[64]{0} add(f32[64]{0} %multiply.797, f32[64]{0} %multiply.800), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.803 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.804 = f32[64]{0} broadcast(f32[] %constant.803), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.805 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.789, f32[64]{0} %broadcast.804), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p123.777 = f32[64]{0} parameter(123), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.802 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.806 = f32[] subtract(f32[] %constant.802, f32[] %constant.803), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.807 = f32[64]{0} broadcast(f32[] %subtract.806), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.808 = f32[64]{0} multiply(f32[64]{0} %p123.777, f32[64]{0} %broadcast.807), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.809 = f32[64]{0} add(f32[64]{0} %multiply.805, f32[64]{0} %multiply.808), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p127.813 = s64[] parameter(127), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.811 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.810 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.812 = s64[] multiply(s64[] %constant.811, s64[] %constant.810), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.814 = s64[] add(s64[] %p127.813, s64[] %multiply.812), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.830 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.831 = f32[384]{0} broadcast(f32[] %constant.830), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.832 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.823, f32[384]{0} %broadcast.831), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p128.815 = f32[384]{0} parameter(128), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.829 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.833 = f32[] subtract(f32[] %constant.829, f32[] %constant.830), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.834 = f32[384]{0} broadcast(f32[] %subtract.833), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.835 = f32[384]{0} multiply(f32[384]{0} %p128.815, f32[384]{0} %broadcast.834), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.836 = f32[384]{0} add(f32[384]{0} %multiply.832, f32[384]{0} %multiply.835), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.838 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.839 = f32[384]{0} broadcast(f32[] %constant.838), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.840 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.824, f32[384]{0} %broadcast.839), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p129.816 = f32[384]{0} parameter(129), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.837 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.841 = f32[] subtract(f32[] %constant.837, f32[] %constant.838), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.842 = f32[384]{0} broadcast(f32[] %subtract.841), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.843 = f32[384]{0} multiply(f32[384]{0} %p129.816, f32[384]{0} %broadcast.842), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.844 = f32[384]{0} add(f32[384]{0} %multiply.840, f32[384]{0} %multiply.843), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p133.848 = s64[] parameter(133), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.846 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.845 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.847 = s64[] multiply(s64[] %constant.846, s64[] %constant.845), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.849 = s64[] add(s64[] %p133.848, s64[] %multiply.847), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.869 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.870 = f32[384]{0} broadcast(f32[] %constant.869), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.871 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.862, f32[384]{0} %broadcast.870), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p134.850 = f32[384]{0} parameter(134), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.868 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.872 = f32[] subtract(f32[] %constant.868, f32[] %constant.869), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.873 = f32[384]{0} broadcast(f32[] %subtract.872), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.874 = f32[384]{0} multiply(f32[384]{0} %p134.850, f32[384]{0} %broadcast.873), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.875 = f32[384]{0} add(f32[384]{0} %multiply.871, f32[384]{0} %multiply.874), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.877 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.878 = f32[384]{0} broadcast(f32[] %constant.877), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.879 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.863, f32[384]{0} %broadcast.878), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p135.851 = f32[384]{0} parameter(135), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.876 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.880 = f32[] subtract(f32[] %constant.876, f32[] %constant.877), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.881 = f32[384]{0} broadcast(f32[] %subtract.880), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.882 = f32[384]{0} multiply(f32[384]{0} %p135.851, f32[384]{0} %broadcast.881), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.883 = f32[384]{0} add(f32[384]{0} %multiply.879, f32[384]{0} %multiply.882), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p139.887 = s64[] parameter(139), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.885 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.884 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.886 = s64[] multiply(s64[] %constant.885, s64[] %constant.884), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.888 = s64[] add(s64[] %p139.887, s64[] %multiply.886), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.908 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.909 = f32[64]{0} broadcast(f32[] %constant.908), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.910 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.901, f32[64]{0} %broadcast.909), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p140.889 = f32[64]{0} parameter(140), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.907 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.911 = f32[] subtract(f32[] %constant.907, f32[] %constant.908), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.912 = f32[64]{0} broadcast(f32[] %subtract.911), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.913 = f32[64]{0} multiply(f32[64]{0} %p140.889, f32[64]{0} %broadcast.912), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.914 = f32[64]{0} add(f32[64]{0} %multiply.910, f32[64]{0} %multiply.913), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.916 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.917 = f32[64]{0} broadcast(f32[] %constant.916), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.918 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.902, f32[64]{0} %broadcast.917), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p141.890 = f32[64]{0} parameter(141), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.915 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.919 = f32[] subtract(f32[] %constant.915, f32[] %constant.916), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.920 = f32[64]{0} broadcast(f32[] %subtract.919), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.921 = f32[64]{0} multiply(f32[64]{0} %p141.890, f32[64]{0} %broadcast.920), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.922 = f32[64]{0} add(f32[64]{0} %multiply.918, f32[64]{0} %multiply.921), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p145.926 = s64[] parameter(145), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.924 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.923 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.925 = s64[] multiply(s64[] %constant.924, s64[] %constant.923), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.927 = s64[] add(s64[] %p145.926, s64[] %multiply.925), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.950 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.951 = f32[384]{0} broadcast(f32[] %constant.950), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.952 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.943, f32[384]{0} %broadcast.951), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p146.928 = f32[384]{0} parameter(146), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.949 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.953 = f32[] subtract(f32[] %constant.949, f32[] %constant.950), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.954 = f32[384]{0} broadcast(f32[] %subtract.953), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.955 = f32[384]{0} multiply(f32[384]{0} %p146.928, f32[384]{0} %broadcast.954), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.956 = f32[384]{0} add(f32[384]{0} %multiply.952, f32[384]{0} %multiply.955), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.958 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.959 = f32[384]{0} broadcast(f32[] %constant.958), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.960 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.944, f32[384]{0} %broadcast.959), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p147.929 = f32[384]{0} parameter(147), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.957 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.961 = f32[] subtract(f32[] %constant.957, f32[] %constant.958), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.962 = f32[384]{0} broadcast(f32[] %subtract.961), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.963 = f32[384]{0} multiply(f32[384]{0} %p147.929, f32[384]{0} %broadcast.962), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.964 = f32[384]{0} add(f32[384]{0} %multiply.960, f32[384]{0} %multiply.963), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p151.968 = s64[] parameter(151), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.966 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.965 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.967 = s64[] multiply(s64[] %constant.966, s64[] %constant.965), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.969 = s64[] add(s64[] %p151.968, s64[] %multiply.967), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.989 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.990 = f32[384]{0} broadcast(f32[] %constant.989), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.991 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.982, f32[384]{0} %broadcast.990), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p152.970 = f32[384]{0} parameter(152), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.988 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.992 = f32[] subtract(f32[] %constant.988, f32[] %constant.989), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.993 = f32[384]{0} broadcast(f32[] %subtract.992), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.994 = f32[384]{0} multiply(f32[384]{0} %p152.970, f32[384]{0} %broadcast.993), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.995 = f32[384]{0} add(f32[384]{0} %multiply.991, f32[384]{0} %multiply.994), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.997 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.998 = f32[384]{0} broadcast(f32[] %constant.997), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.999 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.983, f32[384]{0} %broadcast.998), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p153.971 = f32[384]{0} parameter(153), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.996 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1000 = f32[] subtract(f32[] %constant.996, f32[] %constant.997), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1001 = f32[384]{0} broadcast(f32[] %subtract.1000), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1002 = f32[384]{0} multiply(f32[384]{0} %p153.971, f32[384]{0} %broadcast.1001), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1003 = f32[384]{0} add(f32[384]{0} %multiply.999, f32[384]{0} %multiply.1002), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p157.1007 = s64[] parameter(157), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1005 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1004 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1006 = s64[] multiply(s64[] %constant.1005, s64[] %constant.1004), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1008 = s64[] add(s64[] %p157.1007, s64[] %multiply.1006), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1028 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1029 = f32[64]{0} broadcast(f32[] %constant.1028), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1030 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.1021, f32[64]{0} %broadcast.1029), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p158.1009 = f32[64]{0} parameter(158), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1027 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1031 = f32[] subtract(f32[] %constant.1027, f32[] %constant.1028), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1032 = f32[64]{0} broadcast(f32[] %subtract.1031), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1033 = f32[64]{0} multiply(f32[64]{0} %p158.1009, f32[64]{0} %broadcast.1032), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1034 = f32[64]{0} add(f32[64]{0} %multiply.1030, f32[64]{0} %multiply.1033), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1036 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1037 = f32[64]{0} broadcast(f32[] %constant.1036), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1038 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.1022, f32[64]{0} %broadcast.1037), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p159.1010 = f32[64]{0} parameter(159), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1035 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1039 = f32[] subtract(f32[] %constant.1035, f32[] %constant.1036), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1040 = f32[64]{0} broadcast(f32[] %subtract.1039), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1041 = f32[64]{0} multiply(f32[64]{0} %p159.1010, f32[64]{0} %broadcast.1040), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1042 = f32[64]{0} add(f32[64]{0} %multiply.1038, f32[64]{0} %multiply.1041), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p163.1046 = s64[] parameter(163), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1044 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1043 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1045 = s64[] multiply(s64[] %constant.1044, s64[] %constant.1043), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1047 = s64[] add(s64[] %p163.1046, s64[] %multiply.1045), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1070 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1071 = f32[384]{0} broadcast(f32[] %constant.1070), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1072 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1063, f32[384]{0} %broadcast.1071), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p164.1048 = f32[384]{0} parameter(164), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1069 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1073 = f32[] subtract(f32[] %constant.1069, f32[] %constant.1070), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1074 = f32[384]{0} broadcast(f32[] %subtract.1073), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1075 = f32[384]{0} multiply(f32[384]{0} %p164.1048, f32[384]{0} %broadcast.1074), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1076 = f32[384]{0} add(f32[384]{0} %multiply.1072, f32[384]{0} %multiply.1075), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1078 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1079 = f32[384]{0} broadcast(f32[] %constant.1078), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1080 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1064, f32[384]{0} %broadcast.1079), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p165.1049 = f32[384]{0} parameter(165), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1077 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1081 = f32[] subtract(f32[] %constant.1077, f32[] %constant.1078), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1082 = f32[384]{0} broadcast(f32[] %subtract.1081), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1083 = f32[384]{0} multiply(f32[384]{0} %p165.1049, f32[384]{0} %broadcast.1082), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1084 = f32[384]{0} add(f32[384]{0} %multiply.1080, f32[384]{0} %multiply.1083), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p169.1088 = s64[] parameter(169), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1086 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1085 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1087 = s64[] multiply(s64[] %constant.1086, s64[] %constant.1085), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1089 = s64[] add(s64[] %p169.1088, s64[] %multiply.1087), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1109 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1110 = f32[384]{0} broadcast(f32[] %constant.1109), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1111 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1102, f32[384]{0} %broadcast.1110), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p170.1090 = f32[384]{0} parameter(170), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1108 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1112 = f32[] subtract(f32[] %constant.1108, f32[] %constant.1109), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1113 = f32[384]{0} broadcast(f32[] %subtract.1112), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1114 = f32[384]{0} multiply(f32[384]{0} %p170.1090, f32[384]{0} %broadcast.1113), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1115 = f32[384]{0} add(f32[384]{0} %multiply.1111, f32[384]{0} %multiply.1114), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1117 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1118 = f32[384]{0} broadcast(f32[] %constant.1117), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1119 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1103, f32[384]{0} %broadcast.1118), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p171.1091 = f32[384]{0} parameter(171), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1116 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1120 = f32[] subtract(f32[] %constant.1116, f32[] %constant.1117), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1121 = f32[384]{0} broadcast(f32[] %subtract.1120), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1122 = f32[384]{0} multiply(f32[384]{0} %p171.1091, f32[384]{0} %broadcast.1121), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1123 = f32[384]{0} add(f32[384]{0} %multiply.1119, f32[384]{0} %multiply.1122), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p175.1127 = s64[] parameter(175), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1125 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1124 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1126 = s64[] multiply(s64[] %constant.1125, s64[] %constant.1124), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1128 = s64[] add(s64[] %p175.1127, s64[] %multiply.1126), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1148 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1149 = f32[64]{0} broadcast(f32[] %constant.1148), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1150 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.1141, f32[64]{0} %broadcast.1149), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p176.1129 = f32[64]{0} parameter(176), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1147 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1151 = f32[] subtract(f32[] %constant.1147, f32[] %constant.1148), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1152 = f32[64]{0} broadcast(f32[] %subtract.1151), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1153 = f32[64]{0} multiply(f32[64]{0} %p176.1129, f32[64]{0} %broadcast.1152), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1154 = f32[64]{0} add(f32[64]{0} %multiply.1150, f32[64]{0} %multiply.1153), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1156 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1157 = f32[64]{0} broadcast(f32[] %constant.1156), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1158 = f32[64]{0} multiply(f32[64]{0} %get-tuple-element.1142, f32[64]{0} %broadcast.1157), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p177.1130 = f32[64]{0} parameter(177), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1155 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1159 = f32[] subtract(f32[] %constant.1155, f32[] %constant.1156), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1160 = f32[64]{0} broadcast(f32[] %subtract.1159), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1161 = f32[64]{0} multiply(f32[64]{0} %p177.1130, f32[64]{0} %broadcast.1160), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1162 = f32[64]{0} add(f32[64]{0} %multiply.1158, f32[64]{0} %multiply.1161), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p181.1166 = s64[] parameter(181), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1164 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1163 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1165 = s64[] multiply(s64[] %constant.1164, s64[] %constant.1163), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1167 = s64[] add(s64[] %p181.1166, s64[] %multiply.1165), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1190 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1191 = f32[384]{0} broadcast(f32[] %constant.1190), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1192 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1183, f32[384]{0} %broadcast.1191), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p182.1168 = f32[384]{0} parameter(182), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1189 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1193 = f32[] subtract(f32[] %constant.1189, f32[] %constant.1190), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1194 = f32[384]{0} broadcast(f32[] %subtract.1193), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1195 = f32[384]{0} multiply(f32[384]{0} %p182.1168, f32[384]{0} %broadcast.1194), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1196 = f32[384]{0} add(f32[384]{0} %multiply.1192, f32[384]{0} %multiply.1195), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1198 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1199 = f32[384]{0} broadcast(f32[] %constant.1198), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1200 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1184, f32[384]{0} %broadcast.1199), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p183.1169 = f32[384]{0} parameter(183), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1197 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1201 = f32[] subtract(f32[] %constant.1197, f32[] %constant.1198), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1202 = f32[384]{0} broadcast(f32[] %subtract.1201), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1203 = f32[384]{0} multiply(f32[384]{0} %p183.1169, f32[384]{0} %broadcast.1202), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1204 = f32[384]{0} add(f32[384]{0} %multiply.1200, f32[384]{0} %multiply.1203), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p187.1208 = s64[] parameter(187), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1206 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1205 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1207 = s64[] multiply(s64[] %constant.1206, s64[] %constant.1205), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1209 = s64[] add(s64[] %p187.1208, s64[] %multiply.1207), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1229 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1230 = f32[384]{0} broadcast(f32[] %constant.1229), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1231 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1222, f32[384]{0} %broadcast.1230), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p188.1210 = f32[384]{0} parameter(188), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1228 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1232 = f32[] subtract(f32[] %constant.1228, f32[] %constant.1229), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1233 = f32[384]{0} broadcast(f32[] %subtract.1232), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1234 = f32[384]{0} multiply(f32[384]{0} %p188.1210, f32[384]{0} %broadcast.1233), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1235 = f32[384]{0} add(f32[384]{0} %multiply.1231, f32[384]{0} %multiply.1234), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1237 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1238 = f32[384]{0} broadcast(f32[] %constant.1237), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1239 = f32[384]{0} multiply(f32[384]{0} %get-tuple-element.1223, f32[384]{0} %broadcast.1238), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p189.1211 = f32[384]{0} parameter(189), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1236 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1240 = f32[] subtract(f32[] %constant.1236, f32[] %constant.1237), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1241 = f32[384]{0} broadcast(f32[] %subtract.1240), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1242 = f32[384]{0} multiply(f32[384]{0} %p189.1211, f32[384]{0} %broadcast.1241), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1243 = f32[384]{0} add(f32[384]{0} %multiply.1239, f32[384]{0} %multiply.1242), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p193.1247 = s64[] parameter(193), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1245 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1244 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1246 = s64[] multiply(s64[] %constant.1245, s64[] %constant.1244), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1248 = s64[] add(s64[] %p193.1247, s64[] %multiply.1246), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1268 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1269 = f32[96]{0} broadcast(f32[] %constant.1268), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1270 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.1261, f32[96]{0} %broadcast.1269), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p194.1249 = f32[96]{0} parameter(194), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1267 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1271 = f32[] subtract(f32[] %constant.1267, f32[] %constant.1268), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1272 = f32[96]{0} broadcast(f32[] %subtract.1271), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1273 = f32[96]{0} multiply(f32[96]{0} %p194.1249, f32[96]{0} %broadcast.1272), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1274 = f32[96]{0} add(f32[96]{0} %multiply.1270, f32[96]{0} %multiply.1273), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1276 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1277 = f32[96]{0} broadcast(f32[] %constant.1276), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1278 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.1262, f32[96]{0} %broadcast.1277), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p195.1250 = f32[96]{0} parameter(195), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1275 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1279 = f32[] subtract(f32[] %constant.1275, f32[] %constant.1276), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1280 = f32[96]{0} broadcast(f32[] %subtract.1279), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1281 = f32[96]{0} multiply(f32[96]{0} %p195.1250, f32[96]{0} %broadcast.1280), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1282 = f32[96]{0} add(f32[96]{0} %multiply.1278, f32[96]{0} %multiply.1281), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p199.1286 = s64[] parameter(199), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1284 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1283 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1285 = s64[] multiply(s64[] %constant.1284, s64[] %constant.1283), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1287 = s64[] add(s64[] %p199.1286, s64[] %multiply.1285), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1303 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1304 = f32[576]{0} broadcast(f32[] %constant.1303), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1305 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1296, f32[576]{0} %broadcast.1304), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p200.1288 = f32[576]{0} parameter(200), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1302 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1306 = f32[] subtract(f32[] %constant.1302, f32[] %constant.1303), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1307 = f32[576]{0} broadcast(f32[] %subtract.1306), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1308 = f32[576]{0} multiply(f32[576]{0} %p200.1288, f32[576]{0} %broadcast.1307), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1309 = f32[576]{0} add(f32[576]{0} %multiply.1305, f32[576]{0} %multiply.1308), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1311 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1312 = f32[576]{0} broadcast(f32[] %constant.1311), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1313 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1297, f32[576]{0} %broadcast.1312), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p201.1289 = f32[576]{0} parameter(201), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1310 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1314 = f32[] subtract(f32[] %constant.1310, f32[] %constant.1311), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1315 = f32[576]{0} broadcast(f32[] %subtract.1314), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1316 = f32[576]{0} multiply(f32[576]{0} %p201.1289, f32[576]{0} %broadcast.1315), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1317 = f32[576]{0} add(f32[576]{0} %multiply.1313, f32[576]{0} %multiply.1316), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p205.1321 = s64[] parameter(205), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1319 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1318 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1320 = s64[] multiply(s64[] %constant.1319, s64[] %constant.1318), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1322 = s64[] add(s64[] %p205.1321, s64[] %multiply.1320), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1342 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1343 = f32[576]{0} broadcast(f32[] %constant.1342), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1344 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1335, f32[576]{0} %broadcast.1343), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p206.1323 = f32[576]{0} parameter(206), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1341 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1345 = f32[] subtract(f32[] %constant.1341, f32[] %constant.1342), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1346 = f32[576]{0} broadcast(f32[] %subtract.1345), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1347 = f32[576]{0} multiply(f32[576]{0} %p206.1323, f32[576]{0} %broadcast.1346), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1348 = f32[576]{0} add(f32[576]{0} %multiply.1344, f32[576]{0} %multiply.1347), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1350 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1351 = f32[576]{0} broadcast(f32[] %constant.1350), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1352 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1336, f32[576]{0} %broadcast.1351), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p207.1324 = f32[576]{0} parameter(207), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1349 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1353 = f32[] subtract(f32[] %constant.1349, f32[] %constant.1350), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1354 = f32[576]{0} broadcast(f32[] %subtract.1353), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1355 = f32[576]{0} multiply(f32[576]{0} %p207.1324, f32[576]{0} %broadcast.1354), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1356 = f32[576]{0} add(f32[576]{0} %multiply.1352, f32[576]{0} %multiply.1355), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p211.1360 = s64[] parameter(211), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1358 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1357 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1359 = s64[] multiply(s64[] %constant.1358, s64[] %constant.1357), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1361 = s64[] add(s64[] %p211.1360, s64[] %multiply.1359), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1381 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1382 = f32[96]{0} broadcast(f32[] %constant.1381), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1383 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.1374, f32[96]{0} %broadcast.1382), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p212.1362 = f32[96]{0} parameter(212), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1380 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1384 = f32[] subtract(f32[] %constant.1380, f32[] %constant.1381), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1385 = f32[96]{0} broadcast(f32[] %subtract.1384), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1386 = f32[96]{0} multiply(f32[96]{0} %p212.1362, f32[96]{0} %broadcast.1385), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1387 = f32[96]{0} add(f32[96]{0} %multiply.1383, f32[96]{0} %multiply.1386), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1389 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1390 = f32[96]{0} broadcast(f32[] %constant.1389), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1391 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.1375, f32[96]{0} %broadcast.1390), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p213.1363 = f32[96]{0} parameter(213), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1388 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1392 = f32[] subtract(f32[] %constant.1388, f32[] %constant.1389), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1393 = f32[96]{0} broadcast(f32[] %subtract.1392), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1394 = f32[96]{0} multiply(f32[96]{0} %p213.1363, f32[96]{0} %broadcast.1393), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1395 = f32[96]{0} add(f32[96]{0} %multiply.1391, f32[96]{0} %multiply.1394), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p217.1399 = s64[] parameter(217), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1397 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1396 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1398 = s64[] multiply(s64[] %constant.1397, s64[] %constant.1396), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1400 = s64[] add(s64[] %p217.1399, s64[] %multiply.1398), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1423 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1424 = f32[576]{0} broadcast(f32[] %constant.1423), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1425 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1416, f32[576]{0} %broadcast.1424), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p218.1401 = f32[576]{0} parameter(218), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1422 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1426 = f32[] subtract(f32[] %constant.1422, f32[] %constant.1423), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1427 = f32[576]{0} broadcast(f32[] %subtract.1426), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1428 = f32[576]{0} multiply(f32[576]{0} %p218.1401, f32[576]{0} %broadcast.1427), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1429 = f32[576]{0} add(f32[576]{0} %multiply.1425, f32[576]{0} %multiply.1428), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1431 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1432 = f32[576]{0} broadcast(f32[] %constant.1431), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1433 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1417, f32[576]{0} %broadcast.1432), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p219.1402 = f32[576]{0} parameter(219), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1430 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1434 = f32[] subtract(f32[] %constant.1430, f32[] %constant.1431), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1435 = f32[576]{0} broadcast(f32[] %subtract.1434), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1436 = f32[576]{0} multiply(f32[576]{0} %p219.1402, f32[576]{0} %broadcast.1435), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1437 = f32[576]{0} add(f32[576]{0} %multiply.1433, f32[576]{0} %multiply.1436), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p223.1441 = s64[] parameter(223), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1439 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1438 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1440 = s64[] multiply(s64[] %constant.1439, s64[] %constant.1438), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1442 = s64[] add(s64[] %p223.1441, s64[] %multiply.1440), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1462 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1463 = f32[576]{0} broadcast(f32[] %constant.1462), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1464 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1455, f32[576]{0} %broadcast.1463), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p224.1443 = f32[576]{0} parameter(224), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1461 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1465 = f32[] subtract(f32[] %constant.1461, f32[] %constant.1462), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1466 = f32[576]{0} broadcast(f32[] %subtract.1465), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1467 = f32[576]{0} multiply(f32[576]{0} %p224.1443, f32[576]{0} %broadcast.1466), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1468 = f32[576]{0} add(f32[576]{0} %multiply.1464, f32[576]{0} %multiply.1467), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1470 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1471 = f32[576]{0} broadcast(f32[] %constant.1470), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1472 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1456, f32[576]{0} %broadcast.1471), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p225.1444 = f32[576]{0} parameter(225), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1469 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1473 = f32[] subtract(f32[] %constant.1469, f32[] %constant.1470), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1474 = f32[576]{0} broadcast(f32[] %subtract.1473), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1475 = f32[576]{0} multiply(f32[576]{0} %p225.1444, f32[576]{0} %broadcast.1474), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1476 = f32[576]{0} add(f32[576]{0} %multiply.1472, f32[576]{0} %multiply.1475), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p229.1480 = s64[] parameter(229), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1478 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1477 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1479 = s64[] multiply(s64[] %constant.1478, s64[] %constant.1477), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1481 = s64[] add(s64[] %p229.1480, s64[] %multiply.1479), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1501 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1502 = f32[96]{0} broadcast(f32[] %constant.1501), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1503 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.1494, f32[96]{0} %broadcast.1502), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p230.1482 = f32[96]{0} parameter(230), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1500 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1504 = f32[] subtract(f32[] %constant.1500, f32[] %constant.1501), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1505 = f32[96]{0} broadcast(f32[] %subtract.1504), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1506 = f32[96]{0} multiply(f32[96]{0} %p230.1482, f32[96]{0} %broadcast.1505), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1507 = f32[96]{0} add(f32[96]{0} %multiply.1503, f32[96]{0} %multiply.1506), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1509 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1510 = f32[96]{0} broadcast(f32[] %constant.1509), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1511 = f32[96]{0} multiply(f32[96]{0} %get-tuple-element.1495, f32[96]{0} %broadcast.1510), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p231.1483 = f32[96]{0} parameter(231), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1508 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1512 = f32[] subtract(f32[] %constant.1508, f32[] %constant.1509), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1513 = f32[96]{0} broadcast(f32[] %subtract.1512), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1514 = f32[96]{0} multiply(f32[96]{0} %p231.1483, f32[96]{0} %broadcast.1513), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1515 = f32[96]{0} add(f32[96]{0} %multiply.1511, f32[96]{0} %multiply.1514), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p235.1519 = s64[] parameter(235), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1517 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1516 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1518 = s64[] multiply(s64[] %constant.1517, s64[] %constant.1516), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1520 = s64[] add(s64[] %p235.1519, s64[] %multiply.1518), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1543 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1544 = f32[576]{0} broadcast(f32[] %constant.1543), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1545 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1536, f32[576]{0} %broadcast.1544), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p236.1521 = f32[576]{0} parameter(236), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1542 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1546 = f32[] subtract(f32[] %constant.1542, f32[] %constant.1543), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1547 = f32[576]{0} broadcast(f32[] %subtract.1546), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1548 = f32[576]{0} multiply(f32[576]{0} %p236.1521, f32[576]{0} %broadcast.1547), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1549 = f32[576]{0} add(f32[576]{0} %multiply.1545, f32[576]{0} %multiply.1548), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1551 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1552 = f32[576]{0} broadcast(f32[] %constant.1551), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1553 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1537, f32[576]{0} %broadcast.1552), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p237.1522 = f32[576]{0} parameter(237), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1550 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1554 = f32[] subtract(f32[] %constant.1550, f32[] %constant.1551), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1555 = f32[576]{0} broadcast(f32[] %subtract.1554), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1556 = f32[576]{0} multiply(f32[576]{0} %p237.1522, f32[576]{0} %broadcast.1555), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1557 = f32[576]{0} add(f32[576]{0} %multiply.1553, f32[576]{0} %multiply.1556), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p241.1561 = s64[] parameter(241), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1559 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1558 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1560 = s64[] multiply(s64[] %constant.1559, s64[] %constant.1558), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1562 = s64[] add(s64[] %p241.1561, s64[] %multiply.1560), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1582 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1583 = f32[576]{0} broadcast(f32[] %constant.1582), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1584 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1575, f32[576]{0} %broadcast.1583), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p242.1563 = f32[576]{0} parameter(242), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1581 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1585 = f32[] subtract(f32[] %constant.1581, f32[] %constant.1582), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1586 = f32[576]{0} broadcast(f32[] %subtract.1585), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1587 = f32[576]{0} multiply(f32[576]{0} %p242.1563, f32[576]{0} %broadcast.1586), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1588 = f32[576]{0} add(f32[576]{0} %multiply.1584, f32[576]{0} %multiply.1587), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1590 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1591 = f32[576]{0} broadcast(f32[] %constant.1590), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1592 = f32[576]{0} multiply(f32[576]{0} %get-tuple-element.1576, f32[576]{0} %broadcast.1591), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p243.1564 = f32[576]{0} parameter(243), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1589 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1593 = f32[] subtract(f32[] %constant.1589, f32[] %constant.1590), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1594 = f32[576]{0} broadcast(f32[] %subtract.1593), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1595 = f32[576]{0} multiply(f32[576]{0} %p243.1564, f32[576]{0} %broadcast.1594), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1596 = f32[576]{0} add(f32[576]{0} %multiply.1592, f32[576]{0} %multiply.1595), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p247.1600 = s64[] parameter(247), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1598 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1597 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1599 = s64[] multiply(s64[] %constant.1598, s64[] %constant.1597), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1601 = s64[] add(s64[] %p247.1600, s64[] %multiply.1599), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1621 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1622 = f32[160]{0} broadcast(f32[] %constant.1621), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1623 = f32[160]{0} multiply(f32[160]{0} %get-tuple-element.1614, f32[160]{0} %broadcast.1622), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p248.1602 = f32[160]{0} parameter(248), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1620 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1624 = f32[] subtract(f32[] %constant.1620, f32[] %constant.1621), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1625 = f32[160]{0} broadcast(f32[] %subtract.1624), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1626 = f32[160]{0} multiply(f32[160]{0} %p248.1602, f32[160]{0} %broadcast.1625), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1627 = f32[160]{0} add(f32[160]{0} %multiply.1623, f32[160]{0} %multiply.1626), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1629 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1630 = f32[160]{0} broadcast(f32[] %constant.1629), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1631 = f32[160]{0} multiply(f32[160]{0} %get-tuple-element.1615, f32[160]{0} %broadcast.1630), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p249.1603 = f32[160]{0} parameter(249), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1628 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1632 = f32[] subtract(f32[] %constant.1628, f32[] %constant.1629), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1633 = f32[160]{0} broadcast(f32[] %subtract.1632), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1634 = f32[160]{0} multiply(f32[160]{0} %p249.1603, f32[160]{0} %broadcast.1633), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1635 = f32[160]{0} add(f32[160]{0} %multiply.1631, f32[160]{0} %multiply.1634), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p253.1639 = s64[] parameter(253), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1637 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1636 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1638 = s64[] multiply(s64[] %constant.1637, s64[] %constant.1636), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1640 = s64[] add(s64[] %p253.1639, s64[] %multiply.1638), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1656 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1657 = f32[960]{0} broadcast(f32[] %constant.1656), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1658 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1649, f32[960]{0} %broadcast.1657), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p254.1641 = f32[960]{0} parameter(254), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1655 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1659 = f32[] subtract(f32[] %constant.1655, f32[] %constant.1656), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1660 = f32[960]{0} broadcast(f32[] %subtract.1659), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1661 = f32[960]{0} multiply(f32[960]{0} %p254.1641, f32[960]{0} %broadcast.1660), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1662 = f32[960]{0} add(f32[960]{0} %multiply.1658, f32[960]{0} %multiply.1661), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1664 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1665 = f32[960]{0} broadcast(f32[] %constant.1664), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1666 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1650, f32[960]{0} %broadcast.1665), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p255.1642 = f32[960]{0} parameter(255), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1663 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1667 = f32[] subtract(f32[] %constant.1663, f32[] %constant.1664), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1668 = f32[960]{0} broadcast(f32[] %subtract.1667), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1669 = f32[960]{0} multiply(f32[960]{0} %p255.1642, f32[960]{0} %broadcast.1668), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1670 = f32[960]{0} add(f32[960]{0} %multiply.1666, f32[960]{0} %multiply.1669), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p259.1674 = s64[] parameter(259), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1672 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1671 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1673 = s64[] multiply(s64[] %constant.1672, s64[] %constant.1671), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1675 = s64[] add(s64[] %p259.1674, s64[] %multiply.1673), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1695 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1696 = f32[960]{0} broadcast(f32[] %constant.1695), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1697 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1688, f32[960]{0} %broadcast.1696), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p260.1676 = f32[960]{0} parameter(260), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1694 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1698 = f32[] subtract(f32[] %constant.1694, f32[] %constant.1695), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1699 = f32[960]{0} broadcast(f32[] %subtract.1698), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1700 = f32[960]{0} multiply(f32[960]{0} %p260.1676, f32[960]{0} %broadcast.1699), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1701 = f32[960]{0} add(f32[960]{0} %multiply.1697, f32[960]{0} %multiply.1700), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1703 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1704 = f32[960]{0} broadcast(f32[] %constant.1703), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1705 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1689, f32[960]{0} %broadcast.1704), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p261.1677 = f32[960]{0} parameter(261), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1702 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1706 = f32[] subtract(f32[] %constant.1702, f32[] %constant.1703), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1707 = f32[960]{0} broadcast(f32[] %subtract.1706), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1708 = f32[960]{0} multiply(f32[960]{0} %p261.1677, f32[960]{0} %broadcast.1707), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1709 = f32[960]{0} add(f32[960]{0} %multiply.1705, f32[960]{0} %multiply.1708), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p265.1713 = s64[] parameter(265), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1711 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1710 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1712 = s64[] multiply(s64[] %constant.1711, s64[] %constant.1710), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1714 = s64[] add(s64[] %p265.1713, s64[] %multiply.1712), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1734 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1735 = f32[160]{0} broadcast(f32[] %constant.1734), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1736 = f32[160]{0} multiply(f32[160]{0} %get-tuple-element.1727, f32[160]{0} %broadcast.1735), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p266.1715 = f32[160]{0} parameter(266), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1733 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1737 = f32[] subtract(f32[] %constant.1733, f32[] %constant.1734), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1738 = f32[160]{0} broadcast(f32[] %subtract.1737), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1739 = f32[160]{0} multiply(f32[160]{0} %p266.1715, f32[160]{0} %broadcast.1738), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1740 = f32[160]{0} add(f32[160]{0} %multiply.1736, f32[160]{0} %multiply.1739), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1742 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1743 = f32[160]{0} broadcast(f32[] %constant.1742), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1744 = f32[160]{0} multiply(f32[160]{0} %get-tuple-element.1728, f32[160]{0} %broadcast.1743), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p267.1716 = f32[160]{0} parameter(267), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1741 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1745 = f32[] subtract(f32[] %constant.1741, f32[] %constant.1742), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1746 = f32[160]{0} broadcast(f32[] %subtract.1745), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1747 = f32[160]{0} multiply(f32[160]{0} %p267.1716, f32[160]{0} %broadcast.1746), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1748 = f32[160]{0} add(f32[160]{0} %multiply.1744, f32[160]{0} %multiply.1747), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p271.1752 = s64[] parameter(271), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1750 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1749 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1751 = s64[] multiply(s64[] %constant.1750, s64[] %constant.1749), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1753 = s64[] add(s64[] %p271.1752, s64[] %multiply.1751), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1776 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1777 = f32[960]{0} broadcast(f32[] %constant.1776), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1778 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1769, f32[960]{0} %broadcast.1777), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p272.1754 = f32[960]{0} parameter(272), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1775 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1779 = f32[] subtract(f32[] %constant.1775, f32[] %constant.1776), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1780 = f32[960]{0} broadcast(f32[] %subtract.1779), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1781 = f32[960]{0} multiply(f32[960]{0} %p272.1754, f32[960]{0} %broadcast.1780), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1782 = f32[960]{0} add(f32[960]{0} %multiply.1778, f32[960]{0} %multiply.1781), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1784 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1785 = f32[960]{0} broadcast(f32[] %constant.1784), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1786 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1770, f32[960]{0} %broadcast.1785), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p273.1755 = f32[960]{0} parameter(273), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1783 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1787 = f32[] subtract(f32[] %constant.1783, f32[] %constant.1784), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1788 = f32[960]{0} broadcast(f32[] %subtract.1787), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1789 = f32[960]{0} multiply(f32[960]{0} %p273.1755, f32[960]{0} %broadcast.1788), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1790 = f32[960]{0} add(f32[960]{0} %multiply.1786, f32[960]{0} %multiply.1789), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p277.1794 = s64[] parameter(277), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1792 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1791 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1793 = s64[] multiply(s64[] %constant.1792, s64[] %constant.1791), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1795 = s64[] add(s64[] %p277.1794, s64[] %multiply.1793), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1815 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1816 = f32[960]{0} broadcast(f32[] %constant.1815), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1817 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1808, f32[960]{0} %broadcast.1816), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p278.1796 = f32[960]{0} parameter(278), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1814 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1818 = f32[] subtract(f32[] %constant.1814, f32[] %constant.1815), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1819 = f32[960]{0} broadcast(f32[] %subtract.1818), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1820 = f32[960]{0} multiply(f32[960]{0} %p278.1796, f32[960]{0} %broadcast.1819), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1821 = f32[960]{0} add(f32[960]{0} %multiply.1817, f32[960]{0} %multiply.1820), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1823 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1824 = f32[960]{0} broadcast(f32[] %constant.1823), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1825 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1809, f32[960]{0} %broadcast.1824), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p279.1797 = f32[960]{0} parameter(279), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1822 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1826 = f32[] subtract(f32[] %constant.1822, f32[] %constant.1823), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1827 = f32[960]{0} broadcast(f32[] %subtract.1826), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1828 = f32[960]{0} multiply(f32[960]{0} %p279.1797, f32[960]{0} %broadcast.1827), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1829 = f32[960]{0} add(f32[960]{0} %multiply.1825, f32[960]{0} %multiply.1828), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p283.1833 = s64[] parameter(283), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1831 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1830 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1832 = s64[] multiply(s64[] %constant.1831, s64[] %constant.1830), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1834 = s64[] add(s64[] %p283.1833, s64[] %multiply.1832), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1854 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1855 = f32[160]{0} broadcast(f32[] %constant.1854), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1856 = f32[160]{0} multiply(f32[160]{0} %get-tuple-element.1847, f32[160]{0} %broadcast.1855), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p284.1835 = f32[160]{0} parameter(284), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1853 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1857 = f32[] subtract(f32[] %constant.1853, f32[] %constant.1854), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1858 = f32[160]{0} broadcast(f32[] %subtract.1857), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1859 = f32[160]{0} multiply(f32[160]{0} %p284.1835, f32[160]{0} %broadcast.1858), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1860 = f32[160]{0} add(f32[160]{0} %multiply.1856, f32[160]{0} %multiply.1859), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1862 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1863 = f32[160]{0} broadcast(f32[] %constant.1862), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1864 = f32[160]{0} multiply(f32[160]{0} %get-tuple-element.1848, f32[160]{0} %broadcast.1863), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p285.1836 = f32[160]{0} parameter(285), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1861 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1865 = f32[] subtract(f32[] %constant.1861, f32[] %constant.1862), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1866 = f32[160]{0} broadcast(f32[] %subtract.1865), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1867 = f32[160]{0} multiply(f32[160]{0} %p285.1836, f32[160]{0} %broadcast.1866), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1868 = f32[160]{0} add(f32[160]{0} %multiply.1864, f32[160]{0} %multiply.1867), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p289.1872 = s64[] parameter(289), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1870 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1869 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1871 = s64[] multiply(s64[] %constant.1870, s64[] %constant.1869), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1873 = s64[] add(s64[] %p289.1872, s64[] %multiply.1871), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1896 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1897 = f32[960]{0} broadcast(f32[] %constant.1896), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1898 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1889, f32[960]{0} %broadcast.1897), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p290.1874 = f32[960]{0} parameter(290), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1895 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1899 = f32[] subtract(f32[] %constant.1895, f32[] %constant.1896), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1900 = f32[960]{0} broadcast(f32[] %subtract.1899), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1901 = f32[960]{0} multiply(f32[960]{0} %p290.1874, f32[960]{0} %broadcast.1900), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1902 = f32[960]{0} add(f32[960]{0} %multiply.1898, f32[960]{0} %multiply.1901), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1904 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1905 = f32[960]{0} broadcast(f32[] %constant.1904), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1906 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1890, f32[960]{0} %broadcast.1905), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p291.1875 = f32[960]{0} parameter(291), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1903 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1907 = f32[] subtract(f32[] %constant.1903, f32[] %constant.1904), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1908 = f32[960]{0} broadcast(f32[] %subtract.1907), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1909 = f32[960]{0} multiply(f32[960]{0} %p291.1875, f32[960]{0} %broadcast.1908), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1910 = f32[960]{0} add(f32[960]{0} %multiply.1906, f32[960]{0} %multiply.1909), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p295.1914 = s64[] parameter(295), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1912 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1911 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1913 = s64[] multiply(s64[] %constant.1912, s64[] %constant.1911), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1915 = s64[] add(s64[] %p295.1914, s64[] %multiply.1913), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1935 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1936 = f32[960]{0} broadcast(f32[] %constant.1935), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1937 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1928, f32[960]{0} %broadcast.1936), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p296.1916 = f32[960]{0} parameter(296), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1934 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1938 = f32[] subtract(f32[] %constant.1934, f32[] %constant.1935), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1939 = f32[960]{0} broadcast(f32[] %subtract.1938), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1940 = f32[960]{0} multiply(f32[960]{0} %p296.1916, f32[960]{0} %broadcast.1939), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1941 = f32[960]{0} add(f32[960]{0} %multiply.1937, f32[960]{0} %multiply.1940), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1943 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1944 = f32[960]{0} broadcast(f32[] %constant.1943), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1945 = f32[960]{0} multiply(f32[960]{0} %get-tuple-element.1929, f32[960]{0} %broadcast.1944), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p297.1917 = f32[960]{0} parameter(297), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1942 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1946 = f32[] subtract(f32[] %constant.1942, f32[] %constant.1943), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1947 = f32[960]{0} broadcast(f32[] %subtract.1946), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1948 = f32[960]{0} multiply(f32[960]{0} %p297.1917, f32[960]{0} %broadcast.1947), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1949 = f32[960]{0} add(f32[960]{0} %multiply.1945, f32[960]{0} %multiply.1948), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p301.1953 = s64[] parameter(301), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1951 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1950 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1952 = s64[] multiply(s64[] %constant.1951, s64[] %constant.1950), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1954 = s64[] add(s64[] %p301.1953, s64[] %multiply.1952), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.1974 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1975 = f32[320]{0} broadcast(f32[] %constant.1974), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1976 = f32[320]{0} multiply(f32[320]{0} %get-tuple-element.1967, f32[320]{0} %broadcast.1975), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p302.1955 = f32[320]{0} parameter(302), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1973 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1977 = f32[] subtract(f32[] %constant.1973, f32[] %constant.1974), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1978 = f32[320]{0} broadcast(f32[] %subtract.1977), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1979 = f32[320]{0} multiply(f32[320]{0} %p302.1955, f32[320]{0} %broadcast.1978), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1980 = f32[320]{0} add(f32[320]{0} %multiply.1976, f32[320]{0} %multiply.1979), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1982 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1983 = f32[320]{0} broadcast(f32[] %constant.1982), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1984 = f32[320]{0} multiply(f32[320]{0} %get-tuple-element.1968, f32[320]{0} %broadcast.1983), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p303.1956 = f32[320]{0} parameter(303), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.1981 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.1985 = f32[] subtract(f32[] %constant.1981, f32[] %constant.1982), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.1986 = f32[320]{0} broadcast(f32[] %subtract.1985), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.1987 = f32[320]{0} multiply(f32[320]{0} %p303.1956, f32[320]{0} %broadcast.1986), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.1988 = f32[320]{0} add(f32[320]{0} %multiply.1984, f32[320]{0} %multiply.1987), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p307.1992 = s64[] parameter(307), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.1990 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.1989 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.1991 = s64[] multiply(s64[] %constant.1990, s64[] %constant.1989), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.1993 = s64[] add(s64[] %p307.1992, s64[] %multiply.1991), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.2009 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.2010 = f32[1280]{0} broadcast(f32[] %constant.2009), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.2011 = f32[1280]{0} multiply(f32[1280]{0} %get-tuple-element.2002, f32[1280]{0} %broadcast.2010), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p308.1994 = f32[1280]{0} parameter(308), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2008 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.2012 = f32[] subtract(f32[] %constant.2008, f32[] %constant.2009), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.2013 = f32[1280]{0} broadcast(f32[] %subtract.2012), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.2014 = f32[1280]{0} multiply(f32[1280]{0} %p308.1994, f32[1280]{0} %broadcast.2013), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.2015 = f32[1280]{0} add(f32[1280]{0} %multiply.2011, f32[1280]{0} %multiply.2014), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2017 = f32[] constant(0.1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.2018 = f32[1280]{0} broadcast(f32[] %constant.2017), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.2019 = f32[1280]{0} multiply(f32[1280]{0} %get-tuple-element.2003, f32[1280]{0} %broadcast.2018), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p309.1995 = f32[1280]{0} parameter(309), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="batch_norm@functional.py" source_line=2451}
  %constant.2016 = f32[] constant(1), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %subtract.2020 = f32[] subtract(f32[] %constant.2016, f32[] %constant.2017), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %broadcast.2021 = f32[1280]{0} broadcast(f32[] %subtract.2020), dimensions={}, metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %multiply.2022 = f32[1280]{0} multiply(f32[1280]{0} %p309.1995, f32[1280]{0} %broadcast.2021), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %add.2023 = f32[1280]{0} add(f32[1280]{0} %multiply.2019, f32[1280]{0} %multiply.2022), metadata={op_type="xla__moving_average" op_name="xla__moving_average" source_file="batch_norm@functional.py" source_line=2451}
  %p313.2027 = s64[] parameter(313), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="forward@batchnorm.py" source_line=151}
  %constant.2025 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %constant.2024 = s64[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="forward@batchnorm.py" source_line=151}
  %multiply.2026 = s64[] multiply(s64[] %constant.2025, s64[] %constant.2024), metadata={op_type="aten__mul" op_name="aten__mul" source_file="forward@batchnorm.py" source_line=151}
  %add.2028 = s64[] add(s64[] %p313.2027, s64[] %multiply.2026), metadata={op_type="aten__add" op_name="aten__add" source_file="forward@batchnorm.py" source_line=151}
  %constant.2036 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2042 = f32[1000]{0} reduce(f32[96,1000]{1,0} %broadcast.2035, f32[] %constant.2036), dimensions={0}, to_apply=%AddComputation.2038, metadata={op_type="aten__sum" op_name="aten__sum"}
  %reshape.2043 = f32[1,1000]{1,0} reshape(f32[1000]{0} %reduce.2042), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reshape.2044 = f32[1000]{0} reshape(f32[1,1000]{1,0} %reshape.2043), metadata={op_type="aten__view" op_name="aten__view" source_file="mark_step@xla_model.py" source_line=953}
  %constant.2324 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.2325 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[] %constant.2324), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %broadcast.2326 = f32[96,1280,7,7]{3,2,1,0} broadcast(f32[] %p12.42), dimensions={}, metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %clamp.2327 = f32[96,1280,7,7]{3,2,1,0} clamp(f32[96,1280,7,7]{3,2,1,0} %broadcast.2325, f32[96,1280,7,7]{3,2,1,0} %get-tuple-element.2001, f32[96,1280,7,7]{3,2,1,0} %broadcast.2326), metadata={op_type="aten__clamp" op_name="aten__clamp" source_file="hardtanh@functional.py" source_line=1506}
  %constant.2328 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %reduce.2334 = f32[96,1280]{1,0} reduce(f32[96,1280,7,7]{3,2,1,0} %clamp.2327, f32[] %constant.2328), dimensions={3,2}, to_apply=%AddComputation.2330, metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %constant.2329 = s32[] constant(49), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %constant.2335 = s32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %compare.2337 = pred[] compare(s32[] %constant.2329, s32[] %constant.2335), direction=NE, metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %constant.2336 = f32[] constant(1), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %convert.2338 = f32[] convert(s32[] %constant.2329), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %divide.2339 = f32[] divide(f32[] %constant.2336, f32[] %convert.2338), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %constant.2340 = f32[] constant(nan), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %select.2341 = f32[] select(pred[] %compare.2337, f32[] %divide.2339, f32[] %constant.2340), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %broadcast.2342 = f32[96,1280]{1,0} broadcast(f32[] %select.2341), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %multiply.2343 = f32[96,1280]{1,0} multiply(f32[96,1280]{1,0} %reduce.2334, f32[96,1280]{1,0} %broadcast.2342), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %reshape.2344 = f32[96,1280,1,1]{3,2,1,0} reshape(f32[96,1280]{1,0} %multiply.2343), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %convert.2345 = f32[96,1280,1,1]{3,2,1,0} convert(f32[96,1280,1,1]{3,2,1,0} %reshape.2344), metadata={op_type="aten__mean" op_name="aten__mean" source_file="adaptive_avg_pool2d@functional.py" source_line=1214}
  %reshape.2346 = f32[96,1280]{1,0} reshape(f32[96,1280,1,1]{3,2,1,0} %convert.2345), metadata={op_type="aten__view" op_name="aten__view" source_file="dropout@functional.py" source_line=1252}
  %multiply.2347 = f32[96,1280]{1,0} multiply(f32[96,1280]{1,0} %reshape.2346, f32[96,1280]{1,0} %divide.2323), metadata={op_type="aten__mul" op_name="aten__mul" source_file="dropout@functional.py" source_line=1252}
  %transpose.2348 = f32[1280,96]{0,1} transpose(f32[96,1280]{1,0} %multiply.2347), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %dot.2349 = f32[1280,1000]{1,0} dot(f32[1280,96]{0,1} %transpose.2348, f32[96,1000]{1,0} %broadcast.2035), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm"}
  %transpose.2350 = f32[1000,1280]{0,1} transpose(f32[1280,1000]{1,0} %dot.2349), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="mark_step@xla_model.py" source_line=953}
  %get-tuple-element.2382 = f32[1280]{0} get-tuple-element((f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) %batch-norm-grad.2380), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2383 = f32[1280]{0} get-tuple-element((f32[96,1280,7,7]{3,2,1,0}, f32[1280]{0}, f32[1280]{0}) %batch-norm-grad.2380), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2387 = f32[1,1,320,1280]{3,2,1,0} convolution(f32[96,320,7,7]{3,2,1,0} %get-tuple-element.1966, f32[96,1280,7,7]{3,2,1,0} %get-tuple-element.2381), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2388 = f32[1280,320,1,1]{0,1,3,2} transpose(f32[1,1,320,1280]{3,2,1,0} %convolution.2387), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2404 = f32[320]{0} get-tuple-element((f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) %batch-norm-grad.2402), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2405 = f32[320]{0} get-tuple-element((f32[96,320,7,7]{3,2,1,0}, f32[320]{0}, f32[320]{0}) %batch-norm-grad.2402), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2409 = f32[1,1,960,320]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1963, f32[96,320,7,7]{3,2,1,0} %get-tuple-element.2403), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2410 = f32[320,960,1,1]{0,1,3,2} transpose(f32[1,1,960,320]{3,2,1,0} %convolution.2409), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2436 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2434), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2437 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2434), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2444 = f32[3,3,1,960]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1924, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2435), window={size=7x7 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=960, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2445 = f32[960,1,3,3]{0,1,3,2} transpose(f32[3,3,1,960]{3,2,1,0} %convolution.2444), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2471 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2469), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2472 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2469), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2476 = f32[1,1,160,960]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %add.1885, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2470), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2477 = f32[960,160,1,1]{0,1,3,2} transpose(f32[1,1,160,960]{3,2,1,0} %convolution.2476), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2493 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2491), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2494 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2491), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2498 = f32[1,1,960,160]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1843, f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2492), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2499 = f32[160,960,1,1]{0,1,3,2} transpose(f32[1,1,960,160]{3,2,1,0} %convolution.2498), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2525 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2523), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2526 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2523), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2533 = f32[3,3,1,960]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1804, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2524), window={size=7x7 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=960, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2534 = f32[960,1,3,3]{0,1,3,2} transpose(f32[3,3,1,960]{3,2,1,0} %convolution.2533), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2560 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2558), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2561 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2558), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2565 = f32[1,1,160,960]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %add.1765, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2559), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2566 = f32[960,160,1,1]{0,1,3,2} transpose(f32[1,1,160,960]{3,2,1,0} %convolution.2565), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2589 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2587), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2590 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2587), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2594 = f32[1,1,960,160]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1723, f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2588), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2595 = f32[160,960,1,1]{0,1,3,2} transpose(f32[1,1,960,160]{3,2,1,0} %convolution.2594), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2621 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2619), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2622 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2619), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2629 = f32[3,3,1,960]{3,2,1,0} convolution(f32[96,960,7,7]{3,2,1,0} %clamp.1684, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2620), window={size=7x7 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=960, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2630 = f32[960,1,3,3]{0,1,3,2} transpose(f32[3,3,1,960]{3,2,1,0} %convolution.2629), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2656 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2654), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2657 = f32[960]{0} get-tuple-element((f32[96,960,7,7]{3,2,1,0}, f32[960]{0}, f32[960]{0}) %batch-norm-grad.2654), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2661 = f32[1,1,160,960]{3,2,1,0} convolution(f32[96,160,7,7]{3,2,1,0} %get-tuple-element.1613, f32[96,960,7,7]{3,2,1,0} %get-tuple-element.2655), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2662 = f32[960,160,1,1]{0,1,3,2} transpose(f32[1,1,160,960]{3,2,1,0} %convolution.2661), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2685 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2683), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2686 = f32[160]{0} get-tuple-element((f32[96,160,7,7]{3,2,1,0}, f32[160]{0}, f32[160]{0}) %batch-norm-grad.2683), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2690 = f32[1,1,576,160]{3,2,1,0} convolution(f32[96,576,7,7]{3,2,1,0} %clamp.1610, f32[96,160,7,7]{3,2,1,0} %get-tuple-element.2684), window={size=7x7}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2691 = f32[160,576,1,1]{0,1,3,2} transpose(f32[1,1,576,160]{3,2,1,0} %convolution.2690), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2717 = f32[576]{0} get-tuple-element((f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2715), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2718 = f32[576]{0} get-tuple-element((f32[96,576,7,7]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2715), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2725 = f32[3,3,1,576]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1571, f32[96,576,7,7]{3,2,1,0} %get-tuple-element.2716), window={size=7x7 pad=1_0x1_0 rhs_dilate=2x2}, dim_labels=fb01_io01->01bf, batch_group_count=576, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2726 = f32[576,1,3,3]{0,1,3,2} transpose(f32[3,3,1,576]{3,2,1,0} %convolution.2725), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2752 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2750), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2753 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2750), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2757 = f32[1,1,96,576]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %add.1532, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2751), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2758 = f32[576,96,1,1]{0,1,3,2} transpose(f32[1,1,96,576]{3,2,1,0} %convolution.2757), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2774 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2772), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2775 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2772), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2779 = f32[1,1,576,96]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1490, f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2773), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2780 = f32[96,576,1,1]{0,1,3,2} transpose(f32[1,1,576,96]{3,2,1,0} %convolution.2779), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2806 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2804), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2807 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2804), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2814 = f32[3,3,1,576]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1451, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2805), window={size=14x14 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=576, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2815 = f32[576,1,3,3]{0,1,3,2} transpose(f32[3,3,1,576]{3,2,1,0} %convolution.2814), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2841 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2839), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2842 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2839), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2846 = f32[1,1,96,576]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %add.1412, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2840), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2847 = f32[576,96,1,1]{0,1,3,2} transpose(f32[1,1,96,576]{3,2,1,0} %convolution.2846), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2870 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2868), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2871 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2868), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2875 = f32[1,1,576,96]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1370, f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2869), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2876 = f32[96,576,1,1]{0,1,3,2} transpose(f32[1,1,576,96]{3,2,1,0} %convolution.2875), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2902 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2900), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2903 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2900), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2910 = f32[3,3,1,576]{3,2,1,0} convolution(f32[96,576,14,14]{3,2,1,0} %clamp.1331, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2901), window={size=14x14 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=576, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2911 = f32[576,1,3,3]{0,1,3,2} transpose(f32[3,3,1,576]{3,2,1,0} %convolution.2910), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2937 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2935), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2938 = f32[576]{0} get-tuple-element((f32[96,576,14,14]{3,2,1,0}, f32[576]{0}, f32[576]{0}) %batch-norm-grad.2935), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2942 = f32[1,1,96,576]{3,2,1,0} convolution(f32[96,96,14,14]{3,2,1,0} %get-tuple-element.1260, f32[96,576,14,14]{3,2,1,0} %get-tuple-element.2936), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2943 = f32[576,96,1,1]{0,1,3,2} transpose(f32[1,1,96,576]{3,2,1,0} %convolution.2942), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2966 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2964), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2967 = f32[96]{0} get-tuple-element((f32[96,96,14,14]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.2964), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.2971 = f32[1,1,384,96]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1257, f32[96,96,14,14]{3,2,1,0} %get-tuple-element.2965), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.2972 = f32[96,384,1,1]{0,1,3,2} transpose(f32[1,1,384,96]{3,2,1,0} %convolution.2971), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.2998 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.2996), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.2999 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.2996), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3006 = f32[3,3,1,384]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1218, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.2997), window={size=14x14 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3007 = f32[384,1,3,3]{0,1,3,2} transpose(f32[3,3,1,384]{3,2,1,0} %convolution.3006), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3033 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3031), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3034 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3031), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3038 = f32[1,1,64,384]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %add.1179, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3032), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3039 = f32[384,64,1,1]{0,1,3,2} transpose(f32[1,1,64,384]{3,2,1,0} %convolution.3038), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3055 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3053), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3056 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3053), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3060 = f32[1,1,384,64]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1137, f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3054), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3061 = f32[64,384,1,1]{0,1,3,2} transpose(f32[1,1,384,64]{3,2,1,0} %convolution.3060), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3087 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3085), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3088 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3085), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3095 = f32[3,3,1,384]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1098, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3086), window={size=14x14 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3096 = f32[384,1,3,3]{0,1,3,2} transpose(f32[3,3,1,384]{3,2,1,0} %convolution.3095), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3122 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3120), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3123 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3120), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3127 = f32[1,1,64,384]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %add.1059, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3121), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3128 = f32[384,64,1,1]{0,1,3,2} transpose(f32[1,1,64,384]{3,2,1,0} %convolution.3127), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3151 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3149), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3152 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3149), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3156 = f32[1,1,384,64]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.1017, f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3150), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3157 = f32[64,384,1,1]{0,1,3,2} transpose(f32[1,1,384,64]{3,2,1,0} %convolution.3156), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3183 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3181), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3184 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3181), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3191 = f32[3,3,1,384]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.978, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3182), window={size=14x14 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3192 = f32[384,1,3,3]{0,1,3,2} transpose(f32[3,3,1,384]{3,2,1,0} %convolution.3191), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3218 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3216), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3219 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3216), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3223 = f32[1,1,64,384]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %add.939, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3217), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3224 = f32[384,64,1,1]{0,1,3,2} transpose(f32[1,1,64,384]{3,2,1,0} %convolution.3223), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3247 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3245), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3248 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3245), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3252 = f32[1,1,384,64]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.897, f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3246), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3253 = f32[64,384,1,1]{0,1,3,2} transpose(f32[1,1,384,64]{3,2,1,0} %convolution.3252), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3279 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3277), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3280 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3277), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3287 = f32[3,3,1,384]{3,2,1,0} convolution(f32[96,384,14,14]{3,2,1,0} %clamp.858, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3278), window={size=14x14 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=384, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3288 = f32[384,1,3,3]{0,1,3,2} transpose(f32[3,3,1,384]{3,2,1,0} %convolution.3287), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3314 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3312), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3315 = f32[384]{0} get-tuple-element((f32[96,384,14,14]{3,2,1,0}, f32[384]{0}, f32[384]{0}) %batch-norm-grad.3312), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3319 = f32[1,1,64,384]{3,2,1,0} convolution(f32[96,64,14,14]{3,2,1,0} %get-tuple-element.787, f32[96,384,14,14]{3,2,1,0} %get-tuple-element.3313), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3320 = f32[384,64,1,1]{0,1,3,2} transpose(f32[1,1,64,384]{3,2,1,0} %convolution.3319), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3343 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3341), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3344 = f32[64]{0} get-tuple-element((f32[96,64,14,14]{3,2,1,0}, f32[64]{0}, f32[64]{0}) %batch-norm-grad.3341), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3348 = f32[1,1,192,64]{3,2,1,0} convolution(f32[96,192,14,14]{3,2,1,0} %clamp.784, f32[96,64,14,14]{3,2,1,0} %get-tuple-element.3342), window={size=14x14}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3349 = f32[64,192,1,1]{0,1,3,2} transpose(f32[1,1,192,64]{3,2,1,0} %convolution.3348), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3375 = f32[192]{0} get-tuple-element((f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3373), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3376 = f32[192]{0} get-tuple-element((f32[96,192,14,14]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3373), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3383 = f32[3,3,1,192]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.745, f32[96,192,14,14]{3,2,1,0} %get-tuple-element.3374), window={size=14x14 pad=1_0x1_0 rhs_dilate=2x2}, dim_labels=fb01_io01->01bf, batch_group_count=192, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3384 = f32[192,1,3,3]{0,1,3,2} transpose(f32[3,3,1,192]{3,2,1,0} %convolution.3383), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3410 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3408), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3411 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3408), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3415 = f32[1,1,32,192]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %add.706, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3409), window={size=28x28}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3416 = f32[192,32,1,1]{0,1,3,2} transpose(f32[1,1,32,192]{3,2,1,0} %convolution.3415), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3432 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3430), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3433 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3430), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3437 = f32[1,1,192,32]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.664, f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3431), window={size=28x28}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3438 = f32[32,192,1,1]{0,1,3,2} transpose(f32[1,1,192,32]{3,2,1,0} %convolution.3437), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3464 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3462), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3465 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3462), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3472 = f32[3,3,1,192]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.625, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3463), window={size=28x28 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=192, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3473 = f32[192,1,3,3]{0,1,3,2} transpose(f32[3,3,1,192]{3,2,1,0} %convolution.3472), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3499 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3497), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3500 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3497), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3504 = f32[1,1,32,192]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %add.586, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3498), window={size=28x28}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3505 = f32[192,32,1,1]{0,1,3,2} transpose(f32[1,1,32,192]{3,2,1,0} %convolution.3504), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3528 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3526), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3529 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3526), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3533 = f32[1,1,192,32]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.544, f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3527), window={size=28x28}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3534 = f32[32,192,1,1]{0,1,3,2} transpose(f32[1,1,192,32]{3,2,1,0} %convolution.3533), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3560 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3558), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3561 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3558), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3568 = f32[3,3,1,192]{3,2,1,0} convolution(f32[96,192,28,28]{3,2,1,0} %clamp.505, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3559), window={size=28x28 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=192, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3569 = f32[192,1,3,3]{0,1,3,2} transpose(f32[3,3,1,192]{3,2,1,0} %convolution.3568), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3595 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3593), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3596 = f32[192]{0} get-tuple-element((f32[96,192,28,28]{3,2,1,0}, f32[192]{0}, f32[192]{0}) %batch-norm-grad.3593), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3600 = f32[1,1,32,192]{3,2,1,0} convolution(f32[96,32,28,28]{3,2,1,0} %get-tuple-element.434, f32[96,192,28,28]{3,2,1,0} %get-tuple-element.3594), window={size=28x28}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3601 = f32[192,32,1,1]{0,1,3,2} transpose(f32[1,1,32,192]{3,2,1,0} %convolution.3600), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3624 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3622), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3625 = f32[32]{0} get-tuple-element((f32[96,32,28,28]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3622), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3629 = f32[1,1,144,32]{3,2,1,0} convolution(f32[96,144,28,28]{3,2,1,0} %clamp.431, f32[96,32,28,28]{3,2,1,0} %get-tuple-element.3623), window={size=28x28}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3630 = f32[32,144,1,1]{0,1,3,2} transpose(f32[1,1,144,32]{3,2,1,0} %convolution.3629), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3656 = f32[144]{0} get-tuple-element((f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3654), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3657 = f32[144]{0} get-tuple-element((f32[96,144,28,28]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3654), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3664 = f32[3,3,1,144]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %clamp.392, f32[96,144,28,28]{3,2,1,0} %get-tuple-element.3655), window={size=28x28 pad=1_0x1_0 rhs_dilate=2x2}, dim_labels=fb01_io01->01bf, batch_group_count=144, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3665 = f32[144,1,3,3]{0,1,3,2} transpose(f32[3,3,1,144]{3,2,1,0} %convolution.3664), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3691 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3689), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3692 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3689), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3696 = f32[1,1,24,144]{3,2,1,0} convolution(f32[96,24,56,56]{3,2,1,0} %add.353, f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3690), window={size=56x56}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3697 = f32[144,24,1,1]{0,1,3,2} transpose(f32[1,1,24,144]{3,2,1,0} %convolution.3696), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3713 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-grad.3711), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3714 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-grad.3711), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3718 = f32[1,1,144,24]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %clamp.311, f32[96,24,56,56]{3,2,1,0} %get-tuple-element.3712), window={size=56x56}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3719 = f32[24,144,1,1]{0,1,3,2} transpose(f32[1,1,144,24]{3,2,1,0} %convolution.3718), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3745 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3743), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3746 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3743), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3753 = f32[3,3,1,144]{3,2,1,0} convolution(f32[96,144,56,56]{3,2,1,0} %clamp.272, f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3744), window={size=56x56 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=144, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3754 = f32[144,1,3,3]{0,1,3,2} transpose(f32[3,3,1,144]{3,2,1,0} %convolution.3753), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3780 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3778), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3781 = f32[144]{0} get-tuple-element((f32[96,144,56,56]{3,2,1,0}, f32[144]{0}, f32[144]{0}) %batch-norm-grad.3778), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3785 = f32[1,1,24,144]{3,2,1,0} convolution(f32[96,24,56,56]{3,2,1,0} %get-tuple-element.201, f32[96,144,56,56]{3,2,1,0} %get-tuple-element.3779), window={size=56x56}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3786 = f32[144,24,1,1]{0,1,3,2} transpose(f32[1,1,24,144]{3,2,1,0} %convolution.3785), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3809 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-grad.3807), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3810 = f32[24]{0} get-tuple-element((f32[96,24,56,56]{3,2,1,0}, f32[24]{0}, f32[24]{0}) %batch-norm-grad.3807), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3814 = f32[1,1,96,24]{3,2,1,0} convolution(f32[96,96,56,56]{3,2,1,0} %clamp.198, f32[96,24,56,56]{3,2,1,0} %get-tuple-element.3808), window={size=56x56}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3815 = f32[24,96,1,1]{0,1,3,2} transpose(f32[1,1,96,24]{3,2,1,0} %convolution.3814), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3841 = f32[96]{0} get-tuple-element((f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.3839), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3842 = f32[96]{0} get-tuple-element((f32[96,96,56,56]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.3839), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3849 = f32[3,3,1,96]{3,2,1,0} convolution(f32[96,96,112,112]{3,2,1,0} %clamp.159, f32[96,96,56,56]{3,2,1,0} %get-tuple-element.3840), window={size=56x56 pad=1_0x1_0 rhs_dilate=2x2}, dim_labels=fb01_io01->01bf, batch_group_count=96, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3850 = f32[96,1,3,3]{0,1,3,2} transpose(f32[3,3,1,96]{3,2,1,0} %convolution.3849), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3876 = f32[96]{0} get-tuple-element((f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.3874), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3877 = f32[96]{0} get-tuple-element((f32[96,96,112,112]{3,2,1,0}, f32[96]{0}, f32[96]{0}) %batch-norm-grad.3874), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3881 = f32[1,1,16,96]{3,2,1,0} convolution(f32[96,16,112,112]{3,2,1,0} %get-tuple-element.88, f32[96,96,112,112]{3,2,1,0} %get-tuple-element.3875), window={size=112x112}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3882 = f32[96,16,1,1]{0,1,3,2} transpose(f32[1,1,16,96]{3,2,1,0} %convolution.3881), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3898 = f32[16]{0} get-tuple-element((f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) %batch-norm-grad.3896), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3899 = f32[16]{0} get-tuple-element((f32[96,16,112,112]{3,2,1,0}, f32[16]{0}, f32[16]{0}) %batch-norm-grad.3896), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3903 = f32[1,1,32,16]{3,2,1,0} convolution(f32[96,32,112,112]{3,2,1,0} %clamp.85, f32[96,16,112,112]{3,2,1,0} %get-tuple-element.3897), window={size=112x112}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3904 = f32[16,32,1,1]{0,1,3,2} transpose(f32[1,1,32,16]{3,2,1,0} %convolution.3903), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3930 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3928), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3931 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3928), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3938 = f32[3,3,1,32]{3,2,1,0} convolution(f32[96,32,112,112]{3,2,1,0} %clamp.46, f32[96,32,112,112]{3,2,1,0} %get-tuple-element.3929), window={size=112x112 pad=1_1x1_1}, dim_labels=fb01_io01->01bf, batch_group_count=32, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3939 = f32[32,1,3,3]{0,1,3,2} transpose(f32[3,3,1,32]{3,2,1,0} %convolution.3938), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %get-tuple-element.3965 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3963), index=1, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %get-tuple-element.3966 = f32[32]{0} get-tuple-element((f32[96,32,112,112]{3,2,1,0}, f32[32]{0}, f32[32]{0}) %batch-norm-grad.3963), index=2, metadata={op_type="aten__native_batch_norm_backward" op_name="aten__native_batch_norm_backward"}
  %convolution.3970 = f32[3,3,3,32]{3,2,1,0} convolution(f32[96,3,224,224]{3,2,1,0} %p5.6, f32[96,32,112,112]{3,2,1,0} %get-tuple-element.3964), window={size=112x112 pad=1_0x1_0 rhs_dilate=2x2}, dim_labels=fb01_io01->01bf, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  %transpose.3971 = f32[32,3,3,3]{0,1,3,2} transpose(f32[3,3,3,32]{3,2,1,0} %convolution.3970), dimensions={3,2,0,1}, metadata={op_type="aten__convolution_backward_overrideable" op_name="aten__convolution_backward_overrideable"}
  ROOT %tuple.3978 = (f32[32]{0}, f32[32]{0}, s64[], f32[32]{0}, f32[32]{0}, /*index=5*/s64[], f32[16]{0}, f32[16]{0}, s64[], f32[96]{0}, /*index=10*/f32[96]{0}, s64[], f32[96]{0}, f32[96]{0}, s64[], /*index=15*/f32[24]{0}, f32[24]{0}, s64[], f32[144]{0}, f32[144]{0}, /*index=20*/s64[], f32[144]{0}, f32[144]{0}, s64[], f32[24]{0}, /*index=25*/f32[24]{0}, s64[], f32[144]{0}, f32[144]{0}, s64[], /*index=30*/f32[144]{0}, f32[144]{0}, s64[], f32[32]{0}, f32[32]{0}, /*index=35*/s64[], f32[192]{0}, f32[192]{0}, s64[], f32[192]{0}, /*index=40*/f32[192]{0}, s64[], f32[32]{0}, f32[32]{0}, s64[], /*index=45*/f32[192]{0}, f32[192]{0}, s64[], f32[192]{0}, f32[192]{0}, /*index=50*/s64[], f32[32]{0}, f32[32]{0}, s64[], f32[192]{0}, /*index=55*/f32[192]{0}, s64[], f32[192]{0}, f32[192]{0}, s64[], /*index=60*/f32[64]{0}, f32[64]{0}, s64[], f32[384]{0}, f32[384]{0}, /*index=65*/s64[], f32[384]{0}, f32[384]{0}, s64[], f32[64]{0}, /*index=70*/f32[64]{0}, s64[], f32[384]{0}, f32[384]{0}, s64[], /*index=75*/f32[384]{0}, f32[384]{0}, s64[], f32[64]{0}, f32[64]{0}, /*index=80*/s64[], f32[384]{0}, f32[384]{0}, s64[], f32[384]{0}, /*index=85*/f32[384]{0}, s64[], f32[64]{0}, f32[64]{0}, s64[], /*index=90*/f32[384]{0}, f32[384]{0}, s64[], f32[384]{0}, f32[384]{0}, /*index=95*/s64[], f32[96]{0}, f32[96]{0}, s64[], f32[576]{0}, /*index=100*/f32[576]{0}, s64[], f32[576]{0}, f32[576]{0}, s64[], /*index=105*/f32[96]{0}, f32[96]{0}, s64[], f32[576]{0}, f32[576]{0}, /*index=110*/s64[], f32[576]{0}, f32[576]{0}, s64[], f32[96]{0}, /*index=115*/f32[96]{0}, s64[], f32[576]{0}, f32[576]{0}, s64[], /*index=120*/f32[576]{0}, f32[576]{0}, s64[], f32[160]{0}, f32[160]{0}, /*index=125*/s64[], f32[960]{0}, f32[960]{0}, s64[], f32[960]{0}, /*index=130*/f32[960]{0}, s64[], f32[160]{0}, f32[160]{0}, s64[], /*index=135*/f32[960]{0}, f32[960]{0}, s64[], f32[960]{0}, f32[960]{0}, /*index=140*/s64[], f32[160]{0}, f32[160]{0}, s64[], f32[960]{0}, /*index=145*/f32[960]{0}, s64[], f32[960]{0}, f32[960]{0}, s64[], /*index=150*/f32[320]{0}, f32[320]{0}, s64[], f32[1280]{0}, f32[1280]{0}, /*index=155*/s64[], f32[1000]{0}, f32[1000,1280]{0,1}, f32[1280]{0}, f32[1280]{0}, /*index=160*/f32[1280,320,1,1]{0,1,3,2}, f32[320]{0}, f32[320]{0}, f32[320,960,1,1]{0,1,3,2}, f32[960]{0}, /*index=165*/f32[960]{0}, f32[960,1,3,3]{0,1,3,2}, f32[960]{0}, f32[960]{0}, f32[960,160,1,1]{0,1,3,2}, /*index=170*/f32[160]{0}, f32[160]{0}, f32[160,960,1,1]{0,1,3,2}, f32[960]{0}, f32[960]{0}, /*index=175*/f32[960,1,3,3]{0,1,3,2}, f32[960]{0}, f32[960]{0}, f32[960,160,1,1]{0,1,3,2}, f32[160]{0}, /*index=180*/f32[160]{0}, f32[160,960,1,1]{0,1,3,2}, f32[960]{0}, f32[960]{0}, f32[960,1,3,3]{0,1,3,2}, /*index=185*/f32[960]{0}, f32[960]{0}, f32[960,160,1,1]{0,1,3,2}, f32[160]{0}, f32[160]{0}, /*index=190*/f32[160,576,1,1]{0,1,3,2}, f32[576]{0}, f32[576]{0}, f32[576,1,3,3]{0,1,3,2}, f32[576]{0}, /*index=195*/f32[576]{0}, f32[576,96,1,1]{0,1,3,2}, f32[96]{0}, f32[96]{0}, f32[96,576,1,1]{0,1,3,2}, /*index=200*/f32[576]{0}, f32[576]{0}, f32[576,1,3,3]{0,1,3,2}, f32[576]{0}, f32[576]{0}, /*index=205*/f32[576,96,1,1]{0,1,3,2}, f32[96]{0}, f32[96]{0}, f32[96,576,1,1]{0,1,3,2}, f32[576]{0}, /*index=210*/f32[576]{0}, f32[576,1,3,3]{0,1,3,2}, f32[576]{0}, f32[576]{0}, f32[576,96,1,1]{0,1,3,2}, /*index=215*/f32[96]{0}, f32[96]{0}, f32[96,384,1,1]{0,1,3,2}, f32[384]{0}, f32[384]{0}, /*index=220*/f32[384,1,3,3]{0,1,3,2}, f32[384]{0}, f32[384]{0}, f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, /*index=225*/f32[64]{0}, f32[64,384,1,1]{0,1,3,2}, f32[384]{0}, f32[384]{0}, f32[384,1,3,3]{0,1,3,2}, /*index=230*/f32[384]{0}, f32[384]{0}, f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, f32[64]{0}, /*index=235*/f32[64,384,1,1]{0,1,3,2}, f32[384]{0}, f32[384]{0}, f32[384,1,3,3]{0,1,3,2}, f32[384]{0}, /*index=240*/f32[384]{0}, f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, f32[64]{0}, f32[64,384,1,1]{0,1,3,2}, /*index=245*/f32[384]{0}, f32[384]{0}, f32[384,1,3,3]{0,1,3,2}, f32[384]{0}, f32[384]{0}, /*index=250*/f32[384,64,1,1]{0,1,3,2}, f32[64]{0}, f32[64]{0}, f32[64,192,1,1]{0,1,3,2}, f32[192]{0}, /*index=255*/f32[192]{0}, f32[192,1,3,3]{0,1,3,2}, f32[192]{0}, f32[192]{0}, f32[192,32,1,1]{0,1,3,2}, /*index=260*/f32[32]{0}, f32[32]{0}, f32[32,192,1,1]{0,1,3,2}, f32[192]{0}, f32[192]{0}, /*index=265*/f32[192,1,3,3]{0,1,3,2}, f32[192]{0}, f32[192]{0}, f32[192,32,1,1]{0,1,3,2}, f32[32]{0}, /*index=270*/f32[32]{0}, f32[32,192,1,1]{0,1,3,2}, f32[192]{0}, f32[192]{0}, f32[192,1,3,3]{0,1,3,2}, /*index=275*/f32[192]{0}, f32[192]{0}, f32[192,32,1,1]{0,1,3,2}, f32[32]{0}, f32[32]{0}, /*index=280*/f32[32,144,1,1]{0,1,3,2}, f32[144]{0}, f32[144]{0}, f32[144,1,3,3]{0,1,3,2}, f32[144]{0}, /*index=285*/f32[144]{0}, f32[144,24,1,1]{0,1,3,2}, f32[24]{0}, f32[24]{0}, f32[24,144,1,1]{0,1,3,2}, /*index=290*/f32[144]{0}, f32[144]{0}, f32[144,1,3,3]{0,1,3,2}, f32[144]{0}, f32[144]{0}, /*index=295*/f32[144,24,1,1]{0,1,3,2}, f32[24]{0}, f32[24]{0}, f32[24,96,1,1]{0,1,3,2}, f32[96]{0}, /*index=300*/f32[96]{0}, f32[96,1,3,3]{0,1,3,2}, f32[96]{0}, f32[96]{0}, f32[96,16,1,1]{0,1,3,2}, /*index=305*/f32[16]{0}, f32[16]{0}, f32[16,32,1,1]{0,1,3,2}, f32[32]{0}, f32[32]{0}, /*index=310*/f32[32,1,3,3]{0,1,3,2}, f32[32]{0}, f32[32]{0}, f32[32,3,3,3]{0,1,3,2}) tuple(f32[32]{0} %add.23, f32[32]{0} %add.31, s64[] %add.36, f32[32]{0} %add.63, f32[32]{0} %add.71, /*index=5*/s64[] %add.76, f32[16]{0} %add.102, f32[16]{0} %add.110, s64[] %add.115, f32[96]{0} %add.137, /*index=10*/f32[96]{0} %add.145, s64[] %add.150, f32[96]{0} %add.176, f32[96]{0} %add.184, s64[] %add.189, /*index=15*/f32[24]{0} %add.215, f32[24]{0} %add.223, s64[] %add.228, f32[144]{0} %add.250, f32[144]{0} %add.258, /*index=20*/s64[] %add.263, f32[144]{0} %add.289, f32[144]{0} %add.297, s64[] %add.302, f32[24]{0} %add.328, /*index=25*/f32[24]{0} %add.336, s64[] %add.341, f32[144]{0} %add.370, f32[144]{0} %add.378, s64[] %add.383, /*index=30*/f32[144]{0} %add.409, f32[144]{0} %add.417, s64[] %add.422, f32[32]{0} %add.448, f32[32]{0} %add.456, /*index=35*/s64[] %add.461, f32[192]{0} %add.483, f32[192]{0} %add.491, s64[] %add.496, f32[192]{0} %add.522, /*index=40*/f32[192]{0} %add.530, s64[] %add.535, f32[32]{0} %add.561, f32[32]{0} %add.569, s64[] %add.574, /*index=45*/f32[192]{0} %add.603, f32[192]{0} %add.611, s64[] %add.616, f32[192]{0} %add.642, f32[192]{0} %add.650, /*index=50*/s64[] %add.655, f32[32]{0} %add.681, f32[32]{0} %add.689, s64[] %add.694, f32[192]{0} %add.723, /*index=55*/f32[192]{0} %add.731, s64[] %add.736, f32[192]{0} %add.762, f32[192]{0} %add.770, s64[] %add.775, /*index=60*/f32[64]{0} %add.801, f32[64]{0} %add.809, s64[] %add.814, f32[384]{0} %add.836, f32[384]{0} %add.844, /*index=65*/s64[] %add.849, f32[384]{0} %add.875, f32[384]{0} %add.883, s64[] %add.888, f32[64]{0} %add.914, /*index=70*/f32[64]{0} %add.922, s64[] %add.927, f32[384]{0} %add.956, f32[384]{0} %add.964, s64[] %add.969, /*index=75*/f32[384]{0} %add.995, f32[384]{0} %add.1003, s64[] %add.1008, f32[64]{0} %add.1034, f32[64]{0} %add.1042, /*index=80*/s64[] %add.1047, f32[384]{0} %add.1076, f32[384]{0} %add.1084, s64[] %add.1089, f32[384]{0} %add.1115, /*index=85*/f32[384]{0} %add.1123, s64[] %add.1128, f32[64]{0} %add.1154, f32[64]{0} %add.1162, s64[] %add.1167, /*index=90*/f32[384]{0} %add.1196, f32[384]{0} %add.1204, s64[] %add.1209, f32[384]{0} %add.1235, f32[384]{0} %add.1243, /*index=95*/s64[] %add.1248, f32[96]{0} %add.1274, f32[96]{0} %add.1282, s64[] %add.1287, f32[576]{0} %add.1309, /*index=100*/f32[576]{0} %add.1317, s64[] %add.1322, f32[576]{0} %add.1348, f32[576]{0} %add.1356, s64[] %add.1361, /*index=105*/f32[96]{0} %add.1387, f32[96]{0} %add.1395, s64[] %add.1400, f32[576]{0} %add.1429, f32[576]{0} %add.1437, /*index=110*/s64[] %add.1442, f32[576]{0} %add.1468, f32[576]{0} %add.1476, s64[] %add.1481, f32[96]{0} %add.1507, /*index=115*/f32[96]{0} %add.1515, s64[] %add.1520, f32[576]{0} %add.1549, f32[576]{0} %add.1557, s64[] %add.1562, /*index=120*/f32[576]{0} %add.1588, f32[576]{0} %add.1596, s64[] %add.1601, f32[160]{0} %add.1627, f32[160]{0} %add.1635, /*index=125*/s64[] %add.1640, f32[960]{0} %add.1662, f32[960]{0} %add.1670, s64[] %add.1675, f32[960]{0} %add.1701, /*index=130*/f32[960]{0} %add.1709, s64[] %add.1714, f32[160]{0} %add.1740, f32[160]{0} %add.1748, s64[] %add.1753, /*index=135*/f32[960]{0} %add.1782, f32[960]{0} %add.1790, s64[] %add.1795, f32[960]{0} %add.1821, f32[960]{0} %add.1829, /*index=140*/s64[] %add.1834, f32[160]{0} %add.1860, f32[160]{0} %add.1868, s64[] %add.1873, f32[960]{0} %add.1902, /*index=145*/f32[960]{0} %add.1910, s64[] %add.1915, f32[960]{0} %add.1941, f32[960]{0} %add.1949, s64[] %add.1954, /*index=150*/f32[320]{0} %add.1980, f32[320]{0} %add.1988, s64[] %add.1993, f32[1280]{0} %add.2015, f32[1280]{0} %add.2023, /*index=155*/s64[] %add.2028, f32[1000]{0} %reshape.2044, f32[1000,1280]{0,1} %transpose.2350, f32[1280]{0} %get-tuple-element.2382, f32[1280]{0} %get-tuple-element.2383, /*index=160*/f32[1280,320,1,1]{0,1,3,2} %transpose.2388, f32[320]{0} %get-tuple-element.2404, f32[320]{0} %get-tuple-element.2405, f32[320,960,1,1]{0,1,3,2} %transpose.2410, f32[960]{0} %get-tuple-element.2436, /*index=165*/f32[960]{0} %get-tuple-element.2437, f32[960,1,3,3]{0,1,3,2} %transpose.2445, f32[960]{0} %get-tuple-element.2471, f32[960]{0} %get-tuple-element.2472, f32[960,160,1,1]{0,1,3,2} %transpose.2477, /*index=170*/f32[160]{0} %get-tuple-element.2493, f32[160]{0} %get-tuple-element.2494, f32[160,960,1,1]{0,1,3,2} %transpose.2499, f32[960]{0} %get-tuple-element.2525, f32[960]{0} %get-tuple-element.2526, /*index=175*/f32[960,1,3,3]{0,1,3,2} %transpose.2534, f32[960]{0} %get-tuple-element.2560, f32[960]{0} %get-tuple-element.2561, f32[960,160,1,1]{0,1,3,2} %transpose.2566, f32[160]{0} %get-tuple-element.2589, /*index=180*/f32[160]{0} %get-tuple-element.2590, f32[160,960,1,1]{0,1,3,2} %transpose.2595, f32[960]{0} %get-tuple-element.2621, f32[960]{0} %get-tuple-element.2622, f32[960,1,3,3]{0,1,3,2} %transpose.2630, /*index=185*/f32[960]{0} %get-tuple-element.2656, f32[960]{0} %get-tuple-element.2657, f32[960,160,1,1]{0,1,3,2} %transpose.2662, f32[160]{0} %get-tuple-element.2685, f32[160]{0} %get-tuple-element.2686, /*index=190*/f32[160,576,1,1]{0,1,3,2} %transpose.2691, f32[576]{0} %get-tuple-element.2717, f32[576]{0} %get-tuple-element.2718, f32[576,1,3,3]{0,1,3,2} %transpose.2726, f32[576]{0} %get-tuple-element.2752, /*index=195*/f32[576]{0} %get-tuple-element.2753, f32[576,96,1,1]{0,1,3,2} %transpose.2758, f32[96]{0} %get-tuple-element.2774, f32[96]{0} %get-tuple-element.2775, f32[96,576,1,1]{0,1,3,2} %transpose.2780, /*index=200*/f32[576]{0} %get-tuple-element.2806, f32[576]{0} %get-tuple-element.2807, f32[576,1,3,3]{0,1,3,2} %transpose.2815, f32[576]{0} %get-tuple-element.2841, f32[576]{0} %get-tuple-element.2842, /*index=205*/f32[576,96,1,1]{0,1,3,2} %transpose.2847, f32[96]{0} %get-tuple-element.2870, f32[96]{0} %get-tuple-element.2871, f32[96,576,1,1]{0,1,3,2} %transpose.2876, f32[576]{0} %get-tuple-element.2902, /*index=210*/f32[576]{0} %get-tuple-element.2903, f32[576,1,3,3]{0,1,3,2} %transpose.2911, f32[576]{0} %get-tuple-element.2937, f32[576]{0} %get-tuple-element.2938, f32[576,96,1,1]{0,1,3,2} %transpose.2943, /*index=215*/f32[96]{0} %get-tuple-element.2966, f32[96]{0} %get-tuple-element.2967, f32[96,384,1,1]{0,1,3,2} %transpose.2972, f32[384]{0} %get-tuple-element.2998, f32[384]{0} %get-tuple-element.2999, /*index=220*/f32[384,1,3,3]{0,1,3,2} %transpose.3007, f32[384]{0} %get-tuple-element.3033, f32[384]{0} %get-tuple-element.3034, f32[384,64,1,1]{0,1,3,2} %transpose.3039, f32[64]{0} %get-tuple-element.3055, /*index=225*/f32[64]{0} %get-tuple-element.3056, f32[64,384,1,1]{0,1,3,2} %transpose.3061, f32[384]{0} %get-tuple-element.3087, f32[384]{0} %get-tuple-element.3088, f32[384,1,3,3]{0,1,3,2} %transpose.3096, /*index=230*/f32[384]{0} %get-tuple-element.3122, f32[384]{0} %get-tuple-element.3123, f32[384,64,1,1]{0,1,3,2} %transpose.3128, f32[64]{0} %get-tuple-element.3151, f32[64]{0} %get-tuple-element.3152, /*index=235*/f32[64,384,1,1]{0,1,3,2} %transpose.3157, f32[384]{0} %get-tuple-element.3183, f32[384]{0} %get-tuple-element.3184, f32[384,1,3,3]{0,1,3,2} %transpose.3192, f32[384]{0} %get-tuple-element.3218, /*index=240*/f32[384]{0} %get-tuple-element.3219, f32[384,64,1,1]{0,1,3,2} %transpose.3224, f32[64]{0} %get-tuple-element.3247, f32[64]{0} %get-tuple-element.3248, f32[64,384,1,1]{0,1,3,2} %transpose.3253, /*index=245*/f32[384]{0} %get-tuple-element.3279, f32[384]{0} %get-tuple-element.3280, f32[384,1,3,3]{0,1,3,2} %transpose.3288, f32[384]{0} %get-tuple-element.3314, f32[384]{0} %get-tuple-element.3315, /*index=250*/f32[384,64,1,1]{0,1,3,2} %transpose.3320, f32[64]{0} %get-tuple-element.3343, f32[64]{0} %get-tuple-element.3344, f32[64,192,1,1]{0,1,3,2} %transpose.3349, f32[192]{0} %get-tuple-element.3375, /*index=255*/f32[192]{0} %get-tuple-element.3376, f32[192,1,3,3]{0,1,3,2} %transpose.3384, f32[192]{0} %get-tuple-element.3410, f32[192]{0} %get-tuple-element.3411, f32[192,32,1,1]{0,1,3,2} %transpose.3416, /*index=260*/f32[32]{0} %get-tuple-element.3432, f32[32]{0} %get-tuple-element.3433, f32[32,192,1,1]{0,1,3,2} %transpose.3438, f32[192]{0} %get-tuple-element.3464, f32[192]{0} %get-tuple-element.3465, /*index=265*/f32[192,1,3,3]{0,1,3,2} %transpose.3473, f32[192]{0} %get-tuple-element.3499, f32[192]{0} %get-tuple-element.3500, f32[192,32,1,1]{0,1,3,2} %transpose.3505, f32[32]{0} %get-tuple-element.3528, /*index=270*/f32[32]{0} %get-tuple-element.3529, f32[32,192,1,1]{0,1,3,2} %transpose.3534, f32[192]{0} %get-tuple-element.3560, f32[192]{0} %get-tuple-element.3561, f32[192,1,3,3]{0,1,3,2} %transpose.3569, /*index=275*/f32[192]{0} %get-tuple-element.3595, f32[192]{0} %get-tuple-element.3596, f32[192,32,1,1]{0,1,3,2} %transpose.3601, f32[32]{0} %get-tuple-element.3624, f32[32]{0} %get-tuple-element.3625, /*index=280*/f32[32,144,1,1]{0,1,3,2} %transpose.3630, f32[144]{0} %get-tuple-element.3656, f32[144]{0} %get-tuple-element.3657, f32[144,1,3,3]{0,1,3,2} %transpose.3665, f32[144]{0} %get-tuple-element.3691, /*index=285*/f32[144]{0} %get-tuple-element.3692, f32[144,24,1,1]{0,1,3,2} %transpose.3697, f32[24]{0} %get-tuple-element.3713, f32[24]{0} %get-tuple-element.3714, f32[24,144,1,1]{0,1,3,2} %transpose.3719, /*index=290*/f32[144]{0} %get-tuple-element.3745, f32[144]{0} %get-tuple-element.3746, f32[144,1,3,3]{0,1,3,2} %transpose.3754, f32[144]{0} %get-tuple-element.3780, f32[144]{0} %get-tuple-element.3781, /*index=295*/f32[144,24,1,1]{0,1,3,2} %transpose.3786, f32[24]{0} %get-tuple-element.3809, f32[24]{0} %get-tuple-element.3810, f32[24,96,1,1]{0,1,3,2} %transpose.3815, f32[96]{0} %get-tuple-element.3841, /*index=300*/f32[96]{0} %get-tuple-element.3842, f32[96,1,3,3]{0,1,3,2} %transpose.3850, f32[96]{0} %get-tuple-element.3876, f32[96]{0} %get-tuple-element.3877, f32[96,16,1,1]{0,1,3,2} %transpose.3882, /*index=305*/f32[16]{0} %get-tuple-element.3898, f32[16]{0} %get-tuple-element.3899, f32[16,32,1,1]{0,1,3,2} %transpose.3904, f32[32]{0} %get-tuple-element.3930, f32[32]{0} %get-tuple-element.3931, /*index=310*/f32[32,1,3,3]{0,1,3,2} %transpose.3939, f32[32]{0} %get-tuple-element.3965, f32[32]{0} %get-tuple-element.3966, f32[32,3,3,3]{0,1,3,2} %transpose.3971)
}

