# Copyright 2025 The OpenXLA Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

# Default registry for XLA benchmarks.

benchmarks: [
  {
    name: "gemma3_1b_flax_call"
    description: "Gemma3 1b in Flax."
    owner: "juliagmt-google@"
    input_artifact: {
      input_format: HLO_TEXT
      artifact_gcs_bucket_path: "https://storage.googleapis.com/xla-benchmarking-temp/gemma3_1b_flax_call.hlo"
    }
    model_source_info: ["Gemma3 1B"]
    hardware_execution_configs: [{
      hardware_category: GPU_L4
      topology: { num_hosts: 1, num_devices_per_host: 1, multi_host: false, multi_device: false }
      target_metrics: [GPU_DEVICE_TIME, GPU_DEVICE_MEMCPY_TIME]
      workflow_type: [PRESUBMIT, POSTSUBMIT, SCHEDULED]
      runtime_flags: ["--num_repeats=5"]
    },
    {
      hardware_category: CPU_X86
      topology: { num_hosts: 1, num_devices_per_host: 1, multi_host: false, multi_device: false }
      target_metrics: [CPU_TIME]
      workflow_type: [PRESUBMIT, POSTSUBMIT, SCHEDULED]
      runtime_flags: ["--num_repeats=5"]
    }
    ]
    update_frequency_policy: QUARTERLY
  },
  {
    name: "gemma2_2b_keras_jax"
    description: "Gemma2 2B in Keras."
    owner: "juliagmt-google@"
    input_artifact: {
      input_format: HLO_TEXT
      artifact_gcs_bucket_path: "https://storage.googleapis.com/xla-benchmarking-temp/gemma2_2b_keras_jax.hlo"
    }
    model_source_info: ["Gemma2 2B"]
    hardware_execution_configs: [{
      hardware_category: GPU_L4
      topology: { num_hosts: 1, num_devices_per_host: 1, multi_host: false, multi_device: false }
      target_metrics: [GPU_DEVICE_TIME, GPU_DEVICE_MEMCPY_TIME]
      workflow_type: [PRESUBMIT, POSTSUBMIT]
      runtime_flags: ["--num_repeats=5"]
    },
    {
      hardware_category: GPU_B200
      topology: { num_hosts: 1, num_devices_per_host: 1, multi_host: false, multi_device: false }
      target_metrics: [GPU_DEVICE_TIME, GPU_DEVICE_MEMCPY_TIME]
      workflow_type: [POSTSUBMIT]
      runtime_flags: ["--num_repeats=5"]
    },
    {
      hardware_category: CPU_X86
      topology: { num_hosts: 1, num_devices_per_host: 1, multi_host: false, multi_device: false }
      target_metrics: [CPU_TIME]
      workflow_type: [PRESUBMIT, POSTSUBMIT]
      runtime_flags: ["--num_repeats=5"]
    }]
    update_frequency_policy: QUARTERLY
    # TODO(juliagmt): remove this label once the benchmark is stable.
    github_labels: ["blocking_presubmit_test"]
  },
  {
    name: "nv_maxtext_1n1g_jit_train_step_before_optimization.hlo"
    description: "Nvidia benchmark for Maxtext 1 node 1 gpu config for gpt3-52k model."
    owner: "hmonishN@"
    input_artifact: {
      input_format: HLO_TEXT
      artifact_path: "xla/tools/benchmarks/hlo/nv_maxtext_1n1g_jit_train_step_before_optimization.hlo"
    }
    model_source_info: ["Maxtext Default gpt3-52k"]
    hardware_execution_configs: [{
      hardware_category: GPU_B200
      topology: { num_hosts: 1, num_devices_per_host: 1, multi_host: false, multi_device: false }
      target_metrics: [GPU_DEVICE_TIME, GPU_DEVICE_MEMCPY_TIME]
      workflow_type: [POSTSUBMIT]
      runtime_flags: ["--num_repeats=5", "--xla_gpu_enable_latency_hiding_scheduler", "--xla_gpu_all_reduce_combine_threshold_bytes=1073741824", "--xla_gpu_all_gather_combine_threshold_bytes=1073741824", "--xla_gpu_reduce_scatter_combine_threshold_bytes=134217728", "--xla_gpu_enable_pipelined_all_gather", "--xla_gpu_enable_pipelined_all_reduce", "--xla_gpu_enable_while_loop_double_buffering", "--xla_gpu_enable_all_gather_combine_by_dim=false", "--xla_gpu_enable_reduce_scatter_combine_by_dim=false", "--xla_disable_hlo_passes=rematerialization"]
    }]
    update_frequency_policy: QUARTERLY
  }
]
