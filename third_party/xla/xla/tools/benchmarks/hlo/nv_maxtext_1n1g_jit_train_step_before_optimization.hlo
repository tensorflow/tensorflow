HloModule jit_train_step, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias), {8}: (8, {}, may-alias), {9}: (9, {}, may-alias), {10}: (10, {}, may-alias), {11}: (11, {}, may-alias), {12}: (12, {}, may-alias), {13}: (13, {}, may-alias), {14}: (14, {}, may-alias), {15}: (15, {}, may-alias), {16}: (16, {}, may-alias), {17}: (17, {}, may-alias), {18}: (18, {}, may-alias), {19}: (19, {}, may-alias), {20}: (20, {}, may-alias), {21}: (21, {}, may-alias), {22}: (22, {}, may-alias), {23}: (23, {}, may-alias), {24}: (24, {}, may-alias), {25}: (25, {}, may-alias), {26}: (26, {}, may-alias), {27}: (27, {}, may-alias), {28}: (28, {}, may-alias), {29}: (29, {}, may-alias), {30}: (30, {}, may-alias), {31}: (31, {}, may-alias), {32}: (32, {}, may-alias) }, entry_computation_layout={(s32[], f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, f32[8192,8,2560]{2,1,0}, /*index=5*/f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=10*/f32[32000,2560]{1,0}, s32[], f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, /*index=15*/f32[8192,8,2560]{2,1,0}, f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=20*/f32[2560,8,8,128]{3,2,1,0}, f32[32000,2560]{1,0}, f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, /*index=25*/f32[8192,8,2560]{2,1,0}, f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=30*/f32[2560,8,8,128]{3,2,1,0}, f32[32000,2560]{1,0}, s32[], s32[16,2048]{1,0}, s32[16,2048]{1,0}, /*index=35*/s32[16,2048]{1,0}, s32[16,2048]{1,0}, s32[16,2048]{1,0})->(s32[], f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, f32[8192,8,2560]{2,1,0}, /*index=5*/f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=10*/f32[32000,2560]{1,0}, s32[], f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, /*index=15*/f32[8192,8,2560]{2,1,0}, f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=20*/f32[2560,8,8,128]{3,2,1,0}, f32[32000,2560]{1,0}, f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, /*index=25*/f32[8192,8,2560]{2,1,0}, f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=30*/f32[2560,8,8,128]{3,2,1,0}, f32[32000,2560]{1,0}, s32[], f32[], f32[], /*index=35*/f32[], f32[], f32[], s32[])}

%_where.246 (Arg_0.247: pred[16,1,1,2048,2048], Arg_1.248: f32[], Arg_2.249: f32[]) -> f32[16,1,1,2048,2048] {
  %Arg_0.247 = pred[16,1,1,2048,2048]{4,3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/pjit"}
  %Arg_1.248 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/pjit"}
  %broadcast.250 = f32[16,1,1,2048,2048]{4,3,2,1,0} broadcast(f32[] %Arg_1.248), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=211}
  %Arg_2.249 = f32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/pjit"}
  %broadcast.251 = f32[16,1,1,2048,2048]{4,3,2,1,0} broadcast(f32[] %Arg_2.249), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=211}
  ROOT %select.252 = f32[16,1,1,2048,2048]{4,3,2,1,0} select(pred[16,1,1,2048,2048]{4,3,2,1,0} %Arg_0.247, f32[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.250, f32[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.251), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/jit(_where)/select_n" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=211}
}

%_where_0.255 (Arg_0.256: pred[16,1,1,2048,2048], Arg_1.257: f32[]) -> (pred[16,8,1,2048,2048], bf16[16,8,1,2048,2048]) {
  %Arg_0.256 = pred[16,1,1,2048,2048]{4,3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %broadcast.259 = pred[16,1,1,2048,2048]{4,3,2,1,0} broadcast(pred[16,1,1,2048,2048]{4,3,2,1,0} %Arg_0.256), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %reshape.260 = pred[16,1,2048,2048]{3,2,1,0} reshape(pred[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.259), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %broadcast.261 = pred[16,8,1,2048,2048]{4,3,2,1,0} broadcast(pred[16,1,2048,2048]{3,2,1,0} %reshape.260), dimensions={0,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %Arg_1.257 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %convert.258 = bf16[] convert(f32[] %Arg_1.257), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %broadcast.262 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[] %convert.258), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  ROOT %tuple.263 = (pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) tuple(pred[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.261, bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.262)
}

%region_1.268 (Arg_0.269: f32[], Arg_1.270: f32[]) -> f32[] {
  %Arg_0.269 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/reduce_sum"}
  %Arg_1.270 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/reduce_sum"}
  ROOT %add.271 = f32[] add(f32[] %Arg_0.269, f32[] %Arg_1.270), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
}

%silu.272 (Arg_0.273: bf16[16,2048,8192]) -> bf16[16,2048,8192] {
  %Arg_0.273 = bf16[16,2048,8192]{2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/pjit"}
  %constant.274 = bf16[] constant(1)
  %broadcast.275 = bf16[16,2048,8192]{2,1,0} broadcast(bf16[] %constant.274), dimensions={}
  %negate.276 = bf16[16,2048,8192]{2,1,0} negate(bf16[16,2048,8192]{2,1,0} %Arg_0.273), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/jit(silu)/neg" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %exponential.277 = bf16[16,2048,8192]{2,1,0} exponential(bf16[16,2048,8192]{2,1,0} %negate.276), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/jit(silu)/exp" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %add.278 = bf16[16,2048,8192]{2,1,0} add(bf16[16,2048,8192]{2,1,0} %exponential.277, bf16[16,2048,8192]{2,1,0} %broadcast.275), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/jit(silu)/add" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %divide.279 = bf16[16,2048,8192]{2,1,0} divide(bf16[16,2048,8192]{2,1,0} %broadcast.275, bf16[16,2048,8192]{2,1,0} %add.278), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/jit(silu)/div" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  ROOT %multiply.280 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %Arg_0.273, bf16[16,2048,8192]{2,1,0} %divide.279), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/jit(silu)/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
}

%_where_1.281 (Arg_0.282: bf16[16,8,1,2048,2048], Arg_1.283: pred[16,8,1,2048,2048], Arg_2.284: bf16[16,8,1,2048,2048]) -> bf16[16,8,1,2048,2048] {
  %Arg_1.283 = pred[16,8,1,2048,2048]{4,3,2,1,0} parameter(1), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %Arg_0.282 = bf16[16,8,1,2048,2048]{4,3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %Arg_2.284 = bf16[16,8,1,2048,2048]{4,3,2,1,0} parameter(2), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  ROOT %select.285 = bf16[16,8,1,2048,2048]{4,3,2,1,0} select(pred[16,8,1,2048,2048]{4,3,2,1,0} %Arg_1.283, bf16[16,8,1,2048,2048]{4,3,2,1,0} %Arg_0.282, bf16[16,8,1,2048,2048]{4,3,2,1,0} %Arg_2.284), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/select_n" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
}

%region_2.286 (Arg_0.287: bf16[], Arg_1.288: bf16[]) -> bf16[] {
  %Arg_0.287 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max"}
  %Arg_1.288 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max"}
  ROOT %maximum.289 = bf16[] maximum(bf16[] %Arg_0.287, bf16[] %Arg_1.288), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
}

%region_3.290 (Arg_0.291: f32[], Arg_1.292: f32[]) -> f32[] {
  %Arg_0.291 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  %Arg_1.292 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  ROOT %add.293 = f32[] add(f32[] %Arg_0.291, f32[] %Arg_1.292), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
}

%None.294 (Arg_0.295: bf16[16,2048,1,64], Arg_1.296: bf16[16,2048,1,64], Arg_2.297: bf16[16,2048,1,64], Arg_3.298: bf16[16,2048,1,64], Arg_4.299: pred[16,8,1,2048,2048], Arg_5.300: bf16[16,8,1,2048,2048], Arg_6.301: bf16[16,2048,2560], Arg_7.302: f32[2560,8192], Arg_8.303: f32[2560,8192], Arg_9.304: f32[8192,2560], Arg_10.305: f32[2560], Arg_11.306: f32[2560,8,128], Arg_12.307: f32[8,128,2560], Arg_13.308: f32[2560,8,128], Arg_14.309: f32[2560,8,128]) -> (bf16[16,2048,2560], bf16[16,2048,8,128], bf16[16,2048,8,128], bf16[16,2048,8,128], bf16[16,2048,8192], /*index=5*/bf16[16,2048,8192], bf16[16,2048,2560]) {
  %Arg_6.301 = bf16[16,2048,2560]{2,1,0} parameter(6), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %custom-call.316 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %Arg_6.301), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %convert.317 = f32[16,2048,2560]{2,1,0} convert(bf16[16,2048,2560]{2,1,0} %custom-call.316), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=39}
  %multiply.318 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.317, f32[16,2048,2560]{2,1,0} %convert.317), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/square" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.315 = f32[] constant(0)
  %reduce.319 = f32[16,2048]{1,0} reduce(f32[16,2048,2560]{2,1,0} %multiply.318, f32[] %constant.315), dimensions={2}, to_apply=%region_1.268, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reshape.320 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.319), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.312 = f32[] constant(2560)
  %broadcast.313 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.312), dimensions={}
  %divide.321 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %reshape.320, f32[16,2048,1]{2,1,0} %broadcast.313), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.310 = f32[] constant(1e-05)
  %broadcast.311 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.310), dimensions={}
  %add.322 = f32[16,2048,1]{2,1,0} add(f32[16,2048,1]{2,1,0} %divide.321, f32[16,2048,1]{2,1,0} %broadcast.311), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/add" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %rsqrt.323 = f32[16,2048,1]{2,1,0} rsqrt(f32[16,2048,1]{2,1,0} %add.322), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/rsqrt" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.324 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %rsqrt.323), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.325 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.324), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.326 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.325), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.327 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.317, f32[16,2048,2560]{2,1,0} %broadcast.326), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %convert.328 = bf16[16,2048,2560]{2,1,0} convert(f32[16,2048,2560]{2,1,0} %multiply.327), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %Arg_10.305 = f32[2560]{0} parameter(10), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.329 = bf16[2560]{0} convert(f32[2560]{0} %Arg_10.305), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=50}
  %reshape.330 = bf16[1,1,2560]{2,1,0} reshape(bf16[2560]{0} %convert.329), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.331 = bf16[1,1,2560]{2,1,0} broadcast(bf16[1,1,2560]{2,1,0} %reshape.330), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.332 = bf16[2560]{0} reshape(bf16[1,1,2560]{2,1,0} %broadcast.331), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.333 = bf16[16,2048,2560]{2,1,0} broadcast(bf16[2560]{0} %reshape.332), dimensions={2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %multiply.334 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %convert.328, bf16[16,2048,2560]{2,1,0} %broadcast.333), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %custom-call.335 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %multiply.334), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_7.302 = f32[2560,8192]{1,0} parameter(7), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.336 = bf16[2560,8192]{1,0} convert(f32[2560,8192]{1,0} %Arg_7.302), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wi_0/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.337 = bf16[16,2048,8192]{2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.335, bf16[2560,8192]{1,0} %convert.336), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wi_0/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %reduce-precision.338 = bf16[16,2048,8192]{2,1,0} reduce-precision(bf16[16,2048,8192]{2,1,0} %dot.337), exponent_bits=8, mantissa_bits=7, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wi_0/reduce_precision" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %call.339 = bf16[16,2048,8192]{2,1,0} call(bf16[16,2048,8192]{2,1,0} %reduce-precision.338), to_apply=%silu.272
  %Arg_8.303 = f32[2560,8192]{1,0} parameter(8), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.340 = bf16[2560,8192]{1,0} convert(f32[2560,8192]{1,0} %Arg_8.303), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wi_1/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.341 = bf16[16,2048,8192]{2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.335, bf16[2560,8192]{1,0} %convert.340), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wi_1/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %reduce-precision.342 = bf16[16,2048,8192]{2,1,0} reduce-precision(bf16[16,2048,8192]{2,1,0} %dot.341), exponent_bits=8, mantissa_bits=7, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wi_1/reduce_precision" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %multiply.343 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %call.339, bf16[16,2048,8192]{2,1,0} %reduce-precision.342), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=264}
  %custom-call.344 = bf16[16,2048,8192]{2,1,0} custom-call(bf16[16,2048,8192]{2,1,0} %multiply.343), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_9.304 = f32[8192,2560]{1,0} parameter(9), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.345 = bf16[8192,2560]{1,0} convert(f32[8192,2560]{1,0} %Arg_9.304), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wo/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.346 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8192]{2,1,0} %custom-call.344, bf16[8192,2560]{1,0} %convert.345), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/mlp/wo/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.347 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %dot.346), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %custom-call.348 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %custom-call.335), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_14.309 = f32[2560,8,128]{2,1,0} parameter(14), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.349 = bf16[2560,8,128]{2,1,0} convert(f32[2560,8,128]{2,1,0} %Arg_14.309), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.kv_projection/value/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.350 = bf16[16,2048,8,128]{3,2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.348, bf16[2560,8,128]{2,1,0} %convert.349), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.kv_projection/value/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %reduce-precision.351 = bf16[16,2048,8,128]{3,2,1,0} reduce-precision(bf16[16,2048,8,128]{3,2,1,0} %dot.350), exponent_bits=8, mantissa_bits=7, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.kv_projection/value/reduce_precision" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.352 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.351), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_11.306 = f32[2560,8,128]{2,1,0} parameter(11), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.353 = bf16[2560,8,128]{2,1,0} convert(f32[2560,8,128]{2,1,0} %Arg_11.306), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.kv_projection/key/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.354 = bf16[16,2048,8,128]{3,2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.348, bf16[2560,8,128]{2,1,0} %convert.353), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.kv_projection/key/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %reduce-precision.355 = bf16[16,2048,8,128]{3,2,1,0} reduce-precision(bf16[16,2048,8,128]{3,2,1,0} %dot.354), exponent_bits=8, mantissa_bits=7, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.kv_projection/key/reduce_precision" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %slice.356 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.355), slice={[0:16], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_0.295 = bf16[16,2048,1,64]{3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %broadcast.358 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_0.295), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.359 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.358), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.360 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.359), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.361 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.356, bf16[16,2048,8,64]{3,2,1,0} %broadcast.360), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %slice.357 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.355), slice={[0:16], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_1.296 = bf16[16,2048,1,64]{3,2,1,0} parameter(1), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %broadcast.362 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_1.296), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.363 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.362), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.364 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.363), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.365 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.357, bf16[16,2048,8,64]{3,2,1,0} %broadcast.364), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %subtract.366 = bf16[16,2048,8,64]{3,2,1,0} subtract(bf16[16,2048,8,64]{3,2,1,0} %multiply.361, bf16[16,2048,8,64]{3,2,1,0} %multiply.365), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/sub" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.367 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_0.295), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.368 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.367), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.369 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.368), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.370 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.357, bf16[16,2048,8,64]{3,2,1,0} %broadcast.369), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.371 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_1.296), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.372 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.371), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.373 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.372), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.374 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.356, bf16[16,2048,8,64]{3,2,1,0} %broadcast.373), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %add.375 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.370, bf16[16,2048,8,64]{3,2,1,0} %multiply.374), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %concatenate.376 = bf16[16,2048,8,128]{3,2,1,0} concatenate(bf16[16,2048,8,64]{3,2,1,0} %subtract.366, bf16[16,2048,8,64]{3,2,1,0} %add.375), dimensions={3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/concatenate" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %custom-call.377 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %concatenate.376), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %custom-call.378 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %custom-call.335), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_13.308 = f32[2560,8,128]{2,1,0} parameter(13), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.379 = bf16[2560,8,128]{2,1,0} convert(f32[2560,8,128]{2,1,0} %Arg_13.308), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.query_projection/query/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.380 = bf16[16,2048,8,128]{3,2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.378, bf16[2560,8,128]{2,1,0} %convert.379), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.query_projection/query/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %reduce-precision.381 = bf16[16,2048,8,128]{3,2,1,0} reduce-precision(bf16[16,2048,8,128]{3,2,1,0} %dot.380), exponent_bits=8, mantissa_bits=7, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.query_projection/query/reduce_precision" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %slice.382 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.381), slice={[0:16], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_2.297 = bf16[16,2048,1,64]{3,2,1,0} parameter(2), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %broadcast.384 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_2.297), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.385 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.384), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.386 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.385), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.387 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.382, bf16[16,2048,8,64]{3,2,1,0} %broadcast.386), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %slice.383 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.381), slice={[0:16], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_3.298 = bf16[16,2048,1,64]{3,2,1,0} parameter(3), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %broadcast.388 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_3.298), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.389 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.388), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.390 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.389), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.391 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.383, bf16[16,2048,8,64]{3,2,1,0} %broadcast.390), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %subtract.392 = bf16[16,2048,8,64]{3,2,1,0} subtract(bf16[16,2048,8,64]{3,2,1,0} %multiply.387, bf16[16,2048,8,64]{3,2,1,0} %multiply.391), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/sub" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.393 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_2.297), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.394 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.393), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.395 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.394), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.396 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.383, bf16[16,2048,8,64]{3,2,1,0} %broadcast.395), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.397 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %Arg_3.298), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.398 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.397), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.399 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.398), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.400 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.382, bf16[16,2048,8,64]{3,2,1,0} %broadcast.399), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %add.401 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.396, bf16[16,2048,8,64]{3,2,1,0} %multiply.400), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %concatenate.402 = bf16[16,2048,8,128]{3,2,1,0} concatenate(bf16[16,2048,8,64]{3,2,1,0} %subtract.392, bf16[16,2048,8,64]{3,2,1,0} %add.401), dimensions={3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/concatenate" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %custom-call.403 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %concatenate.402), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %reshape.404 = bf16[16,2048,8,1,128]{4,3,2,1,0} reshape(bf16[16,2048,8,128]{3,2,1,0} %custom-call.403), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=527}
  %dot.405 = bf16[16,8,2048,2048,1]{4,3,2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.377, bf16[16,2048,8,1,128]{4,3,2,1,0} %reshape.404), lhs_batch_dims={0,2}, lhs_contracting_dims={3}, rhs_batch_dims={0,2}, rhs_contracting_dims={4}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %transpose.406 = bf16[16,8,1,2048,2048]{2,3,4,1,0} transpose(bf16[16,8,2048,2048,1]{4,3,2,1,0} %dot.405), dimensions={0,1,4,3,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %Arg_4.299 = pred[16,8,1,2048,2048]{4,3,2,1,0} parameter(4), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %Arg_5.300 = bf16[16,8,1,2048,2048]{4,3,2,1,0} parameter(5), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %call.407 = bf16[16,8,1,2048,2048]{4,3,2,1,0} call(bf16[16,8,1,2048,2048]{2,3,4,1,0} %transpose.406, pred[16,8,1,2048,2048]{4,3,2,1,0} %Arg_4.299, bf16[16,8,1,2048,2048]{4,3,2,1,0} %Arg_5.300), to_apply=%_where_1.281
  %constant.314 = bf16[] constant(-inf)
  %reduce.408 = bf16[16,8,1,2048]{3,2,1,0} reduce(bf16[16,8,1,2048,2048]{4,3,2,1,0} %call.407, bf16[] %constant.314), dimensions={4}, to_apply=%region_2.286, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %reshape.409 = bf16[16,8,1,2048,1]{4,3,2,1,0} reshape(bf16[16,8,1,2048]{3,2,1,0} %reduce.408), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %broadcast.410 = bf16[16,8,1,2048,1]{4,3,2,1,0} broadcast(bf16[16,8,1,2048,1]{4,3,2,1,0} %reshape.409), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %reshape.411 = bf16[16,8,1,2048]{3,2,1,0} reshape(bf16[16,8,1,2048,1]{4,3,2,1,0} %broadcast.410), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %broadcast.412 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[16,8,1,2048]{3,2,1,0} %reshape.411), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %subtract.413 = bf16[16,8,1,2048,2048]{4,3,2,1,0} subtract(bf16[16,8,1,2048,2048]{4,3,2,1,0} %call.407, bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.412), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %exponential.414 = bf16[16,8,1,2048,2048]{4,3,2,1,0} exponential(bf16[16,8,1,2048,2048]{4,3,2,1,0} %subtract.413), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/exp" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %dot.415 = bf16[16,8,128,1,2048]{4,3,2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.352, bf16[16,8,1,2048,2048]{4,3,2,1,0} %exponential.414), lhs_batch_dims={0,2}, lhs_contracting_dims={1}, rhs_batch_dims={0,1}, rhs_contracting_dims={4}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %transpose.416 = bf16[16,2048,8,1,128]{1,3,4,2,0} transpose(bf16[16,8,128,1,2048]{4,3,2,1,0} %dot.415), dimensions={0,4,1,3,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %reshape.417 = bf16[16,2048,8,128]{3,2,1,0} reshape(bf16[16,2048,8,1,128]{1,3,4,2,0} %transpose.416), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=566}
  %convert.418 = f32[16,8,1,2048,2048]{4,3,2,1,0} convert(bf16[16,8,1,2048,2048]{4,3,2,1,0} %exponential.414), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %reduce.419 = f32[16,8,1,2048]{3,2,1,0} reduce(f32[16,8,1,2048,2048]{4,3,2,1,0} %convert.418, f32[] %constant.315), dimensions={4}, to_apply=%region_3.290, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %reshape.420 = f32[16,8,1,2048,1]{4,3,2,1,0} reshape(f32[16,8,1,2048]{3,2,1,0} %reduce.419), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %convert.421 = bf16[16,8,1,2048,1]{4,3,2,1,0} convert(f32[16,8,1,2048,1]{4,3,2,1,0} %reshape.420), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %transpose.422 = bf16[16,2048,8,1,1]{4,1,3,2,0} transpose(bf16[16,8,1,2048,1]{4,3,2,1,0} %convert.421), dimensions={0,3,1,2,4}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=454}
  %reshape.423 = bf16[16,2048,8,1]{3,2,1,0} reshape(bf16[16,2048,8,1,1]{4,1,3,2,0} %transpose.422), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=458}
  %broadcast.424 = bf16[16,2048,8,1]{3,2,1,0} broadcast(bf16[16,2048,8,1]{3,2,1,0} %reshape.423), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.425 = bf16[16,2048,8]{2,1,0} reshape(bf16[16,2048,8,1]{3,2,1,0} %broadcast.424), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %broadcast.426 = bf16[16,2048,8,128]{3,2,1,0} broadcast(bf16[16,2048,8]{2,1,0} %reshape.425), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %divide.427 = bf16[16,2048,8,128]{3,2,1,0} divide(bf16[16,2048,8,128]{3,2,1,0} %reshape.417, bf16[16,2048,8,128]{3,2,1,0} %broadcast.426), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %custom-call.428 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %divide.427), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_12.307 = f32[8,128,2560]{2,1,0} parameter(12), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/closed_call"}
  %convert.429 = bf16[8,128,2560]{2,1,0} convert(f32[8,128,2560]{2,1,0} %Arg_12.307), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.out_projection/out/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.430 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.428, bf16[8,128,2560]{2,1,0} %convert.429), lhs_contracting_dims={2,3}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/self_attention/self_attention.out_projection/out/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.431 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %dot.430), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %add.432 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %custom-call.347, bf16[16,2048,2560]{2,1,0} %custom-call.431), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/add" source_file="/opt/maxtext/MaxText/layers/models.py" source_line=127}
  %add.433 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %add.432, bf16[16,2048,2560]{2,1,0} %custom-call.316), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/add" source_file="/opt/maxtext/MaxText/layers/models.py" source_line=133}
  %custom-call.434 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %add.433), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  ROOT %tuple.435 = (bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) tuple(bf16[16,2048,2560]{2,1,0} %custom-call.434, bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.381, bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.355, bf16[16,2048,8,128]{3,2,1,0} %reduce-precision.351, bf16[16,2048,8192]{2,1,0} %reduce-precision.338, /*index=5*/bf16[16,2048,8192]{2,1,0} %reduce-precision.342, bf16[16,2048,2560]{2,1,0} %Arg_6.301)
}

%region_0.436 (arg_tuple.437: (s32[], bf16[16,2048,2560], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], /*index=5*/bf16[8,16,2048,8192], bf16[8,16,2048,8192], bf16[8,16,2048,2560], f32[8,2560,8192], f32[8,2560,8192], /*index=10*/f32[8,8192,2560], f32[8,2560], f32[8,2560,8,128], f32[8,8,128,2560], f32[8,2560,8,128], /*index=15*/f32[8,2560,8,128], bf16[16,2048,1,64], bf16[16,2048,1,64], bf16[16,2048,1,64], bf16[16,2048,1,64], /*index=20*/pred[16,8,1,2048,2048], bf16[16,8,1,2048,2048])) -> (s32[], bf16[16,2048,2560], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], /*index=5*/bf16[8,16,2048,8192], bf16[8,16,2048,8192], bf16[8,16,2048,2560], f32[8,2560,8192], f32[8,2560,8192], /*index=10*/f32[8,8192,2560], f32[8,2560], f32[8,2560,8,128], f32[8,8,128,2560], f32[8,2560,8,128], /*index=15*/f32[8,2560,8,128], bf16[16,2048,1,64], bf16[16,2048,1,64], bf16[16,2048,1,64], bf16[16,2048,1,64], /*index=20*/pred[16,8,1,2048,2048], bf16[16,8,1,2048,2048]) {
  %arg_tuple.437 = (s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) parameter(0)
  %get-tuple-element.438 = s32[] get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=0
  %constant.460 = s32[] constant(1)
  %add.541 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.460), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.454 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=16
  %get-tuple-element.455 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=17
  %get-tuple-element.456 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=18
  %get-tuple-element.457 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=19
  %get-tuple-element.458 = pred[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=20
  %get-tuple-element.459 = bf16[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=21
  %get-tuple-element.439 = bf16[16,2048,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=1
  %get-tuple-element.446 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=8
  %constant.462 = s32[] constant(0)
  %compare.463 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %constant.461 = s32[] constant(8)
  %add.464 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.465 = s32[] select(pred[] %compare.463, s32[] %add.464, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.466 = f32[1,2560,8192]{2,1,0} dynamic-slice(f32[8,2560,8192]{2,1,0} %get-tuple-element.446, s32[] %select.465, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,2560,8192}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.467 = f32[2560,8192]{1,0} reshape(f32[1,2560,8192]{2,1,0} %dynamic-slice.466), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.447 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=9
  %compare.468 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.469 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.470 = s32[] select(pred[] %compare.468, s32[] %add.469, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.471 = f32[1,2560,8192]{2,1,0} dynamic-slice(f32[8,2560,8192]{2,1,0} %get-tuple-element.447, s32[] %select.470, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,2560,8192}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.472 = f32[2560,8192]{1,0} reshape(f32[1,2560,8192]{2,1,0} %dynamic-slice.471), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.448 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=10
  %compare.473 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.474 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.475 = s32[] select(pred[] %compare.473, s32[] %add.474, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.476 = f32[1,8192,2560]{2,1,0} dynamic-slice(f32[8,8192,2560]{2,1,0} %get-tuple-element.448, s32[] %select.475, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,8192,2560}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.477 = f32[8192,2560]{1,0} reshape(f32[1,8192,2560]{2,1,0} %dynamic-slice.476), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.449 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=11
  %compare.478 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.479 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.480 = s32[] select(pred[] %compare.478, s32[] %add.479, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.481 = f32[1,2560]{1,0} dynamic-slice(f32[8,2560]{1,0} %get-tuple-element.449, s32[] %select.480, s32[] %constant.462), dynamic_slice_sizes={1,2560}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.482 = f32[2560]{0} reshape(f32[1,2560]{1,0} %dynamic-slice.481), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.450 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=12
  %compare.483 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.484 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.485 = s32[] select(pred[] %compare.483, s32[] %add.484, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.486 = f32[1,2560,8,128]{3,2,1,0} dynamic-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.450, s32[] %select.485, s32[] %constant.462, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,2560,8,128}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.487 = f32[2560,8,128]{2,1,0} reshape(f32[1,2560,8,128]{3,2,1,0} %dynamic-slice.486), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.451 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=13
  %compare.488 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.489 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.490 = s32[] select(pred[] %compare.488, s32[] %add.489, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.491 = f32[1,8,128,2560]{3,2,1,0} dynamic-slice(f32[8,8,128,2560]{3,2,1,0} %get-tuple-element.451, s32[] %select.490, s32[] %constant.462, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,8,128,2560}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.492 = f32[8,128,2560]{2,1,0} reshape(f32[1,8,128,2560]{3,2,1,0} %dynamic-slice.491), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.452 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=14
  %compare.493 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.494 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.495 = s32[] select(pred[] %compare.493, s32[] %add.494, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.496 = f32[1,2560,8,128]{3,2,1,0} dynamic-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.452, s32[] %select.495, s32[] %constant.462, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,2560,8,128}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.497 = f32[2560,8,128]{2,1,0} reshape(f32[1,2560,8,128]{3,2,1,0} %dynamic-slice.496), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.453 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=15
  %compare.498 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.499 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.500 = s32[] select(pred[] %compare.498, s32[] %add.499, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.501 = f32[1,2560,8,128]{3,2,1,0} dynamic-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.453, s32[] %select.500, s32[] %constant.462, s32[] %constant.462, s32[] %constant.462), dynamic_slice_sizes={1,2560,8,128}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.502 = f32[2560,8,128]{2,1,0} reshape(f32[1,2560,8,128]{3,2,1,0} %dynamic-slice.501), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %call.503 = (bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) call(bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.454, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.455, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.456, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.457, pred[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.458, /*index=5*/bf16[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.459, bf16[16,2048,2560]{2,1,0} %get-tuple-element.439, f32[2560,8192]{1,0} %reshape.467, f32[2560,8192]{1,0} %reshape.472, f32[8192,2560]{1,0} %reshape.477, /*index=10*/f32[2560]{0} %reshape.482, f32[2560,8,128]{2,1,0} %reshape.487, f32[8,128,2560]{2,1,0} %reshape.492, f32[2560,8,128]{2,1,0} %reshape.497, f32[2560,8,128]{2,1,0} %reshape.502), to_apply=%None.294
  %get-tuple-element.504 = bf16[16,2048,2560]{2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=0
  %get-tuple-element.440 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=2
  %get-tuple-element.505 = bf16[16,2048,8,128]{3,2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=1
  %reshape.511 = bf16[1,16,2048,8,128]{4,3,2,1,0} reshape(bf16[16,2048,8,128]{3,2,1,0} %get-tuple-element.505), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.512 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.513 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.514 = s32[] select(pred[] %compare.512, s32[] %add.513, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.515 = bf16[8,16,2048,8,128]{4,3,2,1,0} dynamic-update-slice(bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.440, bf16[1,16,2048,8,128]{4,3,2,1,0} %reshape.511, s32[] %select.514, s32[] %constant.462, s32[] %constant.462, /*index=5*/s32[] %constant.462, s32[] %constant.462), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.441 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=3
  %get-tuple-element.506 = bf16[16,2048,8,128]{3,2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=2
  %reshape.516 = bf16[1,16,2048,8,128]{4,3,2,1,0} reshape(bf16[16,2048,8,128]{3,2,1,0} %get-tuple-element.506), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.517 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.518 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.519 = s32[] select(pred[] %compare.517, s32[] %add.518, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.520 = bf16[8,16,2048,8,128]{4,3,2,1,0} dynamic-update-slice(bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.441, bf16[1,16,2048,8,128]{4,3,2,1,0} %reshape.516, s32[] %select.519, s32[] %constant.462, s32[] %constant.462, /*index=5*/s32[] %constant.462, s32[] %constant.462), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.442 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=4
  %get-tuple-element.507 = bf16[16,2048,8,128]{3,2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=3
  %reshape.521 = bf16[1,16,2048,8,128]{4,3,2,1,0} reshape(bf16[16,2048,8,128]{3,2,1,0} %get-tuple-element.507), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.522 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.523 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.524 = s32[] select(pred[] %compare.522, s32[] %add.523, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.525 = bf16[8,16,2048,8,128]{4,3,2,1,0} dynamic-update-slice(bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.442, bf16[1,16,2048,8,128]{4,3,2,1,0} %reshape.521, s32[] %select.524, s32[] %constant.462, s32[] %constant.462, /*index=5*/s32[] %constant.462, s32[] %constant.462), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.443 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=5
  %get-tuple-element.508 = bf16[16,2048,8192]{2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=4
  %reshape.526 = bf16[1,16,2048,8192]{3,2,1,0} reshape(bf16[16,2048,8192]{2,1,0} %get-tuple-element.508), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.527 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.528 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.529 = s32[] select(pred[] %compare.527, s32[] %add.528, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.530 = bf16[8,16,2048,8192]{3,2,1,0} dynamic-update-slice(bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.443, bf16[1,16,2048,8192]{3,2,1,0} %reshape.526, s32[] %select.529, s32[] %constant.462, s32[] %constant.462, /*index=5*/s32[] %constant.462), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.444 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=6
  %get-tuple-element.509 = bf16[16,2048,8192]{2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=5
  %reshape.531 = bf16[1,16,2048,8192]{3,2,1,0} reshape(bf16[16,2048,8192]{2,1,0} %get-tuple-element.509), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.532 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.533 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.534 = s32[] select(pred[] %compare.532, s32[] %add.533, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.535 = bf16[8,16,2048,8192]{3,2,1,0} dynamic-update-slice(bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.444, bf16[1,16,2048,8192]{3,2,1,0} %reshape.531, s32[] %select.534, s32[] %constant.462, s32[] %constant.462, /*index=5*/s32[] %constant.462), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.445 = bf16[8,16,2048,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.437), index=7
  %get-tuple-element.510 = bf16[16,2048,2560]{2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8,128]{3,2,1,0}, bf16[16,2048,8192]{2,1,0}, /*index=5*/bf16[16,2048,8192]{2,1,0}, bf16[16,2048,2560]{2,1,0}) %call.503), index=6
  %reshape.536 = bf16[1,16,2048,2560]{3,2,1,0} reshape(bf16[16,2048,2560]{2,1,0} %get-tuple-element.510), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.537 = pred[] compare(s32[] %get-tuple-element.438, s32[] %constant.462), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.538 = s32[] add(s32[] %get-tuple-element.438, s32[] %constant.461), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.539 = s32[] select(pred[] %compare.537, s32[] %add.538, s32[] %get-tuple-element.438), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.540 = bf16[8,16,2048,2560]{3,2,1,0} dynamic-update-slice(bf16[8,16,2048,2560]{3,2,1,0} %get-tuple-element.445, bf16[1,16,2048,2560]{3,2,1,0} %reshape.536, s32[] %select.539, s32[] %constant.462, s32[] %constant.462, /*index=5*/s32[] %constant.462), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  ROOT %tuple.542 = (s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) tuple(s32[] %add.541, bf16[16,2048,2560]{2,1,0} %get-tuple-element.504, bf16[8,16,2048,8,128]{4,3,2,1,0} %dynamic-update-slice.515, bf16[8,16,2048,8,128]{4,3,2,1,0} %dynamic-update-slice.520, bf16[8,16,2048,8,128]{4,3,2,1,0} %dynamic-update-slice.525, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0} %dynamic-update-slice.530, bf16[8,16,2048,8192]{3,2,1,0} %dynamic-update-slice.535, bf16[8,16,2048,2560]{3,2,1,0} %dynamic-update-slice.540, f32[8,2560,8192]{2,1,0} %get-tuple-element.446, f32[8,2560,8192]{2,1,0} %get-tuple-element.447, /*index=10*/f32[8,8192,2560]{2,1,0} %get-tuple-element.448, f32[8,2560]{1,0} %get-tuple-element.449, f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.450, f32[8,8,128,2560]{3,2,1,0} %get-tuple-element.451, f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.452, /*index=15*/f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.453, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.454, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.455, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.456, bf16[16,2048,1,64]{3,2,1,0} %get-tuple-element.457, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.458, bf16[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.459)
}

%region_4.543 (arg_tuple.544: (s32[], bf16[16,2048,2560], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], /*index=5*/bf16[8,16,2048,8192], bf16[8,16,2048,8192], bf16[8,16,2048,2560], f32[8,2560,8192], f32[8,2560,8192], /*index=10*/f32[8,8192,2560], f32[8,2560], f32[8,2560,8,128], f32[8,8,128,2560], f32[8,2560,8,128], /*index=15*/f32[8,2560,8,128], bf16[16,2048,1,64], bf16[16,2048,1,64], bf16[16,2048,1,64], bf16[16,2048,1,64], /*index=20*/pred[16,8,1,2048,2048], bf16[16,8,1,2048,2048])) -> pred[] {
  %arg_tuple.544 = (s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) parameter(0)
  %get-tuple-element.546 = bf16[16,2048,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=1
  %get-tuple-element.547 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=2
  %get-tuple-element.548 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=3
  %get-tuple-element.549 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=4
  %get-tuple-element.550 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=5
  %get-tuple-element.551 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=6
  %get-tuple-element.552 = bf16[8,16,2048,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=7
  %get-tuple-element.553 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=8
  %get-tuple-element.554 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=9
  %get-tuple-element.555 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=10
  %get-tuple-element.556 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=11
  %get-tuple-element.557 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=12
  %get-tuple-element.558 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=13
  %get-tuple-element.559 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=14
  %get-tuple-element.560 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=15
  %get-tuple-element.561 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=16
  %get-tuple-element.562 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=17
  %get-tuple-element.563 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=18
  %get-tuple-element.564 = bf16[16,2048,1,64]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=19
  %get-tuple-element.565 = pred[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=20
  %get-tuple-element.566 = bf16[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=21
  %get-tuple-element.545 = s32[] get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, /*index=10*/f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=15*/f32[8,2560,8,128]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %arg_tuple.544), index=0
  %constant.567 = s32[] constant(8)
  ROOT %compare.568 = pred[] compare(s32[] %get-tuple-element.545, s32[] %constant.567), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while/cond/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
}

%region_5.581 (Arg_0.582: f32[], Arg_1.583: f32[]) -> f32[] {
  %Arg_0.582 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/reduce_sum"}
  %Arg_1.583 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/reduce_sum"}
  ROOT %add.584 = f32[] add(f32[] %Arg_0.582, f32[] %Arg_1.583), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
}

%_one_hot.609 (Arg_0.610: s32[16,2048]) -> f32[16,2048,32000] {
  %Arg_0.610 = s32[16,2048]{1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %reshape.611 = s32[16,2048,1]{2,1,0} reshape(s32[16,2048]{1,0} %Arg_0.610), metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/broadcast_in_dim" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %broadcast.614 = s32[16,2048,1]{2,1,0} broadcast(s32[16,2048,1]{2,1,0} %reshape.611), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %reshape.615 = s32[16,2048]{1,0} reshape(s32[16,2048,1]{2,1,0} %broadcast.614), metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %broadcast.616 = s32[16,2048,32000]{2,1,0} broadcast(s32[16,2048]{1,0} %reshape.615), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %iota.612 = s32[32000]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/iota" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %reshape.613 = s32[1,1,32000]{2,1,0} reshape(s32[32000]{0} %iota.612), metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/iota" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %broadcast.617 = s32[1,1,32000]{2,1,0} broadcast(s32[1,1,32000]{2,1,0} %reshape.613), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %reshape.618 = s32[32000]{0} reshape(s32[1,1,32000]{2,1,0} %broadcast.617), metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %broadcast.619 = s32[16,2048,32000]{2,1,0} broadcast(s32[32000]{0} %reshape.618), dimensions={2}, metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  %compare.620 = pred[16,2048,32000]{2,1,0} compare(s32[16,2048,32000]{2,1,0} %broadcast.616, s32[16,2048,32000]{2,1,0} %broadcast.619), direction=EQ, metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/eq" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
  ROOT %convert.621 = f32[16,2048,32000]{2,1,0} convert(pred[16,2048,32000]{2,1,0} %compare.620), metadata={op_name="jit(train_step)/jit(main)/jvp(jit(_one_hot))/convert_element_type" source_file="/opt/maxtext/MaxText/train.py" source_line=431}
}

%region_6.623 (Arg_0.624: f32[], Arg_1.625: f32[]) -> f32[] {
  %Arg_0.624 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_max"}
  %Arg_1.625 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_max"}
  ROOT %maximum.626 = f32[] maximum(f32[] %Arg_0.624, f32[] %Arg_1.625), metadata={op_name="jit(train_step)/jit(main)/reduce_max" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=876}
}

%region_7.634 (Arg_0.635: f32[], Arg_1.636: f32[]) -> f32[] {
  %Arg_0.635 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.636 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.637 = f32[] add(f32[] %Arg_0.635, f32[] %Arg_1.636), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=879}
}

%region_8.646 (Arg_0.647: f32[], Arg_1.648: f32[]) -> f32[] {
  %Arg_0.647 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.648 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.649 = f32[] add(f32[] %Arg_0.647, f32[] %Arg_1.648), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=881}
}

%region_9.662 (Arg_0.663: f32[], Arg_1.664: f32[]) -> f32[] {
  %Arg_0.663 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.664 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.665 = f32[] add(f32[] %Arg_0.663, f32[] %Arg_1.664), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/train.py" source_line=436}
}

%region_10.669 (Arg_0.670: s32[], Arg_1.671: s32[]) -> s32[] {
  %Arg_0.670 = s32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.671 = s32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.672 = s32[] add(s32[] %Arg_0.670, s32[] %Arg_1.671), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/train.py" source_line=437}
}

%region_11.705 (Arg_0.706: bf16[], Arg_1.707: bf16[]) -> bf16[] {
  %Arg_0.706 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  %Arg_1.707 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  ROOT %add.708 = bf16[] add(bf16[] %Arg_0.706, bf16[] %Arg_1.707), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
}

%region_12.711 (Arg_0.712: bf16[], Arg_1.713: bf16[]) -> bf16[] {
  %Arg_0.712 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  %Arg_1.713 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  ROOT %add.714 = bf16[] add(bf16[] %Arg_0.712, bf16[] %Arg_1.713), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
}

%region_13.723 (Arg_0.724: f32[], Arg_1.725: f32[]) -> f32[] {
  %Arg_0.724 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  %Arg_1.725 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  ROOT %add.726 = f32[] add(f32[] %Arg_0.724, f32[] %Arg_1.725), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
}

%region_14.735 (Arg_0.736: f32[], Arg_1.737: f32[]) -> f32[] {
  %Arg_0.736 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  %Arg_1.737 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum"}
  ROOT %add.738 = f32[] add(f32[] %Arg_0.736, f32[] %Arg_1.737), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
}

%region_16.745 (Arg_0.746: f32[], Arg_1.747: f32[]) -> f32[] {
  %Arg_0.746 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/reduce_sum"}
  %Arg_1.747 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/reduce_sum"}
  ROOT %add.748 = f32[] add(f32[] %Arg_0.746, f32[] %Arg_1.747), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
}

%_where_3.749 (Arg_0.750: pred[16,1,1,2048,2048], Arg_1.751: f32[], Arg_2.752: f32[]) -> f32[16,1,1,2048,2048] {
  %Arg_0.750 = pred[16,1,1,2048,2048]{4,3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/pjit"}
  %Arg_1.751 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/pjit"}
  %broadcast.753 = f32[16,1,1,2048,2048]{4,3,2,1,0} broadcast(f32[] %Arg_1.751), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=211}
  %Arg_2.752 = f32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/pjit"}
  %broadcast.754 = f32[16,1,1,2048,2048]{4,3,2,1,0} broadcast(f32[] %Arg_2.752), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=211}
  ROOT %select.755 = f32[16,1,1,2048,2048]{4,3,2,1,0} select(pred[16,1,1,2048,2048]{4,3,2,1,0} %Arg_0.750, f32[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.753, f32[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.754), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/jit(_where)/select_n" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=211}
}

%_where_4.756 (Arg_0.757: pred[16,1,1,2048,2048], Arg_1.758: bf16[16,8,1,2048,2048], Arg_2.759: f32[]) -> (bf16[16,8,1,2048,2048], pred[16,8,1,2048,2048]) {
  %Arg_0.757 = pred[16,1,1,2048,2048]{4,3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %broadcast.761 = pred[16,1,1,2048,2048]{4,3,2,1,0} broadcast(pred[16,1,1,2048,2048]{4,3,2,1,0} %Arg_0.757), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %reshape.762 = pred[16,1,2048,2048]{3,2,1,0} reshape(pred[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.761), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %broadcast.763 = pred[16,8,1,2048,2048]{4,3,2,1,0} broadcast(pred[16,1,2048,2048]{3,2,1,0} %reshape.762), dimensions={0,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %Arg_1.758 = bf16[16,8,1,2048,2048]{4,3,2,1,0} parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %Arg_2.759 = f32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %convert.760 = bf16[] convert(f32[] %Arg_2.759), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %broadcast.764 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[] %convert.760), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %select.765 = bf16[16,8,1,2048,2048]{4,3,2,1,0} select(pred[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.763, bf16[16,8,1,2048,2048]{4,3,2,1,0} %Arg_1.758, bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.764), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/select_n" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  ROOT %tuple.766 = (bf16[16,8,1,2048,2048]{4,3,2,1,0}, pred[16,8,1,2048,2048]{4,3,2,1,0}) tuple(bf16[16,8,1,2048,2048]{4,3,2,1,0} %select.765, pred[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.763)
}

%region_17.767 (Arg_0.768: bf16[], Arg_1.769: bf16[]) -> bf16[] {
  %Arg_0.768 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max"}
  %Arg_1.769 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max"}
  ROOT %maximum.770 = bf16[] maximum(bf16[] %Arg_0.768, bf16[] %Arg_1.769), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
}

%region_18.771 (Arg_0.772: bf16[], Arg_1.773: bf16[]) -> bf16[] {
  %Arg_0.772 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  %Arg_1.773 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  ROOT %add.774 = bf16[] add(bf16[] %Arg_0.772, bf16[] %Arg_1.773), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
}

%region_19.775 (Arg_0.776: f32[], Arg_1.777: f32[]) -> f32[] {
  %Arg_0.776 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  %Arg_1.777 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  ROOT %add.778 = f32[] add(f32[] %Arg_0.776, f32[] %Arg_1.777), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
}

%silu_5.779 (Arg_0.780: bf16[16,2048,8192]) -> (bf16[16,2048,8192], bf16[16,2048,8192], bf16[16,2048,8192]) {
  %Arg_0.780 = bf16[16,2048,8192]{2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/pjit"}
  %constant.781 = bf16[] constant(1)
  %broadcast.782 = bf16[16,2048,8192]{2,1,0} broadcast(bf16[] %constant.781), dimensions={}
  %negate.783 = bf16[16,2048,8192]{2,1,0} negate(bf16[16,2048,8192]{2,1,0} %Arg_0.780), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/neg" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %exponential.784 = bf16[16,2048,8192]{2,1,0} exponential(bf16[16,2048,8192]{2,1,0} %negate.783), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/exp" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %add.785 = bf16[16,2048,8192]{2,1,0} add(bf16[16,2048,8192]{2,1,0} %exponential.784, bf16[16,2048,8192]{2,1,0} %broadcast.782), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/add" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %divide.786 = bf16[16,2048,8192]{2,1,0} divide(bf16[16,2048,8192]{2,1,0} %broadcast.782, bf16[16,2048,8192]{2,1,0} %add.785), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/div" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %multiply.789 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %Arg_0.780, bf16[16,2048,8192]{2,1,0} %divide.786), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %subtract.787 = bf16[16,2048,8192]{2,1,0} subtract(bf16[16,2048,8192]{2,1,0} %broadcast.782, bf16[16,2048,8192]{2,1,0} %divide.786), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/sub" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %multiply.788 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %divide.786, bf16[16,2048,8192]{2,1,0} %subtract.787), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  ROOT %tuple.790 = (bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}) tuple(bf16[16,2048,8192]{2,1,0} %multiply.789, bf16[16,2048,8192]{2,1,0} %divide.786, bf16[16,2048,8192]{2,1,0} %multiply.788)
}

%region_20.791 (Arg_0.792: bf16[], Arg_1.793: bf16[]) -> bf16[] {
  %Arg_0.792 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/reduce_sum"}
  %Arg_1.793 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/reduce_sum"}
  ROOT %add.794 = bf16[] add(bf16[] %Arg_0.792, bf16[] %Arg_1.793), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
}

%region_21.795 (Arg_0.796: f32[], Arg_1.797: f32[]) -> f32[] {
  %Arg_0.796 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  %Arg_1.797 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  ROOT %add.798 = f32[] add(f32[] %Arg_0.796, f32[] %Arg_1.797), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
}

%region_22.799 (Arg_0.800: bf16[], Arg_1.801: bf16[]) -> bf16[] {
  %Arg_0.800 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  %Arg_1.801 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  ROOT %add.802 = bf16[] add(bf16[] %Arg_0.800, bf16[] %Arg_1.801), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
}

%region_23.803 (Arg_0.804: bf16[], Arg_1.805: bf16[]) -> bf16[] {
  %Arg_0.804 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  %Arg_1.805 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum"}
  ROOT %add.806 = bf16[] add(bf16[] %Arg_0.804, bf16[] %Arg_1.805), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
}

%_where_6.807 (Arg_0.808: pred[16,8,1,2048,2048], Arg_1.809: bf16[16,8,1,2048,2048]) -> bf16[16,8,1,2048,2048] {
  %Arg_0.808 = pred[16,8,1,2048,2048]{4,3,2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %Arg_1.809 = bf16[16,8,1,2048,2048]{4,3,2,1,0} parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/pjit"}
  %constant.810 = bf16[] constant(0)
  %broadcast.811 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[] %constant.810), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  ROOT %select.812 = bf16[16,8,1,2048,2048]{4,3,2,1,0} select(pred[16,8,1,2048,2048]{4,3,2,1,0} %Arg_0.808, bf16[16,8,1,2048,2048]{4,3,2,1,0} %Arg_1.809, bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.811), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/jit(_where)/select_n" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
}

%silu_7.813 (Arg_0.814: bf16[16,2048,8192], Arg_1.815: bf16[16,2048,8192], Arg_2.816: bf16[16,2048,8192], Arg_3.817: bf16[16,2048,8192]) -> bf16[16,2048,8192] {
  %Arg_1.815 = bf16[16,2048,8192]{2,1,0} parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/pjit"}
  %Arg_3.817 = bf16[16,2048,8192]{2,1,0} parameter(3), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/pjit"}
  %multiply.818 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %Arg_1.815, bf16[16,2048,8192]{2,1,0} %Arg_3.817), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %Arg_2.816 = bf16[16,2048,8192]{2,1,0} parameter(2), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/pjit"}
  %multiply.819 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %multiply.818, bf16[16,2048,8192]{2,1,0} %Arg_2.816), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  %Arg_0.814 = bf16[16,2048,8192]{2,1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/pjit"}
  %multiply.820 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %Arg_3.817, bf16[16,2048,8192]{2,1,0} %Arg_0.814), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
  ROOT %add.821 = bf16[16,2048,8192]{2,1,0} add(bf16[16,2048,8192]{2,1,0} %multiply.819, bf16[16,2048,8192]{2,1,0} %multiply.820), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/jit(silu)/add_any" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=260}
}

%region_24.822 (Arg_0.823: bf16[], Arg_1.824: bf16[]) -> bf16[] {
  %Arg_0.823 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  %Arg_1.824 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  ROOT %add.825 = bf16[] add(bf16[] %Arg_0.823, bf16[] %Arg_1.824), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
}

%region_25.826 (Arg_0.827: bf16[], Arg_1.828: bf16[]) -> bf16[] {
  %Arg_0.827 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  %Arg_1.828 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  ROOT %add.829 = bf16[] add(bf16[] %Arg_0.827, bf16[] %Arg_1.828), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
}

%region_26.830 (Arg_0.831: f32[], Arg_1.832: f32[]) -> f32[] {
  %Arg_0.831 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  %Arg_1.832 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  ROOT %add.833 = f32[] add(f32[] %Arg_0.831, f32[] %Arg_1.832), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
}

%region_27.834 (Arg_0.835: f32[], Arg_1.836: f32[]) -> f32[] {
  %Arg_0.835 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  %Arg_1.836 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum"}
  ROOT %add.837 = f32[] add(f32[] %Arg_0.835, f32[] %Arg_1.836), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
}

%None_2.838 (Arg_0.839: s32[16,2048], Arg_1.840: s32[16,2048], Arg_2.841: bf16[16,2048,2560], Arg_3.842: bf16[16,2048,8,128], Arg_4.843: bf16[16,2048,8,128], Arg_5.844: bf16[16,2048,8,128], Arg_6.845: bf16[16,2048,8192], Arg_7.846: bf16[16,2048,8192], Arg_8.847: f32[2560,8192], Arg_9.848: f32[2560,8192], Arg_10.849: f32[8192,2560], Arg_11.850: f32[2560], Arg_12.851: f32[2560,8,128], Arg_13.852: f32[8,128,2560], Arg_14.853: f32[2560,8,128], Arg_15.854: f32[2560,8,128], Arg_16.855: bf16[16,2048,2560]) -> (bf16[16,2048,2560], f32[2560,8192], f32[2560,8192], f32[8192,2560], f32[2560], /*index=5*/f32[2560,8,128], f32[8,128,2560], f32[2560,8,128], f32[2560,8,128]) {
  %Arg_2.841 = bf16[16,2048,2560]{2,1,0} parameter(2), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %custom-call.1057 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %Arg_2.841), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_16.855 = bf16[16,2048,2560]{2,1,0} parameter(16), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %custom-call.878 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %Arg_16.855), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %convert.879 = f32[16,2048,2560]{2,1,0} convert(bf16[16,2048,2560]{2,1,0} %custom-call.878), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=39}
  %Arg_0.839 = s32[16,2048]{1,0} parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %reshape.992 = s32[16,2048,1]{2,1,0} reshape(s32[16,2048]{1,0} %Arg_0.839), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.994 = s32[16,2048,1]{2,1,0} broadcast(s32[16,2048,1]{2,1,0} %reshape.992), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.995 = s32[16,2048]{1,0} reshape(s32[16,2048,1]{2,1,0} %broadcast.994), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.996 = s32[16,2048,2048]{2,1,0} broadcast(s32[16,2048]{1,0} %reshape.995), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.993 = s32[16,1,2048]{2,1,0} reshape(s32[16,2048]{1,0} %Arg_0.839), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.997 = s32[16,1,2048]{2,1,0} broadcast(s32[16,1,2048]{2,1,0} %reshape.993), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.998 = s32[16,2048]{1,0} reshape(s32[16,1,2048]{2,1,0} %broadcast.997), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.999 = s32[16,2048,2048]{2,1,0} broadcast(s32[16,2048]{1,0} %reshape.998), dimensions={0,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %compare.1000 = pred[16,2048,2048]{2,1,0} compare(s32[16,2048,2048]{2,1,0} %broadcast.996, s32[16,2048,2048]{2,1,0} %broadcast.999), direction=EQ, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.1001 = pred[16,1,1,2048,2048]{4,3,2,1,0} reshape(pred[16,2048,2048]{2,1,0} %compare.1000), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=182}
  %iota.1004 = s32[2048]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=191}
  %broadcast.1005 = s32[2048,2048]{1,0} broadcast(s32[2048]{0} %iota.1004), dimensions={1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=191}
  %iota.1002 = s32[2048]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=190}
  %broadcast.1003 = s32[2048,2048]{1,0} broadcast(s32[2048]{0} %iota.1002), dimensions={0}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=190}
  %compare.1006 = pred[2048,2048]{1,0} compare(s32[2048,2048]{1,0} %broadcast.1005, s32[2048,2048]{1,0} %broadcast.1003), direction=LE, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/le" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=192}
  %reshape.1007 = pred[1,1,1,2048,2048]{4,3,2,1,0} reshape(pred[2048,2048]{1,0} %compare.1006), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=192}
  %broadcast.1008 = pred[1,1,1,2048,2048]{4,3,2,1,0} broadcast(pred[1,1,1,2048,2048]{4,3,2,1,0} %reshape.1007), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %reshape.1009 = pred[1,1,2048,2048]{3,2,1,0} reshape(pred[1,1,1,2048,2048]{4,3,2,1,0} %broadcast.1008), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %broadcast.1010 = pred[16,1,1,2048,2048]{4,3,2,1,0} broadcast(pred[1,1,2048,2048]{3,2,1,0} %reshape.1009), dimensions={1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %and.1011 = pred[16,1,1,2048,2048]{4,3,2,1,0} and(pred[16,1,1,2048,2048]{4,3,2,1,0} %reshape.1001, pred[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.1010), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %constant.877 = f32[] constant(0)
  %constant.876 = f32[] constant(-2.38197633e+38)
  %call.1012 = f32[16,1,1,2048,2048]{4,3,2,1,0} call(pred[16,1,1,2048,2048]{4,3,2,1,0} %and.1011, f32[] %constant.877, f32[] %constant.876), to_apply=%_where_3.749
  %constant.858 = f32[] constant(-1.19098816e+38)
  %broadcast.859 = f32[16,1,1,2048,2048]{4,3,2,1,0} broadcast(f32[] %constant.858), dimensions={}
  %compare.1013 = pred[16,1,1,2048,2048]{4,3,2,1,0} compare(f32[16,1,1,2048,2048]{4,3,2,1,0} %call.1012, f32[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.859), direction=GE, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/ge" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %Arg_4.843 = bf16[16,2048,8,128]{3,2,1,0} parameter(4), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %slice.965 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %Arg_4.843), slice={[0:16], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_1.840 = s32[16,2048]{1,0} parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %reshape.951 = s32[16,2048,1,1]{3,2,1,0} reshape(s32[16,2048]{1,0} %Arg_1.840), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=163}
  %convert.952 = f32[16,2048,1,1]{3,2,1,0} convert(s32[16,2048,1,1]{3,2,1,0} %reshape.951), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.954 = f32[16,2048,1,1]{3,2,1,0} broadcast(f32[16,2048,1,1]{3,2,1,0} %convert.952), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.955 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048,1,1]{3,2,1,0} %broadcast.954), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.956 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.955), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %constant.860 = f32[] constant(10000)
  %broadcast.861 = f32[64]{0} broadcast(f32[] %constant.860), dimensions={}
  %iota.946 = s32[64]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/iota" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %constant.864 = s32[] constant(2)
  %broadcast.865 = s32[64]{0} broadcast(s32[] %constant.864), dimensions={}
  %multiply.947 = s32[64]{0} multiply(s32[64]{0} %iota.946, s32[64]{0} %broadcast.865), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %convert.948 = f32[64]{0} convert(s32[64]{0} %multiply.947), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %constant.862 = f32[] constant(128)
  %broadcast.863 = f32[64]{0} broadcast(f32[] %constant.862), dimensions={}
  %divide.949 = f32[64]{0} divide(f32[64]{0} %convert.948, f32[64]{0} %broadcast.863), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %power.950 = f32[64]{0} power(f32[64]{0} %broadcast.861, f32[64]{0} %divide.949), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/pow" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=134}
  %reshape.953 = f32[1,1,1,64]{3,2,1,0} reshape(f32[64]{0} %power.950), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.957 = f32[1,1,1,64]{3,2,1,0} broadcast(f32[1,1,1,64]{3,2,1,0} %reshape.953), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.958 = f32[1,64]{1,0} reshape(f32[1,1,1,64]{3,2,1,0} %broadcast.957), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.959 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[1,64]{1,0} %reshape.958), dimensions={2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %divide.960 = f32[16,2048,1,64]{3,2,1,0} divide(f32[16,2048,1,64]{3,2,1,0} %broadcast.956, f32[16,2048,1,64]{3,2,1,0} %broadcast.959), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %cosine.963 = f32[16,2048,1,64]{3,2,1,0} cosine(f32[16,2048,1,64]{3,2,1,0} %divide.960), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/cos" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %convert.964 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %cosine.963), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %broadcast.967 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.964), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.968 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.967), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.969 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.968), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.970 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.965, bf16[16,2048,8,64]{3,2,1,0} %broadcast.969), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %slice.966 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %Arg_4.843), slice={[0:16], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %sine.961 = f32[16,2048,1,64]{3,2,1,0} sine(f32[16,2048,1,64]{3,2,1,0} %divide.960), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/sin" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %convert.962 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %sine.961), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %broadcast.971 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.962), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.972 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.971), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.973 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.972), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.974 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.966, bf16[16,2048,8,64]{3,2,1,0} %broadcast.973), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %subtract.975 = bf16[16,2048,8,64]{3,2,1,0} subtract(bf16[16,2048,8,64]{3,2,1,0} %multiply.970, bf16[16,2048,8,64]{3,2,1,0} %multiply.974), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/sub" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.976 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.964), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.977 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.976), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.978 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.977), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.979 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.966, bf16[16,2048,8,64]{3,2,1,0} %broadcast.978), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.980 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.962), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.981 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.980), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.982 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.981), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.983 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.965, bf16[16,2048,8,64]{3,2,1,0} %broadcast.982), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %add.984 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.979, bf16[16,2048,8,64]{3,2,1,0} %multiply.983), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %concatenate.985 = bf16[16,2048,8,128]{3,2,1,0} concatenate(bf16[16,2048,8,64]{3,2,1,0} %subtract.975, bf16[16,2048,8,64]{3,2,1,0} %add.984), dimensions={3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/concatenate" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %custom-call.987 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %concatenate.985), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_3.842 = bf16[16,2048,8,128]{3,2,1,0} parameter(3), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %slice.925 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %Arg_3.842), slice={[0:16], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %reshape.911 = s32[16,2048,1,1]{3,2,1,0} reshape(s32[16,2048]{1,0} %Arg_1.840), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=163}
  %convert.912 = f32[16,2048,1,1]{3,2,1,0} convert(s32[16,2048,1,1]{3,2,1,0} %reshape.911), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.914 = f32[16,2048,1,1]{3,2,1,0} broadcast(f32[16,2048,1,1]{3,2,1,0} %convert.912), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.915 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048,1,1]{3,2,1,0} %broadcast.914), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.916 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.915), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %iota.906 = s32[64]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/iota" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %multiply.907 = s32[64]{0} multiply(s32[64]{0} %iota.906, s32[64]{0} %broadcast.865), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %convert.908 = f32[64]{0} convert(s32[64]{0} %multiply.907), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %divide.909 = f32[64]{0} divide(f32[64]{0} %convert.908, f32[64]{0} %broadcast.863), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %power.910 = f32[64]{0} power(f32[64]{0} %broadcast.861, f32[64]{0} %divide.909), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/pow" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=134}
  %reshape.913 = f32[1,1,1,64]{3,2,1,0} reshape(f32[64]{0} %power.910), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.917 = f32[1,1,1,64]{3,2,1,0} broadcast(f32[1,1,1,64]{3,2,1,0} %reshape.913), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.918 = f32[1,64]{1,0} reshape(f32[1,1,1,64]{3,2,1,0} %broadcast.917), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.919 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[1,64]{1,0} %reshape.918), dimensions={2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %divide.920 = f32[16,2048,1,64]{3,2,1,0} divide(f32[16,2048,1,64]{3,2,1,0} %broadcast.916, f32[16,2048,1,64]{3,2,1,0} %broadcast.919), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %cosine.923 = f32[16,2048,1,64]{3,2,1,0} cosine(f32[16,2048,1,64]{3,2,1,0} %divide.920), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/cos" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %convert.924 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %cosine.923), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %broadcast.927 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.924), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.928 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.927), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.929 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.928), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.930 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.925, bf16[16,2048,8,64]{3,2,1,0} %broadcast.929), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %slice.926 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %Arg_3.842), slice={[0:16], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %sine.921 = f32[16,2048,1,64]{3,2,1,0} sine(f32[16,2048,1,64]{3,2,1,0} %divide.920), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/sin" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %convert.922 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %sine.921), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %broadcast.931 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.922), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.932 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.931), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.933 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.932), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.934 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.926, bf16[16,2048,8,64]{3,2,1,0} %broadcast.933), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %subtract.935 = bf16[16,2048,8,64]{3,2,1,0} subtract(bf16[16,2048,8,64]{3,2,1,0} %multiply.930, bf16[16,2048,8,64]{3,2,1,0} %multiply.934), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/sub" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.936 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.924), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.937 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.936), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.938 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.937), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.939 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.926, bf16[16,2048,8,64]{3,2,1,0} %broadcast.938), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.940 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.922), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.941 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.940), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.942 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.941), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.943 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.925, bf16[16,2048,8,64]{3,2,1,0} %broadcast.942), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %add.944 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.939, bf16[16,2048,8,64]{3,2,1,0} %multiply.943), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %concatenate.945 = bf16[16,2048,8,128]{3,2,1,0} concatenate(bf16[16,2048,8,64]{3,2,1,0} %subtract.935, bf16[16,2048,8,64]{3,2,1,0} %add.944), dimensions={3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/concatenate" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %custom-call.986 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %concatenate.945), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %reshape.989 = bf16[16,2048,8,1,128]{4,3,2,1,0} reshape(bf16[16,2048,8,128]{3,2,1,0} %custom-call.986), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=527}
  %dot.990 = bf16[16,8,2048,2048,1]{4,3,2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.987, bf16[16,2048,8,1,128]{4,3,2,1,0} %reshape.989), lhs_batch_dims={0,2}, lhs_contracting_dims={3}, rhs_batch_dims={0,2}, rhs_contracting_dims={4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %transpose.991 = bf16[16,8,1,2048,2048]{2,3,4,1,0} transpose(bf16[16,8,2048,2048,1]{4,3,2,1,0} %dot.990), dimensions={0,1,4,3,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %call.1014 = (bf16[16,8,1,2048,2048]{4,3,2,1,0}, pred[16,8,1,2048,2048]{4,3,2,1,0}) call(pred[16,1,1,2048,2048]{4,3,2,1,0} %compare.1013, bf16[16,8,1,2048,2048]{2,3,4,1,0} %transpose.991, f32[] %constant.876), to_apply=%_where_4.756
  %get-tuple-element.1016 = pred[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((bf16[16,8,1,2048,2048]{4,3,2,1,0}, pred[16,8,1,2048,2048]{4,3,2,1,0}) %call.1014), index=1
  %custom-call.1058 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %custom-call.1057), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_13.852 = f32[8,128,2560]{2,1,0} parameter(13), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.1047 = bf16[8,128,2560]{2,1,0} convert(f32[8,128,2560]{2,1,0} %Arg_13.852), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.out_projection/out/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1062 = bf16[16,2048,8,128]{3,2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.1058, bf16[8,128,2560]{2,1,0} %convert.1047), lhs_contracting_dims={2}, rhs_contracting_dims={2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.out_projection/out/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.1063 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %dot.1062), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %constant.856 = bf16[] constant(1)
  %broadcast.857 = bf16[16,2048,8,1]{3,2,1,0} broadcast(bf16[] %constant.856), dimensions={}
  %get-tuple-element.1015 = bf16[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((bf16[16,8,1,2048,2048]{4,3,2,1,0}, pred[16,8,1,2048,2048]{4,3,2,1,0}) %call.1014), index=0
  %constant.875 = bf16[] constant(-inf)
  %reduce.1017 = bf16[16,8,1,2048]{3,2,1,0} reduce(bf16[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.1015, bf16[] %constant.875), dimensions={4}, to_apply=%region_17.767, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_max" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %reshape.1025 = bf16[16,8,1,2048,1]{4,3,2,1,0} reshape(bf16[16,8,1,2048]{3,2,1,0} %reduce.1017), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %broadcast.1026 = bf16[16,8,1,2048,1]{4,3,2,1,0} broadcast(bf16[16,8,1,2048,1]{4,3,2,1,0} %reshape.1025), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %reshape.1027 = bf16[16,8,1,2048]{3,2,1,0} reshape(bf16[16,8,1,2048,1]{4,3,2,1,0} %broadcast.1026), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %broadcast.1028 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[16,8,1,2048]{3,2,1,0} %reshape.1027), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %subtract.1029 = bf16[16,8,1,2048,2048]{4,3,2,1,0} subtract(bf16[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.1015, bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.1028), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/sub" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %exponential.1030 = bf16[16,8,1,2048,2048]{4,3,2,1,0} exponential(bf16[16,8,1,2048,2048]{4,3,2,1,0} %subtract.1029), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/exp" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %convert.1031 = f32[16,8,1,2048,2048]{4,3,2,1,0} convert(bf16[16,8,1,2048,2048]{4,3,2,1,0} %exponential.1030), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %reduce.1032 = f32[16,8,1,2048]{3,2,1,0} reduce(f32[16,8,1,2048,2048]{4,3,2,1,0} %convert.1031, f32[] %constant.877), dimensions={4}, to_apply=%region_19.775, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %reshape.1033 = f32[16,8,1,2048,1]{4,3,2,1,0} reshape(f32[16,8,1,2048]{3,2,1,0} %reduce.1032), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %convert.1034 = bf16[16,8,1,2048,1]{4,3,2,1,0} convert(f32[16,8,1,2048,1]{4,3,2,1,0} %reshape.1033), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %transpose.1035 = bf16[16,2048,8,1,1]{4,1,3,2,0} transpose(bf16[16,8,1,2048,1]{4,3,2,1,0} %convert.1034), dimensions={0,3,1,2,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=454}
  %reshape.1036 = bf16[16,2048,8,1]{3,2,1,0} reshape(bf16[16,2048,8,1,1]{4,1,3,2,0} %transpose.1035), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=458}
  %multiply.1044 = bf16[16,2048,8,1]{3,2,1,0} multiply(bf16[16,2048,8,1]{3,2,1,0} %reshape.1036, bf16[16,2048,8,1]{3,2,1,0} %reshape.1036), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %divide.1045 = bf16[16,2048,8,1]{3,2,1,0} divide(bf16[16,2048,8,1]{3,2,1,0} %broadcast.857, bf16[16,2048,8,1]{3,2,1,0} %multiply.1044), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %broadcast.1064 = bf16[16,2048,8,1]{3,2,1,0} broadcast(bf16[16,2048,8,1]{3,2,1,0} %divide.1045), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.1065 = bf16[16,2048,8]{2,1,0} reshape(bf16[16,2048,8,1]{3,2,1,0} %broadcast.1064), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %broadcast.1066 = bf16[16,2048,8,128]{3,2,1,0} broadcast(bf16[16,2048,8]{2,1,0} %reshape.1065), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %multiply.1067 = bf16[16,2048,8,128]{3,2,1,0} multiply(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1063, bf16[16,2048,8,128]{3,2,1,0} %broadcast.1066), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %Arg_5.844 = bf16[16,2048,8,128]{3,2,1,0} parameter(5), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %custom-call.988 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %Arg_5.844), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1037 = bf16[16,8,128,1,2048]{4,3,2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.988, bf16[16,8,1,2048,2048]{4,3,2,1,0} %exponential.1030), lhs_batch_dims={0,2}, lhs_contracting_dims={1}, rhs_batch_dims={0,1}, rhs_contracting_dims={4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %transpose.1038 = bf16[16,2048,8,1,128]{1,3,4,2,0} transpose(bf16[16,8,128,1,2048]{4,3,2,1,0} %dot.1037), dimensions={0,4,1,3,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %reshape.1039 = bf16[16,2048,8,128]{3,2,1,0} reshape(bf16[16,2048,8,1,128]{1,3,4,2,0} %transpose.1038), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=566}
  %multiply.1068 = bf16[16,2048,8,128]{3,2,1,0} multiply(bf16[16,2048,8,128]{3,2,1,0} %multiply.1067, bf16[16,2048,8,128]{3,2,1,0} %reshape.1039), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %constant.874 = bf16[] constant(0)
  %reduce.1069 = bf16[16,2048,8]{2,1,0} reduce(bf16[16,2048,8,128]{3,2,1,0} %multiply.1068, bf16[] %constant.874), dimensions={3}, to_apply=%region_20.791, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.1070 = bf16[16,2048,8,1]{3,2,1,0} reshape(bf16[16,2048,8]{2,1,0} %reduce.1069), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %negate.1071 = bf16[16,2048,8,1]{3,2,1,0} negate(bf16[16,2048,8,1]{3,2,1,0} %reshape.1070), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/neg" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.1072 = bf16[16,2048,8,1,1]{4,3,2,1,0} reshape(bf16[16,2048,8,1]{3,2,1,0} %negate.1071), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=458}
  %transpose.1073 = bf16[16,8,1,2048,1]{4,2,1,3,0} transpose(bf16[16,2048,8,1,1]{4,3,2,1,0} %reshape.1072), dimensions={0,2,3,1,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=454}
  %convert.1074 = f32[16,8,1,2048,1]{4,2,1,3,0} convert(bf16[16,8,1,2048,1]{4,2,1,3,0} %transpose.1073), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %reduce.1075 = f32[16,8,2048]{2,1,0} reduce(f32[16,8,1,2048,1]{4,2,1,3,0} %convert.1074, f32[] %constant.877), dimensions={2,4}, to_apply=%region_21.795, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %reshape.1076 = f32[16,8,1,2048]{3,2,1,0} reshape(f32[16,8,2048]{2,1,0} %reduce.1075), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %broadcast.1077 = f32[16,8,1,2048,2048]{4,3,2,1,0} broadcast(f32[16,8,1,2048]{3,2,1,0} %reshape.1076), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %convert.1078 = bf16[16,8,1,2048,2048]{4,3,2,1,0} convert(f32[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.1077), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=452}
  %broadcast.1079 = bf16[16,2048,8,1]{3,2,1,0} broadcast(bf16[16,2048,8,1]{3,2,1,0} %reshape.1036), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.1080 = bf16[16,2048,8]{2,1,0} reshape(bf16[16,2048,8,1]{3,2,1,0} %broadcast.1079), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %broadcast.1081 = bf16[16,2048,8,128]{3,2,1,0} broadcast(bf16[16,2048,8]{2,1,0} %reshape.1080), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %divide.1082 = bf16[16,2048,8,128]{3,2,1,0} divide(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1063, bf16[16,2048,8,128]{3,2,1,0} %broadcast.1081), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.1083 = bf16[16,2048,8,1,128]{4,3,2,1,0} reshape(bf16[16,2048,8,128]{3,2,1,0} %divide.1082), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=566}
  %transpose.1084 = bf16[16,8,128,1,2048]{2,3,1,4,0} transpose(bf16[16,2048,8,1,128]{4,3,2,1,0} %reshape.1083), dimensions={0,2,4,3,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %dot.1085 = bf16[16,8,1,2048,2048]{4,3,2,1,0} dot(bf16[16,8,128,1,2048]{2,3,1,4,0} %transpose.1084, bf16[16,2048,8,128]{3,2,1,0} %custom-call.988), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,2}, rhs_contracting_dims={3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %add.1086 = bf16[16,8,1,2048,2048]{4,3,2,1,0} add(bf16[16,8,1,2048,2048]{4,3,2,1,0} %convert.1078, bf16[16,8,1,2048,2048]{4,3,2,1,0} %dot.1085), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/add_any" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %multiply.1087 = bf16[16,8,1,2048,2048]{4,3,2,1,0} multiply(bf16[16,8,1,2048,2048]{4,3,2,1,0} %add.1086, bf16[16,8,1,2048,2048]{4,3,2,1,0} %exponential.1030), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %negate.1088 = bf16[16,8,1,2048,2048]{4,3,2,1,0} negate(bf16[16,8,1,2048,2048]{4,3,2,1,0} %multiply.1087), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/neg" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %reduce.1089 = bf16[16,8,1,2048]{3,2,1,0} reduce(bf16[16,8,1,2048,2048]{4,3,2,1,0} %negate.1088, bf16[] %constant.874), dimensions={4}, to_apply=%region_22.799, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %reshape.1090 = bf16[16,8,1,2048,1]{4,3,2,1,0} reshape(bf16[16,8,1,2048]{3,2,1,0} %reduce.1089), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=451}
  %reduce.1091 = bf16[16,8,2048]{2,1,0} reduce(bf16[16,8,1,2048,1]{4,3,2,1,0} %reshape.1090, bf16[] %constant.874), dimensions={2,4}, to_apply=%region_23.803, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %reshape.1092 = bf16[16,8,1,2048]{3,2,1,0} reshape(bf16[16,8,2048]{2,1,0} %reduce.1091), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %reshape.1018 = bf16[16,8,1,2048,1]{4,3,2,1,0} reshape(bf16[16,8,1,2048]{3,2,1,0} %reduce.1017), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %broadcast.1019 = bf16[16,8,1,2048,1]{4,3,2,1,0} broadcast(bf16[16,8,1,2048,1]{4,3,2,1,0} %reshape.1018), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %reshape.1020 = bf16[16,8,1,2048]{3,2,1,0} reshape(bf16[16,8,1,2048,1]{4,3,2,1,0} %broadcast.1019), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %broadcast.1021 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[16,8,1,2048]{3,2,1,0} %reshape.1020), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %compare.1022 = pred[16,8,1,2048,2048]{4,3,2,1,0} compare(bf16[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.1015, bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.1021), direction=EQ, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %convert.1023 = bf16[16,8,1,2048,2048]{4,3,2,1,0} convert(pred[16,8,1,2048,2048]{4,3,2,1,0} %compare.1022), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/convert_element_type" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %reduce.1024 = bf16[16,8,1,2048]{3,2,1,0} reduce(bf16[16,8,1,2048,2048]{4,3,2,1,0} %convert.1023, bf16[] %constant.874), dimensions={4}, to_apply=%region_18.771, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/reduce_sum" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %divide.1093 = bf16[16,8,1,2048]{3,2,1,0} divide(bf16[16,8,1,2048]{3,2,1,0} %reshape.1092, bf16[16,8,1,2048]{3,2,1,0} %reduce.1024), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %broadcast.1094 = bf16[16,8,1,2048,2048]{4,3,2,1,0} broadcast(bf16[16,8,1,2048]{3,2,1,0} %divide.1093), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %multiply.1095 = bf16[16,8,1,2048,2048]{4,3,2,1,0} multiply(bf16[16,8,1,2048,2048]{4,3,2,1,0} %broadcast.1094, bf16[16,8,1,2048,2048]{4,3,2,1,0} %convert.1023), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/mul" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %add.1096 = bf16[16,8,1,2048,2048]{4,3,2,1,0} add(bf16[16,8,1,2048,2048]{4,3,2,1,0} %multiply.1087, bf16[16,8,1,2048,2048]{4,3,2,1,0} %multiply.1095), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/add_any" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=450}
  %call.1097 = bf16[16,8,1,2048,2048]{4,3,2,1,0} call(pred[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.1016, bf16[16,8,1,2048,2048]{4,3,2,1,0} %add.1096), to_apply=%_where_6.807
  %transpose.1098 = bf16[16,8,2048,2048,1]{2,3,4,1,0} transpose(bf16[16,8,1,2048,2048]{4,3,2,1,0} %call.1097), dimensions={0,1,4,3,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %dot.1099 = bf16[16,8,2048,1,128]{4,3,2,1,0} dot(bf16[16,8,2048,2048,1]{2,3,4,1,0} %transpose.1098, bf16[16,2048,8,128]{3,2,1,0} %custom-call.987), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %transpose.1100 = bf16[16,2048,8,1,128]{4,3,1,2,0} transpose(bf16[16,8,2048,1,128]{4,3,2,1,0} %dot.1099), dimensions={0,2,1,3,4}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %reshape.1101 = bf16[16,2048,8,128]{3,2,1,0} reshape(bf16[16,2048,8,1,128]{4,3,1,2,0} %transpose.1100), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/reshape" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=527}
  %custom-call.1102 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %reshape.1101), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %slice.1104 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1102), slice={[0:16], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %broadcast.1105 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.922), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.1106 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1105), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.1107 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1106), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.1108 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.1104, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1107), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %slice.1103 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1102), slice={[0:16], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %broadcast.1119 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.924), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.1120 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1119), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1121 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1120), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.1122 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.1103, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1121), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %add.1123 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.1108, bf16[16,2048,8,64]{3,2,1,0} %multiply.1122), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/add_any" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1109 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.924), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.1110 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1109), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.1111 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1110), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.1112 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.1104, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1111), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %negate.1113 = bf16[16,2048,8,64]{3,2,1,0} negate(bf16[16,2048,8,64]{3,2,1,0} %slice.1103), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/neg" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1114 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.922), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.1115 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1114), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1116 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1115), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.1117 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %negate.1113, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1116), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %add.1118 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.1112, bf16[16,2048,8,64]{3,2,1,0} %multiply.1117), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/add_any" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %concatenate.1124 = bf16[16,2048,8,128]{3,2,1,0} concatenate(bf16[16,2048,8,64]{3,2,1,0} %add.1123, bf16[16,2048,8,64]{3,2,1,0} %add.1118), dimensions={3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/concatenate" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_14.853 = f32[2560,8,128]{2,1,0} parameter(14), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.903 = bf16[2560,8,128]{2,1,0} convert(f32[2560,8,128]{2,1,0} %Arg_14.853), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.query_projection/query/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1128 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %concatenate.1124, bf16[2560,8,128]{2,1,0} %convert.903), lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.query_projection/query/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.1129 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %dot.1128), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1130 = bf16[16,8,2048,128]{3,2,1,0} dot(bf16[16,8,2048,2048,1]{2,3,4,1,0} %transpose.1098, bf16[16,2048,8,1,128]{4,3,2,1,0} %reshape.989), lhs_batch_dims={0,1}, lhs_contracting_dims={3,4}, rhs_batch_dims={0,2}, rhs_contracting_dims={1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %transpose.1131 = bf16[16,2048,8,128]{3,1,2,0} transpose(bf16[16,8,2048,128]{3,2,1,0} %dot.1130), dimensions={0,2,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.qk_product/btkgd,bskd->bkgts/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=530}
  %custom-call.1132 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,1,2,0} %transpose.1131), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %slice.1134 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1132), slice={[0:16], [0:2048], [0:8], [64:128]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %broadcast.1135 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.962), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.1136 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1135), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.1137 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1136), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.1138 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.1134, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1137), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %slice.1133 = bf16[16,2048,8,64]{3,2,1,0} slice(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1132), slice={[0:16], [0:2048], [0:8], [0:64]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/split" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=173}
  %broadcast.1149 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.964), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.1150 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1149), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1151 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1150), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.1152 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.1133, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1151), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %add.1153 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.1138, bf16[16,2048,8,64]{3,2,1,0} %multiply.1152), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/add_any" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1139 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.964), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %reshape.1140 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1139), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %broadcast.1141 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1140), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %multiply.1142 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %slice.1134, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1141), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=169}
  %negate.1143 = bf16[16,2048,8,64]{3,2,1,0} negate(bf16[16,2048,8,64]{3,2,1,0} %slice.1133), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/neg" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1144 = bf16[16,2048,1,64]{3,2,1,0} broadcast(bf16[16,2048,1,64]{3,2,1,0} %convert.962), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %reshape.1145 = bf16[16,2048,64]{2,1,0} reshape(bf16[16,2048,1,64]{3,2,1,0} %broadcast.1144), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %broadcast.1146 = bf16[16,2048,8,64]{3,2,1,0} broadcast(bf16[16,2048,64]{2,1,0} %reshape.1145), dimensions={0,1,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %multiply.1147 = bf16[16,2048,8,64]{3,2,1,0} multiply(bf16[16,2048,8,64]{3,2,1,0} %negate.1143, bf16[16,2048,8,64]{3,2,1,0} %broadcast.1146), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %add.1148 = bf16[16,2048,8,64]{3,2,1,0} add(bf16[16,2048,8,64]{3,2,1,0} %multiply.1142, bf16[16,2048,8,64]{3,2,1,0} %multiply.1147), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/add_any" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=168}
  %concatenate.1154 = bf16[16,2048,8,128]{3,2,1,0} concatenate(bf16[16,2048,8,64]{3,2,1,0} %add.1153, bf16[16,2048,8,64]{3,2,1,0} %add.1148), dimensions={3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/concatenate" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=167}
  %Arg_12.851 = f32[2560,8,128]{2,1,0} parameter(12), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.904 = bf16[2560,8,128]{2,1,0} convert(f32[2560,8,128]{2,1,0} %Arg_12.851), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.kv_projection/key/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1158 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %concatenate.1154, bf16[2560,8,128]{2,1,0} %convert.904), lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/key/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %dot.1159 = bf16[16,8,128,2048]{3,2,1,0} dot(bf16[16,8,128,1,2048]{2,3,1,4,0} %transpose.1084, bf16[16,8,1,2048,2048]{4,3,2,1,0} %exponential.1030), lhs_batch_dims={0,1}, lhs_contracting_dims={3,4}, rhs_batch_dims={0,1}, rhs_contracting_dims={2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/dot_general" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %transpose.1160 = bf16[16,2048,8,128]{1,3,2,0} transpose(bf16[16,8,128,2048]{3,2,1,0} %dot.1159), dimensions={0,3,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.compute_local_attention/AttentionOp_0.wv_product/bkgts,bskd->btkgd/transpose" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=564}
  %custom-call.1161 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{1,3,2,0} %transpose.1160), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_15.854 = f32[2560,8,128]{2,1,0} parameter(15), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.905 = bf16[2560,8,128]{2,1,0} convert(f32[2560,8,128]{2,1,0} %Arg_15.854), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/self_attention.kv_projection/value/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1165 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1161, bf16[2560,8,128]{2,1,0} %convert.905), lhs_contracting_dims={2,3}, rhs_contracting_dims={1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/value/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %add.1166 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %dot.1158, bf16[16,2048,2560]{2,1,0} %dot.1165), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/value/add_any" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.1167 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %add.1166), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %add.1168 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %custom-call.1129, bf16[16,2048,2560]{2,1,0} %custom-call.1167), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/add_any" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_6.845 = bf16[16,2048,8192]{2,1,0} parameter(6), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %call.1049 = (bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}) call(bf16[16,2048,8192]{2,1,0} %Arg_6.845), to_apply=%silu_5.779
  %get-tuple-element.1050 = bf16[16,2048,8192]{2,1,0} get-tuple-element((bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}) %call.1049), index=0
  %custom-call.1169 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %custom-call.1057), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_10.849 = f32[8192,2560]{1,0} parameter(10), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.1056 = bf16[8192,2560]{1,0} convert(f32[8192,2560]{1,0} %Arg_10.849), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/wo/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1173 = bf16[16,2048,8192]{2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.1169, bf16[8192,2560]{1,0} %convert.1056), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wo/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.1174 = bf16[16,2048,8192]{2,1,0} custom-call(bf16[16,2048,8192]{2,1,0} %dot.1173), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %multiply.1175 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %get-tuple-element.1050, bf16[16,2048,8192]{2,1,0} %custom-call.1174), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=264}
  %Arg_9.848 = f32[2560,8192]{1,0} parameter(9), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.1053 = bf16[2560,8192]{1,0} convert(f32[2560,8192]{1,0} %Arg_9.848), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/wi_1/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1179 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8192]{2,1,0} %multiply.1175, bf16[2560,8192]{1,0} %convert.1053), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_1/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %add.1180 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %add.1168, bf16[16,2048,2560]{2,1,0} %dot.1179), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_1/add_any" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %get-tuple-element.1051 = bf16[16,2048,8192]{2,1,0} get-tuple-element((bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}) %call.1049), index=1
  %get-tuple-element.1052 = bf16[16,2048,8192]{2,1,0} get-tuple-element((bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}, bf16[16,2048,8192]{2,1,0}) %call.1049), index=2
  %Arg_7.846 = bf16[16,2048,8192]{2,1,0} parameter(7), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %multiply.1181 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %custom-call.1174, bf16[16,2048,8192]{2,1,0} %Arg_7.846), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=264}
  %call.1182 = bf16[16,2048,8192]{2,1,0} call(bf16[16,2048,8192]{2,1,0} %get-tuple-element.1051, bf16[16,2048,8192]{2,1,0} %Arg_6.845, bf16[16,2048,8192]{2,1,0} %get-tuple-element.1052, bf16[16,2048,8192]{2,1,0} %multiply.1181), to_apply=%silu_7.813
  %Arg_8.847 = f32[2560,8192]{1,0} parameter(8), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.1048 = bf16[2560,8192]{1,0} convert(f32[2560,8192]{1,0} %Arg_8.847), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/wi_0/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1186 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,8192]{2,1,0} %call.1182, bf16[2560,8192]{1,0} %convert.1048), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_0/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %add.1187 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %add.1180, bf16[16,2048,2560]{2,1,0} %dot.1186), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_0/add_any" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %custom-call.1188 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %add.1187), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %Arg_11.850 = f32[2560]{0} parameter(11), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/closed_call"}
  %convert.894 = bf16[2560]{0} convert(f32[2560]{0} %Arg_11.850), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=50}
  %reshape.895 = bf16[1,1,2560]{2,1,0} reshape(bf16[2560]{0} %convert.894), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.1194 = bf16[1,1,2560]{2,1,0} broadcast(bf16[1,1,2560]{2,1,0} %reshape.895), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.1195 = bf16[2560]{0} reshape(bf16[1,1,2560]{2,1,0} %broadcast.1194), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.1196 = bf16[16,2048,2560]{2,1,0} broadcast(bf16[2560]{0} %reshape.1195), dimensions={2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %multiply.1197 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %custom-call.1188, bf16[16,2048,2560]{2,1,0} %broadcast.1196), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %convert.1198 = f32[16,2048,2560]{2,1,0} convert(bf16[16,2048,2560]{2,1,0} %multiply.1197), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.1199 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.879, f32[16,2048,2560]{2,1,0} %convert.1198), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reduce.1200 = f32[16,2048]{1,0} reduce(f32[16,2048,2560]{2,1,0} %multiply.1199, f32[] %constant.877), dimensions={2}, to_apply=%region_26.830, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.1201 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.1200), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reshape" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.880 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.879, f32[16,2048,2560]{2,1,0} %convert.879), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/square" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reduce.882 = f32[16,2048]{1,0} reduce(f32[16,2048,2560]{2,1,0} %multiply.880, f32[] %constant.877), dimensions={2}, to_apply=%region_16.745, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reshape.883 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.882), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.870 = f32[] constant(2560)
  %broadcast.871 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.870), dimensions={}
  %divide.884 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %reshape.883, f32[16,2048,1]{2,1,0} %broadcast.871), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.868 = f32[] constant(1e-05)
  %broadcast.869 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.868), dimensions={}
  %add.885 = f32[16,2048,1]{2,1,0} add(f32[16,2048,1]{2,1,0} %divide.884, f32[16,2048,1]{2,1,0} %broadcast.869), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/add" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %rsqrt.886 = f32[16,2048,1]{2,1,0} rsqrt(f32[16,2048,1]{2,1,0} %add.885), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/rsqrt" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %divide.887 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %rsqrt.886, f32[16,2048,1]{2,1,0} %add.885), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %constant.866 = f32[] constant(-0.5)
  %broadcast.867 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.866), dimensions={}
  %multiply.888 = f32[16,2048,1]{2,1,0} multiply(f32[16,2048,1]{2,1,0} %divide.887, f32[16,2048,1]{2,1,0} %broadcast.867), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.1202 = f32[16,2048,1]{2,1,0} multiply(f32[16,2048,1]{2,1,0} %reshape.1201, f32[16,2048,1]{2,1,0} %multiply.888), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %divide.1203 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %multiply.1202, f32[16,2048,1]{2,1,0} %broadcast.871), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reduce.1204 = f32[16,2048]{1,0} reduce(f32[16,2048,1]{2,1,0} %divide.1203, f32[] %constant.877), dimensions={2}, to_apply=%region_27.834, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %broadcast.1205 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reduce.1204), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.872 = f32[] constant(2)
  %broadcast.873 = f32[16,2048,2560]{2,1,0} broadcast(f32[] %constant.872), dimensions={}
  %multiply.881 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.879, f32[16,2048,2560]{2,1,0} %broadcast.873), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %multiply.1206 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %broadcast.1205, f32[16,2048,2560]{2,1,0} %multiply.881), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %broadcast.1207 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %rsqrt.886), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.1208 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.1207), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.1209 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.1208), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.1210 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.1198, f32[16,2048,2560]{2,1,0} %broadcast.1209), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %add.1211 = f32[16,2048,2560]{2,1,0} add(f32[16,2048,2560]{2,1,0} %multiply.1206, f32[16,2048,2560]{2,1,0} %multiply.1210), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/add_any" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %convert.1212 = bf16[16,2048,2560]{2,1,0} convert(f32[16,2048,2560]{2,1,0} %add.1211), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=39}
  %add.1213 = bf16[16,2048,2560]{2,1,0} add(bf16[16,2048,2560]{2,1,0} %custom-call.1057, bf16[16,2048,2560]{2,1,0} %convert.1212), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/add_any" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=39}
  %custom-call.1214 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %add.1213), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %broadcast.889 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %rsqrt.886), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.890 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.889), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.891 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.890), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.892 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.879, f32[16,2048,2560]{2,1,0} %broadcast.891), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %convert.893 = bf16[16,2048,2560]{2,1,0} convert(f32[16,2048,2560]{2,1,0} %multiply.892), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.896 = bf16[1,1,2560]{2,1,0} broadcast(bf16[1,1,2560]{2,1,0} %reshape.895), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.897 = bf16[2560]{0} reshape(bf16[1,1,2560]{2,1,0} %broadcast.896), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.898 = bf16[16,2048,2560]{2,1,0} broadcast(bf16[2560]{0} %reshape.897), dimensions={2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %multiply.899 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %convert.893, bf16[16,2048,2560]{2,1,0} %broadcast.898), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %custom-call.900 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %multiply.899), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1183 = bf16[8192,2560]{1,0} dot(bf16[16,2048,8192]{2,1,0} %call.1182, bf16[16,2048,2560]{2,1,0} %custom-call.900), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_0/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1184 = bf16[2560,8192]{0,1} transpose(bf16[8192,2560]{1,0} %dot.1183), dimensions={1,0}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_0/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1185 = f32[2560,8192]{0,1} convert(bf16[2560,8192]{0,1} %transpose.1184), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_0/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1176 = bf16[8192,2560]{1,0} dot(bf16[16,2048,8192]{2,1,0} %multiply.1175, bf16[16,2048,2560]{2,1,0} %custom-call.900), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_1/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1177 = bf16[2560,8192]{0,1} transpose(bf16[8192,2560]{1,0} %dot.1176), dimensions={1,0}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_1/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1178 = f32[2560,8192]{0,1} convert(bf16[2560,8192]{0,1} %transpose.1177), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wi_1/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %multiply.1054 = bf16[16,2048,8192]{2,1,0} multiply(bf16[16,2048,8192]{2,1,0} %get-tuple-element.1050, bf16[16,2048,8192]{2,1,0} %Arg_7.846), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/mul" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=264}
  %custom-call.1055 = bf16[16,2048,8192]{2,1,0} custom-call(bf16[16,2048,8192]{2,1,0} %multiply.1054), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/mlp/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1170 = bf16[2560,8192]{1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.1169, bf16[16,2048,8192]{2,1,0} %custom-call.1055), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wo/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1171 = bf16[8192,2560]{0,1} transpose(bf16[2560,8192]{1,0} %dot.1170), dimensions={1,0}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wo/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1172 = f32[8192,2560]{0,1} convert(bf16[8192,2560]{0,1} %transpose.1171), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/mlp/wo/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %multiply.1189 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %convert.893, bf16[16,2048,2560]{2,1,0} %custom-call.1188), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reduce.1190 = bf16[2560]{0} reduce(bf16[16,2048,2560]{2,1,0} %multiply.1189, bf16[] %constant.874), dimensions={0,1}, to_apply=%region_24.822, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.1191 = bf16[1,1,2560]{2,1,0} reshape(bf16[2560]{0} %reduce.1190), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reshape" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reduce.1192 = bf16[2560]{0} reduce(bf16[1,1,2560]{2,1,0} %reshape.1191, bf16[] %constant.874), dimensions={0,1}, to_apply=%region_25.826, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %convert.1193 = f32[2560]{0} convert(bf16[2560]{0} %reduce.1192), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/pre_self_attention_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=50}
  %custom-call.902 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %custom-call.900), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1155 = bf16[8,128,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %concatenate.1154, bf16[16,2048,2560]{2,1,0} %custom-call.902), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/key/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1156 = bf16[2560,8,128]{0,2,1} transpose(bf16[8,128,2560]{2,1,0} %dot.1155), dimensions={2,0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/key/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1157 = f32[2560,8,128]{0,2,1} convert(bf16[2560,8,128]{0,2,1} %transpose.1156), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/key/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %broadcast.1040 = bf16[16,2048,8,1]{3,2,1,0} broadcast(bf16[16,2048,8,1]{3,2,1,0} %reshape.1036), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %reshape.1041 = bf16[16,2048,8]{2,1,0} reshape(bf16[16,2048,8,1]{3,2,1,0} %broadcast.1040), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %broadcast.1042 = bf16[16,2048,8,128]{3,2,1,0} broadcast(bf16[16,2048,8]{2,1,0} %reshape.1041), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %divide.1043 = bf16[16,2048,8,128]{3,2,1,0} divide(bf16[16,2048,8,128]{3,2,1,0} %reshape.1039, bf16[16,2048,8,128]{3,2,1,0} %broadcast.1042), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/AttentionOp_0/div" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=1042}
  %custom-call.1046 = bf16[16,2048,8,128]{3,2,1,0} custom-call(bf16[16,2048,8,128]{3,2,1,0} %divide.1043), custom_call_target="Sharding", sharding={devices=[8,1,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1059 = bf16[2560,8,128]{2,1,0} dot(bf16[16,2048,2560]{2,1,0} %custom-call.1058, bf16[16,2048,8,128]{3,2,1,0} %custom-call.1046), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.out_projection/out/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1060 = bf16[8,128,2560]{1,0,2} transpose(bf16[2560,8,128]{2,1,0} %dot.1059), dimensions={1,2,0}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.out_projection/out/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1061 = f32[8,128,2560]{1,0,2} convert(bf16[8,128,2560]{1,0,2} %transpose.1060), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.out_projection/out/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %custom-call.901 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %custom-call.900), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/rematted_computation/layers/self_attention/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %dot.1125 = bf16[8,128,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %concatenate.1124, bf16[16,2048,2560]{2,1,0} %custom-call.901), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.query_projection/query/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1126 = bf16[2560,8,128]{0,2,1} transpose(bf16[8,128,2560]{2,1,0} %dot.1125), dimensions={2,0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.query_projection/query/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1127 = f32[2560,8,128]{0,2,1} convert(bf16[2560,8,128]{0,2,1} %transpose.1126), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.query_projection/query/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  %dot.1162 = bf16[8,128,2560]{2,1,0} dot(bf16[16,2048,8,128]{3,2,1,0} %custom-call.1161, bf16[16,2048,2560]{2,1,0} %custom-call.902), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/value/dot_general" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %transpose.1163 = bf16[2560,8,128]{0,2,1} transpose(bf16[8,128,2560]{2,1,0} %dot.1162), dimensions={2,0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/value/transpose" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=127}
  %convert.1164 = f32[2560,8,128]{0,2,1} convert(bf16[2560,8,128]{0,2,1} %transpose.1163), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/checkpoint/layers/self_attention/self_attention.kv_projection/value/convert_element_type" source_file="/opt/maxtext/MaxText/layers/linears.py" source_line=151}
  ROOT %tuple.1215 = (bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) tuple(bf16[16,2048,2560]{2,1,0} %custom-call.1214, f32[2560,8192]{0,1} %convert.1185, f32[2560,8192]{0,1} %convert.1178, f32[8192,2560]{0,1} %convert.1172, f32[2560]{0} %convert.1193, /*index=5*/f32[2560,8,128]{0,2,1} %convert.1157, f32[8,128,2560]{1,0,2} %convert.1061, f32[2560,8,128]{0,2,1} %convert.1127, f32[2560,8,128]{0,2,1} %convert.1164)
}

%region_15.1216 (arg_tuple.1217: (s32[], bf16[16,2048,2560], f32[8,2560,8192], f32[8,2560,8192], f32[8,8192,2560], /*index=5*/f32[8,2560], f32[8,2560,8,128], f32[8,8,128,2560], f32[8,2560,8,128], f32[8,2560,8,128], /*index=10*/bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8192], bf16[8,16,2048,8192], /*index=15*/f32[8,2560,8192], f32[8,2560,8192], f32[8,8192,2560], f32[8,2560], f32[8,2560,8,128], /*index=20*/f32[8,8,128,2560], f32[8,2560,8,128], f32[8,2560,8,128], bf16[8,16,2048,2560], s32[16,2048], /*index=25*/s32[16,2048])) -> (s32[], bf16[16,2048,2560], f32[8,2560,8192], f32[8,2560,8192], f32[8,8192,2560], /*index=5*/f32[8,2560], f32[8,2560,8,128], f32[8,8,128,2560], f32[8,2560,8,128], f32[8,2560,8,128], /*index=10*/bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8192], bf16[8,16,2048,8192], /*index=15*/f32[8,2560,8192], f32[8,2560,8192], f32[8,8192,2560], f32[8,2560], f32[8,2560,8,128], /*index=20*/f32[8,8,128,2560], f32[8,2560,8,128], f32[8,2560,8,128], bf16[8,16,2048,2560], s32[16,2048], /*index=25*/s32[16,2048]) {
  %arg_tuple.1217 = (s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) parameter(0)
  %get-tuple-element.1218 = s32[] get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=0
  %constant.1245 = s32[] constant(1)
  %add.1369 = s32[] add(s32[] %get-tuple-element.1218, s32[] %constant.1245), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1242 = s32[16,2048]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=24
  %get-tuple-element.1243 = s32[16,2048]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=25
  %get-tuple-element.1219 = bf16[16,2048,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=1
  %get-tuple-element.1228 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=10
  %constant.1246 = s32[] constant(8)
  %subtract.1247 = s32[] subtract(s32[] %constant.1246, s32[] %get-tuple-element.1218), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/sub" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %subtract.1248 = s32[] subtract(s32[] %subtract.1247, s32[] %constant.1245), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/sub" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %constant.1244 = s32[] constant(0)
  %compare.1249 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1250 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1251 = s32[] select(pred[] %compare.1249, s32[] %add.1250, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1252 = bf16[1,16,2048,8,128]{4,3,2,1,0} dynamic-slice(bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.1228, s32[] %select.1251, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), dynamic_slice_sizes={1,16,2048,8,128}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1253 = bf16[16,2048,8,128]{3,2,1,0} reshape(bf16[1,16,2048,8,128]{4,3,2,1,0} %dynamic-slice.1252), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1229 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=11
  %compare.1254 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1255 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1256 = s32[] select(pred[] %compare.1254, s32[] %add.1255, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1257 = bf16[1,16,2048,8,128]{4,3,2,1,0} dynamic-slice(bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.1229, s32[] %select.1256, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), dynamic_slice_sizes={1,16,2048,8,128}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1258 = bf16[16,2048,8,128]{3,2,1,0} reshape(bf16[1,16,2048,8,128]{4,3,2,1,0} %dynamic-slice.1257), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1230 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=12
  %compare.1259 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1260 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1261 = s32[] select(pred[] %compare.1259, s32[] %add.1260, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1262 = bf16[1,16,2048,8,128]{4,3,2,1,0} dynamic-slice(bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.1230, s32[] %select.1261, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), dynamic_slice_sizes={1,16,2048,8,128}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1263 = bf16[16,2048,8,128]{3,2,1,0} reshape(bf16[1,16,2048,8,128]{4,3,2,1,0} %dynamic-slice.1262), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1231 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=13
  %compare.1264 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1265 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1266 = s32[] select(pred[] %compare.1264, s32[] %add.1265, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1267 = bf16[1,16,2048,8192]{3,2,1,0} dynamic-slice(bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.1231, s32[] %select.1266, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,16,2048,8192}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1268 = bf16[16,2048,8192]{2,1,0} reshape(bf16[1,16,2048,8192]{3,2,1,0} %dynamic-slice.1267), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1232 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=14
  %compare.1269 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1270 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1271 = s32[] select(pred[] %compare.1269, s32[] %add.1270, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1272 = bf16[1,16,2048,8192]{3,2,1,0} dynamic-slice(bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.1232, s32[] %select.1271, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,16,2048,8192}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1273 = bf16[16,2048,8192]{2,1,0} reshape(bf16[1,16,2048,8192]{3,2,1,0} %dynamic-slice.1272), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1233 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=15
  %compare.1274 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1275 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1276 = s32[] select(pred[] %compare.1274, s32[] %add.1275, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1277 = f32[1,2560,8192]{2,1,0} dynamic-slice(f32[8,2560,8192]{2,1,0} %get-tuple-element.1233, s32[] %select.1276, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,2560,8192}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1278 = f32[2560,8192]{1,0} reshape(f32[1,2560,8192]{2,1,0} %dynamic-slice.1277), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1234 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=16
  %compare.1279 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1280 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1281 = s32[] select(pred[] %compare.1279, s32[] %add.1280, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1282 = f32[1,2560,8192]{2,1,0} dynamic-slice(f32[8,2560,8192]{2,1,0} %get-tuple-element.1234, s32[] %select.1281, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,2560,8192}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1283 = f32[2560,8192]{1,0} reshape(f32[1,2560,8192]{2,1,0} %dynamic-slice.1282), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1235 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=17
  %compare.1284 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1285 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1286 = s32[] select(pred[] %compare.1284, s32[] %add.1285, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1287 = f32[1,8192,2560]{2,1,0} dynamic-slice(f32[8,8192,2560]{2,1,0} %get-tuple-element.1235, s32[] %select.1286, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,8192,2560}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1288 = f32[8192,2560]{1,0} reshape(f32[1,8192,2560]{2,1,0} %dynamic-slice.1287), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1236 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=18
  %compare.1289 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1290 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1291 = s32[] select(pred[] %compare.1289, s32[] %add.1290, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1292 = f32[1,2560]{1,0} dynamic-slice(f32[8,2560]{1,0} %get-tuple-element.1236, s32[] %select.1291, s32[] %constant.1244), dynamic_slice_sizes={1,2560}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1293 = f32[2560]{0} reshape(f32[1,2560]{1,0} %dynamic-slice.1292), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1237 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=19
  %compare.1294 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1295 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1296 = s32[] select(pred[] %compare.1294, s32[] %add.1295, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1297 = f32[1,2560,8,128]{3,2,1,0} dynamic-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1237, s32[] %select.1296, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,2560,8,128}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1298 = f32[2560,8,128]{2,1,0} reshape(f32[1,2560,8,128]{3,2,1,0} %dynamic-slice.1297), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1238 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=20
  %compare.1299 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1300 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1301 = s32[] select(pred[] %compare.1299, s32[] %add.1300, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1302 = f32[1,8,128,2560]{3,2,1,0} dynamic-slice(f32[8,8,128,2560]{3,2,1,0} %get-tuple-element.1238, s32[] %select.1301, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,8,128,2560}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1303 = f32[8,128,2560]{2,1,0} reshape(f32[1,8,128,2560]{3,2,1,0} %dynamic-slice.1302), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1239 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=21
  %compare.1304 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1305 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1306 = s32[] select(pred[] %compare.1304, s32[] %add.1305, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1307 = f32[1,2560,8,128]{3,2,1,0} dynamic-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1239, s32[] %select.1306, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,2560,8,128}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1308 = f32[2560,8,128]{2,1,0} reshape(f32[1,2560,8,128]{3,2,1,0} %dynamic-slice.1307), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1240 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=22
  %compare.1309 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1310 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1311 = s32[] select(pred[] %compare.1309, s32[] %add.1310, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1312 = f32[1,2560,8,128]{3,2,1,0} dynamic-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1240, s32[] %select.1311, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,2560,8,128}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1313 = f32[2560,8,128]{2,1,0} reshape(f32[1,2560,8,128]{3,2,1,0} %dynamic-slice.1312), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1241 = bf16[8,16,2048,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=23
  %compare.1314 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1315 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1316 = s32[] select(pred[] %compare.1314, s32[] %add.1315, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-slice.1317 = bf16[1,16,2048,2560]{3,2,1,0} dynamic-slice(bf16[8,16,2048,2560]{3,2,1,0} %get-tuple-element.1241, s32[] %select.1316, s32[] %constant.1244, s32[] %constant.1244, s32[] %constant.1244), dynamic_slice_sizes={1,16,2048,2560}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %reshape.1318 = bf16[16,2048,2560]{2,1,0} reshape(bf16[1,16,2048,2560]{3,2,1,0} %dynamic-slice.1317), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/squeeze" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %call.1319 = (bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) call(s32[16,2048]{1,0} %get-tuple-element.1242, s32[16,2048]{1,0} %get-tuple-element.1243, bf16[16,2048,2560]{2,1,0} %get-tuple-element.1219, bf16[16,2048,8,128]{3,2,1,0} %reshape.1253, bf16[16,2048,8,128]{3,2,1,0} %reshape.1258, /*index=5*/bf16[16,2048,8,128]{3,2,1,0} %reshape.1263, bf16[16,2048,8192]{2,1,0} %reshape.1268, bf16[16,2048,8192]{2,1,0} %reshape.1273, f32[2560,8192]{1,0} %reshape.1278, f32[2560,8192]{1,0} %reshape.1283, /*index=10*/f32[8192,2560]{1,0} %reshape.1288, f32[2560]{0} %reshape.1293, f32[2560,8,128]{2,1,0} %reshape.1298, f32[8,128,2560]{2,1,0} %reshape.1303, f32[2560,8,128]{2,1,0} %reshape.1308, /*index=15*/f32[2560,8,128]{2,1,0} %reshape.1313, bf16[16,2048,2560]{2,1,0} %reshape.1318), to_apply=%None_2.838
  %get-tuple-element.1320 = bf16[16,2048,2560]{2,1,0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=0
  %get-tuple-element.1220 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=2
  %get-tuple-element.1321 = f32[2560,8192]{0,1} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=1
  %reshape.1329 = f32[1,2560,8192]{2,1,0} reshape(f32[2560,8192]{0,1} %get-tuple-element.1321), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1330 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1331 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1332 = s32[] select(pred[] %compare.1330, s32[] %add.1331, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1333 = f32[8,2560,8192]{2,1,0} dynamic-update-slice(f32[8,2560,8192]{2,1,0} %get-tuple-element.1220, f32[1,2560,8192]{2,1,0} %reshape.1329, s32[] %select.1332, s32[] %constant.1244, s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1221 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=3
  %get-tuple-element.1322 = f32[2560,8192]{0,1} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=2
  %reshape.1334 = f32[1,2560,8192]{2,1,0} reshape(f32[2560,8192]{0,1} %get-tuple-element.1322), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1335 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1336 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1337 = s32[] select(pred[] %compare.1335, s32[] %add.1336, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1338 = f32[8,2560,8192]{2,1,0} dynamic-update-slice(f32[8,2560,8192]{2,1,0} %get-tuple-element.1221, f32[1,2560,8192]{2,1,0} %reshape.1334, s32[] %select.1337, s32[] %constant.1244, s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1222 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=4
  %get-tuple-element.1323 = f32[8192,2560]{0,1} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=3
  %reshape.1339 = f32[1,8192,2560]{2,1,0} reshape(f32[8192,2560]{0,1} %get-tuple-element.1323), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1340 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1341 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1342 = s32[] select(pred[] %compare.1340, s32[] %add.1341, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1343 = f32[8,8192,2560]{2,1,0} dynamic-update-slice(f32[8,8192,2560]{2,1,0} %get-tuple-element.1222, f32[1,8192,2560]{2,1,0} %reshape.1339, s32[] %select.1342, s32[] %constant.1244, s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1223 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=5
  %get-tuple-element.1324 = f32[2560]{0} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=4
  %reshape.1344 = f32[1,2560]{1,0} reshape(f32[2560]{0} %get-tuple-element.1324), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1345 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1346 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1347 = s32[] select(pred[] %compare.1345, s32[] %add.1346, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1348 = f32[8,2560]{1,0} dynamic-update-slice(f32[8,2560]{1,0} %get-tuple-element.1223, f32[1,2560]{1,0} %reshape.1344, s32[] %select.1347, s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1224 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=6
  %get-tuple-element.1325 = f32[2560,8,128]{0,2,1} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=5
  %reshape.1349 = f32[1,2560,8,128]{3,2,1,0} reshape(f32[2560,8,128]{0,2,1} %get-tuple-element.1325), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1350 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1351 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1352 = s32[] select(pred[] %compare.1350, s32[] %add.1351, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1353 = f32[8,2560,8,128]{3,2,1,0} dynamic-update-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1224, f32[1,2560,8,128]{3,2,1,0} %reshape.1349, s32[] %select.1352, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1225 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=7
  %get-tuple-element.1326 = f32[8,128,2560]{1,0,2} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=6
  %reshape.1354 = f32[1,8,128,2560]{3,2,1,0} reshape(f32[8,128,2560]{1,0,2} %get-tuple-element.1326), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1355 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1356 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1357 = s32[] select(pred[] %compare.1355, s32[] %add.1356, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1358 = f32[8,8,128,2560]{3,2,1,0} dynamic-update-slice(f32[8,8,128,2560]{3,2,1,0} %get-tuple-element.1225, f32[1,8,128,2560]{3,2,1,0} %reshape.1354, s32[] %select.1357, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1226 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=8
  %get-tuple-element.1327 = f32[2560,8,128]{0,2,1} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=7
  %reshape.1359 = f32[1,2560,8,128]{3,2,1,0} reshape(f32[2560,8,128]{0,2,1} %get-tuple-element.1327), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1360 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1361 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1362 = s32[] select(pred[] %compare.1360, s32[] %add.1361, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1363 = f32[8,2560,8,128]{3,2,1,0} dynamic-update-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1226, f32[1,2560,8,128]{3,2,1,0} %reshape.1359, s32[] %select.1362, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1227 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1217), index=9
  %get-tuple-element.1328 = f32[2560,8,128]{0,2,1} get-tuple-element((bf16[16,2048,2560]{2,1,0}, f32[2560,8192]{0,1}, f32[2560,8192]{0,1}, f32[8192,2560]{0,1}, f32[2560]{0}, /*index=5*/f32[2560,8,128]{0,2,1}, f32[8,128,2560]{1,0,2}, f32[2560,8,128]{0,2,1}, f32[2560,8,128]{0,2,1}) %call.1319), index=8
  %reshape.1364 = f32[1,2560,8,128]{3,2,1,0} reshape(f32[2560,8,128]{0,2,1} %get-tuple-element.1328), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/broadcast_in_dim" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %compare.1365 = pred[] compare(s32[] %subtract.1248, s32[] %constant.1244), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %add.1366 = s32[] add(s32[] %subtract.1248, s32[] %constant.1246), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/add" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %select.1367 = s32[] select(pred[] %compare.1365, s32[] %add.1366, s32[] %subtract.1248), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/select_n" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %dynamic-update-slice.1368 = f32[8,2560,8,128]{3,2,1,0} dynamic-update-slice(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1227, f32[1,2560,8,128]{3,2,1,0} %reshape.1364, s32[] %select.1367, s32[] %constant.1244, s32[] %constant.1244, /*index=5*/s32[] %constant.1244), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/body/dynamic_update_slice" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  ROOT %tuple.1370 = (s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) tuple(s32[] %add.1369, bf16[16,2048,2560]{2,1,0} %get-tuple-element.1320, f32[8,2560,8192]{2,1,0} %dynamic-update-slice.1333, f32[8,2560,8192]{2,1,0} %dynamic-update-slice.1338, f32[8,8192,2560]{2,1,0} %dynamic-update-slice.1343, /*index=5*/f32[8,2560]{1,0} %dynamic-update-slice.1348, f32[8,2560,8,128]{3,2,1,0} %dynamic-update-slice.1353, f32[8,8,128,2560]{3,2,1,0} %dynamic-update-slice.1358, f32[8,2560,8,128]{3,2,1,0} %dynamic-update-slice.1363, f32[8,2560,8,128]{3,2,1,0} %dynamic-update-slice.1368, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.1228, bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.1229, bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.1230, bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.1231, bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.1232, /*index=15*/f32[8,2560,8192]{2,1,0} %get-tuple-element.1233, f32[8,2560,8192]{2,1,0} %get-tuple-element.1234, f32[8,8192,2560]{2,1,0} %get-tuple-element.1235, f32[8,2560]{1,0} %get-tuple-element.1236, f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1237, /*index=20*/f32[8,8,128,2560]{3,2,1,0} %get-tuple-element.1238, f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1239, f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1240, bf16[8,16,2048,2560]{3,2,1,0} %get-tuple-element.1241, s32[16,2048]{1,0} %get-tuple-element.1242, /*index=25*/s32[16,2048]{1,0} %get-tuple-element.1243)
}

%region_28.1371 (arg_tuple.1372: (s32[], bf16[16,2048,2560], f32[8,2560,8192], f32[8,2560,8192], f32[8,8192,2560], /*index=5*/f32[8,2560], f32[8,2560,8,128], f32[8,8,128,2560], f32[8,2560,8,128], f32[8,2560,8,128], /*index=10*/bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8,128], bf16[8,16,2048,8192], bf16[8,16,2048,8192], /*index=15*/f32[8,2560,8192], f32[8,2560,8192], f32[8,8192,2560], f32[8,2560], f32[8,2560,8,128], /*index=20*/f32[8,8,128,2560], f32[8,2560,8,128], f32[8,2560,8,128], bf16[8,16,2048,2560], s32[16,2048], /*index=25*/s32[16,2048])) -> pred[] {
  %arg_tuple.1372 = (s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) parameter(0)
  %get-tuple-element.1374 = bf16[16,2048,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=1
  %get-tuple-element.1375 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=2
  %get-tuple-element.1376 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=3
  %get-tuple-element.1377 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=4
  %get-tuple-element.1378 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=5
  %get-tuple-element.1379 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=6
  %get-tuple-element.1380 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=7
  %get-tuple-element.1381 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=8
  %get-tuple-element.1382 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=9
  %get-tuple-element.1383 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=10
  %get-tuple-element.1384 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=11
  %get-tuple-element.1385 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=12
  %get-tuple-element.1386 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=13
  %get-tuple-element.1387 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=14
  %get-tuple-element.1388 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=15
  %get-tuple-element.1389 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=16
  %get-tuple-element.1390 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=17
  %get-tuple-element.1391 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=18
  %get-tuple-element.1392 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=19
  %get-tuple-element.1393 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=20
  %get-tuple-element.1394 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=21
  %get-tuple-element.1395 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=22
  %get-tuple-element.1396 = bf16[8,16,2048,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=23
  %get-tuple-element.1397 = s32[16,2048]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=24
  %get-tuple-element.1398 = s32[16,2048]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=25
  %get-tuple-element.1373 = s32[] get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=20*/f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %arg_tuple.1372), index=0
  %constant.1399 = s32[] constant(8)
  ROOT %compare.1400 = pred[] compare(s32[] %get-tuple-element.1373, s32[] %constant.1399), direction=LT, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while/cond/lt" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
}

%region_29.1421 (Arg_0.1422: bf16[], Arg_1.1423: bf16[]) -> bf16[] {
  %Arg_0.1422 = bf16[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder/scatter-add"}
  %Arg_1.1423 = bf16[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder/scatter-add"}
  ROOT %add.1424 = bf16[] add(bf16[] %Arg_0.1422, bf16[] %Arg_1.1423), metadata={op_name="/add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
}

%region_30.1429 (Arg_0.1430: f32[], Arg_1.1431: f32[]) -> f32[] {
  %Arg_0.1430 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1431 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1432 = f32[] add(f32[] %Arg_0.1430, f32[] %Arg_1.1431), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_31.1435 (Arg_0.1436: f32[], Arg_1.1437: f32[]) -> f32[] {
  %Arg_0.1436 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1437 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1438 = f32[] add(f32[] %Arg_0.1436, f32[] %Arg_1.1437), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_32.1442 (Arg_0.1443: f32[], Arg_1.1444: f32[]) -> f32[] {
  %Arg_0.1443 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1444 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1445 = f32[] add(f32[] %Arg_0.1443, f32[] %Arg_1.1444), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_33.1449 (Arg_0.1450: f32[], Arg_1.1451: f32[]) -> f32[] {
  %Arg_0.1450 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1451 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1452 = f32[] add(f32[] %Arg_0.1450, f32[] %Arg_1.1451), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_34.1456 (Arg_0.1457: f32[], Arg_1.1458: f32[]) -> f32[] {
  %Arg_0.1457 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1458 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1459 = f32[] add(f32[] %Arg_0.1457, f32[] %Arg_1.1458), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_35.1463 (Arg_0.1464: f32[], Arg_1.1465: f32[]) -> f32[] {
  %Arg_0.1464 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1465 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1466 = f32[] add(f32[] %Arg_0.1464, f32[] %Arg_1.1465), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_36.1470 (Arg_0.1471: f32[], Arg_1.1472: f32[]) -> f32[] {
  %Arg_0.1471 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1472 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1473 = f32[] add(f32[] %Arg_0.1471, f32[] %Arg_1.1472), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_37.1477 (Arg_0.1478: f32[], Arg_1.1479: f32[]) -> f32[] {
  %Arg_0.1478 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1479 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1480 = f32[] add(f32[] %Arg_0.1478, f32[] %Arg_1.1479), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_38.1484 (Arg_0.1485: f32[], Arg_1.1486: f32[]) -> f32[] {
  %Arg_0.1485 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1486 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1487 = f32[] add(f32[] %Arg_0.1485, f32[] %Arg_1.1486), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%region_39.1491 (Arg_0.1492: f32[], Arg_1.1493: f32[]) -> f32[] {
  %Arg_0.1492 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1493 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1494 = f32[] add(f32[] %Arg_0.1492, f32[] %Arg_1.1493), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
}

%_where_8.1611 (Arg_0.1612: pred[], Arg_1.1613: s32[], Arg_2.1614: s32[]) -> s32[] {
  %Arg_0.1612 = pred[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_1.1613 = s32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_2.1614 = s32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  ROOT %select.1615 = s32[] select(pred[] %Arg_0.1612, s32[] %Arg_1.1613, s32[] %Arg_2.1614), metadata={op_name="jit(train_step)/jit(main)/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142}
}

%clip.1714 (Arg_0.1715: s32[], Arg_1.1716: s32[], Arg_2.1717: s32[]) -> s32[] {
  %Arg_2.1717 = s32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_1.1716 = s32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_0.1715 = s32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %maximum.1718 = s32[] maximum(s32[] %Arg_1.1716, s32[] %Arg_0.1715), metadata={op_name="jit(train_step)/jit(main)/jit(clip)/max" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=143}
  ROOT %minimum.1719 = s32[] minimum(s32[] %Arg_2.1717, s32[] %maximum.1718), metadata={op_name="jit(train_step)/jit(main)/jit(clip)/min" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=143}
}

%_where_9.1738 (Arg_0.1739: pred[], Arg_1.1740: f32[], Arg_2.1741: f32[]) -> f32[] {
  %Arg_0.1739 = pred[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_1.1740 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_2.1741 = f32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  ROOT %select.1742 = f32[] select(pred[] %Arg_0.1739, f32[] %Arg_1.1740, f32[] %Arg_2.1741), metadata={op_name="jit(train_step)/jit(main)/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_join.py" source_line=43}
}

%_where_8.1767 (Arg_0.1768: pred[], Arg_1.1769: s32[], Arg_2.1770: s32[]) -> s32[] {
  %Arg_0.1768 = pred[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_1.1769 = s32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  %Arg_2.1770 = s32[] parameter(2), metadata={op_name="jit(train_step)/jit(main)/pjit"}
  ROOT %select.1771 = s32[] select(pred[] %Arg_0.1768, s32[] %Arg_1.1769, s32[] %Arg_2.1770), metadata={op_name="jit(train_step)/jit(main)/jit(_where)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142}
}

%region_40.1785 (Arg_0.1786: f32[], Arg_1.1787: f32[]) -> f32[] {
  %Arg_0.1786 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1787 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1788 = f32[] add(f32[] %Arg_0.1786, f32[] %Arg_1.1787), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_41.1791 (Arg_0.1792: f32[], Arg_1.1793: f32[]) -> f32[] {
  %Arg_0.1792 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1793 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1794 = f32[] add(f32[] %Arg_0.1792, f32[] %Arg_1.1793), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_42.1798 (Arg_0.1799: f32[], Arg_1.1800: f32[]) -> f32[] {
  %Arg_0.1799 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1800 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1801 = f32[] add(f32[] %Arg_0.1799, f32[] %Arg_1.1800), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_43.1805 (Arg_0.1806: f32[], Arg_1.1807: f32[]) -> f32[] {
  %Arg_0.1806 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1807 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1808 = f32[] add(f32[] %Arg_0.1806, f32[] %Arg_1.1807), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_44.1812 (Arg_0.1813: f32[], Arg_1.1814: f32[]) -> f32[] {
  %Arg_0.1813 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1814 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1815 = f32[] add(f32[] %Arg_0.1813, f32[] %Arg_1.1814), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_45.1819 (Arg_0.1820: f32[], Arg_1.1821: f32[]) -> f32[] {
  %Arg_0.1820 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1821 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1822 = f32[] add(f32[] %Arg_0.1820, f32[] %Arg_1.1821), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_46.1826 (Arg_0.1827: f32[], Arg_1.1828: f32[]) -> f32[] {
  %Arg_0.1827 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1828 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1829 = f32[] add(f32[] %Arg_0.1827, f32[] %Arg_1.1828), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_47.1833 (Arg_0.1834: f32[], Arg_1.1835: f32[]) -> f32[] {
  %Arg_0.1834 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1835 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1836 = f32[] add(f32[] %Arg_0.1834, f32[] %Arg_1.1835), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_48.1840 (Arg_0.1841: f32[], Arg_1.1842: f32[]) -> f32[] {
  %Arg_0.1841 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1842 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1843 = f32[] add(f32[] %Arg_0.1841, f32[] %Arg_1.1842), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_49.1847 (Arg_0.1848: f32[], Arg_1.1849: f32[]) -> f32[] {
  %Arg_0.1848 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1849 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1850 = f32[] add(f32[] %Arg_0.1848, f32[] %Arg_1.1849), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_50.1855 (Arg_0.1856: f32[], Arg_1.1857: f32[]) -> f32[] {
  %Arg_0.1856 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1857 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1858 = f32[] add(f32[] %Arg_0.1856, f32[] %Arg_1.1857), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_51.1861 (Arg_0.1862: f32[], Arg_1.1863: f32[]) -> f32[] {
  %Arg_0.1862 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1863 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1864 = f32[] add(f32[] %Arg_0.1862, f32[] %Arg_1.1863), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_52.1868 (Arg_0.1869: f32[], Arg_1.1870: f32[]) -> f32[] {
  %Arg_0.1869 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1870 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1871 = f32[] add(f32[] %Arg_0.1869, f32[] %Arg_1.1870), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_53.1875 (Arg_0.1876: f32[], Arg_1.1877: f32[]) -> f32[] {
  %Arg_0.1876 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1877 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1878 = f32[] add(f32[] %Arg_0.1876, f32[] %Arg_1.1877), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_54.1882 (Arg_0.1883: f32[], Arg_1.1884: f32[]) -> f32[] {
  %Arg_0.1883 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1884 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1885 = f32[] add(f32[] %Arg_0.1883, f32[] %Arg_1.1884), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_55.1889 (Arg_0.1890: f32[], Arg_1.1891: f32[]) -> f32[] {
  %Arg_0.1890 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1891 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1892 = f32[] add(f32[] %Arg_0.1890, f32[] %Arg_1.1891), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_56.1896 (Arg_0.1897: f32[], Arg_1.1898: f32[]) -> f32[] {
  %Arg_0.1897 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1898 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1899 = f32[] add(f32[] %Arg_0.1897, f32[] %Arg_1.1898), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_57.1903 (Arg_0.1904: f32[], Arg_1.1905: f32[]) -> f32[] {
  %Arg_0.1904 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1905 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1906 = f32[] add(f32[] %Arg_0.1904, f32[] %Arg_1.1905), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_58.1910 (Arg_0.1911: f32[], Arg_1.1912: f32[]) -> f32[] {
  %Arg_0.1911 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1912 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1913 = f32[] add(f32[] %Arg_0.1911, f32[] %Arg_1.1912), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_59.1917 (Arg_0.1918: f32[], Arg_1.1919: f32[]) -> f32[] {
  %Arg_0.1918 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1919 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1920 = f32[] add(f32[] %Arg_0.1918, f32[] %Arg_1.1919), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_60.1925 (Arg_0.1926: f32[], Arg_1.1927: f32[]) -> f32[] {
  %Arg_0.1926 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1927 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1928 = f32[] add(f32[] %Arg_0.1926, f32[] %Arg_1.1927), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_61.1931 (Arg_0.1932: f32[], Arg_1.1933: f32[]) -> f32[] {
  %Arg_0.1932 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1933 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1934 = f32[] add(f32[] %Arg_0.1932, f32[] %Arg_1.1933), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_62.1938 (Arg_0.1939: f32[], Arg_1.1940: f32[]) -> f32[] {
  %Arg_0.1939 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1940 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1941 = f32[] add(f32[] %Arg_0.1939, f32[] %Arg_1.1940), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_63.1945 (Arg_0.1946: f32[], Arg_1.1947: f32[]) -> f32[] {
  %Arg_0.1946 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1947 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1948 = f32[] add(f32[] %Arg_0.1946, f32[] %Arg_1.1947), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_64.1952 (Arg_0.1953: f32[], Arg_1.1954: f32[]) -> f32[] {
  %Arg_0.1953 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1954 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1955 = f32[] add(f32[] %Arg_0.1953, f32[] %Arg_1.1954), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_65.1959 (Arg_0.1960: f32[], Arg_1.1961: f32[]) -> f32[] {
  %Arg_0.1960 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1961 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1962 = f32[] add(f32[] %Arg_0.1960, f32[] %Arg_1.1961), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_66.1966 (Arg_0.1967: f32[], Arg_1.1968: f32[]) -> f32[] {
  %Arg_0.1967 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1968 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1969 = f32[] add(f32[] %Arg_0.1967, f32[] %Arg_1.1968), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_67.1973 (Arg_0.1974: f32[], Arg_1.1975: f32[]) -> f32[] {
  %Arg_0.1974 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1975 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1976 = f32[] add(f32[] %Arg_0.1974, f32[] %Arg_1.1975), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_68.1980 (Arg_0.1981: f32[], Arg_1.1982: f32[]) -> f32[] {
  %Arg_0.1981 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1982 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1983 = f32[] add(f32[] %Arg_0.1981, f32[] %Arg_1.1982), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

%region_69.1987 (Arg_0.1988: f32[], Arg_1.1989: f32[]) -> f32[] {
  %Arg_0.1988 = f32[] parameter(0), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  %Arg_1.1989 = f32[] parameter(1), metadata={op_name="jit(train_step)/jit(main)/reduce_sum"}
  ROOT %add.1990 = f32[] add(f32[] %Arg_0.1988, f32[] %Arg_1.1989), metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
}

ENTRY %main.2034 (Arg_0.1: s32[], Arg_1.2: f32[2560], Arg_2.3: f32[2560,8,8192], Arg_3.4: f32[2560,8,8192], Arg_4.5: f32[8192,8,2560], Arg_5.6: f32[2560,8], Arg_6.7: f32[2560,8,8,128], Arg_7.8: f32[8,8,128,2560], Arg_8.9: f32[2560,8,8,128], Arg_9.10: f32[2560,8,8,128], Arg_10.11: f32[32000,2560], Arg_11.12: s32[], Arg_12.13: f32[2560], Arg_13.14: f32[2560,8,8192], Arg_14.15: f32[2560,8,8192], Arg_15.16: f32[8192,8,2560], Arg_16.17: f32[2560,8], Arg_17.18: f32[2560,8,8,128], Arg_18.19: f32[8,8,128,2560], Arg_19.20: f32[2560,8,8,128], Arg_20.21: f32[2560,8,8,128], Arg_21.22: f32[32000,2560], Arg_22.23: f32[2560], Arg_23.24: f32[2560,8,8192], Arg_24.25: f32[2560,8,8192], Arg_25.26: f32[8192,8,2560], Arg_26.27: f32[2560,8], Arg_27.28: f32[2560,8,8,128], Arg_28.29: f32[8,8,128,2560], Arg_29.30: f32[2560,8,8,128], Arg_30.31: f32[2560,8,8,128], Arg_31.32: f32[32000,2560], Arg_32.33: s32[], Arg_33.34: s32[16,2048], Arg_34.35: s32[16,2048], Arg_35.36: s32[16,2048], Arg_36.37: s32[16,2048], Arg_37.38: s32[16,2048]) -> (s32[], f32[2560], f32[2560,8,8192], f32[2560,8,8192], f32[8192,8,2560], /*index=5*/f32[2560,8], f32[2560,8,8,128], f32[8,8,128,2560], f32[2560,8,8,128], f32[2560,8,8,128], /*index=10*/f32[32000,2560], s32[], f32[2560], f32[2560,8,8192], f32[2560,8,8192], /*index=15*/f32[8192,8,2560], f32[2560,8], f32[2560,8,8,128], f32[8,8,128,2560], f32[2560,8,8,128], /*index=20*/f32[2560,8,8,128], f32[32000,2560], f32[2560], f32[2560,8,8192], f32[2560,8,8192], /*index=25*/f32[8192,8,2560], f32[2560,8], f32[2560,8,8,128], f32[8,8,128,2560], f32[2560,8,8,128], /*index=30*/f32[2560,8,8,128], f32[32000,2560], s32[], f32[], f32[], /*index=35*/f32[], f32[], f32[], s32[]) {
  %constant.172 = s32[] constant(0)
  %Arg_10.11 = f32[32000,2560]{1,0} parameter(10), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'token_embedder\'][\'embedding\']"}
  %convert.173 = bf16[32000,2560]{1,0} convert(f32[32000,2560]{1,0} %Arg_10.11), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %Arg_33.34 = s32[16,2048]{1,0} parameter(33), sharding={devices=[8,1]<=[8]}, metadata={op_name="data[\'inputs\']"}
  %constant.151 = s32[] constant(0)
  %broadcast.152 = s32[16,2048]{1,0} broadcast(s32[] %constant.151), dimensions={}
  %compare.174 = pred[16,2048]{1,0} compare(s32[16,2048]{1,0} %Arg_33.34, s32[16,2048]{1,0} %broadcast.152), direction=LT, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/lt" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %constant.149 = s32[] constant(32000)
  %broadcast.150 = s32[16,2048]{1,0} broadcast(s32[] %constant.149), dimensions={}
  %add.175 = s32[16,2048]{1,0} add(s32[16,2048]{1,0} %Arg_33.34, s32[16,2048]{1,0} %broadcast.150), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %select.176 = s32[16,2048]{1,0} select(pred[16,2048]{1,0} %compare.174, s32[16,2048]{1,0} %add.175, s32[16,2048]{1,0} %Arg_33.34), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/select_n" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %reshape.177 = s32[16,2048,1]{2,1,0} reshape(s32[16,2048]{1,0} %select.176), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %gather.178 = bf16[16,2048,2560]{2,1,0} gather(bf16[32000,2560]{1,0} %convert.173, s32[16,2048,1]{2,1,0} %reshape.177), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2560}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/gather" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %custom-call.179 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %gather.178), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %constant.139 = bf16[] constant(0)
  %broadcast.140 = bf16[8,16,2048,8,128]{4,3,2,1,0} broadcast(bf16[] %constant.139), dimensions={}
  %constant.137 = bf16[] constant(0)
  %broadcast.138 = bf16[8,16,2048,8192]{3,2,1,0} broadcast(bf16[] %constant.137), dimensions={}
  %constant.135 = bf16[] constant(0)
  %broadcast.136 = bf16[8,16,2048,2560]{3,2,1,0} broadcast(bf16[] %constant.135), dimensions={}
  %Arg_2.3 = f32[2560,8,8192]{2,1,0} parameter(2), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_0\'][\'kernel\']"}
  %transpose.180 = f32[8,2560,8192]{2,0,1} transpose(f32[2560,8,8192]{2,1,0} %Arg_2.3), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_3.4 = f32[2560,8,8192]{2,1,0} parameter(3), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_1\'][\'kernel\']"}
  %transpose.181 = f32[8,2560,8192]{2,0,1} transpose(f32[2560,8,8192]{2,1,0} %Arg_3.4), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_4.5 = f32[8192,8,2560]{2,1,0} parameter(4), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wo\'][\'kernel\']"}
  %transpose.182 = f32[8,8192,2560]{2,0,1} transpose(f32[8192,8,2560]{2,1,0} %Arg_4.5), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_5.6 = f32[2560,8]{1,0} parameter(5), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'pre_self_attention_norm\'][\'scale\']"}
  %transpose.183 = f32[8,2560]{0,1} transpose(f32[2560,8]{1,0} %Arg_5.6), dimensions={1,0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_6.7 = f32[2560,8,8,128]{3,2,1,0} parameter(6), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'key\'][\'kernel\']"}
  %transpose.184 = f32[8,2560,8,128]{3,2,0,1} transpose(f32[2560,8,8,128]{3,2,1,0} %Arg_6.7), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_7.8 = f32[8,8,128,2560]{3,2,1,0} parameter(7), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'out\'][\'kernel\']"}
  %transpose.185 = f32[8,8,128,2560]{3,2,0,1} transpose(f32[8,8,128,2560]{3,2,1,0} %Arg_7.8), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_8.9 = f32[2560,8,8,128]{3,2,1,0} parameter(8), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'query\'][\'kernel\']"}
  %transpose.186 = f32[8,2560,8,128]{3,2,0,1} transpose(f32[2560,8,8,128]{3,2,1,0} %Arg_8.9), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_9.10 = f32[2560,8,8,128]{3,2,1,0} parameter(9), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'value\'][\'kernel\']"}
  %transpose.187 = f32[8,2560,8,128]{3,2,0,1} transpose(f32[2560,8,8,128]{3,2,1,0} %Arg_9.10), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %Arg_34.35 = s32[16,2048]{1,0} parameter(34), sharding={devices=[8,1]<=[8]}, metadata={op_name="data[\'inputs_position\']"}
  %reshape.212 = s32[16,2048,1,1]{3,2,1,0} reshape(s32[16,2048]{1,0} %Arg_34.35), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=163}
  %convert.213 = f32[16,2048,1,1]{3,2,1,0} convert(s32[16,2048,1,1]{3,2,1,0} %reshape.212), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.215 = f32[16,2048,1,1]{3,2,1,0} broadcast(f32[16,2048,1,1]{3,2,1,0} %convert.213), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.216 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048,1,1]{3,2,1,0} %broadcast.215), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.217 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.216), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %constant.143 = f32[] constant(10000)
  %broadcast.144 = f32[64]{0} broadcast(f32[] %constant.143), dimensions={}
  %iota.207 = s32[64]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/iota" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %constant.147 = s32[] constant(2)
  %broadcast.148 = s32[64]{0} broadcast(s32[] %constant.147), dimensions={}
  %multiply.208 = s32[64]{0} multiply(s32[64]{0} %iota.207, s32[64]{0} %broadcast.148), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %convert.209 = f32[64]{0} convert(s32[64]{0} %multiply.208), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %constant.145 = f32[] constant(128)
  %broadcast.146 = f32[64]{0} broadcast(f32[] %constant.145), dimensions={}
  %divide.210 = f32[64]{0} divide(f32[64]{0} %convert.209, f32[64]{0} %broadcast.146), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %power.211 = f32[64]{0} power(f32[64]{0} %broadcast.144, f32[64]{0} %divide.210), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary.setup/pow" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=134}
  %reshape.214 = f32[1,1,1,64]{3,2,1,0} reshape(f32[64]{0} %power.211), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.218 = f32[1,1,1,64]{3,2,1,0} broadcast(f32[1,1,1,64]{3,2,1,0} %reshape.214), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.219 = f32[1,64]{1,0} reshape(f32[1,1,1,64]{3,2,1,0} %broadcast.218), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.220 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[1,64]{1,0} %reshape.219), dimensions={2,3}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %divide.221 = f32[16,2048,1,64]{3,2,1,0} divide(f32[16,2048,1,64]{3,2,1,0} %broadcast.217, f32[16,2048,1,64]{3,2,1,0} %broadcast.220), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %cosine.224 = f32[16,2048,1,64]{3,2,1,0} cosine(f32[16,2048,1,64]{3,2,1,0} %divide.221), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/cos" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %convert.225 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %cosine.224), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %sine.222 = f32[16,2048,1,64]{3,2,1,0} sine(f32[16,2048,1,64]{3,2,1,0} %divide.221), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/sin" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %convert.223 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %sine.222), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/key_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %reshape.193 = s32[16,2048,1,1]{3,2,1,0} reshape(s32[16,2048]{1,0} %Arg_34.35), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=163}
  %convert.194 = f32[16,2048,1,1]{3,2,1,0} convert(s32[16,2048,1,1]{3,2,1,0} %reshape.193), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.196 = f32[16,2048,1,1]{3,2,1,0} broadcast(f32[16,2048,1,1]{3,2,1,0} %convert.194), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.197 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048,1,1]{3,2,1,0} %broadcast.196), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.198 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.197), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %iota.188 = s32[64]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/iota" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %multiply.189 = s32[64]{0} multiply(s32[64]{0} %iota.188, s32[64]{0} %broadcast.148), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/mul" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %convert.190 = f32[64]{0} convert(s32[64]{0} %multiply.189), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %divide.191 = f32[64]{0} divide(f32[64]{0} %convert.190, f32[64]{0} %broadcast.146), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=133}
  %power.192 = f32[64]{0} power(f32[64]{0} %broadcast.144, f32[64]{0} %divide.191), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary.setup/pow" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=134}
  %reshape.195 = f32[1,1,1,64]{3,2,1,0} reshape(f32[64]{0} %power.192), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.199 = f32[1,1,1,64]{3,2,1,0} broadcast(f32[1,1,1,64]{3,2,1,0} %reshape.195), dimensions={0,1,2,3}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %reshape.200 = f32[1,64]{1,0} reshape(f32[1,1,1,64]{3,2,1,0} %broadcast.199), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %broadcast.201 = f32[16,2048,1,64]{3,2,1,0} broadcast(f32[1,64]{1,0} %reshape.200), dimensions={2,3}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %divide.202 = f32[16,2048,1,64]{3,2,1,0} divide(f32[16,2048,1,64]{3,2,1,0} %broadcast.198, f32[16,2048,1,64]{3,2,1,0} %broadcast.201), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/div" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=164}
  %cosine.205 = f32[16,2048,1,64]{3,2,1,0} cosine(f32[16,2048,1,64]{3,2,1,0} %divide.202), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/cos" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %convert.206 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %cosine.205), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=166}
  %sine.203 = f32[16,2048,1,64]{3,2,1,0} sine(f32[16,2048,1,64]{3,2,1,0} %divide.202), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/sin" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %convert.204 = bf16[16,2048,1,64]{3,2,1,0} convert(f32[16,2048,1,64]{3,2,1,0} %sine.203), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/self_attention.apply_rotary_embedding/query_rotary/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=165}
  %Arg_35.36 = s32[16,2048]{1,0} parameter(35), sharding={devices=[8,1]<=[8]}, metadata={op_name="data[\'inputs_segmentation\']"}
  %reshape.226 = s32[16,2048,1]{2,1,0} reshape(s32[16,2048]{1,0} %Arg_35.36), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.228 = s32[16,2048,1]{2,1,0} broadcast(s32[16,2048,1]{2,1,0} %reshape.226), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.229 = s32[16,2048]{1,0} reshape(s32[16,2048,1]{2,1,0} %broadcast.228), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.230 = s32[16,2048,2048]{2,1,0} broadcast(s32[16,2048]{1,0} %reshape.229), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.227 = s32[16,1,2048]{2,1,0} reshape(s32[16,2048]{1,0} %Arg_35.36), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.231 = s32[16,1,2048]{2,1,0} broadcast(s32[16,1,2048]{2,1,0} %reshape.227), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.232 = s32[16,2048]{1,0} reshape(s32[16,1,2048]{2,1,0} %broadcast.231), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %broadcast.233 = s32[16,2048,2048]{2,1,0} broadcast(s32[16,2048]{1,0} %reshape.232), dimensions={0,2}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %compare.234 = pred[16,2048,2048]{2,1,0} compare(s32[16,2048,2048]{2,1,0} %broadcast.230, s32[16,2048,2048]{2,1,0} %broadcast.233), direction=EQ, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/eq" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=181}
  %reshape.235 = pred[16,1,1,2048,2048]{4,3,2,1,0} reshape(pred[16,2048,2048]{2,1,0} %compare.234), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=182}
  %iota.238 = s32[2048]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=191}
  %broadcast.239 = s32[2048,2048]{1,0} broadcast(s32[2048]{0} %iota.238), dimensions={1}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=191}
  %iota.236 = s32[2048]{0} iota(), iota_dimension=0, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=190}
  %broadcast.237 = s32[2048,2048]{1,0} broadcast(s32[2048]{0} %iota.236), dimensions={0}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/iota" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=190}
  %compare.240 = pred[2048,2048]{1,0} compare(s32[2048,2048]{1,0} %broadcast.239, s32[2048,2048]{1,0} %broadcast.237), direction=LE, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/le" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=192}
  %reshape.241 = pred[1,1,1,2048,2048]{4,3,2,1,0} reshape(pred[2048,2048]{1,0} %compare.240), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=192}
  %broadcast.242 = pred[1,1,1,2048,2048]{4,3,2,1,0} broadcast(pred[1,1,1,2048,2048]{4,3,2,1,0} %reshape.241), dimensions={0,1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %reshape.243 = pred[1,1,2048,2048]{3,2,1,0} reshape(pred[1,1,1,2048,2048]{4,3,2,1,0} %broadcast.242), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %broadcast.244 = pred[16,1,1,2048,2048]{4,3,2,1,0} broadcast(pred[1,1,2048,2048]{3,2,1,0} %reshape.243), dimensions={1,2,3,4}, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %and.245 = pred[16,1,1,2048,2048]{4,3,2,1,0} and(pred[16,1,1,2048,2048]{4,3,2,1,0} %reshape.235, pred[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.244), metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/AttentionOp_0.generate_attention_mask/and" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=197}
  %constant.170 = f32[] constant(0)
  %constant.169 = f32[] constant(-2.38197633e+38)
  %call.253 = f32[16,1,1,2048,2048]{4,3,2,1,0} call(pred[16,1,1,2048,2048]{4,3,2,1,0} %and.245, f32[] %constant.170, f32[] %constant.169), to_apply=%_where.246
  %constant.141 = f32[] constant(-1.19098816e+38)
  %broadcast.142 = f32[16,1,1,2048,2048]{4,3,2,1,0} broadcast(f32[] %constant.141), dimensions={}
  %compare.254 = pred[16,1,1,2048,2048]{4,3,2,1,0} compare(f32[16,1,1,2048,2048]{4,3,2,1,0} %call.253, f32[16,1,1,2048,2048]{4,3,2,1,0} %broadcast.142), direction=GE, metadata={op_name="jit(train_step)/jit(main)/layers/self_attention/AttentionOp_0/AttentionOp_0.apply_attention/AttentionOp_0.apply_attention_dot/ge" source_file="/opt/maxtext/MaxText/layers/attentions.py" source_line=131}
  %call.264 = (pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) call(pred[16,1,1,2048,2048]{4,3,2,1,0} %compare.254, f32[] %constant.169), to_apply=%_where_0.255
  %get-tuple-element.265 = pred[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %call.264), index=0
  %get-tuple-element.266 = bf16[16,8,1,2048,2048]{4,3,2,1,0} get-tuple-element((pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %call.264), index=1
  %tuple.267 = (s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) tuple(s32[] %constant.172, bf16[16,2048,2560]{2,1,0} %custom-call.179, bf16[8,16,2048,8,128]{4,3,2,1,0} %broadcast.140, bf16[8,16,2048,8,128]{4,3,2,1,0} %broadcast.140, bf16[8,16,2048,8,128]{4,3,2,1,0} %broadcast.140, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0} %broadcast.138, bf16[8,16,2048,8192]{3,2,1,0} %broadcast.138, bf16[8,16,2048,2560]{3,2,1,0} %broadcast.136, f32[8,2560,8192]{2,0,1} %transpose.180, f32[8,2560,8192]{2,0,1} %transpose.181, /*index=10*/f32[8,8192,2560]{2,0,1} %transpose.182, f32[8,2560]{0,1} %transpose.183, f32[8,2560,8,128]{3,2,0,1} %transpose.184, f32[8,8,128,2560]{3,2,0,1} %transpose.185, f32[8,2560,8,128]{3,2,0,1} %transpose.186, /*index=15*/f32[8,2560,8,128]{3,2,0,1} %transpose.187, bf16[16,2048,1,64]{3,2,1,0} %convert.225, bf16[16,2048,1,64]{3,2,1,0} %convert.223, bf16[16,2048,1,64]{3,2,1,0} %convert.206, bf16[16,2048,1,64]{3,2,1,0} %convert.204, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.265, bf16[16,8,1,2048,2048]{4,3,2,1,0} %get-tuple-element.266), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %while.569 = (s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) while((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %tuple.267), condition=%region_4.543, body=%region_0.436, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.570 = s32[] get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=0, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %constant.171 = f32[] constant(1)
  %Arg_37.38 = s32[16,2048]{1,0} parameter(37), sharding={devices=[8,1]<=[8]}, metadata={op_name="data[\'targets_segmentation\']"}
  %compare.667 = pred[16,2048]{1,0} compare(s32[16,2048]{1,0} %Arg_37.38, s32[16,2048]{1,0} %broadcast.152), direction=NE, metadata={op_name="jit(train_step)/jit(main)/ne" source_file="/opt/maxtext/MaxText/train.py" source_line=437}
  %convert.668 = s32[16,2048]{1,0} convert(pred[16,2048]{1,0} %compare.667), metadata={op_name="jit(train_step)/jit(main)/convert_element_type" source_file="/opt/maxtext/MaxText/train.py" source_line=437}
  %reduce.673 = s32[] reduce(s32[16,2048]{1,0} %convert.668, s32[] %constant.172), dimensions={0,1}, to_apply=%region_10.669, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/train.py" source_line=437}
  %convert.674 = f32[] convert(s32[] %reduce.673), metadata={op_name="jit(train_step)/jit(main)/convert_element_type" source_file="/opt/maxtext/MaxText/train.py" source_line=438}
  %constant.165 = f32[] constant(1e-08)
  %add.675 = f32[] add(f32[] %convert.674, f32[] %constant.165), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/train.py" source_line=438}
  %divide.677 = f32[] divide(f32[] %constant.171, f32[] %add.675), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/train.py" source_line=438}
  %broadcast.678 = f32[16,2048]{1,0} broadcast(f32[] %divide.677), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/train.py" source_line=436}
  %compare.659 = pred[16,2048]{1,0} compare(s32[16,2048]{1,0} %Arg_37.38, s32[16,2048]{1,0} %broadcast.152), direction=NE, metadata={op_name="jit(train_step)/jit(main)/ne" source_file="/opt/maxtext/MaxText/train.py" source_line=435}
  %convert.660 = f32[16,2048]{1,0} convert(pred[16,2048]{1,0} %compare.659), metadata={op_name="jit(train_step)/jit(main)/convert_element_type" source_file="/opt/maxtext/MaxText/train.py" source_line=435}
  %multiply.679 = f32[16,2048]{1,0} multiply(f32[16,2048]{1,0} %broadcast.678, f32[16,2048]{1,0} %convert.660), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=435}
  %custom-call.680 = f32[16,2048]{1,0} custom-call(f32[16,2048]{1,0} %multiply.679), custom_call_target="Sharding", sharding={devices=[8,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %reshape.693 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %custom-call.680), metadata={op_name="jit(train_step)/jit(main)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %broadcast.694 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.693), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %reshape.695 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.694), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %broadcast.696 = f32[16,2048,32000]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.695), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %get-tuple-element.571 = bf16[16,2048,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=1, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %convert.578 = f32[16,2048,2560]{2,1,0} convert(bf16[16,2048,2560]{2,1,0} %get-tuple-element.571), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=39}
  %multiply.579 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.578, f32[16,2048,2560]{2,1,0} %convert.578), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/square" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reduce.585 = f32[16,2048]{1,0} reduce(f32[16,2048,2560]{2,1,0} %multiply.579, f32[] %constant.170), dimensions={2}, to_apply=%region_5.581, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reshape.586 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.585), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.131 = f32[] constant(2560)
  %broadcast.132 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.131), dimensions={}
  %divide.587 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %reshape.586, f32[16,2048,1]{2,1,0} %broadcast.132), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.129 = f32[] constant(1e-05)
  %broadcast.130 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.129), dimensions={}
  %add.588 = f32[16,2048,1]{2,1,0} add(f32[16,2048,1]{2,1,0} %divide.587, f32[16,2048,1]{2,1,0} %broadcast.130), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/add" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %rsqrt.589 = f32[16,2048,1]{2,1,0} rsqrt(f32[16,2048,1]{2,1,0} %add.588), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/rsqrt" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.592 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %rsqrt.589), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.593 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.592), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.594 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.593), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.595 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.578, f32[16,2048,2560]{2,1,0} %broadcast.594), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %convert.596 = bf16[16,2048,2560]{2,1,0} convert(f32[16,2048,2560]{2,1,0} %multiply.595), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %Arg_1.2 = f32[2560]{0} parameter(1), sharding={replicated}, metadata={op_name="state.params[\'params\'][\'decoder\'][\'decoder_norm\'][\'scale\']"}
  %convert.597 = bf16[2560]{0} convert(f32[2560]{0} %Arg_1.2), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=50}
  %reshape.598 = bf16[1,1,2560]{2,1,0} reshape(bf16[2560]{0} %convert.597), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.599 = bf16[1,1,2560]{2,1,0} broadcast(bf16[1,1,2560]{2,1,0} %reshape.598), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.600 = bf16[2560]{0} reshape(bf16[1,1,2560]{2,1,0} %broadcast.599), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.601 = bf16[16,2048,2560]{2,1,0} broadcast(bf16[2560]{0} %reshape.600), dimensions={2}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %multiply.602 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %convert.596, bf16[16,2048,2560]{2,1,0} %broadcast.601), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %convert.603 = bf16[32000,2560]{1,0} convert(f32[32000,2560]{1,0} %Arg_10.11), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder.attend/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=104}
  %transpose.604 = bf16[2560,32000]{0,1} transpose(bf16[32000,2560]{1,0} %convert.603), dimensions={1,0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder.attend/transpose" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=104}
  %dot.605 = bf16[16,2048,32000]{2,1,0} dot(bf16[16,2048,2560]{2,1,0} %multiply.602, bf16[2560,32000]{0,1} %transpose.604), lhs_contracting_dims={2}, rhs_contracting_dims={0}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/token_embedder.attend/dot_general" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=104}
  %constant.125 = bf16[] constant(50.5)
  %broadcast.126 = bf16[16,2048,32000]{2,1,0} broadcast(bf16[] %constant.125), dimensions={}
  %divide.606 = bf16[16,2048,32000]{2,1,0} divide(bf16[16,2048,32000]{2,1,0} %dot.605, bf16[16,2048,32000]{2,1,0} %broadcast.126), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/div" source_file="/opt/maxtext/MaxText/layers/models.py" source_line=419}
  %custom-call.607 = bf16[16,2048,32000]{2,1,0} custom-call(bf16[16,2048,32000]{2,1,0} %divide.606), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %convert.608 = f32[16,2048,32000]{2,1,0} convert(bf16[16,2048,32000]{2,1,0} %custom-call.607), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/convert_element_type" source_file="/opt/maxtext/MaxText/layers/models.py" source_line=438}
  %constant.166 = f32[] constant(-inf)
  %reduce.627 = f32[16,2048]{1,0} reduce(f32[16,2048,32000]{2,1,0} %convert.608, f32[] %constant.166), dimensions={2}, to_apply=%region_6.623, metadata={op_name="jit(train_step)/jit(main)/reduce_max" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=876}
  %reshape.628 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.627), metadata={op_name="jit(train_step)/jit(main)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=876}
  %broadcast.629 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.628), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=877}
  %reshape.630 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.629), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=877}
  %broadcast.631 = f32[16,2048,32000]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.630), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=877}
  %subtract.632 = f32[16,2048,32000]{2,1,0} subtract(f32[16,2048,32000]{2,1,0} %convert.608, f32[16,2048,32000]{2,1,0} %broadcast.631), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=877}
  %exponential.633 = f32[16,2048,32000]{2,1,0} exponential(f32[16,2048,32000]{2,1,0} %subtract.632), metadata={op_name="jit(train_step)/jit(main)/exp" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=878}
  %reduce.638 = f32[16,2048]{1,0} reduce(f32[16,2048,32000]{2,1,0} %exponential.633, f32[] %constant.170), dimensions={2}, to_apply=%region_7.634, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=879}
  %reshape.639 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.638), metadata={op_name="jit(train_step)/jit(main)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=879}
  %log.652 = f32[16,2048,1]{2,1,0} log(f32[16,2048,1]{2,1,0} %reshape.639), metadata={op_name="jit(train_step)/jit(main)/log" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=883}
  %add.653 = f32[16,2048,1]{2,1,0} add(f32[16,2048,1]{2,1,0} %log.652, f32[16,2048,1]{2,1,0} %reshape.628), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=883}
  %reshape.654 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %add.653), metadata={op_name="jit(train_step)/jit(main)/squeeze" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=883}
  %constant.123 = f32[] constant(0)
  %broadcast.124 = f32[16,2048]{1,0} broadcast(f32[] %constant.123), dimensions={}
  %multiply.681 = f32[16,2048]{1,0} multiply(f32[16,2048]{1,0} %reshape.654, f32[16,2048]{1,0} %broadcast.124), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %constant.121 = f32[] constant(1)
  %broadcast.122 = f32[16,2048]{1,0} broadcast(f32[] %constant.121), dimensions={}
  %add.682 = f32[16,2048]{1,0} add(f32[16,2048]{1,0} %multiply.681, f32[16,2048]{1,0} %broadcast.122), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %reshape.683 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %add.682), metadata={op_name="jit(train_step)/jit(main)/broadcast_in_dim" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %broadcast.684 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.683), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %reshape.685 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.684), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %broadcast.686 = f32[16,2048,32000]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.685), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %multiply.687 = f32[16,2048,32000]{2,1,0} multiply(f32[16,2048,32000]{2,1,0} %broadcast.686, f32[16,2048,32000]{2,1,0} %exponential.633), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %broadcast.688 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %reshape.639), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %reshape.689 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.688), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %broadcast.690 = f32[16,2048,32000]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.689), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %divide.691 = f32[16,2048,32000]{2,1,0} divide(f32[16,2048,32000]{2,1,0} %multiply.687, f32[16,2048,32000]{2,1,0} %broadcast.690), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %Arg_36.37 = s32[16,2048]{1,0} parameter(36), sharding={devices=[8,1]<=[8]}, metadata={op_name="data[\'targets\']"}
  %call.622 = f32[16,2048,32000]{2,1,0} call(s32[16,2048]{1,0} %Arg_36.37), to_apply=%_one_hot.609
  %subtract.692 = f32[16,2048,32000]{2,1,0} subtract(f32[16,2048,32000]{2,1,0} %divide.691, f32[16,2048,32000]{2,1,0} %call.622), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %multiply.697 = f32[16,2048,32000]{2,1,0} multiply(f32[16,2048,32000]{2,1,0} %broadcast.696, f32[16,2048,32000]{2,1,0} %subtract.692), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=432}
  %convert.698 = bf16[16,2048,32000]{2,1,0} convert(f32[16,2048,32000]{2,1,0} %multiply.697), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/convert_element_type" source_file="/opt/maxtext/MaxText/layers/models.py" source_line=438}
  %custom-call.699 = bf16[16,2048,32000]{2,1,0} custom-call(bf16[16,2048,32000]{2,1,0} %convert.698), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %divide.700 = bf16[16,2048,32000]{2,1,0} divide(bf16[16,2048,32000]{2,1,0} %custom-call.699, bf16[16,2048,32000]{2,1,0} %broadcast.126), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/div" source_file="/opt/maxtext/MaxText/layers/models.py" source_line=419}
  %dot.702 = bf16[16,2048,2560]{2,1,0} dot(bf16[16,2048,32000]{2,1,0} %divide.700, bf16[2560,32000]{0,1} %transpose.604), lhs_contracting_dims={2}, rhs_contracting_dims={1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder.attend/dot_general" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=104}
  %broadcast.716 = bf16[1,1,2560]{2,1,0} broadcast(bf16[1,1,2560]{2,1,0} %reshape.598), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.717 = bf16[2560]{0} reshape(bf16[1,1,2560]{2,1,0} %broadcast.716), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %broadcast.718 = bf16[16,2048,2560]{2,1,0} broadcast(bf16[2560]{0} %reshape.717), dimensions={2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %multiply.719 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %dot.702, bf16[16,2048,2560]{2,1,0} %broadcast.718), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %convert.721 = f32[16,2048,2560]{2,1,0} convert(bf16[16,2048,2560]{2,1,0} %multiply.719), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.729 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %rsqrt.589), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.730 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.729), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %broadcast.731 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.730), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.732 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.721, f32[16,2048,2560]{2,1,0} %broadcast.731), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.722 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.578, f32[16,2048,2560]{2,1,0} %convert.721), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reduce.727 = f32[16,2048]{1,0} reduce(f32[16,2048,2560]{2,1,0} %multiply.722, f32[] %constant.170), dimensions={2}, to_apply=%region_13.723, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %reshape.728 = f32[16,2048,1]{2,1,0} reshape(f32[16,2048]{1,0} %reduce.727), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reshape" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %divide.590 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %rsqrt.589, f32[16,2048,1]{2,1,0} %add.588), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %constant.127 = f32[] constant(-0.5)
  %broadcast.128 = f32[16,2048,1]{2,1,0} broadcast(f32[] %constant.127), dimensions={}
  %multiply.591 = f32[16,2048,1]{2,1,0} multiply(f32[16,2048,1]{2,1,0} %divide.590, f32[16,2048,1]{2,1,0} %broadcast.128), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %multiply.733 = f32[16,2048,1]{2,1,0} multiply(f32[16,2048,1]{2,1,0} %reshape.728, f32[16,2048,1]{2,1,0} %multiply.591), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=42}
  %divide.734 = f32[16,2048,1]{2,1,0} divide(f32[16,2048,1]{2,1,0} %multiply.733, f32[16,2048,1]{2,1,0} %broadcast.132), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/div" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %reduce.739 = f32[16,2048]{1,0} reduce(f32[16,2048,1]{2,1,0} %divide.734, f32[] %constant.170), dimensions={2}, to_apply=%region_14.735, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %broadcast.740 = f32[16,2048,2560]{2,1,0} broadcast(f32[16,2048]{1,0} %reduce.739), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/broadcast_in_dim" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %constant.133 = f32[] constant(2)
  %broadcast.134 = f32[16,2048,2560]{2,1,0} broadcast(f32[] %constant.133), dimensions={}
  %multiply.580 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %convert.578, f32[16,2048,2560]{2,1,0} %broadcast.134), metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %multiply.741 = f32[16,2048,2560]{2,1,0} multiply(f32[16,2048,2560]{2,1,0} %broadcast.740, f32[16,2048,2560]{2,1,0} %multiply.580), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %add.742 = f32[16,2048,2560]{2,1,0} add(f32[16,2048,2560]{2,1,0} %multiply.732, f32[16,2048,2560]{2,1,0} %multiply.741), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/add_any" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=41}
  %convert.743 = bf16[16,2048,2560]{2,1,0} convert(f32[16,2048,2560]{2,1,0} %add.742), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=39}
  %constant.119 = f32[] constant(0)
  %broadcast.120 = f32[8,2560,8192]{2,1,0} broadcast(f32[] %constant.119), dimensions={}
  %constant.117 = f32[] constant(0)
  %broadcast.118 = f32[8,8192,2560]{2,1,0} broadcast(f32[] %constant.117), dimensions={}
  %constant.115 = f32[] constant(0)
  %broadcast.116 = f32[8,2560]{1,0} broadcast(f32[] %constant.115), dimensions={}
  %constant.113 = f32[] constant(0)
  %broadcast.114 = f32[8,2560,8,128]{3,2,1,0} broadcast(f32[] %constant.113), dimensions={}
  %constant.111 = f32[] constant(0)
  %broadcast.112 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %constant.111), dimensions={}
  %get-tuple-element.572 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=2, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.573 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=3, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.574 = bf16[8,16,2048,8,128]{4,3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=4, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.575 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=5, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.576 = bf16[8,16,2048,8192]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=6, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.577 = bf16[8,16,2048,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, /*index=5*/bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,2560]{3,2,1,0}, f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, /*index=10*/f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=15*/f32[8,2560,8,128]{3,2,0,1}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, bf16[16,2048,1,64]{3,2,1,0}, /*index=20*/pred[16,8,1,2048,2048]{4,3,2,1,0}, bf16[16,8,1,2048,2048]{4,3,2,1,0}) %while.569), index=7, metadata={op_name="jit(train_step)/jit(main)/jvp(Transformer)/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %tuple.744 = (s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) tuple(s32[] %constant.172, bf16[16,2048,2560]{2,1,0} %convert.743, f32[8,2560,8192]{2,1,0} %broadcast.120, f32[8,2560,8192]{2,1,0} %broadcast.120, f32[8,8192,2560]{2,1,0} %broadcast.118, /*index=5*/f32[8,2560]{1,0} %broadcast.116, f32[8,2560,8,128]{3,2,1,0} %broadcast.114, f32[8,8,128,2560]{3,2,1,0} %broadcast.112, f32[8,2560,8,128]{3,2,1,0} %broadcast.114, f32[8,2560,8,128]{3,2,1,0} %broadcast.114, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.572, bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.573, bf16[8,16,2048,8,128]{4,3,2,1,0} %get-tuple-element.574, bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.575, bf16[8,16,2048,8192]{3,2,1,0} %get-tuple-element.576, /*index=15*/f32[8,2560,8192]{2,0,1} %transpose.180, f32[8,2560,8192]{2,0,1} %transpose.181, f32[8,8192,2560]{2,0,1} %transpose.182, f32[8,2560]{0,1} %transpose.183, f32[8,2560,8,128]{3,2,0,1} %transpose.184, /*index=20*/f32[8,8,128,2560]{3,2,0,1} %transpose.185, f32[8,2560,8,128]{3,2,0,1} %transpose.186, f32[8,2560,8,128]{3,2,0,1} %transpose.187, bf16[8,16,2048,2560]{3,2,1,0} %get-tuple-element.577, s32[16,2048]{1,0} %Arg_35.36, /*index=25*/s32[16,2048]{1,0} %Arg_34.35), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %while.1401 = (s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) while((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %tuple.744), condition=%region_28.1371, body=%region_15.1216, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %get-tuple-element.1402 = s32[] get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=0, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %Arg_0.1 = s32[] parameter(0), sharding={replicated}, metadata={op_name="state.step"}
  %constant.167 = s32[] constant(1)
  %add.1783 = s32[] add(s32[] %Arg_0.1, s32[] %constant.167), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/flax/flax/training/train_state.py" source_line=118}
  %reshape.1994 = s32[] reshape(s32[] %add.1783), sharding={replicated}
  %Arg_32.33 = s32[] parameter(32), sharding={replicated}, metadata={op_name="state.opt_state[2].count"}
  %constant.161 = s32[] constant(10)
  %compare.1726 = pred[] compare(s32[] %Arg_32.33, s32[] %constant.161), direction=LT, metadata={op_name="jit(train_step)/jit(main)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_join.py" source_line=43}
  %subtract.1713 = s32[] subtract(s32[] %Arg_32.33, s32[] %constant.172), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=143}
  %call.1720 = s32[] call(s32[] %subtract.1713, s32[] %constant.172, s32[] %constant.161), to_apply=%clip.1714
  %convert.1721 = f32[] convert(s32[] %call.1720), metadata={op_name="jit(train_step)/jit(main)/convert_element_type" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=144}
  %constant.160 = f32[] constant(10)
  %divide.1722 = f32[] divide(f32[] %convert.1721, f32[] %constant.160), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=144}
  %subtract.1723 = f32[] subtract(f32[] %constant.171, f32[] %divide.1722), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=144}
  %constant.159 = f32[] constant(-3e-05)
  %multiply.1724 = f32[] multiply(f32[] %subtract.1723, f32[] %constant.159), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=145}
  %constant.158 = f32[] constant(3e-05)
  %add.1725 = f32[] add(f32[] %multiply.1724, f32[] %constant.158), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_schedule.py" source_line=145}
  %subtract.1727 = s32[] subtract(s32[] %Arg_32.33, s32[] %constant.161), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/schedules/_join.py" source_line=43}
  %convert.1728 = f32[] convert(s32[] %subtract.1727), metadata={op_name="jit(train_step)/jit(main)/convert_element_type" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=800}
  %constant.157 = f32[] constant(90)
  %divide.1729 = f32[] divide(f32[] %convert.1728, f32[] %constant.157), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=800}
  %constant.156 = f32[] constant(3.14159274)
  %multiply.1730 = f32[] multiply(f32[] %divide.1729, f32[] %constant.156), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=801}
  %cosine.1731 = f32[] cosine(f32[] %multiply.1730), metadata={op_name="jit(train_step)/jit(main)/cos" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=801}
  %add.1732 = f32[] add(f32[] %cosine.1731, f32[] %constant.171), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=801}
  %constant.155 = f32[] constant(0.5)
  %multiply.1733 = f32[] multiply(f32[] %add.1732, f32[] %constant.155), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=801}
  %multiply.1734 = f32[] multiply(f32[] %multiply.1733, f32[] %constant.158), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=802}
  %subtract.1735 = f32[] subtract(f32[] %constant.171, f32[] %multiply.1733), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=802}
  %constant.154 = f32[] constant(3e-06)
  %multiply.1736 = f32[] multiply(f32[] %subtract.1735, f32[] %constant.154), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=802}
  %add.1737 = f32[] add(f32[] %multiply.1734, f32[] %multiply.1736), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=802}
  %call.1743 = f32[] call(pred[] %compare.1726, f32[] %add.1725, f32[] %add.1737), to_apply=%_where_9.1738
  %constant.153 = f32[] constant(-1)
  %multiply.1744 = f32[] multiply(f32[] %call.1743, f32[] %constant.153), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=958}
  %broadcast.1745 = f32[2560]{0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %multiply.704 = bf16[16,2048,2560]{2,1,0} multiply(bf16[16,2048,2560]{2,1,0} %convert.596, bf16[16,2048,2560]{2,1,0} %dot.702), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/mul" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %constant.168 = bf16[] constant(0)
  %reduce.709 = bf16[2560]{0} reduce(bf16[16,2048,2560]{2,1,0} %multiply.704, bf16[] %constant.168), dimensions={0,1}, to_apply=%region_11.705, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reshape.710 = bf16[1,1,2560]{2,1,0} reshape(bf16[2560]{0} %reduce.709), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reshape" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %reduce.715 = bf16[2560]{0} reduce(bf16[1,1,2560]{2,1,0} %reshape.710, bf16[] %constant.168), dimensions={0,1}, to_apply=%region_12.711, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/reduce_sum" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=51}
  %convert.720 = f32[2560]{0} convert(bf16[2560]{0} %reduce.715), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/decoder_norm/convert_element_type" source_file="/opt/maxtext/MaxText/layers/normalizations.py" source_line=50}
  %multiply.1428 = f32[2560]{0} multiply(f32[2560]{0} %convert.720, f32[2560]{0} %convert.720), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1433 = f32[] reduce(f32[2560]{0} %multiply.1428, f32[] %constant.170), dimensions={0}, to_apply=%region_30.1429, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1404 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=2, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1419 = f32[2560,8,8192]{2,0,1} transpose(f32[8,2560,8192]{2,1,0} %get-tuple-element.1404), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1434 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %transpose.1419, f32[2560,8,8192]{2,0,1} %transpose.1419), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1439 = f32[] reduce(f32[2560,8,8192]{2,0,1} %multiply.1434, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_31.1435, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1440 = f32[] add(f32[] %reduce.1433, f32[] %reduce.1439), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1405 = f32[8,2560,8192]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=3, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1418 = f32[2560,8,8192]{2,0,1} transpose(f32[8,2560,8192]{2,1,0} %get-tuple-element.1405), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1441 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %transpose.1418, f32[2560,8,8192]{2,0,1} %transpose.1418), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1446 = f32[] reduce(f32[2560,8,8192]{2,0,1} %multiply.1441, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_32.1442, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1447 = f32[] add(f32[] %add.1440, f32[] %reduce.1446), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1406 = f32[8,8192,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=4, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1417 = f32[8192,8,2560]{2,0,1} transpose(f32[8,8192,2560]{2,1,0} %get-tuple-element.1406), dimensions={1,0,2}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1448 = f32[8192,8,2560]{2,0,1} multiply(f32[8192,8,2560]{2,0,1} %transpose.1417, f32[8192,8,2560]{2,0,1} %transpose.1417), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1453 = f32[] reduce(f32[8192,8,2560]{2,0,1} %multiply.1448, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_33.1449, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1454 = f32[] add(f32[] %add.1447, f32[] %reduce.1453), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1407 = f32[8,2560]{1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=5, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1416 = f32[2560,8]{0,1} transpose(f32[8,2560]{1,0} %get-tuple-element.1407), dimensions={1,0}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1455 = f32[2560,8]{0,1} multiply(f32[2560,8]{0,1} %transpose.1416, f32[2560,8]{0,1} %transpose.1416), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1460 = f32[] reduce(f32[2560,8]{0,1} %multiply.1455, f32[] %constant.170), dimensions={0,1}, to_apply=%region_34.1456, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1461 = f32[] add(f32[] %add.1454, f32[] %reduce.1460), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1408 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=6, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1415 = f32[2560,8,8,128]{3,2,0,1} transpose(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1408), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1462 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %transpose.1415, f32[2560,8,8,128]{3,2,0,1} %transpose.1415), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1467 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1462, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_35.1463, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1468 = f32[] add(f32[] %add.1461, f32[] %reduce.1467), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1409 = f32[8,8,128,2560]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=7, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1414 = f32[8,8,128,2560]{3,2,0,1} transpose(f32[8,8,128,2560]{3,2,1,0} %get-tuple-element.1409), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1469 = f32[8,8,128,2560]{3,2,0,1} multiply(f32[8,8,128,2560]{3,2,0,1} %transpose.1414, f32[8,8,128,2560]{3,2,0,1} %transpose.1414), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1474 = f32[] reduce(f32[8,8,128,2560]{3,2,0,1} %multiply.1469, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_36.1470, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1475 = f32[] add(f32[] %add.1468, f32[] %reduce.1474), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1410 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=8, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1413 = f32[2560,8,8,128]{3,2,0,1} transpose(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1410), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1476 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %transpose.1413, f32[2560,8,8,128]{3,2,0,1} %transpose.1413), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1481 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1476, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_37.1477, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1482 = f32[] add(f32[] %add.1475, f32[] %reduce.1481), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %get-tuple-element.1411 = f32[8,2560,8,128]{3,2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=9, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %transpose.1412 = f32[2560,8,8,128]{3,2,0,1} transpose(f32[8,2560,8,128]{3,2,1,0} %get-tuple-element.1411), dimensions={1,0,2,3}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/transpose" source_file="/opt/flax/flax/core/axes_scan.py" source_line=102}
  %multiply.1483 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %transpose.1412, f32[2560,8,8,128]{3,2,0,1} %transpose.1412), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1488 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1483, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_38.1484, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1489 = f32[] add(f32[] %add.1482, f32[] %reduce.1488), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %dot.701 = bf16[32000,2560]{1,0} dot(bf16[16,2048,32000]{2,1,0} %divide.700, bf16[16,2048,2560]{2,1,0} %multiply.602), lhs_contracting_dims={0,1}, rhs_contracting_dims={0,1}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder.attend/dot_general" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=104}
  %convert.703 = f32[32000,2560]{1,0} convert(bf16[32000,2560]{1,0} %dot.701), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder.attend/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=104}
  %constant.109 = bf16[] constant(0)
  %broadcast.110 = bf16[32000,2560]{1,0} broadcast(bf16[] %constant.109), dimensions={}
  %get-tuple-element.1403 = bf16[16,2048,2560]{2,1,0} get-tuple-element((s32[], bf16[16,2048,2560]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,2560,8192]{2,1,0}, f32[8,8192,2560]{2,1,0}, /*index=5*/f32[8,2560]{1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, f32[8,2560,8,128]{3,2,1,0}, /*index=10*/bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8,128]{4,3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, bf16[8,16,2048,8192]{3,2,1,0}, /*index=15*/f32[8,2560,8192]{2,0,1}, f32[8,2560,8192]{2,0,1}, f32[8,8192,2560]{2,0,1}, f32[8,2560]{0,1}, f32[8,2560,8,128]{3,2,0,1}, /*index=20*/f32[8,8,128,2560]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, f32[8,2560,8,128]{3,2,0,1}, bf16[8,16,2048,2560]{3,2,1,0}, s32[16,2048]{1,0}, /*index=25*/s32[16,2048]{1,0}) %while.1401), index=1, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/while" source_file="/opt/flax/flax/core/axes_scan.py" source_line=177}
  %custom-call.1420 = bf16[16,2048,2560]{2,1,0} custom-call(bf16[16,2048,2560]{2,1,0} %get-tuple-element.1403), custom_call_target="Sharding", sharding={devices=[8,1,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %scatter.1425 = bf16[32000,2560]{1,0} scatter(bf16[32000,2560]{1,0} %broadcast.110, s32[16,2048,1]{2,1,0} %reshape.177, bf16[16,2048,2560]{2,1,0} %custom-call.1420), update_window_dims={2}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=2, to_apply=%region_29.1421, metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder/scatter-add" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %convert.1426 = f32[32000,2560]{1,0} convert(bf16[32000,2560]{1,0} %scatter.1425), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder/convert_element_type" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %add.1427 = f32[32000,2560]{1,0} add(f32[32000,2560]{1,0} %convert.703, f32[32000,2560]{1,0} %convert.1426), metadata={op_name="jit(train_step)/jit(main)/transpose(jvp(Transformer))/decoder/token_embedder/add_any" source_file="/opt/maxtext/MaxText/layers/embeddings.py" source_line=84}
  %multiply.1490 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %add.1427, f32[32000,2560]{1,0} %add.1427), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=45}
  %reduce.1495 = f32[] reduce(f32[32000,2560]{1,0} %multiply.1490, f32[] %constant.170), dimensions={0,1}, to_apply=%region_39.1491, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %add.1496 = f32[] add(f32[] %add.1489, f32[] %reduce.1495), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=38}
  %sqrt.1497 = f32[] sqrt(f32[] %add.1496), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/linear_algebra.py" source_line=37}
  %compare.1498 = pred[] compare(f32[] %sqrt.1497, f32[] %constant.171), direction=LT, metadata={op_name="jit(train_step)/jit(main)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=96}
  %broadcast.1501 = pred[2560]{0} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1499 = f32[2560]{0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1500 = f32[2560]{0} divide(f32[2560]{0} %convert.720, f32[2560]{0} %broadcast.1499), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1502 = f32[2560]{0} select(pred[2560]{0} %broadcast.1501, f32[2560]{0} %convert.720, f32[2560]{0} %divide.1500), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.107 = f32[] constant(0.1)
  %broadcast.108 = f32[2560]{0} broadcast(f32[] %constant.107), dimensions={}
  %multiply.1539 = f32[2560]{0} multiply(f32[2560]{0} %select.1502, f32[2560]{0} %broadcast.108), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_12.13 = f32[2560]{0} parameter(12), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'decoder_norm\'][\'scale\']"}
  %constant.105 = f32[] constant(0.9)
  %broadcast.106 = f32[2560]{0} broadcast(f32[] %constant.105), dimensions={}
  %multiply.1540 = f32[2560]{0} multiply(f32[2560]{0} %Arg_12.13, f32[2560]{0} %broadcast.106), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1541 = f32[2560]{0} add(f32[2560]{0} %multiply.1539, f32[2560]{0} %multiply.1540), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %constant.164 = f32[] constant(0.9)
  %Arg_11.12 = s32[] parameter(11), sharding={replicated}, metadata={op_name="state.opt_state[0].count"}
  %constant.162 = s32[] constant(2147483647)
  %compare.1609 = pred[] compare(s32[] %Arg_11.12, s32[] %constant.162), direction=LT, metadata={op_name="jit(train_step)/jit(main)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142}
  %add.1610 = s32[] add(s32[] %Arg_11.12, s32[] %constant.167), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142}
  %call.1616 = s32[] call(pred[] %compare.1609, s32[] %add.1610, s32[] %constant.162), to_apply=%_where_8.1611
  %convert.1617 = f32[] convert(s32[] %call.1616), metadata={op_name="jit(train_step)/jit(main)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %power.1618 = f32[] power(f32[] %constant.164, f32[] %convert.1617), metadata={op_name="jit(train_step)/jit(main)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %subtract.1619 = f32[] subtract(f32[] %constant.171, f32[] %power.1618), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %broadcast.1620 = f32[2560]{0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1621 = f32[2560]{0} divide(f32[2560]{0} %add.1541, f32[2560]{0} %broadcast.1620), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1569 = f32[2560]{0} multiply(f32[2560]{0} %select.1502, f32[2560]{0} %select.1502), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.79 = f32[] constant(0.05)
  %broadcast.80 = f32[2560]{0} broadcast(f32[] %constant.79), dimensions={}
  %multiply.1570 = f32[2560]{0} multiply(f32[2560]{0} %multiply.1569, f32[2560]{0} %broadcast.80), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_22.23 = f32[2560]{0} parameter(22), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'decoder_norm\'][\'scale\']"}
  %constant.77 = f32[] constant(0.95)
  %broadcast.78 = f32[2560]{0} broadcast(f32[] %constant.77), dimensions={}
  %multiply.1571 = f32[2560]{0} multiply(f32[2560]{0} %Arg_22.23, f32[2560]{0} %broadcast.78), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1572 = f32[2560]{0} add(f32[2560]{0} %multiply.1570, f32[2560]{0} %multiply.1571), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %constant.163 = f32[] constant(0.95)
  %convert.1640 = f32[] convert(s32[] %call.1616), metadata={op_name="jit(train_step)/jit(main)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %power.1641 = f32[] power(f32[] %constant.163, f32[] %convert.1640), metadata={op_name="jit(train_step)/jit(main)/pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %subtract.1642 = f32[] subtract(f32[] %constant.171, f32[] %power.1641), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %broadcast.1643 = f32[2560]{0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1644 = f32[2560]{0} divide(f32[2560]{0} %add.1572, f32[2560]{0} %broadcast.1643), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1663 = f32[2560]{0} sqrt(f32[2560]{0} %divide.1644), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.51 = f32[] constant(1e-08)
  %broadcast.52 = f32[2560]{0} broadcast(f32[] %constant.51), dimensions={}
  %add.1664 = f32[2560]{0} add(f32[2560]{0} %sqrt.1663, f32[2560]{0} %broadcast.52), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1665 = f32[2560]{0} divide(f32[2560]{0} %divide.1621, f32[2560]{0} %add.1664), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1693 = f32[2560]{0} multiply(f32[2560]{0} %Arg_1.2, f32[2560]{0} %broadcast.108), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1694 = f32[2560]{0} add(f32[2560]{0} %divide.1665, f32[2560]{0} %multiply.1693), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1746 = f32[2560]{0} multiply(f32[2560]{0} %broadcast.1745, f32[2560]{0} %add.1694), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1773 = f32[2560]{0} add(f32[2560]{0} %Arg_1.2, f32[2560]{0} %multiply.1746), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.1995 = f32[2560]{0} reshape(f32[2560]{0} %add.1773), sharding={replicated}
  %broadcast.1747 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1505 = pred[2560,8,8192]{2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1503 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1504 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %transpose.1419, f32[2560,8,8192]{2,1,0} %broadcast.1503), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1506 = f32[2560,8,8192]{2,0,1} select(pred[2560,8,8192]{2,0,1} %broadcast.1505, f32[2560,8,8192]{2,0,1} %transpose.1419, f32[2560,8,8192]{2,0,1} %divide.1504), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.103 = f32[] constant(0.1)
  %broadcast.104 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %constant.103), dimensions={}
  %multiply.1542 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %select.1506, f32[2560,8,8192]{2,1,0} %broadcast.104), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_13.14 = f32[2560,8,8192]{2,1,0} parameter(13), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_0\'][\'kernel\']"}
  %constant.101 = f32[] constant(0.9)
  %broadcast.102 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %constant.101), dimensions={}
  %multiply.1543 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %Arg_13.14, f32[2560,8,8192]{2,1,0} %broadcast.102), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1544 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %multiply.1542, f32[2560,8,8192]{2,1,0} %multiply.1543), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1622 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1623 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %add.1544, f32[2560,8,8192]{2,1,0} %broadcast.1622), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1573 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %select.1506, f32[2560,8,8192]{2,0,1} %select.1506), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.75 = f32[] constant(0.05)
  %broadcast.76 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %constant.75), dimensions={}
  %multiply.1574 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %multiply.1573, f32[2560,8,8192]{2,1,0} %broadcast.76), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_23.24 = f32[2560,8,8192]{2,1,0} parameter(23), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_0\'][\'kernel\']"}
  %constant.73 = f32[] constant(0.95)
  %broadcast.74 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %constant.73), dimensions={}
  %multiply.1575 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %Arg_23.24, f32[2560,8,8192]{2,1,0} %broadcast.74), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1576 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %multiply.1574, f32[2560,8,8192]{2,1,0} %multiply.1575), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1645 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1646 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %add.1576, f32[2560,8,8192]{2,1,0} %broadcast.1645), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1666 = f32[2560,8,8192]{2,0,1} sqrt(f32[2560,8,8192]{2,0,1} %divide.1646), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.49 = f32[] constant(1e-08)
  %broadcast.50 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %constant.49), dimensions={}
  %add.1667 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %sqrt.1666, f32[2560,8,8192]{2,1,0} %broadcast.50), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1668 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %divide.1623, f32[2560,8,8192]{2,0,1} %add.1667), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1695 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %Arg_2.3, f32[2560,8,8192]{2,1,0} %broadcast.104), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1696 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %divide.1668, f32[2560,8,8192]{2,1,0} %multiply.1695), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1748 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %broadcast.1747, f32[2560,8,8192]{2,0,1} %add.1696), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1774 = f32[2560,8,8192]{2,1,0} add(f32[2560,8,8192]{2,1,0} %Arg_2.3, f32[2560,8,8192]{2,1,0} %multiply.1748), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.1996 = f32[2560,8,8192]{2,1,0} reshape(f32[2560,8,8192]{2,1,0} %add.1774), sharding={replicated}
  %broadcast.1749 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1509 = pred[2560,8,8192]{2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1507 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1508 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %transpose.1418, f32[2560,8,8192]{2,1,0} %broadcast.1507), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1510 = f32[2560,8,8192]{2,0,1} select(pred[2560,8,8192]{2,0,1} %broadcast.1509, f32[2560,8,8192]{2,0,1} %transpose.1418, f32[2560,8,8192]{2,0,1} %divide.1508), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %multiply.1545 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %select.1510, f32[2560,8,8192]{2,1,0} %broadcast.104), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_14.15 = f32[2560,8,8192]{2,1,0} parameter(14), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_1\'][\'kernel\']"}
  %multiply.1546 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %Arg_14.15, f32[2560,8,8192]{2,1,0} %broadcast.102), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1547 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %multiply.1545, f32[2560,8,8192]{2,1,0} %multiply.1546), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1624 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1625 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %add.1547, f32[2560,8,8192]{2,1,0} %broadcast.1624), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1577 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %select.1510, f32[2560,8,8192]{2,0,1} %select.1510), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %multiply.1578 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %multiply.1577, f32[2560,8,8192]{2,1,0} %broadcast.76), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_24.25 = f32[2560,8,8192]{2,1,0} parameter(24), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wi_1\'][\'kernel\']"}
  %multiply.1579 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %Arg_24.25, f32[2560,8,8192]{2,1,0} %broadcast.74), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1580 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %multiply.1578, f32[2560,8,8192]{2,1,0} %multiply.1579), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1647 = f32[2560,8,8192]{2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1648 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %add.1580, f32[2560,8,8192]{2,1,0} %broadcast.1647), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1669 = f32[2560,8,8192]{2,0,1} sqrt(f32[2560,8,8192]{2,0,1} %divide.1648), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %add.1670 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %sqrt.1669, f32[2560,8,8192]{2,1,0} %broadcast.50), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1671 = f32[2560,8,8192]{2,0,1} divide(f32[2560,8,8192]{2,0,1} %divide.1625, f32[2560,8,8192]{2,0,1} %add.1670), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1697 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %Arg_3.4, f32[2560,8,8192]{2,1,0} %broadcast.104), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1698 = f32[2560,8,8192]{2,0,1} add(f32[2560,8,8192]{2,0,1} %divide.1671, f32[2560,8,8192]{2,1,0} %multiply.1697), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1750 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %broadcast.1749, f32[2560,8,8192]{2,0,1} %add.1698), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1775 = f32[2560,8,8192]{2,1,0} add(f32[2560,8,8192]{2,1,0} %Arg_3.4, f32[2560,8,8192]{2,1,0} %multiply.1750), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.1997 = f32[2560,8,8192]{2,1,0} reshape(f32[2560,8,8192]{2,1,0} %add.1775), sharding={replicated}
  %broadcast.1751 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1513 = pred[8192,8,2560]{2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1511 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1512 = f32[8192,8,2560]{2,0,1} divide(f32[8192,8,2560]{2,0,1} %transpose.1417, f32[8192,8,2560]{2,1,0} %broadcast.1511), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1514 = f32[8192,8,2560]{2,0,1} select(pred[8192,8,2560]{2,0,1} %broadcast.1513, f32[8192,8,2560]{2,0,1} %transpose.1417, f32[8192,8,2560]{2,0,1} %divide.1512), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.99 = f32[] constant(0.1)
  %broadcast.100 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %constant.99), dimensions={}
  %multiply.1548 = f32[8192,8,2560]{2,0,1} multiply(f32[8192,8,2560]{2,0,1} %select.1514, f32[8192,8,2560]{2,1,0} %broadcast.100), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_15.16 = f32[8192,8,2560]{2,1,0} parameter(15), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wo\'][\'kernel\']"}
  %constant.97 = f32[] constant(0.9)
  %broadcast.98 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %constant.97), dimensions={}
  %multiply.1549 = f32[8192,8,2560]{2,1,0} multiply(f32[8192,8,2560]{2,1,0} %Arg_15.16, f32[8192,8,2560]{2,1,0} %broadcast.98), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1550 = f32[8192,8,2560]{2,0,1} add(f32[8192,8,2560]{2,0,1} %multiply.1548, f32[8192,8,2560]{2,1,0} %multiply.1549), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1626 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1627 = f32[8192,8,2560]{2,0,1} divide(f32[8192,8,2560]{2,0,1} %add.1550, f32[8192,8,2560]{2,1,0} %broadcast.1626), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1581 = f32[8192,8,2560]{2,0,1} multiply(f32[8192,8,2560]{2,0,1} %select.1514, f32[8192,8,2560]{2,0,1} %select.1514), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.71 = f32[] constant(0.05)
  %broadcast.72 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %constant.71), dimensions={}
  %multiply.1582 = f32[8192,8,2560]{2,0,1} multiply(f32[8192,8,2560]{2,0,1} %multiply.1581, f32[8192,8,2560]{2,1,0} %broadcast.72), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_25.26 = f32[8192,8,2560]{2,1,0} parameter(25), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'mlp\'][\'wo\'][\'kernel\']"}
  %constant.69 = f32[] constant(0.95)
  %broadcast.70 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %constant.69), dimensions={}
  %multiply.1583 = f32[8192,8,2560]{2,1,0} multiply(f32[8192,8,2560]{2,1,0} %Arg_25.26, f32[8192,8,2560]{2,1,0} %broadcast.70), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1584 = f32[8192,8,2560]{2,0,1} add(f32[8192,8,2560]{2,0,1} %multiply.1582, f32[8192,8,2560]{2,1,0} %multiply.1583), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1649 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1650 = f32[8192,8,2560]{2,0,1} divide(f32[8192,8,2560]{2,0,1} %add.1584, f32[8192,8,2560]{2,1,0} %broadcast.1649), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1672 = f32[8192,8,2560]{2,0,1} sqrt(f32[8192,8,2560]{2,0,1} %divide.1650), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.47 = f32[] constant(1e-08)
  %broadcast.48 = f32[8192,8,2560]{2,1,0} broadcast(f32[] %constant.47), dimensions={}
  %add.1673 = f32[8192,8,2560]{2,0,1} add(f32[8192,8,2560]{2,0,1} %sqrt.1672, f32[8192,8,2560]{2,1,0} %broadcast.48), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1674 = f32[8192,8,2560]{2,0,1} divide(f32[8192,8,2560]{2,0,1} %divide.1627, f32[8192,8,2560]{2,0,1} %add.1673), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1699 = f32[8192,8,2560]{2,1,0} multiply(f32[8192,8,2560]{2,1,0} %Arg_4.5, f32[8192,8,2560]{2,1,0} %broadcast.100), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1700 = f32[8192,8,2560]{2,0,1} add(f32[8192,8,2560]{2,0,1} %divide.1674, f32[8192,8,2560]{2,1,0} %multiply.1699), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1752 = f32[8192,8,2560]{2,1,0} multiply(f32[8192,8,2560]{2,1,0} %broadcast.1751, f32[8192,8,2560]{2,0,1} %add.1700), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1776 = f32[8192,8,2560]{2,1,0} add(f32[8192,8,2560]{2,1,0} %Arg_4.5, f32[8192,8,2560]{2,1,0} %multiply.1752), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.1998 = f32[8192,8,2560]{2,1,0} reshape(f32[8192,8,2560]{2,1,0} %add.1776), sharding={replicated}
  %broadcast.1753 = f32[2560,8]{1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1517 = pred[2560,8]{0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1515 = f32[2560,8]{1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1516 = f32[2560,8]{0,1} divide(f32[2560,8]{0,1} %transpose.1416, f32[2560,8]{1,0} %broadcast.1515), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1518 = f32[2560,8]{0,1} select(pred[2560,8]{0,1} %broadcast.1517, f32[2560,8]{0,1} %transpose.1416, f32[2560,8]{0,1} %divide.1516), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.95 = f32[] constant(0.1)
  %broadcast.96 = f32[2560,8]{1,0} broadcast(f32[] %constant.95), dimensions={}
  %multiply.1551 = f32[2560,8]{0,1} multiply(f32[2560,8]{0,1} %select.1518, f32[2560,8]{1,0} %broadcast.96), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_16.17 = f32[2560,8]{1,0} parameter(16), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'pre_self_attention_norm\'][\'scale\']"}
  %constant.93 = f32[] constant(0.9)
  %broadcast.94 = f32[2560,8]{1,0} broadcast(f32[] %constant.93), dimensions={}
  %multiply.1552 = f32[2560,8]{1,0} multiply(f32[2560,8]{1,0} %Arg_16.17, f32[2560,8]{1,0} %broadcast.94), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1553 = f32[2560,8]{0,1} add(f32[2560,8]{0,1} %multiply.1551, f32[2560,8]{1,0} %multiply.1552), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1628 = f32[2560,8]{1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1629 = f32[2560,8]{0,1} divide(f32[2560,8]{0,1} %add.1553, f32[2560,8]{1,0} %broadcast.1628), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1585 = f32[2560,8]{0,1} multiply(f32[2560,8]{0,1} %select.1518, f32[2560,8]{0,1} %select.1518), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.67 = f32[] constant(0.05)
  %broadcast.68 = f32[2560,8]{1,0} broadcast(f32[] %constant.67), dimensions={}
  %multiply.1586 = f32[2560,8]{0,1} multiply(f32[2560,8]{0,1} %multiply.1585, f32[2560,8]{1,0} %broadcast.68), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_26.27 = f32[2560,8]{1,0} parameter(26), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'pre_self_attention_norm\'][\'scale\']"}
  %constant.65 = f32[] constant(0.95)
  %broadcast.66 = f32[2560,8]{1,0} broadcast(f32[] %constant.65), dimensions={}
  %multiply.1587 = f32[2560,8]{1,0} multiply(f32[2560,8]{1,0} %Arg_26.27, f32[2560,8]{1,0} %broadcast.66), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1588 = f32[2560,8]{0,1} add(f32[2560,8]{0,1} %multiply.1586, f32[2560,8]{1,0} %multiply.1587), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1651 = f32[2560,8]{1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1652 = f32[2560,8]{0,1} divide(f32[2560,8]{0,1} %add.1588, f32[2560,8]{1,0} %broadcast.1651), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1675 = f32[2560,8]{0,1} sqrt(f32[2560,8]{0,1} %divide.1652), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.45 = f32[] constant(1e-08)
  %broadcast.46 = f32[2560,8]{1,0} broadcast(f32[] %constant.45), dimensions={}
  %add.1676 = f32[2560,8]{0,1} add(f32[2560,8]{0,1} %sqrt.1675, f32[2560,8]{1,0} %broadcast.46), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1677 = f32[2560,8]{0,1} divide(f32[2560,8]{0,1} %divide.1629, f32[2560,8]{0,1} %add.1676), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1701 = f32[2560,8]{1,0} multiply(f32[2560,8]{1,0} %Arg_5.6, f32[2560,8]{1,0} %broadcast.96), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1702 = f32[2560,8]{0,1} add(f32[2560,8]{0,1} %divide.1677, f32[2560,8]{1,0} %multiply.1701), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1754 = f32[2560,8]{1,0} multiply(f32[2560,8]{1,0} %broadcast.1753, f32[2560,8]{0,1} %add.1702), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1777 = f32[2560,8]{1,0} add(f32[2560,8]{1,0} %Arg_5.6, f32[2560,8]{1,0} %multiply.1754), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.1999 = f32[2560,8]{1,0} reshape(f32[2560,8]{1,0} %add.1777), sharding={replicated}
  %broadcast.1755 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1521 = pred[2560,8,8,128]{3,2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1519 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1520 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %transpose.1415, f32[2560,8,8,128]{3,2,1,0} %broadcast.1519), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1522 = f32[2560,8,8,128]{3,2,0,1} select(pred[2560,8,8,128]{3,2,0,1} %broadcast.1521, f32[2560,8,8,128]{3,2,0,1} %transpose.1415, f32[2560,8,8,128]{3,2,0,1} %divide.1520), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.91 = f32[] constant(0.1)
  %broadcast.92 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %constant.91), dimensions={}
  %multiply.1554 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1522, f32[2560,8,8,128]{3,2,1,0} %broadcast.92), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_17.18 = f32[2560,8,8,128]{3,2,1,0} parameter(17), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'key\'][\'kernel\']"}
  %constant.89 = f32[] constant(0.9)
  %broadcast.90 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %constant.89), dimensions={}
  %multiply.1555 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_17.18, f32[2560,8,8,128]{3,2,1,0} %broadcast.90), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1556 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %multiply.1554, f32[2560,8,8,128]{3,2,1,0} %multiply.1555), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1630 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1631 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %add.1556, f32[2560,8,8,128]{3,2,1,0} %broadcast.1630), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1589 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1522, f32[2560,8,8,128]{3,2,0,1} %select.1522), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.63 = f32[] constant(0.05)
  %broadcast.64 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %constant.63), dimensions={}
  %multiply.1590 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %multiply.1589, f32[2560,8,8,128]{3,2,1,0} %broadcast.64), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_27.28 = f32[2560,8,8,128]{3,2,1,0} parameter(27), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'key\'][\'kernel\']"}
  %constant.61 = f32[] constant(0.95)
  %broadcast.62 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %constant.61), dimensions={}
  %multiply.1591 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_27.28, f32[2560,8,8,128]{3,2,1,0} %broadcast.62), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1592 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %multiply.1590, f32[2560,8,8,128]{3,2,1,0} %multiply.1591), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1653 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1654 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %add.1592, f32[2560,8,8,128]{3,2,1,0} %broadcast.1653), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1678 = f32[2560,8,8,128]{3,2,0,1} sqrt(f32[2560,8,8,128]{3,2,0,1} %divide.1654), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.43 = f32[] constant(1e-08)
  %broadcast.44 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %constant.43), dimensions={}
  %add.1679 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %sqrt.1678, f32[2560,8,8,128]{3,2,1,0} %broadcast.44), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1680 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %divide.1631, f32[2560,8,8,128]{3,2,0,1} %add.1679), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1703 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_6.7, f32[2560,8,8,128]{3,2,1,0} %broadcast.92), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1704 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %divide.1680, f32[2560,8,8,128]{3,2,1,0} %multiply.1703), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1756 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %broadcast.1755, f32[2560,8,8,128]{3,2,0,1} %add.1704), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1778 = f32[2560,8,8,128]{3,2,1,0} add(f32[2560,8,8,128]{3,2,1,0} %Arg_6.7, f32[2560,8,8,128]{3,2,1,0} %multiply.1756), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.2000 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,1,0} %add.1778), sharding={replicated}
  %broadcast.1757 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1525 = pred[8,8,128,2560]{3,2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1523 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1524 = f32[8,8,128,2560]{3,2,0,1} divide(f32[8,8,128,2560]{3,2,0,1} %transpose.1414, f32[8,8,128,2560]{3,2,1,0} %broadcast.1523), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1526 = f32[8,8,128,2560]{3,2,0,1} select(pred[8,8,128,2560]{3,2,0,1} %broadcast.1525, f32[8,8,128,2560]{3,2,0,1} %transpose.1414, f32[8,8,128,2560]{3,2,0,1} %divide.1524), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.87 = f32[] constant(0.1)
  %broadcast.88 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %constant.87), dimensions={}
  %multiply.1557 = f32[8,8,128,2560]{3,2,0,1} multiply(f32[8,8,128,2560]{3,2,0,1} %select.1526, f32[8,8,128,2560]{3,2,1,0} %broadcast.88), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_18.19 = f32[8,8,128,2560]{3,2,1,0} parameter(18), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'out\'][\'kernel\']"}
  %constant.85 = f32[] constant(0.9)
  %broadcast.86 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %constant.85), dimensions={}
  %multiply.1558 = f32[8,8,128,2560]{3,2,1,0} multiply(f32[8,8,128,2560]{3,2,1,0} %Arg_18.19, f32[8,8,128,2560]{3,2,1,0} %broadcast.86), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1559 = f32[8,8,128,2560]{3,2,0,1} add(f32[8,8,128,2560]{3,2,0,1} %multiply.1557, f32[8,8,128,2560]{3,2,1,0} %multiply.1558), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1632 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1633 = f32[8,8,128,2560]{3,2,0,1} divide(f32[8,8,128,2560]{3,2,0,1} %add.1559, f32[8,8,128,2560]{3,2,1,0} %broadcast.1632), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1593 = f32[8,8,128,2560]{3,2,0,1} multiply(f32[8,8,128,2560]{3,2,0,1} %select.1526, f32[8,8,128,2560]{3,2,0,1} %select.1526), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.59 = f32[] constant(0.05)
  %broadcast.60 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %constant.59), dimensions={}
  %multiply.1594 = f32[8,8,128,2560]{3,2,0,1} multiply(f32[8,8,128,2560]{3,2,0,1} %multiply.1593, f32[8,8,128,2560]{3,2,1,0} %broadcast.60), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_28.29 = f32[8,8,128,2560]{3,2,1,0} parameter(28), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'out\'][\'kernel\']"}
  %constant.57 = f32[] constant(0.95)
  %broadcast.58 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %constant.57), dimensions={}
  %multiply.1595 = f32[8,8,128,2560]{3,2,1,0} multiply(f32[8,8,128,2560]{3,2,1,0} %Arg_28.29, f32[8,8,128,2560]{3,2,1,0} %broadcast.58), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1596 = f32[8,8,128,2560]{3,2,0,1} add(f32[8,8,128,2560]{3,2,0,1} %multiply.1594, f32[8,8,128,2560]{3,2,1,0} %multiply.1595), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1655 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1656 = f32[8,8,128,2560]{3,2,0,1} divide(f32[8,8,128,2560]{3,2,0,1} %add.1596, f32[8,8,128,2560]{3,2,1,0} %broadcast.1655), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1681 = f32[8,8,128,2560]{3,2,0,1} sqrt(f32[8,8,128,2560]{3,2,0,1} %divide.1656), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.41 = f32[] constant(1e-08)
  %broadcast.42 = f32[8,8,128,2560]{3,2,1,0} broadcast(f32[] %constant.41), dimensions={}
  %add.1682 = f32[8,8,128,2560]{3,2,0,1} add(f32[8,8,128,2560]{3,2,0,1} %sqrt.1681, f32[8,8,128,2560]{3,2,1,0} %broadcast.42), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1683 = f32[8,8,128,2560]{3,2,0,1} divide(f32[8,8,128,2560]{3,2,0,1} %divide.1633, f32[8,8,128,2560]{3,2,0,1} %add.1682), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1705 = f32[8,8,128,2560]{3,2,1,0} multiply(f32[8,8,128,2560]{3,2,1,0} %Arg_7.8, f32[8,8,128,2560]{3,2,1,0} %broadcast.88), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1706 = f32[8,8,128,2560]{3,2,0,1} add(f32[8,8,128,2560]{3,2,0,1} %divide.1683, f32[8,8,128,2560]{3,2,1,0} %multiply.1705), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1758 = f32[8,8,128,2560]{3,2,1,0} multiply(f32[8,8,128,2560]{3,2,1,0} %broadcast.1757, f32[8,8,128,2560]{3,2,0,1} %add.1706), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1779 = f32[8,8,128,2560]{3,2,1,0} add(f32[8,8,128,2560]{3,2,1,0} %Arg_7.8, f32[8,8,128,2560]{3,2,1,0} %multiply.1758), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.2001 = f32[8,8,128,2560]{3,2,1,0} reshape(f32[8,8,128,2560]{3,2,1,0} %add.1779), sharding={replicated}
  %broadcast.1759 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1529 = pred[2560,8,8,128]{3,2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1527 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1528 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %transpose.1413, f32[2560,8,8,128]{3,2,1,0} %broadcast.1527), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1530 = f32[2560,8,8,128]{3,2,0,1} select(pred[2560,8,8,128]{3,2,0,1} %broadcast.1529, f32[2560,8,8,128]{3,2,0,1} %transpose.1413, f32[2560,8,8,128]{3,2,0,1} %divide.1528), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %multiply.1560 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1530, f32[2560,8,8,128]{3,2,1,0} %broadcast.92), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_19.20 = f32[2560,8,8,128]{3,2,1,0} parameter(19), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'query\'][\'kernel\']"}
  %multiply.1561 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_19.20, f32[2560,8,8,128]{3,2,1,0} %broadcast.90), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1562 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %multiply.1560, f32[2560,8,8,128]{3,2,1,0} %multiply.1561), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1634 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1635 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %add.1562, f32[2560,8,8,128]{3,2,1,0} %broadcast.1634), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1597 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1530, f32[2560,8,8,128]{3,2,0,1} %select.1530), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %multiply.1598 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %multiply.1597, f32[2560,8,8,128]{3,2,1,0} %broadcast.64), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_29.30 = f32[2560,8,8,128]{3,2,1,0} parameter(29), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'query\'][\'kernel\']"}
  %multiply.1599 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_29.30, f32[2560,8,8,128]{3,2,1,0} %broadcast.62), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1600 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %multiply.1598, f32[2560,8,8,128]{3,2,1,0} %multiply.1599), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1657 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1658 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %add.1600, f32[2560,8,8,128]{3,2,1,0} %broadcast.1657), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1684 = f32[2560,8,8,128]{3,2,0,1} sqrt(f32[2560,8,8,128]{3,2,0,1} %divide.1658), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %add.1685 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %sqrt.1684, f32[2560,8,8,128]{3,2,1,0} %broadcast.44), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1686 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %divide.1635, f32[2560,8,8,128]{3,2,0,1} %add.1685), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1707 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_8.9, f32[2560,8,8,128]{3,2,1,0} %broadcast.92), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1708 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %divide.1686, f32[2560,8,8,128]{3,2,1,0} %multiply.1707), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1760 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %broadcast.1759, f32[2560,8,8,128]{3,2,0,1} %add.1708), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1780 = f32[2560,8,8,128]{3,2,1,0} add(f32[2560,8,8,128]{3,2,1,0} %Arg_8.9, f32[2560,8,8,128]{3,2,1,0} %multiply.1760), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.2002 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,1,0} %add.1780), sharding={replicated}
  %broadcast.1761 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1533 = pred[2560,8,8,128]{3,2,0,1} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1531 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1532 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %transpose.1412, f32[2560,8,8,128]{3,2,1,0} %broadcast.1531), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1534 = f32[2560,8,8,128]{3,2,0,1} select(pred[2560,8,8,128]{3,2,0,1} %broadcast.1533, f32[2560,8,8,128]{3,2,0,1} %transpose.1412, f32[2560,8,8,128]{3,2,0,1} %divide.1532), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %multiply.1563 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1534, f32[2560,8,8,128]{3,2,1,0} %broadcast.92), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_20.21 = f32[2560,8,8,128]{3,2,1,0} parameter(20), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'value\'][\'kernel\']"}
  %multiply.1564 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_20.21, f32[2560,8,8,128]{3,2,1,0} %broadcast.90), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1565 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %multiply.1563, f32[2560,8,8,128]{3,2,1,0} %multiply.1564), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1636 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1637 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %add.1565, f32[2560,8,8,128]{3,2,1,0} %broadcast.1636), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1601 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1534, f32[2560,8,8,128]{3,2,0,1} %select.1534), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %multiply.1602 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %multiply.1601, f32[2560,8,8,128]{3,2,1,0} %broadcast.64), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_30.31 = f32[2560,8,8,128]{3,2,1,0} parameter(30), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'decoder\'][\'layers\'][\'self_attention\'][\'value\'][\'kernel\']"}
  %multiply.1603 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_30.31, f32[2560,8,8,128]{3,2,1,0} %broadcast.62), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1604 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %multiply.1602, f32[2560,8,8,128]{3,2,1,0} %multiply.1603), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1659 = f32[2560,8,8,128]{3,2,1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1660 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %add.1604, f32[2560,8,8,128]{3,2,1,0} %broadcast.1659), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1687 = f32[2560,8,8,128]{3,2,0,1} sqrt(f32[2560,8,8,128]{3,2,0,1} %divide.1660), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %add.1688 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %sqrt.1687, f32[2560,8,8,128]{3,2,1,0} %broadcast.44), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1689 = f32[2560,8,8,128]{3,2,0,1} divide(f32[2560,8,8,128]{3,2,0,1} %divide.1637, f32[2560,8,8,128]{3,2,0,1} %add.1688), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1709 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %Arg_9.10, f32[2560,8,8,128]{3,2,1,0} %broadcast.92), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1710 = f32[2560,8,8,128]{3,2,0,1} add(f32[2560,8,8,128]{3,2,0,1} %divide.1689, f32[2560,8,8,128]{3,2,1,0} %multiply.1709), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1762 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %broadcast.1761, f32[2560,8,8,128]{3,2,0,1} %add.1710), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1781 = f32[2560,8,8,128]{3,2,1,0} add(f32[2560,8,8,128]{3,2,1,0} %Arg_9.10, f32[2560,8,8,128]{3,2,1,0} %multiply.1762), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.2003 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,1,0} %add.1781), sharding={replicated}
  %broadcast.1763 = f32[32000,2560]{1,0} broadcast(f32[] %multiply.1744), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %broadcast.1537 = pred[32000,2560]{1,0} broadcast(pred[] %compare.1498), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %broadcast.1535 = f32[32000,2560]{1,0} broadcast(f32[] %sqrt.1497), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %divide.1536 = f32[32000,2560]{1,0} divide(f32[32000,2560]{1,0} %add.1427, f32[32000,2560]{1,0} %broadcast.1535), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %select.1538 = f32[32000,2560]{1,0} select(pred[32000,2560]{1,0} %broadcast.1537, f32[32000,2560]{1,0} %add.1427, f32[32000,2560]{1,0} %divide.1536), metadata={op_name="jit(train_step)/jit(main)/select_n" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_clipping.py" source_line=100}
  %constant.83 = f32[] constant(0.1)
  %broadcast.84 = f32[32000,2560]{1,0} broadcast(f32[] %constant.83), dimensions={}
  %multiply.1566 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %select.1538, f32[32000,2560]{1,0} %broadcast.84), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %Arg_21.22 = f32[32000,2560]{1,0} parameter(21), sharding={replicated}, metadata={op_name="state.opt_state[0].mu[\'params\'][\'token_embedder\'][\'embedding\']"}
  %constant.81 = f32[] constant(0.9)
  %broadcast.82 = f32[32000,2560]{1,0} broadcast(f32[] %constant.81), dimensions={}
  %multiply.1567 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %Arg_21.22, f32[32000,2560]{1,0} %broadcast.82), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %add.1568 = f32[32000,2560]{1,0} add(f32[32000,2560]{1,0} %multiply.1566, f32[32000,2560]{1,0} %multiply.1567), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=308}
  %broadcast.1638 = f32[32000,2560]{1,0} broadcast(f32[] %subtract.1619), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %divide.1639 = f32[32000,2560]{1,0} divide(f32[32000,2560]{1,0} %add.1568, f32[32000,2560]{1,0} %broadcast.1638), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=294}
  %multiply.1605 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %select.1538, f32[32000,2560]{1,0} %select.1538), metadata={op_name="jit(train_step)/jit(main)/integer_pow" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=333}
  %constant.55 = f32[] constant(0.05)
  %broadcast.56 = f32[32000,2560]{1,0} broadcast(f32[] %constant.55), dimensions={}
  %multiply.1606 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %multiply.1605, f32[32000,2560]{1,0} %broadcast.56), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %Arg_31.32 = f32[32000,2560]{1,0} parameter(31), sharding={replicated}, metadata={op_name="state.opt_state[0].nu[\'params\'][\'token_embedder\'][\'embedding\']"}
  %constant.53 = f32[] constant(0.95)
  %broadcast.54 = f32[32000,2560]{1,0} broadcast(f32[] %constant.53), dimensions={}
  %multiply.1607 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %Arg_31.32, f32[32000,2560]{1,0} %broadcast.54), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %add.1608 = f32[32000,2560]{1,0} add(f32[32000,2560]{1,0} %multiply.1606, f32[32000,2560]{1,0} %multiply.1607), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/tree_utils/_tree_math.py" source_line=343}
  %broadcast.1661 = f32[32000,2560]{1,0} broadcast(f32[] %subtract.1642), dimensions={}, metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %divide.1662 = f32[32000,2560]{1,0} divide(f32[32000,2560]{1,0} %add.1608, f32[32000,2560]{1,0} %broadcast.1661), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=298}
  %sqrt.1690 = f32[32000,2560]{1,0} sqrt(f32[32000,2560]{1,0} %divide.1662), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %constant.39 = f32[] constant(1e-08)
  %broadcast.40 = f32[32000,2560]{1,0} broadcast(f32[] %constant.39), dimensions={}
  %add.1691 = f32[32000,2560]{1,0} add(f32[32000,2560]{1,0} %sqrt.1690, f32[32000,2560]{1,0} %broadcast.40), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %divide.1692 = f32[32000,2560]{1,0} divide(f32[32000,2560]{1,0} %divide.1639, f32[32000,2560]{1,0} %add.1691), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=300}
  %multiply.1711 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %Arg_10.11, f32[32000,2560]{1,0} %broadcast.84), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %add.1712 = f32[32000,2560]{1,0} add(f32[32000,2560]{1,0} %divide.1692, f32[32000,2560]{1,0} %multiply.1711), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/transforms/_adding.py" source_line=50}
  %multiply.1764 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %broadcast.1763, f32[32000,2560]{1,0} %add.1712), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/transform.py" source_line=983}
  %add.1782 = f32[32000,2560]{1,0} add(f32[32000,2560]{1,0} %Arg_10.11, f32[32000,2560]{1,0} %multiply.1764), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/update.py" source_line=43}
  %reshape.2004 = f32[32000,2560]{1,0} reshape(f32[32000,2560]{1,0} %add.1782), sharding={replicated}
  %reshape.2005 = s32[] reshape(s32[] %call.1616), sharding={replicated}
  %reshape.2006 = f32[2560]{0} reshape(f32[2560]{0} %add.1541), sharding={replicated}
  %reshape.2007 = f32[2560,8,8192]{2,1,0} reshape(f32[2560,8,8192]{2,0,1} %add.1544), sharding={replicated}
  %reshape.2008 = f32[2560,8,8192]{2,1,0} reshape(f32[2560,8,8192]{2,0,1} %add.1547), sharding={replicated}
  %reshape.2009 = f32[8192,8,2560]{2,1,0} reshape(f32[8192,8,2560]{2,0,1} %add.1550), sharding={replicated}
  %reshape.2010 = f32[2560,8]{1,0} reshape(f32[2560,8]{0,1} %add.1553), sharding={replicated}
  %reshape.2011 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,0,1} %add.1556), sharding={replicated}
  %reshape.2012 = f32[8,8,128,2560]{3,2,1,0} reshape(f32[8,8,128,2560]{3,2,0,1} %add.1559), sharding={replicated}
  %reshape.2013 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,0,1} %add.1562), sharding={replicated}
  %reshape.2014 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,0,1} %add.1565), sharding={replicated}
  %reshape.2015 = f32[32000,2560]{1,0} reshape(f32[32000,2560]{1,0} %add.1568), sharding={replicated}
  %reshape.2016 = f32[2560]{0} reshape(f32[2560]{0} %add.1572), sharding={replicated}
  %reshape.2017 = f32[2560,8,8192]{2,1,0} reshape(f32[2560,8,8192]{2,0,1} %add.1576), sharding={replicated}
  %reshape.2018 = f32[2560,8,8192]{2,1,0} reshape(f32[2560,8,8192]{2,0,1} %add.1580), sharding={replicated}
  %reshape.2019 = f32[8192,8,2560]{2,1,0} reshape(f32[8192,8,2560]{2,0,1} %add.1584), sharding={replicated}
  %reshape.2020 = f32[2560,8]{1,0} reshape(f32[2560,8]{0,1} %add.1588), sharding={replicated}
  %reshape.2021 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,0,1} %add.1592), sharding={replicated}
  %reshape.2022 = f32[8,8,128,2560]{3,2,1,0} reshape(f32[8,8,128,2560]{3,2,0,1} %add.1596), sharding={replicated}
  %reshape.2023 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,0,1} %add.1600), sharding={replicated}
  %reshape.2024 = f32[2560,8,8,128]{3,2,1,0} reshape(f32[2560,8,8,128]{3,2,0,1} %add.1604), sharding={replicated}
  %reshape.2025 = f32[32000,2560]{1,0} reshape(f32[32000,2560]{1,0} %add.1608), sharding={replicated}
  %compare.1765 = pred[] compare(s32[] %Arg_32.33, s32[] %constant.162), direction=LT, metadata={op_name="jit(train_step)/jit(main)/lt" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142}
  %add.1766 = s32[] add(s32[] %Arg_32.33, s32[] %constant.167), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/usr/local/lib/python3.12/dist-packages/optax/_src/numerics.py" source_line=142}
  %call.1772 = s32[] call(pred[] %compare.1765, s32[] %add.1766, s32[] %constant.162), to_apply=%_where_8.1767
  %reshape.2026 = s32[] reshape(s32[] %call.1772), sharding={replicated}
  %multiply.1784 = f32[2560]{0} multiply(f32[2560]{0} %select.1502, f32[2560]{0} %select.1502), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1789 = f32[] reduce(f32[2560]{0} %multiply.1784, f32[] %constant.170), dimensions={0}, to_apply=%region_40.1785, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1790 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %select.1506, f32[2560,8,8192]{2,0,1} %select.1506), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1795 = f32[] reduce(f32[2560,8,8192]{2,0,1} %multiply.1790, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_41.1791, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1796 = f32[] add(f32[] %reduce.1789, f32[] %reduce.1795), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1797 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %select.1510, f32[2560,8,8192]{2,0,1} %select.1510), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1802 = f32[] reduce(f32[2560,8,8192]{2,0,1} %multiply.1797, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_42.1798, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1803 = f32[] add(f32[] %add.1796, f32[] %reduce.1802), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1804 = f32[8192,8,2560]{2,0,1} multiply(f32[8192,8,2560]{2,0,1} %select.1514, f32[8192,8,2560]{2,0,1} %select.1514), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1809 = f32[] reduce(f32[8192,8,2560]{2,0,1} %multiply.1804, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_43.1805, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1810 = f32[] add(f32[] %add.1803, f32[] %reduce.1809), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1811 = f32[2560,8]{0,1} multiply(f32[2560,8]{0,1} %select.1518, f32[2560,8]{0,1} %select.1518), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1816 = f32[] reduce(f32[2560,8]{0,1} %multiply.1811, f32[] %constant.170), dimensions={0,1}, to_apply=%region_44.1812, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1817 = f32[] add(f32[] %add.1810, f32[] %reduce.1816), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1818 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1522, f32[2560,8,8,128]{3,2,0,1} %select.1522), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1823 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1818, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_45.1819, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1824 = f32[] add(f32[] %add.1817, f32[] %reduce.1823), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1825 = f32[8,8,128,2560]{3,2,0,1} multiply(f32[8,8,128,2560]{3,2,0,1} %select.1526, f32[8,8,128,2560]{3,2,0,1} %select.1526), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1830 = f32[] reduce(f32[8,8,128,2560]{3,2,0,1} %multiply.1825, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_46.1826, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1831 = f32[] add(f32[] %add.1824, f32[] %reduce.1830), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1832 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1530, f32[2560,8,8,128]{3,2,0,1} %select.1530), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1837 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1832, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_47.1833, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1838 = f32[] add(f32[] %add.1831, f32[] %reduce.1837), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1839 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %select.1534, f32[2560,8,8,128]{3,2,0,1} %select.1534), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1844 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1839, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_48.1840, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1845 = f32[] add(f32[] %add.1838, f32[] %reduce.1844), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1846 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %select.1538, f32[32000,2560]{1,0} %select.1538), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1851 = f32[] reduce(f32[32000,2560]{1,0} %multiply.1846, f32[] %constant.170), dimensions={0,1}, to_apply=%region_49.1847, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1852 = f32[] add(f32[] %add.1845, f32[] %reduce.1851), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %sqrt.1853 = f32[] sqrt(f32[] %add.1852), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reshape.2027 = f32[] reshape(f32[] %sqrt.1853)
  %log.640 = f32[16,2048,1]{2,1,0} log(f32[16,2048,1]{2,1,0} %reshape.639), metadata={op_name="jit(train_step)/jit(main)/log" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=880}
  %broadcast.641 = f32[16,2048,1]{2,1,0} broadcast(f32[16,2048,1]{2,1,0} %log.640), dimensions={0,1,2}, metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=880}
  %reshape.642 = f32[16,2048]{1,0} reshape(f32[16,2048,1]{2,1,0} %broadcast.641), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=880}
  %broadcast.643 = f32[16,2048,32000]{2,1,0} broadcast(f32[16,2048]{1,0} %reshape.642), dimensions={0,1}, metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=880}
  %subtract.644 = f32[16,2048,32000]{2,1,0} subtract(f32[16,2048,32000]{2,1,0} %subtract.632, f32[16,2048,32000]{2,1,0} %broadcast.643), metadata={op_name="jit(train_step)/jit(main)/sub" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=880}
  %multiply.645 = f32[16,2048,32000]{2,1,0} multiply(f32[16,2048,32000]{2,1,0} %call.622, f32[16,2048,32000]{2,1,0} %subtract.644), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=881}
  %reduce.650 = f32[16,2048]{1,0} reduce(f32[16,2048,32000]{2,1,0} %multiply.645, f32[] %constant.170), dimensions={2}, to_apply=%region_8.646, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=881}
  %negate.651 = f32[16,2048]{1,0} negate(f32[16,2048]{1,0} %reduce.650), metadata={op_name="jit(train_step)/jit(main)/neg" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=881}
  %multiply.655 = f32[16,2048]{1,0} multiply(f32[16,2048]{1,0} %reshape.654, f32[16,2048]{1,0} %reshape.654), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=884}
  %multiply.656 = f32[16,2048]{1,0} multiply(f32[16,2048]{1,0} %multiply.655, f32[16,2048]{1,0} %broadcast.124), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=884}
  %add.657 = f32[16,2048]{1,0} add(f32[16,2048]{1,0} %negate.651, f32[16,2048]{1,0} %multiply.656), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=885}
  %custom-call.658 = f32[16,2048]{1,0} custom-call(f32[16,2048]{1,0} %add.657), custom_call_target="Sharding", sharding={devices=[8,1]<=[8]}, metadata={op_name="jit(train_step)/jit(main)/sharding_constraint" source_file="/opt/flax/flax/linen/spmd.py" source_line=208}
  %multiply.661 = f32[16,2048]{1,0} multiply(f32[16,2048]{1,0} %custom-call.658, f32[16,2048]{1,0} %convert.660), metadata={op_name="jit(train_step)/jit(main)/mul" source_file="/opt/maxtext/MaxText/train.py" source_line=435}
  %reduce.666 = f32[] reduce(f32[16,2048]{1,0} %multiply.661, f32[] %constant.170), dimensions={0,1}, to_apply=%region_9.662, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/train.py" source_line=436}
  %divide.676 = f32[] divide(f32[] %reduce.666, f32[] %add.675), metadata={op_name="jit(train_step)/jit(main)/div" source_file="/opt/maxtext/MaxText/train.py" source_line=438}
  %reshape.2028 = f32[] reshape(f32[] %divide.676)
  %reshape.2029 = f32[] reshape(f32[] %constant.170)
  %multiply.1924 = f32[2560]{0} multiply(f32[2560]{0} %add.1773, f32[2560]{0} %add.1773), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1929 = f32[] reduce(f32[2560]{0} %multiply.1924, f32[] %constant.170), dimensions={0}, to_apply=%region_60.1925, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1930 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %add.1774, f32[2560,8,8192]{2,1,0} %add.1774), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1935 = f32[] reduce(f32[2560,8,8192]{2,1,0} %multiply.1930, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_61.1931, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1936 = f32[] add(f32[] %reduce.1929, f32[] %reduce.1935), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1937 = f32[2560,8,8192]{2,1,0} multiply(f32[2560,8,8192]{2,1,0} %add.1775, f32[2560,8,8192]{2,1,0} %add.1775), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1942 = f32[] reduce(f32[2560,8,8192]{2,1,0} %multiply.1937, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_62.1938, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1943 = f32[] add(f32[] %add.1936, f32[] %reduce.1942), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1944 = f32[8192,8,2560]{2,1,0} multiply(f32[8192,8,2560]{2,1,0} %add.1776, f32[8192,8,2560]{2,1,0} %add.1776), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1949 = f32[] reduce(f32[8192,8,2560]{2,1,0} %multiply.1944, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_63.1945, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1950 = f32[] add(f32[] %add.1943, f32[] %reduce.1949), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1951 = f32[2560,8]{1,0} multiply(f32[2560,8]{1,0} %add.1777, f32[2560,8]{1,0} %add.1777), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1956 = f32[] reduce(f32[2560,8]{1,0} %multiply.1951, f32[] %constant.170), dimensions={0,1}, to_apply=%region_64.1952, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1957 = f32[] add(f32[] %add.1950, f32[] %reduce.1956), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1958 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %add.1778, f32[2560,8,8,128]{3,2,1,0} %add.1778), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1963 = f32[] reduce(f32[2560,8,8,128]{3,2,1,0} %multiply.1958, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_65.1959, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1964 = f32[] add(f32[] %add.1957, f32[] %reduce.1963), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1965 = f32[8,8,128,2560]{3,2,1,0} multiply(f32[8,8,128,2560]{3,2,1,0} %add.1779, f32[8,8,128,2560]{3,2,1,0} %add.1779), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1970 = f32[] reduce(f32[8,8,128,2560]{3,2,1,0} %multiply.1965, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_66.1966, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1971 = f32[] add(f32[] %add.1964, f32[] %reduce.1970), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1972 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %add.1780, f32[2560,8,8,128]{3,2,1,0} %add.1780), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1977 = f32[] reduce(f32[2560,8,8,128]{3,2,1,0} %multiply.1972, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_67.1973, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1978 = f32[] add(f32[] %add.1971, f32[] %reduce.1977), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1979 = f32[2560,8,8,128]{3,2,1,0} multiply(f32[2560,8,8,128]{3,2,1,0} %add.1781, f32[2560,8,8,128]{3,2,1,0} %add.1781), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1984 = f32[] reduce(f32[2560,8,8,128]{3,2,1,0} %multiply.1979, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_68.1980, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1985 = f32[] add(f32[] %add.1978, f32[] %reduce.1984), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1986 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %add.1782, f32[32000,2560]{1,0} %add.1782), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1991 = f32[] reduce(f32[32000,2560]{1,0} %multiply.1986, f32[] %constant.170), dimensions={0,1}, to_apply=%region_69.1987, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1992 = f32[] add(f32[] %add.1985, f32[] %reduce.1991), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %sqrt.1993 = f32[] sqrt(f32[] %add.1992), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reshape.2030 = f32[] reshape(f32[] %sqrt.1993)
  %multiply.1854 = f32[2560]{0} multiply(f32[2560]{0} %convert.720, f32[2560]{0} %convert.720), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1859 = f32[] reduce(f32[2560]{0} %multiply.1854, f32[] %constant.170), dimensions={0}, to_apply=%region_50.1855, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1860 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %transpose.1419, f32[2560,8,8192]{2,0,1} %transpose.1419), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1865 = f32[] reduce(f32[2560,8,8192]{2,0,1} %multiply.1860, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_51.1861, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1866 = f32[] add(f32[] %reduce.1859, f32[] %reduce.1865), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1867 = f32[2560,8,8192]{2,0,1} multiply(f32[2560,8,8192]{2,0,1} %transpose.1418, f32[2560,8,8192]{2,0,1} %transpose.1418), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1872 = f32[] reduce(f32[2560,8,8192]{2,0,1} %multiply.1867, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_52.1868, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1873 = f32[] add(f32[] %add.1866, f32[] %reduce.1872), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1874 = f32[8192,8,2560]{2,0,1} multiply(f32[8192,8,2560]{2,0,1} %transpose.1417, f32[8192,8,2560]{2,0,1} %transpose.1417), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1879 = f32[] reduce(f32[8192,8,2560]{2,0,1} %multiply.1874, f32[] %constant.170), dimensions={0,1,2}, to_apply=%region_53.1875, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1880 = f32[] add(f32[] %add.1873, f32[] %reduce.1879), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1881 = f32[2560,8]{0,1} multiply(f32[2560,8]{0,1} %transpose.1416, f32[2560,8]{0,1} %transpose.1416), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1886 = f32[] reduce(f32[2560,8]{0,1} %multiply.1881, f32[] %constant.170), dimensions={0,1}, to_apply=%region_54.1882, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1887 = f32[] add(f32[] %add.1880, f32[] %reduce.1886), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1888 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %transpose.1415, f32[2560,8,8,128]{3,2,0,1} %transpose.1415), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1893 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1888, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_55.1889, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1894 = f32[] add(f32[] %add.1887, f32[] %reduce.1893), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1895 = f32[8,8,128,2560]{3,2,0,1} multiply(f32[8,8,128,2560]{3,2,0,1} %transpose.1414, f32[8,8,128,2560]{3,2,0,1} %transpose.1414), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1900 = f32[] reduce(f32[8,8,128,2560]{3,2,0,1} %multiply.1895, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_56.1896, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1901 = f32[] add(f32[] %add.1894, f32[] %reduce.1900), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1902 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %transpose.1413, f32[2560,8,8,128]{3,2,0,1} %transpose.1413), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1907 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1902, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_57.1903, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1908 = f32[] add(f32[] %add.1901, f32[] %reduce.1907), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1909 = f32[2560,8,8,128]{3,2,0,1} multiply(f32[2560,8,8,128]{3,2,0,1} %transpose.1412, f32[2560,8,8,128]{3,2,0,1} %transpose.1412), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1914 = f32[] reduce(f32[2560,8,8,128]{3,2,0,1} %multiply.1909, f32[] %constant.170), dimensions={0,1,2,3}, to_apply=%region_58.1910, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1915 = f32[] add(f32[] %add.1908, f32[] %reduce.1914), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %multiply.1916 = f32[32000,2560]{1,0} multiply(f32[32000,2560]{1,0} %add.1427, f32[32000,2560]{1,0} %add.1427), metadata={op_name="jit(train_step)/jit(main)/square" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reduce.1921 = f32[] reduce(f32[32000,2560]{1,0} %multiply.1916, f32[] %constant.170), dimensions={0,1}, to_apply=%region_59.1917, metadata={op_name="jit(train_step)/jit(main)/reduce_sum" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %add.1922 = f32[] add(f32[] %add.1915, f32[] %reduce.1921), metadata={op_name="jit(train_step)/jit(main)/add" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %sqrt.1923 = f32[] sqrt(f32[] %add.1922), metadata={op_name="jit(train_step)/jit(main)/sqrt" source_file="/opt/maxtext/MaxText/max_utils.py" source_line=82}
  %reshape.2031 = f32[] reshape(f32[] %sqrt.1923)
  %reshape.2032 = s32[] reshape(s32[] %reduce.673)
  ROOT %tuple.2033 = (s32[], f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, f32[8192,8,2560]{2,1,0}, /*index=5*/f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=10*/f32[32000,2560]{1,0}, s32[], f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, /*index=15*/f32[8192,8,2560]{2,1,0}, f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=20*/f32[2560,8,8,128]{3,2,1,0}, f32[32000,2560]{1,0}, f32[2560]{0}, f32[2560,8,8192]{2,1,0}, f32[2560,8,8192]{2,1,0}, /*index=25*/f32[8192,8,2560]{2,1,0}, f32[2560,8]{1,0}, f32[2560,8,8,128]{3,2,1,0}, f32[8,8,128,2560]{3,2,1,0}, f32[2560,8,8,128]{3,2,1,0}, /*index=30*/f32[2560,8,8,128]{3,2,1,0}, f32[32000,2560]{1,0}, s32[], f32[], f32[], /*index=35*/f32[], f32[], f32[], s32[]) tuple(s32[] %reshape.1994, f32[2560]{0} %reshape.1995, f32[2560,8,8192]{2,1,0} %reshape.1996, f32[2560,8,8192]{2,1,0} %reshape.1997, f32[8192,8,2560]{2,1,0} %reshape.1998, /*index=5*/f32[2560,8]{1,0} %reshape.1999, f32[2560,8,8,128]{3,2,1,0} %reshape.2000, f32[8,8,128,2560]{3,2,1,0} %reshape.2001, f32[2560,8,8,128]{3,2,1,0} %reshape.2002, f32[2560,8,8,128]{3,2,1,0} %reshape.2003, /*index=10*/f32[32000,2560]{1,0} %reshape.2004, s32[] %reshape.2005, f32[2560]{0} %reshape.2006, f32[2560,8,8192]{2,1,0} %reshape.2007, f32[2560,8,8192]{2,1,0} %reshape.2008, /*index=15*/f32[8192,8,2560]{2,1,0} %reshape.2009, f32[2560,8]{1,0} %reshape.2010, f32[2560,8,8,128]{3,2,1,0} %reshape.2011, f32[8,8,128,2560]{3,2,1,0} %reshape.2012, f32[2560,8,8,128]{3,2,1,0} %reshape.2013, /*index=20*/f32[2560,8,8,128]{3,2,1,0} %reshape.2014, f32[32000,2560]{1,0} %reshape.2015, f32[2560]{0} %reshape.2016, f32[2560,8,8192]{2,1,0} %reshape.2017, f32[2560,8,8192]{2,1,0} %reshape.2018, /*index=25*/f32[8192,8,2560]{2,1,0} %reshape.2019, f32[2560,8]{1,0} %reshape.2020, f32[2560,8,8,128]{3,2,1,0} %reshape.2021, f32[8,8,128,2560]{3,2,1,0} %reshape.2022, f32[2560,8,8,128]{3,2,1,0} %reshape.2023, /*index=30*/f32[2560,8,8,128]{3,2,1,0} %reshape.2024, f32[32000,2560]{1,0} %reshape.2025, s32[] %reshape.2026, f32[] %reshape.2027, f32[] %reshape.2028, /*index=35*/f32[] %reshape.2029, f32[] %reshape.2030, f32[] %reshape.2031, s32[] %reshape.2032), sharding={{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=5*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=10*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=15*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=20*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=25*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=30*/{replicated}, {replicated}, {replicated}, {replicated}, {replicated}, /*index=35*/{replicated}, {replicated}, {replicated}, {replicated}}
}

