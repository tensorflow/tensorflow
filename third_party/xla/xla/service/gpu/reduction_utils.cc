/* Copyright 2023 The OpenXLA Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#include "xla/service/gpu/reduction_utils.h"

#include <algorithm>
#include <cstdint>
#include <ostream>

#include "absl/algorithm/container.h"
#include "absl/log/check.h"
#include "absl/strings/str_join.h"
#include "absl/types/span.h"
#include "xla/hlo/ir/hlo_instruction.h"
#include "xla/hlo/ir/hlo_opcode.h"
#include "xla/layout_util.h"
#include "xla/service/gpu/ir_emission_utils.h"
#include "xla/shape.h"
#include "xla/shape_util.h"
#include "xla/stream_executor/device_description.h"
#include "xla/util.h"
#include "tsl/platform/logging.h"

namespace xla {
namespace gpu {

namespace {
// Given a shape and a group of contiguous dimensions in the shape, returns
// a tuple of three values (major, middle, minor), where major is the size of
// the dimensions more major then the given dimensions, minor is the size of
// dimensions more minor then the given dimensions, and middle is the size of
// the given dimensions.
Vector3 PartitionShapeByMiddleDimensions(
    const Shape& shape, absl::Span<const int64_t> dims_middle) {
  CHECK(LayoutUtil::AreDimensionsConsecutive(shape.layout(), dims_middle));
  Vector3 values = {1, 1, 1};
  enum Segment { kMajor = 0, kMiddle = 1, kMinor = 2 };
  Segment cur_segment = kMinor;

  for (int64_t cur_dim : LayoutUtil::MinorToMajor(shape)) {
    if (cur_segment != kMajor) {
      // Handle change of segments.
      bool cur_dim_in_middle = absl::c_linear_search(dims_middle, cur_dim);
      if (cur_segment == kMinor) {
        if (cur_dim_in_middle) {
          cur_segment = kMiddle;
        }
      } else if (cur_segment == kMiddle) {
        if (!cur_dim_in_middle) {
          cur_segment = kMajor;
        }
      }
    }
    values[cur_segment] *= shape.dimensions(cur_dim);
  }
  return values;
}

}  // namespace

Vector3 GetReductionTiling(const ReductionDimensions& reduction_dimensions) {
  if (reduction_dimensions.is_row_reduction) {
    int64_t tile_z = std::min(reduction_dimensions.dimensions[0],
                              BatchedReductionRaceFreeBound());
    return {tile_z, 1, 16};
  }

  // Column reduction.
  return {1, 128, 1};
}

int64_t ReductionDimensionRaceFreeBound(
    const ReductionDimensions& reduction_dimensions,
    const se::DeviceDescription& device_description) {
  Vector3 reduction_tiling = GetReductionTiling(reduction_dimensions);
  if (reduction_dimensions.is_row_reduction) {
    return MinThreadsXRowReduction() * reduction_tiling[2];
  }
  return WarpSize(device_description) * reduction_tiling[1];
}

bool IsUnnestedReductionFasterThanElemental(
    const ReductionDimensions& reduction_dimensions,
    const se::DeviceDescription& device_description) {
  const int64_t warp_size = WarpSize(device_description);
  if (reduction_dimensions.is_row_reduction) {
    // For row reduction, the tile block is 1 x tile_size_x, and we are reducing
    // along tile_size_x which needs to be large enough to make the tiling
    // implementation efficient.
    // For very small reductions with a power-of-two size, we can fit multiple
    // reductions inside a single warp, which is more efficient than a loop.
    return (reduction_dimensions.dimensions[2] >= warp_size) ||
           ((warp_size % reduction_dimensions.dimensions[2]) == 0);
  }

  // For column reduction, the tile block is tile_size_y x tile_size_x, and we
  // are reducing along tile_size_y. Only tile_size_y needs to be
  // large enough to make the tiling implementation efficient.
  int64_t major_size = reduction_dimensions.dimensions[1];
  int64_t minor_size = reduction_dimensions.dimensions[2];

  // Rule generated by sweeping the search space of small column reductions.
  bool prefer_elemental_emitter =
      (major_size < warp_size) ||
      (major_size < 2 * warp_size && minor_size < warp_size) ||
      (major_size < 4 * warp_size && minor_size < 8) ||
      (major_size < 8 * warp_size && minor_size < 3);

  return !prefer_elemental_emitter;
}

bool IsReductionFromOrToContiguousDimensions(
    const HloInstruction& reduce,
    const se::DeviceDescription& device_description) {
  if (reduce.opcode() != HloOpcode::kReduce) {
    return false;
  }

  const Shape& operand_shape = reduce.operand(0)->shape();
  absl::Span<const int64_t> dims_to_reduce = reduce.dimensions();
  DimensionVector dims_to_keep;
  for (int64_t dim = 0; dim < operand_shape.dimensions().size(); ++dim) {
    if (!absl::c_linear_search(dims_to_reduce, dim)) {
      dims_to_keep.push_back(dim);
    }
  }

  // We support fast codegen for three cases:
  // 1) Row reduction: (K, R)
  // 2) Column reduction: (K, R, K)
  // 3) "Batched" row reduction: (R, K, R)
  return (LayoutUtil::AreDimensionsConsecutive(operand_shape.layout(),
                                               dims_to_keep) ||
          LayoutUtil::AreDimensionsConsecutive(operand_shape.layout(),
                                               dims_to_reduce)) &&
         IsUnnestedReductionFasterThanElemental(
             GetReductionKindAndContiguousComponents(reduce),
             device_description);
}

bool ReductionIsRaceFree(const ReductionDimensions& reduction_dimensions,
                         const se::DeviceDescription& device_description) {
  if (reduction_dimensions.is_row_reduction) {
    return reduction_dimensions.dimensions[2] <=
               ReductionDimensionRaceFreeBound(reduction_dimensions,
                                               device_description) &&
           reduction_dimensions.dimensions[0] <=
               BatchedReductionRaceFreeBound();
  }

  // Column reduction.
  return reduction_dimensions.dimensions[1] <=
         ReductionDimensionRaceFreeBound(reduction_dimensions,
                                         device_description);
}

std::ostream& operator<<(std::ostream& os,
                         const ReductionDimensions& reduction_dimensions) {
  bool is_row_reduction = reduction_dimensions.is_row_reduction;
  os << (is_row_reduction ? "row " : "column ") << "reduction ["
     << absl::StrJoin(reduction_dimensions.dimensions, ",") << "] -> ["
     << reduction_dimensions.dimensions[0] << ", "
     << reduction_dimensions
            .dimensions[is_row_reduction
                            ? ReductionDimensions::kRowKeptDimension
                            : ReductionDimensions::kColMinorKeptDimension]
     << "]";
  return os;
}

ReductionDimensions GetReductionKindAndContiguousComponents(
    const HloInstruction& reduce) {
  Shape input_shape = reduce.operand(0)->shape();
  absl::Span<const int64_t> dims_to_reduce = reduce.dimensions();
  DimensionVector dims_to_keep;
  for (int64_t dim = 0; dim < input_shape.dimensions().size(); ++dim) {
    if (!absl::c_linear_search(dims_to_reduce, dim)) {
      dims_to_keep.push_back(dim);
    }
  }

  if (dims_to_keep.empty()) {
    return {/*is_row_reduction=*/true,
            {1, 1, ShapeUtil::ElementsIn(input_shape)}};
  }

  if (LayoutUtil::AreDimensionsConsecutive(input_shape.layout(),
                                           dims_to_keep)) {
    Vector3 shape_partition =
        PartitionShapeByMiddleDimensions(input_shape, dims_to_keep);
    if (shape_partition[1] == 1) {
      return {/*is_row_reduction=*/true,
              {1, 1, shape_partition[0] * shape_partition[2]}};
    }
    if (shape_partition[2] == 1) {
      return {/*is_row_reduction=*/false,
              {1, shape_partition[0], shape_partition[1]}};
    }
    return {/*is_row_reduction=*/true, shape_partition};
  }

  Vector3 shape_partition =
      PartitionShapeByMiddleDimensions(input_shape, dims_to_reduce);

  if (shape_partition[2] == 1) {
    return {/*is_row_reduction=*/true,
            {1, shape_partition[0], shape_partition[1]}};
  }
  return {/*is_row_reduction=*/false, shape_partition};
}

bool IsRealReductionHero(const HloInstruction& root, const HloInstruction& hero,
                         const se::DeviceDescription& device_description) {
  if (!IsReductionFromOrToContiguousDimensions(hero, device_description)) {
    return false;
  }
  return &root == &hero ||
         ReductionIsRaceFree(GetReductionKindAndContiguousComponents(hero),
                             device_description);
}

bool AreReductionsMultiOutputFusionCompatible(
    const HloInstruction* reduce_hero, const HloInstruction* first_reduce) {
  // The reduction kind must be the same for all reduce heroes inside of a
  // multioutput fusion.
  return GetReductionKindAndContiguousComponents(*reduce_hero) ==
         GetReductionKindAndContiguousComponents(*first_reduce);
}

}  // namespace gpu
}  // namespace xla
