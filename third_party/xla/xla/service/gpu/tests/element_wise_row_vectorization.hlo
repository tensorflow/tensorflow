// RUN: hlo-opt %s --platform=gpu --stage=ptx --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/v100.txtpb --split-input-file | FileCheck %s
// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb --split-input-file | FileCheck --check-prefixes=CHECK-LLVM %s
// We check that the row loads are vectorized.

HloModule SimpleAddRowBroadcasting, is_scheduled=true

%fused_computation.0 (param_0: f32[672], param_1: f32[512,14,14,672]) -> f32[512,14,14,672]{
  %param_0 = f32[672]{0} parameter(0)
  %broadcast = f32[512,14,14,672]{3,2,1,0} broadcast(%param_0), dimensions={3}
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)
  ROOT %add = f32[512,14,14,672]{3,2,1,0} add(%broadcast, %param_1)
}

ENTRY main {
  %param_0 = f32[672]{0} parameter(0)
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)

  ROOT %fusion.0 = f32[512,14,14,672]{3,2,1,0} fusion(%param_0, %param_1), kind=kLoop, calls=%fused_computation.0
}

// CHECK-LABEL: fusion_0
// CHECK: .reqntid 168, 1, 1
// CHECK-NOT: ld.global.nc.f
// CHECK-NOT: ld.global.nc.b

// -----

HloModule SimpleAddSmallRowBroadcasting, is_scheduled=true

%fused_computation.0 (param_0: f32[48], param_1: f32[512,14,14,48]) -> f32[512,14,14,48]{
  %param_0 = f32[48]{0} parameter(0)
  %broadcast = f32[512,14,14,48]{3,2,1,0} broadcast(%param_0), dimensions={3}
  %param_1 = f32[512,14,14,48]{3,2,1,0} parameter(1)
  ROOT %add = f32[512,14,14,48]{3,2,1,0} add(%broadcast, %param_1)
}

ENTRY main {
  %param_0 = f32[48]{0} parameter(0)
  %param_1 = f32[512,14,14,48]{3,2,1,0} parameter(1)

  ROOT %fusion.0_small = f32[512,14,14,48]{3,2,1,0} fusion(%param_0, %param_1), kind=kLoop, calls=%fused_computation.0
}

// CHECK-LABEL: fusion_0_small
// CHECK: .reqntid 12, 11, 1
// CHECK-NOT: ld.global.nc.f
// CHECK-NOT: ld.global.nc.b

// -----

// This test an BatchNorm fused kernel found in EfficientNet.
HloModule EfficientNetSwish, is_scheduled=true

%fused_computation.1 (param_0.89: f32[672], param_1: f32[672], param_2: f32[672], param_3: f32[672], param_4: f16[512,14,14,672], param_5: f32[672], param_6: f16[512,14,14,672], param_7: f32[672]) -> f16[512,14,14,672] {
  %param_2 = f32[672]{0} parameter(2)
  %constant_157 = f32[] constant(1), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %broadcast.186 = f32[672]{0} broadcast(f32[] %constant_157), dimensions={}, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %param_5 = f32[672]{0} parameter(5)
  %constant_56 = f32[] constant(9.96492327e-06)
  %broadcast.185 = f32[672]{0} broadcast(f32[] %constant_56), dimensions={}
  %multiply.155 = f32[672]{0} multiply(f32[672]{0} %param_5, f32[672]{0} %broadcast.185), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %param_3 = f32[672]{0} parameter(3)
  %multiply.154 = f32[672]{0} multiply(f32[672]{0} %param_3, f32[672]{0} %broadcast.185), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %multiply.153 = f32[672]{0} multiply(f32[672]{0} %multiply.154, f32[672]{0} %multiply.154), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %subtract.15 = f32[672]{0} subtract(f32[672]{0} %multiply.155, f32[672]{0} %multiply.153), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %constant_155 = f32[] constant(0.001), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %broadcast.184 = f32[672]{0} broadcast(f32[] %constant_155), dimensions={}
  %add.14 = f32[672]{0} add(f32[672]{0} %subtract.15, f32[672]{0} %broadcast.184), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %rsqrt.23 = f32[672]{0} rsqrt(f32[672]{0} %add.14), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %multiply.152 = f32[672]{0} multiply(f32[672]{0} %rsqrt.23, f32[672]{0} %rsqrt.23), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %divide.14 = f32[672]{0} divide(f32[672]{0} %broadcast.186, f32[672]{0} %multiply.152), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %rsqrt.7 = f32[672]{0} rsqrt(f32[672]{0} %divide.14), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %multiply.29 = f32[672]{0} multiply(f32[672]{0} %param_2, f32[672]{0} %rsqrt.7), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %multiply.28 = f32[672]{0} multiply(f32[672]{0} %multiply.29, f32[672]{0} %broadcast.185), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %broadcast.47 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %multiply.28), dimensions={3}
  %param_6 = f16[512,14,14,672]{3,2,1,0} parameter(6)
  %constant_194 = f16[] constant(1), metadata={op_type="AddV2" op_name="add"}
  %broadcast.256 = f16[512,14,14,672]{3,2,1,0} broadcast(f16[] %constant_194), dimensions={}
  %param_4 = f16[512,14,14,672]{3,2,1,0} parameter(4)
  %convert.66 = f32[512,14,14,672]{3,2,1,0} convert(f16[512,14,14,672]{3,2,1,0} %param_4), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %broadcast.254 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %multiply.154), dimensions={3}, metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %subtract.82 = f32[512,14,14,672]{3,2,1,0} subtract(f32[512,14,14,672]{3,2,1,0} %convert.66, f32[512,14,14,672]{3,2,1,0} %broadcast.254), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %broadcast.251 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %rsqrt.23), dimensions={3}
  %multiply.219 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %subtract.82, f32[512,14,14,672]{3,2,1,0} %broadcast.251), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %broadcast.250 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_2), dimensions={3}, metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %multiply.218 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %multiply.219, f32[512,14,14,672]{3,2,1,0} %broadcast.250), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %param_7 = f32[672]{0} parameter(7)
  %broadcast.249 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_7), dimensions={3}, metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %add.79 = f32[512,14,14,672]{3,2,1,0} add(f32[512,14,14,672]{3,2,1,0} %multiply.218, f32[512,14,14,672]{3,2,1,0} %broadcast.249), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %convert.65 = f16[512,14,14,672]{3,2,1,0} convert(f32[512,14,14,672]{3,2,1,0} %add.79), metadata={op_type="FusedBatchNormV3" op_name="foo/batch_normalization/FusedBatchNormV3"}
  %negate.12 = f16[512,14,14,672]{3,2,1,0} negate(f16[512,14,14,672]{3,2,1,0} %convert.65)
  %exponential.10 = f16[512,14,14,672]{3,2,1,0} exponential(f16[512,14,14,672]{3,2,1,0} %negate.12)
  %add.78 = f16[512,14,14,672]{3,2,1,0} add(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %exponential.10)
  %divide.20 = f16[512,14,14,672]{3,2,1,0} divide(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %add.78), metadata={op_type="Sigmoid" op_name="foo/activation/Sigmoid"}
  %subtract.77 = f16[512,14,14,672]{3,2,1,0} subtract(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %divide.20), metadata={op_type="Sub" op_name="sub"}
  %multiply.211 = f16[512,14,14,672]{3,2,1,0} multiply(f16[512,14,14,672]{3,2,1,0} %convert.65, f16[512,14,14,672]{3,2,1,0} %subtract.77), metadata={op_type="Mul" op_name="mul"}
  %add.75 = f16[512,14,14,672]{3,2,1,0} add(f16[512,14,14,672]{3,2,1,0} %broadcast.256, f16[512,14,14,672]{3,2,1,0} %multiply.211), metadata={op_type="AddV2" op_name="add"}
  %multiply.210 = f16[512,14,14,672]{3,2,1,0} multiply(f16[512,14,14,672]{3,2,1,0} %divide.20, f16[512,14,14,672]{3,2,1,0} %add.75), metadata={op_type="Mul" op_name="mul_1"}
  %multiply.209 = f16[512,14,14,672]{3,2,1,0} multiply(f16[512,14,14,672]{3,2,1,0} %param_6, f16[512,14,14,672]{3,2,1,0} %multiply.210), metadata={op_type="Mul" op_name="mul_2"}
  %convert.8 = f32[512,14,14,672]{3,2,1,0} convert(f16[512,14,14,672]{3,2,1,0} %multiply.209), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %constant_48 = f32[] constant(100352), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %broadcast.46 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[] %constant_48), dimensions={}, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %multiply.27 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %convert.8, f32[512,14,14,672]{3,2,1,0} %broadcast.46), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %param_1 = f32[672]{0} parameter(1)
  %broadcast.45 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_1), dimensions={3}, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %subtract.10 = f32[512,14,14,672]{3,2,1,0} subtract(f32[512,14,14,672]{3,2,1,0} %multiply.27, f32[512,14,14,672]{3,2,1,0} %broadcast.45), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %param_0.89 = f32[672]{0} parameter(0)
  %broadcast.44 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %param_0.89), dimensions={3}, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %multiply.26 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %broadcast.44, f32[512,14,14,672]{3,2,1,0} %subtract.82), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %broadcast.42 = f32[512,14,14,672]{3,2,1,0} broadcast(f32[672]{0} %divide.14), dimensions={3}
  %divide.6 = f32[512,14,14,672]{3,2,1,0} divide(f32[512,14,14,672]{3,2,1,0} %multiply.26, f32[512,14,14,672]{3,2,1,0} %broadcast.42), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %subtract.9 = f32[512,14,14,672]{3,2,1,0} subtract(f32[512,14,14,672]{3,2,1,0} %subtract.10, f32[512,14,14,672]{3,2,1,0} %divide.6), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  %multiply.25 = f32[512,14,14,672]{3,2,1,0} multiply(f32[512,14,14,672]{3,2,1,0} %broadcast.47, f32[512,14,14,672]{3,2,1,0} %subtract.9), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
  ROOT %convert.7 = f16[512,14,14,672]{3,2,1,0} convert(f32[512,14,14,672]{3,2,1,0} %multiply.25), metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
}

ENTRY main {
  %param_0 = f32[672]{0} parameter(0)
  %param_1 = f32[672]{0} parameter(1)
  %param_2 = f32[672]{0} parameter(2)
  %param_3 = f32[672]{0} parameter(3)
  %param_4 = f16[512,14,14,672]{3,2,1,0} parameter(4)
  %param_5 = f32[672]{0} parameter(5)
  %param_6 = f16[512,14,14,672]{3,2,1,0} parameter(6)
  %param_7 = f32[672]{0} parameter(7)

  ROOT %fusion.1 = f16[512,14,14,672]{3,2,1,0} fusion(f32[672]{0} %param_0, f32[672]{0} %param_1, f32[672]{0} %param_2, f32[672]{0} %param_3, f16[512,14,14,672]{3,2,1,0} %param_4, f32[672]{0} %param_5, f16[512,14,14,672]{3,2,1,0} %param_6, f32[672]{0} %param_7), kind=kLoop, calls=%fused_computation.1, metadata={op_type="FusedBatchNormGradV3" op_name="gradient_tape/foo/batch_normalization/FusedBatchNormGradV3"}
}

// CHECK-LABEL: fusion_1
// CHECK: .reqntid 168, 1, 1
// CHECK-NOT: ld.global.nc.f
// CHECK-NOT: ld.global.nc.b

// -----

HloModule TransposeOutput, is_scheduled=true

%fused_computation.2 (param_0: f32[672], param_1: f32[512,14,14,672]) -> f32[512,14,14,672] {
  %param_0 = f32[672]{0} parameter(0)
  %broadcast = f32[512,14,14,672]{3,2,1,0} broadcast(%param_0), dimensions={3}
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)
  %add = f32[512,14,14,672]{3,2,1,0} add(%broadcast, %param_1)
  ROOT  %copy = f32[512,14,14,672]{0,2,3,1} copy(%add)
}

ENTRY main {
  %param_0 = f32[672]{0} parameter(0)
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)

  ROOT %fusion.2 = f32[512,14,14,672]{0,2,3,1} fusion(%param_0, %param_1), kind=kLoop, calls=%fused_computation.2
}
// Check that we didn't do anything. The block size didn't change.
// CHECK-LABEL: fusion_2
// CHECK: .reqntid 128, 1, 1
// CHECK: ld.global.nc.f

// -----

HloModule TransposeInput, is_scheduled=true

%fused_computation.3 (param_0: f32[672], param_1: f32[512,14,14,672]) -> f32[512,14,14,672] {
  %param_0 = f32[672]{0} parameter(0)
  %broadcast = f32[512,14,14,672]{3,2,1,0} broadcast(%param_0), dimensions={3}
  %param_1 = f32[512,14,14,672]{0,2,3,1} parameter(1)
  %copy = f32[512,14,14,672]{3,2,1,0} copy(%param_1)
  ROOT %add = f32[512,14,14,672]{3,2,1,0} add(%broadcast, %copy)
}

ENTRY main {
  %param_0 = f32[672]{0} parameter(0)
  %param_1 = f32[512,14,14,672]{0,2,3,1} parameter(1)

  ROOT %fusion.3 = f32[512,14,14,672]{3,2,1,0} fusion(%param_0, %param_1), kind=kLoop, calls=%fused_computation.3
}
// Check that we didn't do anything. The block size didn't change.
// CHECK-LABEL: fusion_3
// CHECK: .reqntid 128, 1, 1
// CHECK: ld.global.nc.f

// -----

HloModule ScalarBroadcasting, is_scheduled=true

%fused_computation.5 (param_0: f32[], param_1: f32[512,14,14,672]) -> f32[512,14,14,672] {
  %param_0 = f32[] parameter(0)
  %broadcast = f32[512,14,14,672]{3,2,1,0} broadcast(%param_0), dimensions={}
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)
  ROOT %add = f32[512,14,14,672]{3,2,1,0} add(%broadcast, %param_1)
}

ENTRY main {
  %param_0 = f32[] parameter(0)
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)

  ROOT %fusion.5 = f32[512,14,14,672] fusion(%param_0, %param_1), kind=kLoop, calls=%fused_computation.5
}

// CHECK-LABEL: fusion_5
// CHECK: .reqntid 128, 1, 1
// CHECK: ld.global.nc.f

// -----

HloModule NotSupportedBroadcasting, is_scheduled=true

%fused_computation.6 (param_0: f32[14,672], param_1: f32[512,14,14,672]) -> f32[512,14,14,672] {
  %param_0 = f32[14,672]{1,0} parameter(0)
  %broadcast = f32[512,14,14,672]{3,2,1,0} broadcast(%param_0), dimensions={2,3}
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)
  ROOT %add = f32[512,14,14,672]{3,2,1,0} add(%broadcast, %param_1)
}

ENTRY main {
  %param_0 = f32[14,672]{1,0} parameter(0)
  %param_1 = f32[512,14,14,672]{3,2,1,0} parameter(1)

  ROOT %fusion.6 = f32[512,14,14,672] fusion(%param_0, %param_1), kind=kLoop, calls=%fused_computation.6
}

// Check that we didn't do anything. The block size didn't change.
// CHECK-LABEL: fusion_6
// CHECK: .reqntid 128, 1, 1
// CHECK: ld.global.nc.f

// -----
HloModule Module, is_scheduled=true

%fused_computation.7 {
  %constant_2 = f32[] constant(0)
  %broadcast.1 = f32[32,7,7,352]{2,1,3,0} broadcast(f32[] %constant_2), dimensions={}
  %param_1.2 = f32[32,7,7,320]{2,1,3,0} parameter(1)
  %param_2.1 = f32[32,7,7,224]{2,1,3,0} parameter(2)
  %param_3.1 = f32[32,7,7,128]{2,1,3,0} parameter(3)
  %tmp_8.1 = f32[32,7,7,1024]{2,1,3,0} concatenate(f32[32,7,7,352]{2,1,3,0} %broadcast.1, f32[32,7,7,320]{2,1,3,0} %param_1.2, f32[32,7,7,224]{2,1,3,0} %param_2.1, f32[32,7,7,128]{2,1,3,0} %param_3.1), dimensions={3}
  %param_0.1 = f32[32,7,7,1024]{2,1,3,0} parameter(0)
  ROOT %tmp_10.1 = f32[32,7,7,1024]{2,1,3,0} add(f32[32,7,7,1024]{2,1,3,0} %tmp_8.1, f32[32,7,7,1024]{2,1,3,0} %param_0.1)
}

ENTRY %computation {
  %tmp_0 = u8[32,224,224,3]{3,2,1,0} parameter(0)
  %tmp_9 = f32[32,7,7,1024]{2,1,3,0} constant({...})
  %tmp_5 = f32[32,7,7,320]{2,1,3,0} constant({...})
  %tmp_6 = f32[32,7,7,224]{2,1,3,0} constant({...})
  %tmp_7 = f32[32,7,7,128]{2,1,3,0} constant({...})
  ROOT %fusion.7 = f32[32,7,7,1024]{2,1,3,0} fusion(f32[32,7,7,1024]{2,1,3,0} %tmp_9, f32[32,7,7,320]{2,1,3,0} %tmp_5, f32[32,7,7,224]{2,1,3,0} %tmp_6, f32[32,7,7,128]{2,1,3,0} %tmp_7), kind=kLoop, calls=%fused_computation.7
}


// This graph triggered a bug where the new indexing was generated
// CHECK-LLVM-LABEL: @fusion_7
// CHECK-LLVM-NOT: row_index

// -----
HloModule RowToLong, is_scheduled=true

%fused_computation.1 {
  %p0 = f32[2025]{0} parameter(0)
  ROOT %r = f32[3025,2025]{1,0} broadcast(%p0), dimensions={1}
}

ENTRY main {
  %param_0 = f32[2025]{0} parameter(0)
  ROOT %fusion.8 = f32[3025,2025]{1,0} fusion(%param_0), kind=kLoop, calls=%fused_computation.1

}
// Check that we didn't emit the simpler row broadcasting.
// CHECK-LLVM-LABEL: @fusion_8
// CHECK-LLVM-NOT: row_index

// -----

HloModule module, is_scheduled=true

%fused_computation.1 {
  %p0 = f16[5000,64,64,32] parameter(0)
  %p1 = f16[] parameter(1)
  ROOT %pad1 = f16[5000,65,65,32] pad(%p0, %p1), padding=0_0x0_1x0_1x0_0
}

ENTRY computation {
  p0 = f16[5000,64,64,32] parameter(0)
  zero = f16[] constant(0)

  ROOT %fusion.9 = f16[5000,65,65,32] fusion(p0, zero), kind=kLoop, calls=%fused_computation.1
}

// Our codegen can't emit a vectorized load here, but it can emit a vectorized
// store.
// CHECK-LABEL: .visible .entry fusion_9
// CHECK-COUNT-4: ld.global.nc.u16
// CHECK: st.global.v4.b16
