// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb --split-input-file | FileCheck --check-prefixes=CHECK,CHECK-%{PTX} %s

// All fusions must reuse the same kernel:
// CHECK-LABEL: target triple
// CHECK-PTX: define void
// CHECK-GCN: define amdgpu_kernel void
// CHECK-PTX-NOT: define void
// CHECK-GCN-NOT: define amdgpu_kernel void

HloModule KernelReuse, is_scheduled=true

fused_computation {
  param_0.2 = f32[5,5]{1,0} parameter(0)
  sqrt.11 = f32[5,5]{1,0} sqrt(param_0.2)
  sqrt.10 = f32[5,5]{1,0} sqrt(sqrt.11)
  ROOT sqrt.9 = f32[5,5]{1,0} sqrt(sqrt.10)
}

fused_computation.1 {
  param_0.5 = f32[5,5]{1,0} parameter(0)
  sqrt.14 = f32[5,5]{1,0} sqrt(param_0.5)
  sqrt.13 = f32[5,5]{1,0} sqrt(sqrt.14)
  ROOT sqrt.12 = f32[5,5]{1,0} sqrt(sqrt.13)
}

fused_computation.2 {
  param_0.8 = f32[5,5]{1,0} parameter(0)
  sqrt.17 = f32[5,5]{1,0} sqrt(param_0.8)
  sqrt.16 = f32[5,5]{1,0} sqrt(sqrt.17)
  ROOT sqrt.15 = f32[5,5]{1,0} sqrt(sqrt.16)
}

ENTRY main {
  a = f32[5,5]{1,0} parameter(0)
  custom-call = f32[5,5]{1,0} custom-call(a, a), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion.2 = f32[5,5]{1,0} fusion(custom-call), kind=kLoop, calls=fused_computation.2
  custom-call.1 = f32[5,5]{1,0} custom-call(fusion.2, fusion.2), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion.1 = f32[5,5]{1,0} fusion(custom-call.1), kind=kLoop, calls=fused_computation.1
  custom-call.2 = f32[5,5]{1,0} custom-call(fusion.1, fusion.1), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion = f32[5,5]{1,0} fusion(custom-call.2), kind=kLoop, calls=fused_computation
  custom-call.3 = f32[5,5]{1,0} custom-call(fusion, fusion), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  ROOT tuple = (f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}) tuple(custom-call, custom-call.1, custom-call.2, custom-call.3)
}

// -----

// All (Triton) fusions must reuse the same kernel:
// CHECK-LABEL: target triple
// CHECK-PTX-NOT: define void
// CHECK-GCN-NOT: define amdgpu_kernel void
// CHECK-PTX: define void @triton_gemm_dot1(
// CHECK-GCN: define amdgpu_kernel void @triton_gemm_dot1(
// CHECK-PTX-NOT: define void
// CHECK-GCN-NOT: define amdgpu_kernel void

HloModule t, is_scheduled=true

triton_gemm_dot0 {
  parameter_1 = f16[15,19]{1,0} parameter(1)
  parameter_0 = s8[19,17]{1,0} parameter(0)
  cp1.1 = f16[19,17]{1,0} convert(parameter_0)
  ROOT dot0.1 = f16[15,17]{1,0} dot(parameter_1, cp1.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}
}

triton_gemm_dot1 {
  parameter_1.1 = f16[15,19]{1,0} parameter(1)
  parameter_0.1 = s8[19,17]{1,0} parameter(0)
  cp3.1 = f16[19,17]{1,0} convert(parameter_0.1)
  ROOT dot1.1 = f16[15,17]{1,0} dot(parameter_1.1, cp3.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}
}

ENTRY e {
  p3 = s8[19,17]{1,0} parameter(3)
  p2 = f16[15,19]{1,0} parameter(2)
  p1 = s8[19,17]{1,0} parameter(1)
  p0 = f16[15,19]{1,0} parameter(0)
  triton_gemm_dot1 = f16[15,17]{1,0} fusion(p3, p2), kind=kCustom, calls=triton_gemm_dot1, backend_config="{ \"fusion_backend_config\": {kind: \"__triton_gemm\", triton_gemm_config: {\"block_m\":\"64\",\"block_n\":\"32\",\"block_k\":\"64\",\"split_k\":\"1\",\"num_stages\":\"4\",\"num_warps\":\"4\",\"num_ctas\":\"1\"}}}"
  triton_gemm_dot0 = f16[15,17]{1,0} fusion(p1, p0), kind=kCustom, calls=triton_gemm_dot0, backend_config="{ \"fusion_backend_config\": {kind: \"__triton_gemm\", triton_gemm_config: {\"block_m\":\"64\",\"block_n\":\"32\",\"block_k\":\"64\",\"split_k\":\"1\",\"num_stages\":\"4\",\"num_warps\":\"4\",\"num_ctas\":\"1\"}}}"
  ROOT tuple = (f16[15,17]{1,0}, f16[15,17]{1,0}) tuple(triton_gemm_dot0, triton_gemm_dot1)
}

// -----

// We need 2 different kernels:
// - @fusion_2's %arg0 must have align 16, because we are passing a module input
// - @fusion_1's %arg0 must have align 128, because we are passing an internal buffer
// CHECK-LABEL: target triple
// CHECK-PTX-DAG: define void @fusion_2(ptr noalias align 16 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-GCN-DAG: define amdgpu_kernel void @fusion_2(ptr noalias align 16 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-PTX-DAG: define void @fusion_1(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-GCN-DAG: define amdgpu_kernel void @fusion_1(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-PTX-NOT: define void
// CHECK-GCN-NOT: define amdgpu_kernel void

HloModule KernelReuse, is_scheduled=true

fused_computation {
  param_0.2 = f32[5,5]{1,0} parameter(0)
  sqrt.11 = f32[5,5]{1,0} sqrt(param_0.2)
  sqrt.10 = f32[5,5]{1,0} sqrt(sqrt.11)
  ROOT sqrt.9 = f32[5,5]{1,0} sqrt(sqrt.10)
}

fused_computation.1 {
  param_0.5 = f32[5,5]{1,0} parameter(0)
  sqrt.14 = f32[5,5]{1,0} sqrt(param_0.5)
  sqrt.13 = f32[5,5]{1,0} sqrt(sqrt.14)
  ROOT sqrt.12 = f32[5,5]{1,0} sqrt(sqrt.13)
}

fused_computation.2 {
  param_0.8 = f32[5,5]{1,0} parameter(0)
  sqrt.17 = f32[5,5]{1,0} sqrt(param_0.8)
  sqrt.16 = f32[5,5]{1,0} sqrt(sqrt.17)
  ROOT sqrt.15 = f32[5,5]{1,0} sqrt(sqrt.16)
}

ENTRY main {
  a = f32[5,5]{1,0} parameter(0)
  fusion.2 = f32[5,5]{1,0} fusion(a), kind=kLoop, calls=fused_computation.2
  custom-call.1 = f32[5,5]{1,0} custom-call(fusion.2, fusion.2), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion.1 = f32[5,5]{1,0} fusion(custom-call.1), kind=kLoop, calls=fused_computation.1
  custom-call.2 = f32[5,5]{1,0} custom-call(fusion.1, fusion.1), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion = f32[5,5]{1,0} fusion(custom-call.2), kind=kLoop, calls=fused_computation
  custom-call.3 = f32[5,5]{1,0} custom-call(fusion, fusion), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  ROOT tuple = (f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}) tuple(custom-call.1, custom-call.2, custom-call.3)
}

// -----

// We need 2 different kernels:
// The first has just 2 parameters (1 input, 1 output) and the second has 3 (2 input, 1 output).
// All the parameters are noalias, because we are not passing the same argument twice to the kernel.
// CHECK-LABEL: target triple
// CHECK-PTX-DAG: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-GCN-DAG: define amdgpu_kernel void @fusion_2(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-PTX-DAG: define void @fusion_1(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-GCN-DAG: define amdgpu_kernel void @fusion_1(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-NOT: define void

HloModule KernelReuse, is_scheduled=true

fused_computation {
  a = f32[5,5]{1,0} parameter(0)
  b = f32[5,5]{1,0} parameter(1)
  ROOT c = f32[5,5]{1,0} add(a, b)
}

fused_computation.1 {
  a = f32[5,5]{1,0} parameter(0)
  b = f32[5,5]{1,0} parameter(1)
  ROOT c = f32[5,5]{1,0} add(a, b)
}

fused_computation.2 {
  a = f32[5,5]{1,0} parameter(0)
  b = f32[5,5]{1,0} parameter(1)
  ROOT c = f32[5,5]{1,0} add(a, b)
}

ENTRY main {
  a = f32[5,5]{1,0} parameter(0)
  custom-call = f32[5,5]{1,0} custom-call(a, a), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion.2 = f32[5,5]{1,0} fusion(custom-call, custom-call), kind=kLoop, calls=fused_computation.2
  custom-call.1 = f32[5,5]{1,0} custom-call(fusion.2, fusion.2), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion.1 = f32[5,5]{1,0} fusion(custom-call, custom-call.1), kind=kLoop, calls=fused_computation.1
  custom-call.2 = f32[5,5]{1,0} custom-call(fusion.1, fusion.1), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion = f32[5,5]{1,0} fusion(custom-call.1, custom-call.2), kind=kLoop, calls=fused_computation
  custom-call.3 = f32[5,5]{1,0} custom-call(fusion, fusion), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  ROOT tuple = (f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}) tuple(custom-call, custom-call.1, custom-call.2, custom-call.3)
}

// -----

// We need 2 different kernels:
// In the first one we modify the input buffer in place.
// In the others, we have a separate input and output buffer, so we can use
// "!invariant.load" (thanks to ir_array.MarkInvariantOverWholeProgram).
//
// CHECK-LABEL: target triple
// CHECK-PTX: define void @fusion_2(ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-GCN: define amdgpu_kernel void @fusion_2(ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-NOT: !invariant.load
// CHECK-PTX: define void @fusion(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-GCN: define amdgpu_kernel void @fusion(ptr noalias align 128 dereferenceable(100) %{{.*}}, ptr noalias align 128 dereferenceable(100) %{{.*}})
// CHECK-PTX-NOT: define void
// CHECK-GCN-NOT: define amdgpu_kernel void
// CHECK: !invariant.load
// CHECK-PTX-NOT: define void
// CHECK-GCN-NOT: define amdgpu_kernel void

HloModule KernelReuse, is_scheduled=true

fused_computation {
  a = f32[5,5]{1,0} parameter(0)
  ROOT b = f32[5,5]{1,0} sqrt(a)
}

fused_computation.1 {
  a = f32[5,5]{1,0} parameter(0)
  ROOT b = f32[5,5]{1,0} sqrt(a)
}

fused_computation.2 {
  a = f32[5,5]{1,0} parameter(0)
  ROOT b = f32[5,5]{1,0} sqrt(a)
}

ENTRY main {
  a = f32[5,5]{1,0} parameter(0)
  custom-call = f32[5,5]{1,0} custom-call(a, a), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion.2 = f32[5,5]{1,0} fusion(custom-call), kind=kLoop, calls=fused_computation.2
  custom-call.1 = f32[5,5]{1,0} custom-call(fusion.2, fusion.2), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  fusion = f32[5,5]{1,0} fusion(custom-call.1), kind=kLoop, calls=fused_computation
  fusion.1 = f32[5,5]{1,0} fusion(custom-call.1), kind=kLoop, calls=fused_computation.1
  custom-call.2 = f32[5,5]{1,0} custom-call(fusion.1, fusion.1), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  custom-call.3 = f32[5,5]{1,0} custom-call(fusion, fusion), custom_call_target="__cublas$gemm", backend_config="{ \"gemm_backend_config\": {\"alpha_real\":1,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"alpha_imag\":0,\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}}"
  // We don't output custom-call, so fusion.2 can change its input.
  ROOT tuple = (f32[5,5]{1,0}, f32[5,5]{1,0}, f32[5,5]{1,0}) tuple(custom-call.1, custom-call.2, custom-call.3)
}

// We don't have a test case for aliasing, because it is hard or impossible to
// create a situation when parameters are aliased, but not the same and at least
// one of them is written.
