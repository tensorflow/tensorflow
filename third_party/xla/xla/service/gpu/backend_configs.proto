syntax = "proto3";

package xla.gpu;

import "xla/autotuning.proto";
import "xla/tsl/protobuf/dnn.proto";
import "xla/xla_data.proto";

// Backend configs for XLA:GPU.
//
// These are metadata that the GPU backend attaches to HloInstructions and later
// uses during e.g. codegen.
//
// GpuBackendConfig serves as a parent config for all backend configs so
// configs won't overwrite each other. Any new backend config proto
// should be added to and used in GpuBackendConfig.
//
// Remember that proto3 doesn't give clients a way to tell the difference
// between a field not being present and a field having the default value.
// Choose your defaults carefully.
//
// No guarantee is made about the stability of these protos.
//
// See HloInstruction::backend_config() for more info.

// Backend config for a convolution that runs through cudnn.
message CudnnConvBackendConfig {
  reserved 1, 2;

  // Opaque algorithm number and tuning knobs chosen for this conv.
  stream_executor.dnn.AlgorithmProto algorithm = 6;

  // The scaling factor multiplied with the convolution result.
  double conv_result_scale = 4;

  // Below are the fields related to cuDNN's fused convolution. Refer to
  // GpuConvParams for their meanings.

  // The requested activation (e.g. relu) after the convolution.
  stream_executor.dnn.ActivationMode activation_mode = 3;

  // The scaling factor multiplied with the side input. If no side input buffer
  // is provided, this field must be 0.
  double side_input_scale = 5;

  // The negative slope coefficient alpha for leaky_relu activation, used only
  // when activation_mode is kLeakyRelu.
  //
  // leakyrelu(x) is defined as x > 0 ? x : alpha * x.
  //
  // Since this is a proto3 proto, leakyrelu_alpha is 0 if not specified (in
  // which case the leakyrelu activation is equivalent to relu).
  double leakyrelu_alpha = 8;

  // If the filter (and bias, if present) have been reordered, set this flag.
  // It's placed into a `oneof` block to skip the serialization when not set.
  oneof filter_and_bias_reordering_oneof {
    // cuDNN int8x32 vectorized convolutions (NCHW_VECT_C data layout) can be
    // optimized by reordering the filter and bias (if present). The logical
    // layout stays the same, but the data is shuffled in a way that is
    // compatible with NVidia's IMMA instruction (sm75+).
    bool reordered_int8_nchw_vect = 7;
  }

  // Serialization of the graph described by the convolution and adjacent
  // pointwise ops.
  optional string serialized_graph = 9;
}

// Backend config for the GEMM operation running through cuBLAS.
message GemmBackendConfig {
  // Opaque optional algorithm number. No chosen number indicates that a
  // different cuBLAS API will be used, which does not allow for choosing an
  // algorithm.
  oneof algorithm {
    int64 selected_algorithm = 1;
  }

  double alpha_real = 2;
  double alpha_imag = 9;

  double beta = 3;

  xla.DotDimensionNumbers dot_dimension_numbers = 7;

  xla.PrecisionConfig precision_config = 12;

  // cublasLt matmul epilogue.
  enum Epilogue {
    DEFAULT = 0;
    BIAS = 1;
    RELU = 2;
    BIAS_RELU = 3;
    GELU = 4;
    GELU_AUX = 5;
    BIAS_GELU = 6;
    BIAS_GELU_AUX = 7;
  }

  Epilogue epilogue = 13;

  optional int64 lhs_stride = 14;
  optional int64 rhs_stride = 15;

  optional bool grad_x = 16;
  optional bool grad_y = 17;
  bool damax_output = 18;
}

// Backend config for bitcast operation generated from MLIR MHLO dialect.
message BitcastBackendConfig {
  LayoutProto source_layout = 1;
  LayoutProto result_layout = 2;
}

// Backend config for async collective operations. Note that for is_sync will
// be false by default, so even if a backend config is not explicitly attached
// to the HLOInstruction, getting the backend_config will yield a default valued
// proto which will have is_sync = false. Attribute no_parallel_custom_call
// asserts that an asynchronous collective operation does not execute in
// parallel with custom-calls, which can trigger device synchronization . This
// attribute will also be false by default and should lead to conversative
// runtime behavior.
message CollectiveBackendConfig {
  bool is_sync = 1;
  bool no_parallel_custom_call = 2;
  // Determines whether the collective op of interested has been pipelined
  // within a loop.
  bool is_pipelined = 3;
  // Cost model prediction.
  ReificationCost reification_cost = 4;
}

// Backend config for cost model estimates.
message ReificationCost {
  // Total execution time of the reified op.
  double end_to_end_cycles = 1;

  // Estimated overall kernel execution in microseconds.
  //
  // GPU Cost Model estimates compute and memory access time separately. Exec
  // time is a combined metric of the two.
  double exec_time_us = 2;

  // Estimate for compute time in microseconds.
  double compute_time_us = 3;

  // Estimate for memory access (read+write) time in microseconds.
  double memory_access_time_us = 4;
}

// Backend config for a custom fusion (pre-compiled device kernel implementing a
// fusion computation).
message CustomFusionConfig {
  string name = 1;

  // When a custom fusion has multiple kernels, this field specifies which
  // kernel to use. Default value is to select the first kernel, i.e.
  // kernel_index = 0.
  int32 kernel_index = 2;
}

message CuDnnFusionConfig {
  int64 plan_id = 1;
}

// Block-level parameters for a fusion.
message BlockLevelFusionConfig {
  // The output tile sizes of the associated instruction. The length of this
  // field is expected to be the rank of the output shape.
  repeated int64 output_tile_sizes = 1;

  // The number of warps to use for the kernel.
  int64 num_warps = 2;
}

message FusionBackendConfig {
  // kLoop, kInput, or kOutput (from HloInstruction::FusionKind), or your own
  // custom string.
  //
  // Don't put "kCustom" in here -- just put a string describing the custom
  // fusion, like "__triton_gemm".
  //
  // This is somewhat redundant with HloInstruction::fusion_kind().  We need it
  // here because LMHLO does not have the concept of a fusion kind, and we use
  // this same backend-config proto for both HLO and LMHLO.
  string kind = 1;

  // Only valid when kind == "__triton_gemm".  Even then it's optional: If not
  // present, we use the default Triton config.
  AutotuneResult.TritonGemmKey triton_gemm_config = 2;

  // Only valid when kind == "__triton" for now. Code generation of such
  // fusions will fail if this field is not set.
  BlockLevelFusionConfig block_level_fusion_config = 6;

  // Only valid when kind == "__custom_fusion".
  CustomFusionConfig custom_fusion_config = 4;

  // Cost model prediction.
  ReificationCost reification_cost = 3;

  CuDnnFusionConfig cudnn_fusion_config = 5;
}

// Backed config for norm executed by cuDNN.
message CudnnNormBackendConfig {
  // Epsilon parameter.
  double epsilon = 1;

  // Opaque algorithm number.
  stream_executor.dnn.AlgorithmProto algorithm = 2;

  // Norm type.
  enum Kind {
    LAYER_FWD_INFER = 0;
    LAYER_FWD_TRAIN = 1;
    LAYER_BWD = 2;
  }
  Kind kind = 3;
}

// Backend config for a fused Multi-Headed Attention (fMHA) that runs through
// cudnn.
message CudnnfMHABackendConfig {
  // Opaque algorithm number and tuning knobs chosen for this fMHA.
  stream_executor.dnn.AlgorithmProto algorithm = 8;

  // The scaling factor multiplied with the BMM1 result. fmha_scale is 1 if the
  // MHA pattern has no scaling.
  double fmha_scale = 10;

  // Dropout factor in MHA
  double dropout_rate = 13;

  // Configs for mha bmms in the forward graph
  xla.DotDimensionNumbers bmm1_dot_dimension_numbers = 11;
  xla.DotDimensionNumbers bmm2_dot_dimension_numbers = 12;

  xla.ShapeProto intermediate_tensor_shape = 14;

  // Configs for mha bmms in the backward graph
  xla.DotDimensionNumbers bmm1_grad_gemm1_dot_dimension_numbers = 16;
  xla.DotDimensionNumbers bmm1_grad_gemm2_dot_dimension_numbers = 17;
  xla.DotDimensionNumbers bmm2_grad_gemm1_dot_dimension_numbers = 18;
  xla.DotDimensionNumbers bmm2_grad_gemm2_dot_dimension_numbers = 19;

  // Random seed used by dropout
  int64 seed = 15;

  // Is flash attention
  bool is_flash_attention = 20;

  // Is causal mask
  bool is_causal_mask = 21;

  // mask type
  enum MaskType {
    NO_MASK = 0;
    PADDING = 1;
    CAUSAL = 2;
    PADDING_CAUSAL = 3;
    ALIBI = 4;
  }
  MaskType mask_type = 22;

  // Whether force deterministic for bwd
  bool force_deterministic = 23;

  // Sliding window length
  // ignored if the value <= 0
  int32 sliding_window_length = 24;

  // The maximum number of segments in each batch
  // Only used with packed layout
  // ignored if the valued <= 1
  int32 max_seg_per_batch = 25;
}

// Backend config for a general custom call instruction, e.g. XLA FFI.
message CustomCallBackendConfig {
  // Generic configurations that can be parsed by XLA.
  oneof raw_backend_config_oneof {
    // An opaque ASCII string could be parsed by the compiler.
    string opaque = 1;
    // Attributes parsed by XLA FFI.
    string attributes = 2;
  }
}

// Generic backend config for XLA:GPU
// Next-Id: 12
message GpuBackendConfig {
  // Specifies which operation queue the current instruction will run on.
  // A backend may have multiple operation queues to run instructions
  // concurrently, use this to signal the backend which queue to dispatch to.
  // The backend should keep a mapping of
  // operation_queue_id->actual_hardware_queue_id if runtime will create
  // different IDs.
  int64 operation_queue_id = 1;

  // Specifies which operation queues to await for data when running with
  // multiple operation queues.
  repeated int64 wait_on_operation_queues = 2;

  oneof backend_config {
    CudnnConvBackendConfig cudnn_conv_backend_config = 3;

    GemmBackendConfig gemm_backend_config = 4;

    BitcastBackendConfig bitcast_backend_config = 5;

    CollectiveBackendConfig collective_backend_config = 6;

    FusionBackendConfig fusion_backend_config = 7;

    CudnnNormBackendConfig cudnn_norm_backend_config = 8;

    CudnnfMHABackendConfig cudnn_fmha_backend_config = 9;

    CustomCallBackendConfig custom_call_backend_config = 11;
  }

  // This attribute instructs the latency-hiding scheduler to
  // schedule this particular instruction to the earliest position.
  // Note that setting this to true will make this instruction scheduled
  // at the very beginning of the parent computation before
  // every other nodes.
  // An example use case would be deciding to schedule between collective
  // or an async compute. LHS might put either one at the first place
  // depending on the cost, but it'd be more beneficial if the collective
  // is always scheduled first as it's not SM-heavy.
  // In this case we can use this flag to enforce the ordering.
  bool force_earliest_schedule = 10;
}
