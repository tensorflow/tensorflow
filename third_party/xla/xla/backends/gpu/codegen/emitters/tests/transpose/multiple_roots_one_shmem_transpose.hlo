// RUN: fusion_to_mlir %s | emitters_opt -xla-gpu-test-optimize |\
// RUN:   FileCheck %s
// RUN: test_correctness %s

HloModule m

fusion {
  p0 = bf16[22,512,16,256] parameter(0)
  p1 = bf16[22,512,4096] parameter(1)

  transpose = bf16[22,16,512,256] transpose(p0), dimensions={0,2,1,3}
  shmem_transpose = bf16[22,4096,512] transpose(p1), dimensions={0,2,1}
  ROOT tuple = (bf16[22,16,512,256], bf16[22,4096,512])
    tuple(transpose, shmem_transpose)
}
// Check that the packed transpose was selected. The packed transpose requires
// 32 * BITWIDTH x 32 x BITWIDTH shmem.
// CHECK: xla_gpu.allocate_shared : tensor<64x64xbf16>
// CHECK:  vector.transfer_read
// CHECK-SAME: tensor<64x64xbf16>, vector<2xbf16>