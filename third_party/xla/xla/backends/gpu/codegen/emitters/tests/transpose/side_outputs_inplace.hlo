// RUN: fusion_to_mlir %s | emitters_opt -xla-gpu-test-optimize |\
// RUN:   FileCheck %s
// RUN: test_correctness %s

HloModule module, input_output_alias={ {0}: (0, {}) }

transpose_fusion {
  p0 = f32[1024,2048]{1,0} parameter(0)
  p1 = f32[1024,2048]{1,0} parameter(1)
  add = f32[1024,2048]{1,0} add(p0, p1)
  bitcast = f32[2097152]{0} bitcast(p0)
  transpose = f32[2048,1024]{1,0} transpose(p1), dimensions={1,0}
  ROOT res = (f32[1024,2048]{1,0}, f32[2048,1024]{1,0}, f32[2097152]{0}) tuple(add, transpose, bitcast)
}

ENTRY module {
  param0 = f32[1024,2048]{1,0} parameter(0)
  param1 = f32[1024,2048]{1,0} parameter(1)
  ROOT f = (f32[1024,2048]{1,0}, f32[2048,1024]{1,0}, f32[2097152]{0}) fusion(param0, param1), kind=kInput, calls=transpose_fusion
}

// CHECK: xla_gpu.allocate_shared
