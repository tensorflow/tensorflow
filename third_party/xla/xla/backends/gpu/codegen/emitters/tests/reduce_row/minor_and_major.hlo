// RUN: fusion_to_mlir %s | emitters_opt -xla-gpu-test-optimize \
// RUN:   --inline="default-pipeline='cse'" | FileCheck %s
// RUN: test_correctness %s --bijection_inputs=reduce:0 \
// RUN:   --bijection_outputs=reduce

add {
  lhs = f32[] parameter(0)
  rhs = f32[] parameter(1)
  ROOT add = f32[] add(lhs, rhs)
}

fusion {
  param_0 = f32[7,100,128] parameter(0)
  param_1 = f32[] parameter(1)
  ROOT reduce = f32[100] reduce(param_0, param_1), dimensions={0,2}, to_apply=add
}

// Our codegen doesn't support parallelizing the major reduction dimension. In
// principle, this could be done via shared memory.
// CHECK-NOT: allocate_shared
// CHECK: shuffle_reduce
// CHECK-NOT: allocate_shared
