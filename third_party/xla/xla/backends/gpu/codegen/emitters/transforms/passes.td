/* Copyright 2024 The OpenXLA Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

#ifndef XLA_SERVICE_GPU_FUSIONS_TRANSFORMS_PASSES_TD_
#define XLA_SERVICE_GPU_FUSIONS_TRANSFORMS_PASSES_TD_

include "mlir/Pass/PassBase.td"

def ConvertIndexTypePass : Pass<"xla-gpu-convert-index-type", "mlir::ModuleOp"> {
  let summary = "Converts index types to module data layout index type.";

  let description = [{
      Converts types of arith ops that are potentially generated by indexing
      maps to integers of module's data layout index size. That is necessary
      because some of the backends either want to stop supporting arithmetic ops
      on IndexType or hardcode 32-bits for their LLVM lowering, which might be
      not sufficient.
  }];

  let dependentDialects = [
    "mlir::arith::ArithDialect",
  ];

  let constructor = "CreateConvertIndexTypePass()";
}

def ConvertFloatNvidiaPass : Pass<"xla-gpu-convert-float-nvidia", "mlir::ModuleOp"> {
  let summary = "Convert floating point types using NVidia intrinsics.";

  let dependentDialects = [
    "mlir::LLVM::LLVMDialect",
    "mlir::arith::ArithDialect",
  ];

  let constructor = "CreateConvertFloatNvidiaPass()";
}

def FuseLoopsPass : Pass<"xla-gpu-fuse-loops", "mlir::func::FuncOp"> {
  let summary = "Fuse xla_gpu.loop.";
  let description = [{
    This pass fuses similar xla_gpu.loops into one if the second one is
    extracting the same value from a vector in which the first one inserts to.

    Before fuse-loops:
      %loop0 = xla_gpu.loop (%tid, %bid) -> (%ra, %rb, %rc)[%i, %j]
        in #indexing_map iter_args(%iter = %cst) -> (vector<8x1xf32>) {
          %extracted = tensor.extract %arg0[%ra, %rb, %rc]
          %1 = vector.insert %extracted, %iter [%i, %j]
          xla_gpu.yield %1
      }
      %loop1 = xla_gpu.loop (%tid, %bid) -> (%ra, %rb)[%i, %j]
        in #indexing_map1 iter_args(%iter = %shmem) -> (tensor<32x33xf32>) {
          %2 = vector.extract %loop0  [%i, %j]
          %inserted = tensor.insert %iter[%ra, %rb]
          xla_gpu.yield %extracted
      }

    After fuse-loops:
      %loop = xla_gpu.loop (%tid, %bid) -> (%ra, %rb, %rc, %rd, %re)[%i, %j]
        in #indexing_map iter_args(%iter = %shmem) -> (tensor<32x33xf32>) {
          %extracted = tensor.extract %arg0[%ra, %rb, %rc]
          %inserted = tensor.insert %extracted into %iter[%rd, %re]
          xla_gpu.yield %inserted
      }
  }];
  let dependentDialects = ["xla::gpu::XlaGpuDialect", "xla::XlaDialect"];
  let constructor = "CreateFuseLoopsPass()";
}

def PeelLoopsPass : Pass<"xla-gpu-peel-loops", "mlir::func::FuncOp"> {
  let summary = "Peels xla_gpu.loop.";
  let description = [{
      Attempts to split each loop dimension [0, NUM_ITERATIONS)
      as [0, NUM_ITERATIONS - 1) and [NUM_ITERATIONS - 1, NUM_ITERATIONS)
      if it removes a constraint.
  }];
  let dependentDialects = ["xla::gpu::XlaGpuDialect", "xla::XlaDialect"];
  let constructor = "CreatePeelLoopsPass()";
}

def OptimizeLoopsPass :
   Pass<"xla-gpu-optimize-loops", "mlir::func::FuncOp"> {
  let summary = "Unrolls and pipelines loops.";

  let description = [{
    Unrolls loops with a small trip count. Pipelines loops with a large trip
    count.
  }];

  let dependentDialects = [
    "mlir::vector::VectorDialect",
    "xla::gpu::XlaGpuDialect",
    "xla::XlaDialect",
  ];

  let constructor = "CreateOptimizeLoopsPass()";
}

#endif  // XLA_SERVICE_GPU_FUSIONS_TRANSFORMS_PASSES_TD_
