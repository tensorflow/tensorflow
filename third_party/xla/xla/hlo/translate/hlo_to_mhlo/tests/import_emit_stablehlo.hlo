// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py
//
// The script is designed to make adding checks to
// a test case fast, it is *not* designed to be authoritative
// minimized and named to reflect the test intent.

// Regnerate them using the following command:
// $ TFILE=/path/to/import_emit_stablehlo.hlo
// $ DELIM="Hlo Module" # Remove the space in the middle when running cmd. This comment needs the space since the source file is regex matched.
// $ xla-translate $TFILE -hlo-text-to-mlir-hlo --emit-stablehlo --split-input-file --hlo-import-all-computations | \
//     third_party/llvm/llvm-project/mlir/utils/generate-test-checks.py --source $TFILE --source_delim_regex="$DELIM" --starts_from_scope=0 -i

// RUN: xla-translate %s -hlo-text-to-mlir-hlo --emit-stablehlo --split-input-file --hlo-import-all-computations | FileCheck %s

// CHECK-LABEL: module @foo attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<i1>) -> tensor<i1> {
// CHECK:           return %[[VAL_0]] : tensor<i1>
// CHECK:         }
// CHECK:       }
HloModule foo, entry_computation_layout={(pred[])->pred[]}

ENTRY %main.2 (Arg_0.1: pred[]) -> pred[] {
  ROOT %Arg_0.1 = pred[] parameter(0)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xi1>) -> tensor<2xi1> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.xor %[[VAL_0]], %[[VAL_0]] : tensor<2xi1>
// CHECK:           return %[[VAL_1]] : tensor<2xi1>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(pred[2]{0})->pred[2]{0}}

ENTRY %main.3 (Arg_0.1: pred[2]) -> pred[2] {
  %Arg_0.1 = pred[2] parameter(0)
  ROOT %xor.2 = pred[2] xor(%Arg_0.1, %Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token, %[[VAL_1:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_2:.*]] = mhlo.after_all %[[VAL_0]], %[[VAL_1]] {xla_shape = "token[]"} : !mhlo.token
// CHECK:           return %[[VAL_2]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[], token[])->token[]}

ENTRY %main.4 (Arg_0.1: token[], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = token[] parameter(0)
  %Arg_1.2 = token[] parameter(1)
  ROOT %after-all.3 = token[] after-all(%Arg_0.1, %Arg_1.2)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main() -> !mhlo.token {
// CHECK:           %[[VAL_0:.*]] = mhlo.create_token {xla_shape = "token[]"} : !mhlo.token
// CHECK:           return %[[VAL_0]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={()->token[]}

ENTRY %main.2 () -> token[] {
  ROOT %after-all.1 = token[] after-all()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<5xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.reduce_scatter"(%[[VAL_3]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 0>, replica_groups = dense<{{\[\[}}0, 2], [1, 3]]> : tensor<2x2xi64>, scatter_dimension = 0 : i64}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.maximum %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<5xf32>
// CHECK:           return %[[VAL_4]] : tensor<5xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[5]{0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
  ROOT %maximum.5 = f32[] maximum(%Arg_0.3, %Arg_1.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[5] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %reduce-scatter.6 = f32[5] reduce-scatter(%Arg_0.1), channel_id=5, replica_groups={{0,2},{1,3}}, dimensions={0}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<128x32xf32>) -> tensor<128x128xf32> {
// CHECK:           %[[VAL_1:.*]] = "mhlo.all_gather"(%[[VAL_0]]) <{all_gather_dim = 1 : i64, channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\[\[}}0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>}> : (tensor<128x32xf32>) -> tensor<128x128xf32>
// CHECK:           return %[[VAL_1]] : tensor<128x128xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[128,32]{1,0})->f32[128,128]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[128,32]) -> f32[128,128] {
  %Arg_0.1 = f32[128,32] parameter(0)
  ROOT %all-gather.2 = f32[128,128] all-gather(%Arg_0.1), channel_id=1, replica_groups={{0,2,4,6},{1,3,5,7}}, dimensions={1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<128x32xf32>) -> tensor<128x128xf32> {
// CHECK:           %[[VAL_1:.*]] = "mhlo.all_gather"(%[[VAL_0]]) <{all_gather_dim = 1 : i64, channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\[\[}}0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>, use_global_device_ids}> : (tensor<128x32xf32>) -> tensor<128x128xf32>
// CHECK:           return %[[VAL_1]] : tensor<128x128xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[128,32]{1,0})->f32[128,128]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[128,32]) -> f32[128,128] {
  %Arg_0.1 = f32[128,32] parameter(0)
  ROOT %all-gather.2 = f32[128,128] all-gather(%Arg_0.1), channel_id=1, replica_groups={{0,2,4,6},{1,3,5,7}}, dimensions={1}, use_global_device_ids=true
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<8x2xf32>, %[[VAL_1:.*]]: tensor<8x4xf32>) -> tuple<tensor<8x8xf32>, tensor<8x16xf32>> {
// CHECK:           %[[VAL_2:.*]]:2 = "mhlo.all_gather"(%[[VAL_0]], %[[VAL_1]]) <{all_gather_dim = 1 : i64, channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\[\[}}0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>, use_global_device_ids}> : (tensor<8x2xf32>, tensor<8x4xf32>) -> (tensor<8x8xf32>, tensor<8x16xf32>)
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple %[[VAL_2]]#0, %[[VAL_2]]#1 {xla_shape = "(f32[8,8]{1,0}, f32[8,16]{1,0})"} : tuple<tensor<8x8xf32>, tensor<8x16xf32>>
// CHECK:           return %[[VAL_3]] : tuple<tensor<8x8xf32>, tensor<8x16xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[8,2]{1,0}, f32[8,4]{1,0})->(f32[8,8]{1,0}, f32[8,16]{1,0})}

ENTRY %main.10 (Arg_0.1: f32[8,2], Arg_1.2: f32[8,4]) -> (f32[8,8], f32[8,16]) {
  %Arg_0.1 = f32[8,2] parameter(0)
  %Arg_1.2 = f32[8,4] parameter(1)
  %tuple.3 = (f32[8,2], f32[8,4]) tuple(%Arg_0.1, %Arg_1.2)
  %get-tuple-element.4 = f32[8,2] get-tuple-element(%tuple.3), index=0
  %get-tuple-element.5 = f32[8,4] get-tuple-element(%tuple.3), index=1
  %all-gather.6 = (f32[8,8], f32[8,16]) all-gather(%get-tuple-element.4, %get-tuple-element.5), channel_id=1, replica_groups={{0,2,4,6},{1,3,5,7}}, dimensions={1}, use_global_device_ids=true
  %get-tuple-element.7 = f32[8,8] get-tuple-element(%all-gather.6), index=0
  %get-tuple-element.8 = f32[8,16] get-tuple-element(%all-gather.6), index=1
  ROOT %tuple.9 = (f32[8,8], f32[8,16]) tuple(%get-tuple-element.7, %get-tuple-element.8)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<10xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.all_reduce"(%[[VAL_3]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 0>, replica_groups = dense<{{\[\[}}0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.maximum %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<10xf32>
// CHECK:           return %[[VAL_4]] : tensor<10xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[10]{0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
  ROOT %maximum.5 = f32[] maximum(%Arg_0.3, %Arg_1.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[10] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %all-reduce.6 = f32[10] all-reduce(%Arg_0.1), channel_id=5, replica_groups={{0,2,4,6},{1,3,5,7}}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<10xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.all_reduce"(%[[VAL_3]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 0>, replica_groups = dense<{{\[\[}}0, 2, 4, -1], [1, 3, 5, 6]]> : tensor<2x4xi64>}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.maximum %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<10xf32>
// CHECK:           return %[[VAL_4]] : tensor<10xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[10]{0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
  ROOT %maximum.5 = f32[] maximum(%Arg_0.3, %Arg_1.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[10] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %all-reduce.6 = f32[10] all-reduce(%Arg_0.1), channel_id=5, replica_groups={{0,2,4},{1,3,5,6}}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<10xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.all_reduce"(%[[VAL_3]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 0>, replica_groups = dense<{{\[\[}}0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>, use_global_device_ids}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.maximum %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<10xf32>
// CHECK:           return %[[VAL_4]] : tensor<10xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[10]{0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
  ROOT %maximum.5 = f32[] maximum(%Arg_0.3, %Arg_1.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[10] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %all-reduce.6 = f32[10] all-reduce(%Arg_0.1), channel_id=5, replica_groups={{0,2,4,6},{1,3,5,7}}, use_global_device_ids=true, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.6(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<8xf32>, %[[VAL_4:.*]]: tensor<f32>) -> tuple<tensor<8xf32>, tensor<f32>> {
// CHECK:           %[[VAL_5:.*]]:2 = "mhlo.all_reduce"(%[[VAL_3]], %[[VAL_4]]) <{replica_groups = dense<> : tensor<0x0xi64>}> ({
// CHECK:           ^bb0(%[[VAL_6:.*]]: tensor<f32>, %[[VAL_7:.*]]: tensor<f32>):
// CHECK:             %[[VAL_8:.*]] = stablehlo.add %[[VAL_6]], %[[VAL_7]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_8]] : tensor<f32>
// CHECK:           }) : (tensor<8xf32>, tensor<f32>) -> (tensor<8xf32>, tensor<f32>)
// CHECK:           %[[VAL_9:.*]] = mhlo.tuple %[[VAL_10:.*]]#0, %[[VAL_10]]#1 {xla_shape = "(f32[8]{0}, f32[])"} : tuple<tensor<8xf32>, tensor<f32>>
// CHECK:           return %[[VAL_9]] : tuple<tensor<8xf32>, tensor<f32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[8]{0}, f32[])->(f32[8]{0}, f32[])}

%region_0.6 (Arg_0.7: f32[], Arg_1.8: f32[]) -> f32[] {
  %Arg_0.7 = f32[] parameter(0)
  %Arg_1.8 = f32[] parameter(1)
  ROOT %add.9 = f32[] add(%Arg_0.7, %Arg_1.8)
}

ENTRY %main.14 (Arg_0.1: f32[8], Arg_1.2: f32[]) -> (f32[8], f32[]) {
  %Arg_0.1 = f32[8] parameter(0)
  %Arg_1.2 = f32[] parameter(1)
  %tuple.3 = (f32[8], f32[]) tuple(%Arg_0.1, %Arg_1.2)
  %get-tuple-element.4 = f32[8] get-tuple-element(%tuple.3), index=0
  %get-tuple-element.5 = f32[] get-tuple-element(%tuple.3), index=1
  %all-reduce.10 = (f32[8], f32[]) all-reduce(%get-tuple-element.4, %get-tuple-element.5), replica_groups={}, to_apply=%region_0.6
  %get-tuple-element.11 = f32[8] get-tuple-element(%all-reduce.10), index=0
  %get-tuple-element.12 = f32[] get-tuple-element(%all-reduce.10), index=1
  ROOT %tuple.13 = (f32[8], f32[]) tuple(%get-tuple-element.11, %get-tuple-element.12)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<5xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.reduce_scatter"(%[[VAL_3]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 0>, replica_groups = dense<{{\[\[}}0, 2], [1, 3]]> : tensor<2x2xi64>, scatter_dimension = 0 : i64}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.maximum %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<5xf32>
// CHECK:           return %[[VAL_4]] : tensor<5xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[5]{0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
  ROOT %maximum.5 = f32[] maximum(%Arg_0.3, %Arg_1.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[5] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %reduce-scatter.6 = f32[5] reduce-scatter(%Arg_0.1), channel_id=5, replica_groups={{0,2},{1,3}}, dimensions={0}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<5xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.reduce_scatter"(%[[VAL_3]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 0>, replica_groups = dense<{{\[\[}}0, 2], [1, 3]]> : tensor<2x2xi64>, scatter_dimension = 0 : i64, use_global_device_ids}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.maximum %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<5xf32>
// CHECK:           return %[[VAL_4]] : tensor<5xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[5]{0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
  ROOT %maximum.5 = f32[] maximum(%Arg_0.3, %Arg_1.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[5] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %reduce-scatter.6 = f32[5] reduce-scatter(%Arg_0.1), channel_id=5, replica_groups={{0,2},{1,3}}, use_global_device_ids=true, dimensions={0}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x2x2x2xf32>, %[[VAL_1:.*]]: tensor<2xf32>, %[[VAL_2:.*]]: tensor<2xf32>, %[[VAL_3:.*]]: tensor<2xf32>, %[[VAL_4:.*]]: tensor<2x2x2x2xf32>) -> tuple<tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>> {
// CHECK:           %[[VAL_5:.*]], %[[VAL_6:.*]], %[[VAL_7:.*]] = "stablehlo.batch_norm_grad"(%[[VAL_0]], %[[VAL_1]], %[[VAL_2]], %[[VAL_3]], %[[VAL_4]]) <{epsilon = 1.000000e-03 : f32, feature_index = 0 : i64}> : (tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2xf32>, tensor<2x2x2x2xf32>) -> (tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>)
// CHECK:           %[[VAL_8:.*]] = mhlo.tuple %[[VAL_5]], %[[VAL_6]], %[[VAL_7]] {xla_shape = "(f32[2,2,2,2]{3,2,1,0}, f32[2]{0}, f32[2]{0})"} : tuple<tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>>
// CHECK:           return %[[VAL_8]] : tuple<tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,2,2,2]{3,2,1,0}, f32[2]{0}, f32[2]{0}, f32[2]{0}, f32[2,2,2,2]{3,2,1,0})->(f32[2,2,2,2]{3,2,1,0}, f32[2]{0}, f32[2]{0})}

ENTRY %main.11 (Arg_0.1: f32[2,2,2,2], Arg_1.2: f32[2], Arg_2.3: f32[2], Arg_3.4: f32[2], Arg_4.5: f32[2,2,2,2]) -> (f32[2,2,2,2], f32[2], f32[2]) {
  %Arg_0.1 = f32[2,2,2,2] parameter(0)
  %Arg_1.2 = f32[2] parameter(1)
  %Arg_2.3 = f32[2] parameter(2)
  %Arg_3.4 = f32[2] parameter(3)
  %Arg_4.5 = f32[2,2,2,2] parameter(4)
  %batch-norm-grad.6 = (f32[2,2,2,2], f32[2], f32[2]) batch-norm-grad(%Arg_0.1, %Arg_1.2, %Arg_2.3, %Arg_3.4, %Arg_4.5), epsilon=0.001, feature_index=0
  %get-tuple-element.7 = f32[2,2,2,2] get-tuple-element(%batch-norm-grad.6), index=0
  %get-tuple-element.8 = f32[2] get-tuple-element(%batch-norm-grad.6), index=1
  %get-tuple-element.9 = f32[2] get-tuple-element(%batch-norm-grad.6), index=2
  ROOT %tuple.10 = (f32[2,2,2,2], f32[2], f32[2]) tuple(%get-tuple-element.7, %get-tuple-element.8, %get-tuple-element.9)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x2x2x2xf32>, %[[VAL_1:.*]]: tensor<2xf32>, %[[VAL_2:.*]]: tensor<2xf32>) -> tuple<tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>> {
// CHECK:           %[[VAL_3:.*]], %[[VAL_4:.*]], %[[VAL_5:.*]] = "stablehlo.batch_norm_training"(%[[VAL_0]], %[[VAL_1]], %[[VAL_2]]) <{epsilon = 1.000000e-03 : f32, feature_index = 3 : i64}> : (tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>) -> (tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>)
// CHECK:           %[[VAL_6:.*]] = mhlo.tuple %[[VAL_3]], %[[VAL_4]], %[[VAL_5]] {xla_shape = "(f32[2,2,2,2]{3,2,1,0}, f32[2]{0}, f32[2]{0})"} : tuple<tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>>
// CHECK:           return %[[VAL_6]] : tuple<tensor<2x2x2x2xf32>, tensor<2xf32>, tensor<2xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,2,2,2]{3,2,1,0}, f32[2]{0}, f32[2]{0})->(f32[2,2,2,2]{3,2,1,0}, f32[2]{0}, f32[2]{0})}

ENTRY %main.9 (Arg_0.1: f32[2,2,2,2], Arg_1.2: f32[2], Arg_2.3: f32[2]) -> (f32[2,2,2,2], f32[2], f32[2]) {
  %Arg_0.1 = f32[2,2,2,2] parameter(0)
  %Arg_1.2 = f32[2] parameter(1)
  %Arg_2.3 = f32[2] parameter(2)
  %batch-norm-training.4 = (f32[2,2,2,2], f32[2], f32[2]) batch-norm-training(%Arg_0.1, %Arg_1.2, %Arg_2.3), epsilon=0.001, feature_index=3
  %get-tuple-element.5 = f32[2,2,2,2] get-tuple-element(%batch-norm-training.4), index=0
  %get-tuple-element.6 = f32[2] get-tuple-element(%batch-norm-training.4), index=1
  %get-tuple-element.7 = f32[2] get-tuple-element(%batch-norm-training.4), index=2
  ROOT %tuple.8 = (f32[2,2,2,2], f32[2], f32[2]) tuple(%get-tuple-element.5, %get-tuple-element.6, %get-tuple-element.7)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xf32>, %[[VAL_1:.*]]: tensor<4xf32>, %[[VAL_2:.*]]: tensor<4xi32>, %[[VAL_3:.*]]: tensor<4xi32>) -> tuple<tensor<4xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>> {
// CHECK:           %[[VAL_4:.*]] = stablehlo.atan2 %[[VAL_0]], %[[VAL_1]] : tensor<4xf32>
// CHECK:           %[[VAL_5:.*]] = stablehlo.shift_left %[[VAL_2]], %[[VAL_3]] : tensor<4xi32>
// CHECK:           %[[VAL_6:.*]] = stablehlo.shift_right_arithmetic %[[VAL_2]], %[[VAL_3]] : tensor<4xi32>
// CHECK:           %[[VAL_7:.*]] = stablehlo.shift_right_logical %[[VAL_2]], %[[VAL_3]] : tensor<4xi32>
// CHECK:           %[[VAL_8:.*]] = mhlo.tuple %[[VAL_4]], %[[VAL_5]], %[[VAL_6]], %[[VAL_7]] {xla_shape = "(f32[4]{0}, s32[4]{0}, s32[4]{0}, s32[4]{0})"} : tuple<tensor<4xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>>
// CHECK:           return %[[VAL_8]] : tuple<tensor<4xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4]{0}, f32[4]{0}, s32[4]{0}, s32[4]{0})->(f32[4]{0}, s32[4]{0}, s32[4]{0}, s32[4]{0})}

ENTRY %main.10 (Arg_0.1: f32[4], Arg_1.2: f32[4], Arg_2.3: s32[4], Arg_3.4: s32[4]) -> (f32[4], s32[4], s32[4], s32[4]) {
  %Arg_0.1 = f32[4] parameter(0)
  %Arg_1.2 = f32[4] parameter(1)
  %atan2.5 = f32[4] atan2(%Arg_0.1, %Arg_1.2)
  %Arg_2.3 = s32[4] parameter(2)
  %Arg_3.4 = s32[4] parameter(3)
  %shift-left.6 = s32[4] shift-left(%Arg_2.3, %Arg_3.4)
  %shift-right-arithmetic.7 = s32[4] shift-right-arithmetic(%Arg_2.3, %Arg_3.4)
  %shift-right-logical.8 = s32[4] shift-right-logical(%Arg_2.3, %Arg_3.4)
  ROOT %tuple.9 = (f32[4], s32[4], s32[4], s32[4]) tuple(%atan2.5, %shift-left.6, %shift-right-arithmetic.7, %shift-right-logical.8)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xi32>) -> tensor<2xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.bitcast_convert %[[VAL_0]] : (tensor<2xi32>) -> tensor<2xf32>
// CHECK:           return %[[VAL_1]] : tensor<2xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[2]{0})->f32[2]{0}}

ENTRY %main.3 (Arg_0.1: s32[2]) -> f32[2] {
  %Arg_0.1 = s32[2] parameter(0)
  ROOT %bitcast-convert.2 = f32[2] bitcast-convert(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xi32>) -> tensor<1x2x3x4xi32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.broadcast_in_dim %[[VAL_0]], dims = [3] : (tensor<4xi32>) -> tensor<1x2x3x4xi32>
// CHECK:           return %[[VAL_1]] : tensor<1x2x3x4xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[4]{0})->s32[1,2,3,4]{3,2,1,0}}

ENTRY %main.3 (Arg_0.1: s32[4]) -> s32[1,2,3,4] {
  %Arg_0.1 = s32[4] parameter(0)
  ROOT %broadcast.2 = s32[1,2,3,4] broadcast(%Arg_0.1), dimensions={3}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<1xf32>) -> tensor<1x10xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.broadcast_in_dim %[[VAL_0]], dims = [0] : (tensor<1xf32>) -> tensor<1x10xf32>
// CHECK:           return %[[VAL_1]] : tensor<1x10xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[1]{0})->f32[1,10]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[1]) -> f32[1,10] {
  %Arg_0.1 = f32[1] parameter(0)
  ROOT %broadcast.2 = f32[1,10] broadcast(%Arg_0.1), dimensions={0}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main() -> !mhlo.token {
// CHECK:           %[[VAL_0:.*]] = mhlo.create_token {xla_shape = "token[]"} : !mhlo.token
// CHECK:           return %[[VAL_0]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={()->token[]}

ENTRY %main.2 () -> token[] {
  ROOT %after-all.1 = token[] after-all()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @empty_callee.2() -> tuple<> {
// CHECK:           %[[VAL_0:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_0]] : tuple<>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_1:.*]]: tensor<4xi32>) -> tensor<4xi32> {
// CHECK:           %[[VAL_2:.*]] = call @empty_callee.2() {xla_shape = "()"} : () -> tuple<>
// CHECK:           return %[[VAL_1]] : tensor<4xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[4]{0})->s32[4]{0}}

%empty_callee.2 () -> () {
  ROOT %tuple.3 = () tuple()
}

ENTRY %main.5 (Arg_0.1: s32[4]) -> s32[4] {
  ROOT %Arg_0.1 = s32[4] parameter(0)
  %call.4 = () call(), to_apply=%empty_callee.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @callee.2(%[[VAL_0:.*]]: tensor<4xi32>, %[[VAL_1:.*]]: tensor<4xi32>) -> tensor<4xi32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<4xi32>
// CHECK:           return %[[VAL_2]] : tensor<4xi32>
// CHECK:         }
// CHECK:         func.func private @callee.7(%[[VAL_3:.*]]: tensor<4xi32>, %[[VAL_4:.*]]: tensor<4xi32>) -> tensor<4xi32> {
// CHECK:           %[[VAL_5:.*]] = stablehlo.add %[[VAL_3]], %[[VAL_4]] : tensor<4xi32>
// CHECK:           return %[[VAL_5]] : tensor<4xi32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_6:.*]]: tensor<4xi32>) -> tensor<4xi32> {
// CHECK:           %[[VAL_7:.*]] = call @callee.2(%[[VAL_6]], %[[VAL_6]]) : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
// CHECK:           %[[VAL_8:.*]] = call @callee.7(%[[VAL_7]], %[[VAL_7]]) : (tensor<4xi32>, tensor<4xi32>) -> tensor<4xi32>
// CHECK:           return %[[VAL_8]] : tensor<4xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[4]{0})->s32[4]{0}}

%callee.2 (Arg_0.3: s32[4], Arg_1.4: s32[4]) -> s32[4] {
  %Arg_0.3 = s32[4] parameter(0)
  %Arg_1.4 = s32[4] parameter(1)
  ROOT %add.5 = s32[4] add(%Arg_0.3, %Arg_1.4)
}

%callee.7 (Arg_0.8: s32[4], Arg_1.9: s32[4]) -> s32[4] {
  %Arg_0.8 = s32[4] parameter(0)
  %Arg_1.9 = s32[4] parameter(1)
  ROOT %add.10 = s32[4] add(%Arg_0.8, %Arg_1.9)
}

ENTRY %main.12 (Arg_0.1: s32[4]) -> s32[4] {
  %Arg_0.1 = s32[4] parameter(0)
  %call.6 = s32[4] call(%Arg_0.1, %Arg_0.1), to_apply=%callee.2
  ROOT %call.11 = s32[4] call(%call.6, %call.6), to_apply=%callee.7
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @callee.2(%[[VAL_0:.*]]: tensor<4xi32>, %[[VAL_1:.*]]: tensor<4xi32>) -> tuple<tensor<4xi32>, tensor<4xi32>> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<4xi32>
// CHECK:           %[[VAL_3:.*]] = stablehlo.multiply %[[VAL_0]], %[[VAL_1]] : tensor<4xi32>
// CHECK:           %[[VAL_4:.*]] = mhlo.tuple %[[VAL_2]], %[[VAL_3]] {xla_shape = "(s32[4]{0}, s32[4]{0})"} : tuple<tensor<4xi32>, tensor<4xi32>>
// CHECK:           return %[[VAL_4]] : tuple<tensor<4xi32>, tensor<4xi32>>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_5:.*]]: tensor<4xi32>) -> tuple<tensor<4xi32>, tensor<4xi32>> {
// CHECK:           %[[VAL_6:.*]] = call @callee.2(%[[VAL_5]], %[[VAL_5]]) {xla_shape = "(s32[4]{0}, s32[4]{0})"} : (tensor<4xi32>, tensor<4xi32>) -> tuple<tensor<4xi32>, tensor<4xi32>>
// CHECK:           %[[VAL_7:.*]] = stablehlo.get_tuple_element %[[VAL_6]][0] : (tuple<tensor<4xi32>, tensor<4xi32>>) -> tensor<4xi32>
// CHECK:           %[[VAL_8:.*]] = stablehlo.get_tuple_element %[[VAL_6]][1] : (tuple<tensor<4xi32>, tensor<4xi32>>) -> tensor<4xi32>
// CHECK:           %[[VAL_9:.*]] = mhlo.tuple %[[VAL_7]], %[[VAL_8]] {xla_shape = "(s32[4]{0}, s32[4]{0})"} : tuple<tensor<4xi32>, tensor<4xi32>>
// CHECK:           return %[[VAL_9]] : tuple<tensor<4xi32>, tensor<4xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[4]{0})->(s32[4]{0}, s32[4]{0})}

%callee.2 (Arg_0.3: s32[4], Arg_1.4: s32[4]) -> (s32[4], s32[4]) {
  %Arg_0.3 = s32[4] parameter(0)
  %Arg_1.4 = s32[4] parameter(1)
  %add.5 = s32[4] add(%Arg_0.3, %Arg_1.4)
  %multiply.6 = s32[4] multiply(%Arg_0.3, %Arg_1.4)
  ROOT %tuple.7 = (s32[4], s32[4]) tuple(%add.5, %multiply.6)
}

ENTRY %main.12 (Arg_0.1: s32[4]) -> (s32[4], s32[4]) {
  %Arg_0.1 = s32[4] parameter(0)
  %call.8 = (s32[4], s32[4]) call(%Arg_0.1, %Arg_0.1), to_apply=%callee.2
  %get-tuple-element.9 = s32[4] get-tuple-element(%call.8), index=0
  %get-tuple-element.10 = s32[4] get-tuple-element(%call.8), index=1
  ROOT %tuple.11 = (s32[4], s32[4]) tuple(%get-tuple-element.9, %get-tuple-element.10)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<1x16x16x3xf32>) -> tensor<1x16x16x3xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.cosine %[[VAL_0]] : tensor<1x16x16x3xf32>
// CHECK:           return %[[VAL_1]] : tensor<1x16x16x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[1,16,16,3]{3,2,1,0})->f32[1,16,16,3]{3,2,1,0}}

ENTRY %main.3 (Arg_0.1: f32[1,16,16,3]) -> f32[1,16,16,3] {
  %Arg_0.1 = f32[1,16,16,3] parameter(0)
  ROOT %cosine.2 = f32[1,16,16,3] cosine(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<1x16x16x3xf32>) -> tensor<1x16x16x3xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.sine %[[VAL_0]] : tensor<1x16x16x3xf32>
// CHECK:           return %[[VAL_1]] : tensor<1x16x16x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[1,16,16,3]{3,2,1,0})->f32[1,16,16,3]{3,2,1,0}}

ENTRY %main.3 (Arg_0.1: f32[1,16,16,3]) -> f32[1,16,16,3] {
  %Arg_0.1 = f32[1,16,16,3] parameter(0)
  ROOT %sine.2 = f32[1,16,16,3] sine(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.exponential %[[VAL_0]] {result_accuracy = #mhlo.result_accuracy<ulps = 10, mode = #mhlo.result_accuracy_mode<TOLERANCE>>} : tensor<f32>
// CHECK:           return %[[VAL_1]] : tensor<f32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[])->f32[]}

ENTRY %main.3 (Arg_0.1: f32[]) -> f32[] {
  %Arg_0.1 = f32[] parameter(0)
  ROOT %exponential.2 = f32[] exponential(%Arg_0.1), result_accuracy={tolerance={atol=0,rtol=0,ulps=10}}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<128x32xf32>) -> tensor<128x32xf32> {
// CHECK:           %[[VAL_1:.*]] = "mhlo.collective_broadcast"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\[\[}}0, 1], [2, 3]]> : tensor<2x2xi64>}> : (tensor<128x32xf32>) -> tensor<128x32xf32>
// CHECK:           return %[[VAL_1]] : tensor<128x32xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[128,32]{1,0})->f32[128,32]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[128,32]) -> f32[128,32] {
  %Arg_0.1 = f32[128,32] parameter(0)
  ROOT %collective-broadcast.2 = f32[128,32] collective-broadcast(%Arg_0.1), channel_id=1, replica_groups={{0,1},{2,3}}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<128x32xf32>) -> tensor<128x32xf32> {
// CHECK:           %[[VAL_1:.*]] = "mhlo.collective_permute"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, source_target_pairs = dense<{{\[\[}}0, 1], [1, 2], [2, 3]]> : tensor<3x2xi64>}> : (tensor<128x32xf32>) -> tensor<128x32xf32>
// CHECK:           return %[[VAL_1]] : tensor<128x32xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[128,32]{1,0})->f32[128,32]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[128,32]) -> f32[128,32] {
  %Arg_0.1 = f32[128,32] parameter(0)
  ROOT %collective-permute.2 = f32[128,32] collective-permute(%Arg_0.1), channel_id=1, source_target_pairs={{0,1},{1,2},{2,3}}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<5x2xf32>, %[[VAL_1:.*]]: tensor<5x5xf32>, %[[VAL_2:.*]]: tensor<5x7xf32>) -> tensor<5x14xf32> {
// CHECK:           %[[VAL_3:.*]] = stablehlo.concatenate %[[VAL_0]], %[[VAL_1]], %[[VAL_2]], dim = 1 : (tensor<5x2xf32>, tensor<5x5xf32>, tensor<5x7xf32>) -> tensor<5x14xf32>
// CHECK:           return %[[VAL_3]] : tensor<5x14xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[5,2]{1,0}, f32[5,5]{1,0}, f32[5,7]{1,0})->f32[5,14]{1,0}}

ENTRY %main.5 (Arg_0.1: f32[5,2], Arg_1.2: f32[5,5], Arg_2.3: f32[5,7]) -> f32[5,14] {
  %Arg_0.1 = f32[5,2] parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  %Arg_2.3 = f32[5,7] parameter(2)
  ROOT %concatenate.4 = f32[5,14] concatenate(%Arg_0.1, %Arg_1.2, %Arg_2.3), dimensions={1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main() -> tuple<> {
// CHECK:           %[[VAL_0:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_0]] : tuple<>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={()->()}

ENTRY %main.2 () -> () {
  ROOT %tuple.1 = () tuple()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<100x26x26x32xf32>, %[[VAL_1:.*]]: tensor<3x3x1x32xf32>) -> tensor<100x28x28x1xf32> {
// CHECK:           %[[VAL_2:.*]] = mhlo.convolution(%[[VAL_0]], %[[VAL_1]]) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = {{\[\[}}2, 2], [2, 2]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x26x26x32xf32>, tensor<3x3x1x32xf32>) -> tensor<100x28x28x1xf32>
// CHECK:           return %[[VAL_2]] : tensor<100x28x28x1xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[100,26,26,32]{3,2,1,0}, f32[3,3,1,32]{3,2,1,0})->f32[100,28,28,1]{3,2,1,0}}

ENTRY %main.4 (Arg_0.1: f32[100,26,26,32], Arg_1.2: f32[3,3,1,32]) -> f32[100,28,28,1] {
  %Arg_0.1 = f32[100,26,26,32] parameter(0)
  %Arg_1.2 = f32[3,3,1,32] parameter(1)
  ROOT %convolution.3 = f32[100,28,28,1] convolution(%Arg_0.1, %Arg_1.2), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01oi->b01f
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<100x26x26x32xi8>, %[[VAL_1:.*]]: tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32> {
// CHECK:           %[[VAL_2:.*]] = mhlo.convolution(%[[VAL_0]], %[[VAL_1]]) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = {{\[\[}}2, 2], [2, 2]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [0, 0]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x26x26x32xi8>, tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32>
// CHECK:           return %[[VAL_2]] : tensor<100x28x28x1xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s8[100,26,26,32]{3,2,1,0}, s8[3,3,1,32]{3,2,1,0})->s32[100,28,28,1]{3,2,1,0}}

ENTRY %main.4 (Arg_0.1: s8[100,26,26,32], Arg_1.2: s8[3,3,1,32]) -> s32[100,28,28,1] {
  %Arg_0.1 = s8[100,26,26,32] parameter(0)
  %Arg_1.2 = s8[3,3,1,32] parameter(1)
  ROOT %convolution.3 = s32[100,28,28,1] convolution(%Arg_0.1, %Arg_1.2), window={size=3x3 pad=2_2x2_2}, dim_labels=b01f_01oi->b01f
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<100x26x26x32xi8>, %[[VAL_1:.*]]: tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32> {
// CHECK:           %[[VAL_2:.*]] = mhlo.convolution(%[[VAL_0]], %[[VAL_1]]) dim_numbers = [b, 0, 1, f]x[0, 1, o, i]->[b, 0, 1, f], window = {stride = [1, 1], pad = {{\[\[}}2, 2], [2, 2]], lhs_dilate = [1, 1], rhs_dilate = [1, 1], reverse = [1, 1]} {batch_group_count = 1 : i64, feature_group_count = 1 : i64, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]} : (tensor<100x26x26x32xi8>, tensor<3x3x1x32xi8>) -> tensor<100x28x28x1xi32>
// CHECK:           return %[[VAL_2]] : tensor<100x28x28x1xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s8[100,26,26,32]{3,2,1,0}, s8[3,3,1,32]{3,2,1,0})->s32[100,28,28,1]{3,2,1,0}}

ENTRY %main.4 (Arg_0.1: s8[100,26,26,32], Arg_1.2: s8[3,3,1,32]) -> s32[100,28,28,1] {
  %Arg_0.1 = s8[100,26,26,32] parameter(0)
  %Arg_1.2 = s8[3,3,1,32] parameter(1)
  ROOT %convolution.3 = s32[100,28,28,1] convolution(%Arg_0.1, %Arg_1.2), window={size=3x3 pad=2_2x2_2 rhs_reversal=1x1}, dim_labels=b01f_01oi->b01f
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xi32>) -> tensor<2xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.convert %[[VAL_0]] : (tensor<2xi32>) -> tensor<2xf32>
// CHECK:           return %[[VAL_1]] : tensor<2xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[2]{0})->f32[2]{0}}

ENTRY %main.3 (Arg_0.1: s32[2]) -> f32[2] {
  %Arg_0.1 = s32[2] parameter(0)
  ROOT %convert.2 = f32[2] convert(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xf32>) -> tensor<2xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.convert %[[VAL_0]] : (tensor<2xf32>) -> tensor<2xf8E5M2>
// CHECK:           %[[VAL_2:.*]] = stablehlo.convert %[[VAL_1]] : (tensor<2xf8E5M2>) -> tensor<2xf8E4M3FN>
// CHECK:           %[[VAL_3:.*]] = stablehlo.convert %[[VAL_2]] : (tensor<2xf8E4M3FN>) -> tensor<2xf8E4M3FNUZ>
// CHECK:           %[[VAL_4:.*]] = stablehlo.convert %[[VAL_3]] : (tensor<2xf8E4M3FNUZ>) -> tensor<2xf8E5M2FNUZ>
// CHECK:           %[[VAL_5:.*]] = stablehlo.convert %[[VAL_4]] : (tensor<2xf8E5M2FNUZ>) -> tensor<2xf8E4M3>
// CHECK:           %[[VAL_6:.*]] = stablehlo.convert %[[VAL_5]] : (tensor<2xf8E4M3>) -> tensor<2xf8E3M4>
// CHECK:           %[[VAL_7:.*]] = stablehlo.convert %[[VAL_6]] : (tensor<2xf8E3M4>) -> tensor<2xf4E2M1FN>
// CHECK:           %[[VAL_8:.*]] = stablehlo.convert %[[VAL_7]] : (tensor<2xf4E2M1FN>) -> tensor<2xf32>
// CHECK:           return %[[VAL_8]] : tensor<2xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2]{0})->f32[2]{0}}

ENTRY %main.10 (Arg_0.1: f32[2]) -> f32[2] {
  %Arg_0.1 = f32[2] parameter(0)
  %convert.2 = f8e5m2[2] convert(%Arg_0.1)
  %convert.3 = f8e4m3fn[2] convert(%convert.2)
  %convert.4 = f8e4m3fnuz[2] convert(%convert.3)
  %convert.5 = f8e5m2fnuz[2] convert(%convert.4)
  %convert.6 = f8e4m3[2] convert(%convert.5)
  %convert.7 = f8e3m4[2] convert(%convert.6)
  %convert.8 = f4e2m1fn[2] convert(%convert.7)
  ROOT %convert.9 = f32[2] convert(%convert.8)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<5x5xf32>, %[[VAL_1:.*]]: tensor<5x5xui32>) -> tensor<5x5xi8> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.stochastic_convert"(%[[VAL_0]], %[[VAL_1]]) : (tensor<5x5xf32>, tensor<5x5xui32>) -> tensor<5x5xi8>
// CHECK:           return %[[VAL_2]] : tensor<5x5xi8>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[5,5]{1,0}, u32[5,5]{1,0})->s8[5,5]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[5,5], Arg_1.2: u32[5,5]) -> s8[5,5] {
  %Arg_0.1 = f32[5,5] parameter(0)
  %Arg_1.2 = u32[5,5] parameter(1)
  ROOT %stochastic-convert.3 = s8[5,5] stochastic-convert(%Arg_0.1, %Arg_1.2)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xi32>) -> tensor<2xi32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.copy %[[VAL_0]] : tensor<2xi32>
// CHECK:           return %[[VAL_1]] : tensor<2xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[2]{0})->s32[2]{0}}

ENTRY %main.3 (Arg_0.1: s32[2]) -> s32[2] {
  %Arg_0.1 = s32[2] parameter(0)
  ROOT %copy.2 = s32[2] copy(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @sum.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<10xf32>) -> tensor<10xf32> {
// CHECK:           %[[VAL_4:.*]] = "mhlo.all_reduce"(%[[VAL_3]]) <{replica_groups = dense<{{\[\[}}0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>}> ({
// CHECK:           ^bb0(%[[VAL_5:.*]]: tensor<f32>, %[[VAL_6:.*]]: tensor<f32>):
// CHECK:             %[[VAL_7:.*]] = stablehlo.add %[[VAL_5]], %[[VAL_6]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_7]] : tensor<f32>
// CHECK:           }) : (tensor<10xf32>) -> tensor<10xf32>
// CHECK:           return %[[VAL_4]] : tensor<10xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10]{0})->f32[10]{0}}

%sum.2 (x.3: f32[], y.4: f32[]) -> f32[] {
  %x.3 = f32[] parameter(0)
  %y.4 = f32[] parameter(1)
  ROOT %add.5 = f32[] add(%x.3, %y.4)
}

ENTRY %main.7 (Arg_0.1: f32[10]) -> f32[10] {
  %Arg_0.1 = f32[10] parameter(0)
  ROOT %all-reduce.6 = f32[10] all-reduce(%Arg_0.1), replica_groups={{0,2,4,6},{1,3,5,7}}, to_apply=%sum.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>) -> tensor<2x3xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.custom_call @SetBound(%[[VAL_0]]) {backend_config = "", mhlo.literal = dense<1> : tensor<i32>} : (tensor<2x3xf32>) -> tensor<2x3xf32>
// CHECK:           return %[[VAL_1]] : tensor<2x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0})->f32[2,3]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[2,3]) -> f32[2,3] {
  %Arg_0.1 = f32[2,3] parameter(0)
  ROOT %custom-call.2 = f32[2,3] custom-call(%Arg_0.1), custom_call_target="SetBound", literal=s32[] 1
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<6xf32>, %[[VAL_1:.*]]: tensor<6xf32>, %[[VAL_2:.*]]: tensor<3xi32>, %[[VAL_3:.*]]: tensor<3xi32>, %[[VAL_4:.*]]: tensor<3xi32>, %[[VAL_5:.*]]: tensor<3xi32>) -> tensor<6xf32> {
// CHECK:           %[[VAL_6:.*]] = stablehlo.custom_call @ragged_all_to_all(%[[VAL_0]], %[[VAL_1]], %[[VAL_2]], %[[VAL_3]], %[[VAL_4]], %[[VAL_5]]) {api_version = 4 : i32, backend_config = {channel_id = 1 : i64, replica_groups = dense<{{\[\[}}0, 1, 2]]> : tensor<1x3xi64>}} : (tensor<6xf32>, tensor<6xf32>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> tensor<6xf32>
// CHECK:           return %[[VAL_6]] : tensor<6xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[6]{0}, f32[6]{0}, s32[3]{0}, s32[3]{0}, s32[3]{0}, /*index=5*/s32[3]{0})->f32[6]{0}}

ENTRY %main.8 (Arg_0.1: f32[6], Arg_1.2: f32[6], Arg_2.3: s32[3], Arg_3.4: s32[3], Arg_4.5: s32[3], Arg_5.6: s32[3]) -> f32[6] {
  %Arg_0.1 = f32[6] parameter(0)
  %Arg_1.2 = f32[6] parameter(1)
  %Arg_2.3 = s32[3] parameter(2)
  %Arg_3.4 = s32[3] parameter(3)
  %Arg_4.5 = s32[3] parameter(4)
  %Arg_5.6 = s32[3] parameter(5)
  ROOT %ragged-all-to-all.7 = f32[6] ragged-all-to-all(%Arg_0.1, %Arg_1.2, %Arg_2.3, %Arg_3.4, %Arg_4.5, /*index=5*/%Arg_5.6), channel_id=1, replica_groups={{0,1,2}}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @top_k_gt_comparator.5(%[[VAL_0:.*]]: tensor<bf16>, %[[VAL_1:.*]]: tensor<bf16>, %[[VAL_2:.*]]: tensor<i32>, %[[VAL_3:.*]]: tensor<i32>) -> tensor<i1> {
// CHECK:           %[[VAL_4:.*]] = mhlo.compare  GT, %[[VAL_0]], %[[VAL_1]] : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
// CHECK:           return %[[VAL_4]] : tensor<i1>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_5:.*]]: tensor<16x256xbf16>, %[[VAL_6:.*]]: tensor<i32>, %[[VAL_7:.*]]: tensor<16x256xi32>, %[[VAL_8:.*]]: tensor<bf16>) -> tuple<tensor<16x4xbf16>, tensor<16x4xi32>> {
// CHECK:           %[[VAL_9:.*]]:2 = "stablehlo.sort"(%[[VAL_5]], %[[VAL_7]]) <{dimension = 1 : i64, is_stable = false}> ({
// CHECK:           ^bb0(%[[VAL_10:.*]]: tensor<bf16>, %[[VAL_11:.*]]: tensor<bf16>, %[[VAL_12:.*]]: tensor<i32>, %[[VAL_13:.*]]: tensor<i32>):
// CHECK:             %[[VAL_14:.*]] = mhlo.compare  GT, %[[VAL_10]], %[[VAL_11]] : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
// CHECK:             stablehlo.return %[[VAL_14]] : tensor<i1>
// CHECK:           }) : (tensor<16x256xbf16>, tensor<16x256xi32>) -> (tensor<16x256xbf16>, tensor<16x256xi32>)
// CHECK:           %[[VAL_15:.*]] = stablehlo.slice %[[VAL_16:.*]]#0 [0:16, 0:4] : (tensor<16x256xbf16>) -> tensor<16x4xbf16>
// CHECK:           %[[VAL_17:.*]] = stablehlo.slice %[[VAL_16]]#1 [0:16, 0:4] : (tensor<16x256xi32>) -> tensor<16x4xi32>
// CHECK:           %[[VAL_18:.*]] = mhlo.tuple %[[VAL_15]], %[[VAL_17]] {xla_shape = "(bf16[16,4]{1,0}, s32[16,4]{1,0})"} : tuple<tensor<16x4xbf16>, tensor<16x4xi32>>
// CHECK:           return %[[VAL_18]] : tuple<tensor<16x4xbf16>, tensor<16x4xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(bf16[16,256]{1,0}, s32[], s32[16,256]{1,0}, bf16[])->(bf16[16,4]{1,0}, s32[16,4]{1,0})}

%top_k_gt_comparator.5 (Arg_0.6: bf16[], Arg_1.7: bf16[], Arg_2.8: s32[], Arg_3.9: s32[]) -> pred[] {
  %Arg_2.8 = s32[] parameter(2)
  %Arg_3.9 = s32[] parameter(3)
  %Arg_0.6 = bf16[] parameter(0)
  %Arg_1.7 = bf16[] parameter(1)
  ROOT %compare.10 = pred[] compare(%Arg_0.6, %Arg_1.7), direction=GT
}

ENTRY %main.20 (Arg_0.1: bf16[16,256], Arg_1.2: s32[], Arg_2.3: s32[16,256], Arg_3.4: bf16[]) -> (bf16[16,4], s32[16,4]) {
  %Arg_1.2 = s32[] parameter(1)
  %Arg_3.4 = bf16[] parameter(3)
  %Arg_0.1 = bf16[16,256] parameter(0)
  %Arg_2.3 = s32[16,256] parameter(2)
  %sort.11 = (bf16[16,256], s32[16,256]) sort(%Arg_0.1, %Arg_2.3), dimensions={1}, to_apply=%top_k_gt_comparator.5
  %get-tuple-element.12 = bf16[16,256] get-tuple-element(%sort.11), index=0
  %slice.13 = bf16[16,4] slice(%get-tuple-element.12), slice={[0:16], [0:4]}
  %get-tuple-element.14 = s32[16,256] get-tuple-element(%sort.11), index=1
  %slice.15 = s32[16,4] slice(%get-tuple-element.14), slice={[0:16], [0:4]}
  %tuple.16 = (bf16[16,4], s32[16,4]) tuple(%slice.13, %slice.15)
  %get-tuple-element.17 = bf16[16,4] get-tuple-element(%tuple.16), index=0
  %get-tuple-element.18 = s32[16,4] get-tuple-element(%tuple.16), index=1
  ROOT %tuple.19 = (bf16[16,4], s32[16,4]) tuple(%get-tuple-element.17, %get-tuple-element.18)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @top_k_gt_comparator.5(%[[VAL_0:.*]]: tensor<bf16>, %[[VAL_1:.*]]: tensor<bf16>, %[[VAL_2:.*]]: tensor<i32>, %[[VAL_3:.*]]: tensor<i32>) -> tensor<i1> {
// CHECK:           %[[VAL_4:.*]] = mhlo.compare  GT, %[[VAL_0]], %[[VAL_1]] : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
// CHECK:           return %[[VAL_4]] : tensor<i1>
// CHECK:         }
// CHECK:         func.func private @top_k_gt_comparator.14(%[[VAL_5:.*]]: tensor<bf16>, %[[VAL_6:.*]]: tensor<bf16>, %[[VAL_7:.*]]: tensor<i32>, %[[VAL_8:.*]]: tensor<i32>) -> tensor<i1> {
// CHECK:           %[[VAL_9:.*]] = mhlo.compare  GT, %[[VAL_5]], %[[VAL_6]] : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
// CHECK:           return %[[VAL_9]] : tensor<i1>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_10:.*]]: tensor<16x256xbf16>, %[[VAL_11:.*]]: tensor<i32>, %[[VAL_12:.*]]: tensor<16x256xi32>, %[[VAL_13:.*]]: tensor<bf16>) -> tuple<tensor<16x4xbf16>, tensor<16x4xi32>> {
// CHECK:           %[[VAL_14:.*]] = mhlo.custom_call @PartialReduce(%[[VAL_10]], %[[VAL_12]], %[[VAL_13]], %[[VAL_11]]) {backend_config = "{\22log2_reduction\22: 1, \22reduction_dim\22: 1, \22to_apply_type\22: \22comparator\22, \22top_k\22: 4, \22recall_target\22: 0.949218}", called_computations = [@top_k_gt_comparator.5], xla_shape = "(bf16[16,128]{1,0}, s32[16,128]{1,0})"} : (tensor<16x256xbf16>, tensor<16x256xi32>, tensor<bf16>, tensor<i32>) -> tuple<tensor<16x128xbf16>, tensor<16x128xi32>>
// CHECK:           %[[VAL_15:.*]] = stablehlo.get_tuple_element %[[VAL_14]][0] : (tuple<tensor<16x128xbf16>, tensor<16x128xi32>>) -> tensor<16x128xbf16>
// CHECK:           %[[VAL_16:.*]] = stablehlo.get_tuple_element %[[VAL_14]][1] : (tuple<tensor<16x128xbf16>, tensor<16x128xi32>>) -> tensor<16x128xi32>
// CHECK:           %[[VAL_17:.*]]:2 = "stablehlo.sort"(%[[VAL_15]], %[[VAL_16]]) <{dimension = 1 : i64, is_stable = false}> ({
// CHECK:           ^bb0(%[[VAL_18:.*]]: tensor<bf16>, %[[VAL_19:.*]]: tensor<bf16>, %[[VAL_20:.*]]: tensor<i32>, %[[VAL_21:.*]]: tensor<i32>):
// CHECK:             %[[VAL_22:.*]] = mhlo.compare  GT, %[[VAL_18]], %[[VAL_19]] : (tensor<bf16>, tensor<bf16>) -> tensor<i1>
// CHECK:             stablehlo.return %[[VAL_22]] : tensor<i1>
// CHECK:           }) : (tensor<16x128xbf16>, tensor<16x128xi32>) -> (tensor<16x128xbf16>, tensor<16x128xi32>)
// CHECK:           %[[VAL_23:.*]] = stablehlo.slice %[[VAL_24:.*]]#0 [0:16, 0:4] : (tensor<16x128xbf16>) -> tensor<16x4xbf16>
// CHECK:           %[[VAL_25:.*]] = stablehlo.slice %[[VAL_24]]#1 [0:16, 0:4] : (tensor<16x128xi32>) -> tensor<16x4xi32>
// CHECK:           %[[VAL_26:.*]] = mhlo.tuple %[[VAL_23]], %[[VAL_25]] {xla_shape = "(bf16[16,4]{1,0}, s32[16,4]{1,0})"} : tuple<tensor<16x4xbf16>, tensor<16x4xi32>>
// CHECK:           return %[[VAL_26]] : tuple<tensor<16x4xbf16>, tensor<16x4xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(bf16[16,256]{1,0}, s32[], s32[16,256]{1,0}, bf16[])->(bf16[16,4]{1,0}, s32[16,4]{1,0})}

%top_k_gt_comparator.5 (Arg_0.6: bf16[], Arg_1.7: bf16[], Arg_2.8: s32[], Arg_3.9: s32[]) -> pred[] {
  %Arg_2.8 = s32[] parameter(2)
  %Arg_3.9 = s32[] parameter(3)
  %Arg_0.6 = bf16[] parameter(0)
  %Arg_1.7 = bf16[] parameter(1)
  ROOT %compare.10 = pred[] compare(%Arg_0.6, %Arg_1.7), direction=GT
}

%top_k_gt_comparator.14 (Arg_0.15: bf16[], Arg_1.16: bf16[], Arg_2.17: s32[], Arg_3.18: s32[]) -> pred[] {
  %Arg_2.17 = s32[] parameter(2)
  %Arg_3.18 = s32[] parameter(3)
  %Arg_0.15 = bf16[] parameter(0)
  %Arg_1.16 = bf16[] parameter(1)
  ROOT %compare.19 = pred[] compare(%Arg_0.15, %Arg_1.16), direction=GT
}

ENTRY %main.29 (Arg_0.1: bf16[16,256], Arg_1.2: s32[], Arg_2.3: s32[16,256], Arg_3.4: bf16[]) -> (bf16[16,4], s32[16,4]) {
  %Arg_0.1 = bf16[16,256] parameter(0)
  %Arg_2.3 = s32[16,256] parameter(2)
  %Arg_3.4 = bf16[] parameter(3)
  %Arg_1.2 = s32[] parameter(1)
  %custom-call.11 = (bf16[16,128], s32[16,128]) custom-call(%Arg_0.1, %Arg_2.3, %Arg_3.4, %Arg_1.2), custom_call_target="PartialReduce", called_computations={%top_k_gt_comparator.5}, backend_config={"log2_reduction": 1, "reduction_dim": 1, "to_apply_type": "comparator", "top_k": 4, "recall_target": 0.949218}
  %get-tuple-element.12 = bf16[16,128] get-tuple-element(%custom-call.11), index=0
  %get-tuple-element.13 = s32[16,128] get-tuple-element(%custom-call.11), index=1
  %sort.20 = (bf16[16,128], s32[16,128]) sort(%get-tuple-element.12, %get-tuple-element.13), dimensions={1}, to_apply=%top_k_gt_comparator.14
  %get-tuple-element.21 = bf16[16,128] get-tuple-element(%sort.20), index=0
  %slice.22 = bf16[16,4] slice(%get-tuple-element.21), slice={[0:16], [0:4]}
  %get-tuple-element.23 = s32[16,128] get-tuple-element(%sort.20), index=1
  %slice.24 = s32[16,4] slice(%get-tuple-element.23), slice={[0:16], [0:4]}
  %tuple.25 = (bf16[16,4], s32[16,4]) tuple(%slice.22, %slice.24)
  %get-tuple-element.26 = bf16[16,4] get-tuple-element(%tuple.25), index=0
  %get-tuple-element.27 = s32[16,4] get-tuple-element(%tuple.25), index=1
  ROOT %tuple.28 = (bf16[16,4], s32[16,4]) tuple(%get-tuple-element.26, %get-tuple-element.27)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>, %[[VAL_1:.*]]: tensor<5x5xf32>) -> tensor<1x2x3xf32> {
// CHECK:           %[[VAL_2:.*]] = mhlo.custom_call @foo(%[[VAL_0]], %[[VAL_1]]) {backend_config = "bar", custom_call_schedule = #mhlo<custom_call_schedule LATEST>, has_side_effect = true} : (tensor<2x3xf32>, tensor<5x5xf32>) -> tensor<1x2x3xf32>
// CHECK:           return %[[VAL_2]] : tensor<1x2x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0}, f32[5,5]{1,0})->f32[1,2,3]{2,1,0}}

ENTRY %main.4 (Arg_0.1: f32[2,3], Arg_1.2: f32[5,5]) -> f32[1,2,3] {
  %Arg_0.1 = f32[2,3] parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  ROOT %custom-call.3 = f32[1,2,3] custom-call(%Arg_0.1, %Arg_1.2), custom_call_target="foo", custom_call_has_side_effect=true, schedule=SCHEDULE_LATEST, backend_config="bar"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>, %[[VAL_1:.*]]: tensor<5x5xf32>) -> tensor<1x2x3xf32> {
// CHECK:           %[[VAL_2:.*]] = mhlo.custom_call @foo(%[[VAL_0]], %[[VAL_1]]) {backend_config = "bar", custom_call_schedule = #mhlo<custom_call_schedule EARLIEST>, has_side_effect = true} : (tensor<2x3xf32>, tensor<5x5xf32>) -> tensor<1x2x3xf32>
// CHECK:           return %[[VAL_2]] : tensor<1x2x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0}, f32[5,5]{1,0})->f32[1,2,3]{2,1,0}}

ENTRY %main.4 (Arg_0.1: f32[2,3], Arg_1.2: f32[5,5]) -> f32[1,2,3] {
  %Arg_0.1 = f32[2,3] parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  ROOT %custom-call.3 = f32[1,2,3] custom-call(%Arg_0.1, %Arg_1.2), custom_call_target="foo", custom_call_has_side_effect=true, schedule=SCHEDULE_EARLIEST, backend_config="bar"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>) -> tuple<tensor<2x3xf32>> {
// CHECK:           %[[VAL_1:.*]] = mhlo.custom_call @foo(%[[VAL_0]]) {backend_config = "", xla_shape = "(f32[2,3]{1,0})"} : (tensor<2x3xf32>) -> tuple<tensor<2x3xf32>>
// CHECK:           return %[[VAL_1]] : tuple<tensor<2x3xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0})->(f32[2,3]{1,0})}

ENTRY %main.3 (Arg_0.1: f32[2,3]) -> (f32[2,3]) {
  %Arg_0.1 = f32[2,3] parameter(0)
  ROOT %custom-call.2 = (f32[2,3]) custom-call(%Arg_0.1), custom_call_target="foo"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>) -> tuple<tensor<2x3xf32>, tensor<4x5xf16>> {
// CHECK:           %[[VAL_1:.*]] = mhlo.custom_call @foo(%[[VAL_0]]) {backend_config = "", xla_shape = "(f32[2,3]{1,0}, f16[4,5]{1,0})"} : (tensor<2x3xf32>) -> tuple<tensor<2x3xf32>, tensor<4x5xf16>>
// CHECK:           return %[[VAL_1]] : tuple<tensor<2x3xf32>, tensor<4x5xf16>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0})->(f32[2,3]{1,0}, f16[4,5]{1,0})}

ENTRY %main.3 (Arg_0.1: f32[2,3]) -> (f32[2,3], f16[4,5]) {
  %Arg_0.1 = f32[2,3] parameter(0)
  ROOT %custom-call.2 = (f32[2,3], f16[4,5]) custom-call(%Arg_0.1), custom_call_target="foo"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>) -> tuple<tensor<2x3xf32>, tensor<4x5xf16>> {
// CHECK:           %[[VAL_1:.*]] = mhlo.custom_call @foo(%[[VAL_0]]) {backend_config = "", xla_shape = "(f32[2,3]{1,0}, f16[4,5]{1,0})"} : (tensor<2x3xf32>) -> tuple<tensor<2x3xf32>, tensor<4x5xf16>>
// CHECK:           %[[VAL_2:.*]] = stablehlo.get_tuple_element %[[VAL_1]][0] : (tuple<tensor<2x3xf32>, tensor<4x5xf16>>) -> tensor<2x3xf32>
// CHECK:           %[[VAL_3:.*]] = stablehlo.get_tuple_element %[[VAL_1]][1] : (tuple<tensor<2x3xf32>, tensor<4x5xf16>>) -> tensor<4x5xf16>
// CHECK:           %[[VAL_4:.*]] = mhlo.tuple %[[VAL_2]], %[[VAL_3]] {xla_shape = "(f32[2,3]{1,0}, f16[4,5]{1,0})"} : tuple<tensor<2x3xf32>, tensor<4x5xf16>>
// CHECK:           return %[[VAL_4]] : tuple<tensor<2x3xf32>, tensor<4x5xf16>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0})->(f32[2,3]{1,0}, f16[4,5]{1,0})}

ENTRY %main.6 (Arg_0.1: f32[2,3]) -> (f32[2,3], f16[4,5]) {
  %Arg_0.1 = f32[2,3] parameter(0)
  %custom-call.2 = (f32[2,3], f16[4,5]) custom-call(%Arg_0.1), custom_call_target="foo"
  %get-tuple-element.3 = f32[2,3] get-tuple-element(%custom-call.2), index=0
  %get-tuple-element.4 = f16[4,5] get-tuple-element(%custom-call.2), index=1
  ROOT %tuple.5 = (f32[2,3], f16[4,5]) tuple(%get-tuple-element.3, %get-tuple-element.4)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3xi8>, %[[VAL_1:.*]]: tensor<3xi8>) -> tensor<i64> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.dot"(%[[VAL_0]], %[[VAL_1]]) <{precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]}> : (tensor<3xi8>, tensor<3xi8>) -> tensor<i64>
// CHECK:           return %[[VAL_2]] : tensor<i64>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s8[3]{0}, s8[3]{0})->s64[]}

ENTRY %main.4 (Arg_0.1: s8[3], Arg_1.2: s8[3]) -> s64[] {
  %Arg_0.1 = s8[3] parameter(0)
  %Arg_1.2 = s8[3] parameter(1)
  ROOT %dot.3 = s64[] dot(%Arg_0.1, %Arg_1.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3xi4>, %[[VAL_1:.*]]: tensor<3xi4>) -> tensor<i8> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.dot"(%[[VAL_0]], %[[VAL_1]]) <{precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]}> : (tensor<3xi4>, tensor<3xi4>) -> tensor<i8>
// CHECK:           return %[[VAL_2]] : tensor<i8>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s4[3]{0}, s4[3]{0})->s8[]}

ENTRY %main.4 (Arg_0.1: s4[3], Arg_1.2: s4[3]) -> s8[] {
  %Arg_0.1 = s4[3] parameter(0)
  %Arg_1.2 = s4[3] parameter(1)
  ROOT %dot.3 = s8[] dot(%Arg_0.1, %Arg_1.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3xui4>, %[[VAL_1:.*]]: tensor<3xui4>) -> tensor<ui8> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.dot"(%[[VAL_0]], %[[VAL_1]]) <{precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]}> : (tensor<3xui4>, tensor<3xui4>) -> tensor<ui8>
// CHECK:           return %[[VAL_2]] : tensor<ui8>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(u4[3]{0}, u4[3]{0})->u8[]}

ENTRY %main.4 (Arg_0.1: u4[3], Arg_1.2: u4[3]) -> u8[] {
  %Arg_0.1 = u4[3] parameter(0)
  %Arg_1.2 = u4[3] parameter(1)
  ROOT %dot.3 = u8[] dot(%Arg_0.1, %Arg_1.2), lhs_contracting_dims={0}, rhs_contracting_dims={0}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x2x2xi8>, %[[VAL_1:.*]]: tensor<2x2x3xi8>) -> tensor<2x2x3xi32> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.dot_general"(%[[VAL_0]], %[[VAL_1]]) <{dot_dimension_numbers = #mhlo.dot<lhs_batching_dimensions = [0], rhs_batching_dimensions = [0], lhs_contracting_dimensions = [2], rhs_contracting_dimensions = [1]>, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]}> : (tensor<2x2x2xi8>, tensor<2x2x3xi8>) -> tensor<2x2x3xi32>
// CHECK:           return %[[VAL_2]] : tensor<2x2x3xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s8[2,2,2]{2,1,0}, s8[2,2,3]{2,1,0})->s32[2,2,3]{2,1,0}}

ENTRY %main.4 (Arg_0.1: s8[2,2,2], Arg_1.2: s8[2,2,3]) -> s32[2,2,3] {
  %Arg_0.1 = s8[2,2,2] parameter(0)
  %Arg_1.2 = s8[2,2,3] parameter(1)
  ROOT %dot.3 = s32[2,2,3] dot(%Arg_0.1, %Arg_1.2), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<10x16xbf16>, %[[VAL_1:.*]]: tensor<32x20xbf16>, %[[VAL_2:.*]]: tensor<10x2xui16>) -> tensor<10x20xf32> {
// CHECK:           %[[VAL_3:.*]] = "mhlo.sparse_dot"(%[[VAL_0]], %[[VAL_1]], %[[VAL_2]]) <{dot_dimension_numbers = #mhlo.dot<lhs_contracting_dimensions = [1], rhs_contracting_dimensions = [0]>, lhs_sparsity = #mhlo.sparsity<dimension = 1, n = 2, m = 4>, precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]}> : (tensor<10x16xbf16>, tensor<32x20xbf16>, tensor<10x2xui16>) -> tensor<10x20xf32>
// CHECK:           return %[[VAL_3]] : tensor<10x20xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(bf16[10,16]{1,0}, bf16[32,20]{1,0}, u16[10,2]{1,0})->f32[10,20]{1,0}}

ENTRY %main.5 (Arg_0.1: bf16[10,16], Arg_1.2: bf16[32,20], Arg_2.3: u16[10,2]) -> f32[10,20] {
  %Arg_0.1 = bf16[10,16] parameter(0)
  %Arg_1.2 = bf16[32,20] parameter(1)
  %Arg_2.3 = u16[10,2] parameter(2)
  ROOT %dot.4 = f32[10,20] dot(%Arg_0.1, %Arg_1.2, %Arg_2.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}, sparsity=L.1@2:4
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xi32>, %[[VAL_1:.*]]: tensor<4x5xi32>) -> tensor<3x5xi32> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.dot"(%[[VAL_0]], %[[VAL_1]]) <{precision_config = [#mhlo<precision DEFAULT>, #mhlo<precision DEFAULT>]}> {mhlo.frontend_attributes = {grad_x = "false", grad_y = "false"}} : (tensor<3x4xi32>, tensor<4x5xi32>) -> tensor<3x5xi32>
// CHECK:           %[[VAL_3:.*]] = stablehlo.transpose %[[VAL_2]], dims = [0, 1] : (tensor<3x5xi32>) -> tensor<3x5xi32>
// CHECK:           return %[[VAL_3]] : tensor<3x5xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3,4]{1,0}, s32[4,5]{1,0})->s32[3,5]{1,0}}

ENTRY %main.5 (Arg_0.1: s32[3,4], Arg_1.2: s32[4,5]) -> s32[3,5] {
  %Arg_0.1 = s32[3,4] parameter(0)
  %Arg_1.2 = s32[4,5] parameter(1)
  %dot.3 = s32[3,5] dot(%Arg_0.1, %Arg_1.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, frontend_attributes={grad_x="false",grad_y="false"}
  ROOT %transpose.4 = s32[3,5] transpose(%dot.3), dimensions={0,1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x9xf32>) -> tensor<3x5xcomplex<f32>> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.fft %[[VAL_0]], type =  RFFT, length = [9] : (tensor<3x9xf32>) -> tensor<3x5xcomplex<f32>>
// CHECK:           return %[[VAL_1]] : tensor<3x5xcomplex<f32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,9]{1,0})->c64[3,5]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[3,9]) -> c64[3,5] {
  %Arg_0.1 = f32[3,9] parameter(0)
  ROOT %fft.2 = c64[3,5] fft(%Arg_0.1), fft_type=RFFT, fft_length={9}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<200x100x300xf32>, %[[VAL_1:.*]]: tensor<10x2xi32>) -> tensor<10x300xf32> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.gather"(%[[VAL_0]], %[[VAL_1]]) <{dimension_numbers = #mhlo.gather<offset_dims = [1], collapsed_slice_dims = [0, 1], start_index_map = [0, 1], index_vector_dim = 1>, indices_are_sorted = true, slice_sizes = dense<[1, 1, 300]> : tensor<3xi64>}> : (tensor<200x100x300xf32>, tensor<10x2xi32>) -> tensor<10x300xf32>
// CHECK:           return %[[VAL_2]] : tensor<10x300xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[200,100,300]{2,1,0}, s32[10,2]{1,0})->f32[10,300]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[200,100,300], Arg_1.2: s32[10,2]) -> f32[10,300] {
  %Arg_0.1 = f32[200,100,300] parameter(0)
  %Arg_1.2 = s32[10,2] parameter(1)
  ROOT %gather.3 = f32[10,300] gather(%Arg_0.1, %Arg_1.2), offset_dims={1}, collapsed_slice_dims={0,1}, start_index_map={0,1}, index_vector_dim=1, slice_sizes={1,1,300}, indices_are_sorted=true
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<200x100x300xf32>, %[[VAL_1:.*]]: tensor<100x200x1xi32>) -> tensor<100x200x300xf32> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.gather"(%[[VAL_0]], %[[VAL_1]]) <{dimension_numbers = #mhlo.gather<offset_dims = [2], operand_batching_dims = [0, 1], start_indices_batching_dims = [1, 0], start_index_map = [2], index_vector_dim = 2>, indices_are_sorted = true, slice_sizes = dense<[1, 1, 300]> : tensor<3xi64>}> : (tensor<200x100x300xf32>, tensor<100x200x1xi32>) -> tensor<100x200x300xf32>
// CHECK:           return %[[VAL_2]] : tensor<100x200x300xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[200,100,300]{2,1,0}, s32[100,200,1]{2,1,0})->f32[100,200,300]{2,1,0}}

ENTRY %main.4 (Arg_0.1: f32[200,100,300], Arg_1.2: s32[100,200,1]) -> f32[100,200,300] {
  %Arg_0.1 = f32[200,100,300] parameter(0)
  %Arg_1.2 = s32[100,200,1] parameter(1)
  ROOT %gather.3 = f32[100,200,300] gather(%Arg_0.1, %Arg_1.2), offset_dims={2}, collapsed_slice_dims={}, start_index_map={2}, operand_batching_dims={0,1}, start_indices_batching_dims={1,0}, index_vector_dim=2, slice_sizes={1,1,300}, indices_are_sorted=true
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x2xf32>, %[[VAL_1:.*]]: tensor<i32>) -> tensor<i32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.constant dense<2> : tensor<i32>
// CHECK:           return %[[VAL_2]] : tensor<i32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,2]{1,0}, s32[])->s32[]}

ENTRY %main.4 (Arg_0.1: f32[4,2], Arg_1.2: s32[]) -> s32[] {
  %Arg_0.1 = f32[4,2] parameter(0)
  %Arg_1.2 = s32[] parameter(1)
  ROOT %constant.3 = s32[] constant(2)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x4xf32>, %[[VAL_1:.*]]: tensor<i32>) -> tensor<4x?xf32, #mhlo.type_extensions<bounds = [?, 4]>> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.set_dimension_size %[[VAL_0]], %[[VAL_1]], dim = 1 : (tensor<4x4xf32>, tensor<i32>) -> tensor<4x?xf32, #mhlo.type_extensions<bounds = [?, 4]>>
// CHECK:           return %[[VAL_2]] : tensor<4x?xf32, #mhlo.type_extensions<bounds = [?, 4]>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,4]{1,0}, s32[])->f32[4,<=4]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[4,4], Arg_1.2: s32[]) -> f32[4,<=4] {
  %Arg_0.1 = f32[4,4] parameter(0)
  %Arg_1.2 = s32[] parameter(1)
  ROOT %set-dimension-size.3 = f32[4,<=4] set-dimension-size(%Arg_0.1, %Arg_1.2), dimensions={1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tuple<tensor<f32>, tensor<i32>>) -> tensor<f32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.get_tuple_element %[[VAL_0]][0] : (tuple<tensor<f32>, tensor<i32>>) -> tensor<f32>
// CHECK:           return %[[VAL_1]] : tensor<f32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={((f32[], s32[]))->f32[]}

ENTRY %main.3 (Arg_0.1: (f32[], s32[])) -> f32[] {
  %Arg_0.1 = (f32[], s32[]) parameter(0)
  ROOT %get-tuple-element.2 = f32[] get-tuple-element(%Arg_0.1), index=0
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> tuple<tuple<tensor<3x3xi32>, tensor<i1>>, !mhlo.token> {
// CHECK:           %[[VAL_1:.*]]:3 = "mhlo.infeed"(%[[VAL_0]]) <{infeed_config = "foobar", layout = {{\[\[}}1, 0], []]}> : (!mhlo.token) -> (tensor<3x3xi32>, tensor<i1>, !mhlo.token)
// CHECK:           %[[VAL_2:.*]] = mhlo.tuple %[[VAL_1]]#0, %[[VAL_1]]#1 {xla_shape = "(s32[3,3]{1,0}, pred[])"} : tuple<tensor<3x3xi32>, tensor<i1>>
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple %[[VAL_2]], %[[VAL_1]]#2 {xla_shape = "((s32[3,3]{1,0}, pred[]), token[])"} : tuple<tuple<tensor<3x3xi32>, tensor<i1>>, !mhlo.token>
// CHECK:           return %[[VAL_3]] : tuple<tuple<tensor<3x3xi32>, tensor<i1>>, !mhlo.token>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->((s32[3,3]{1,0}, pred[]), token[])}

ENTRY %main.9 (Arg_0.1: token[]) -> ((s32[3,3], pred[]), token[]) {
  %Arg_0.1 = token[] parameter(0)
  %infeed.2 = ((s32[3,3], pred[]), token[]) infeed(%Arg_0.1), infeed_config="foobar"
  %get-tuple-element.3 = (s32[3,3], pred[]) get-tuple-element(%infeed.2), index=0
  %get-tuple-element.4 = s32[3,3] get-tuple-element(%get-tuple-element.3), index=0
  %get-tuple-element.5 = pred[] get-tuple-element(%get-tuple-element.3), index=1
  %tuple.7 = (s32[3,3], pred[]) tuple(%get-tuple-element.4, %get-tuple-element.5)
  %get-tuple-element.6 = token[] get-tuple-element(%infeed.2), index=1
  ROOT %tuple.8 = ((s32[3,3], pred[]), token[]) tuple(%tuple.7, %get-tuple-element.6)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, mhlo.xla_entry_computation_result_layout = [dense<[0, 1]> : tensor<2xindex>], mhlo.xla_entry_computation_result_tiles = {{\[\[}}]]} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> tensor<3x3xi32> {
// CHECK:           %[[VAL_1:.*]]:2 = "mhlo.infeed"(%[[VAL_0]]) <{infeed_config = "foobar", layout = {{\[\[}}1, 0]]}> : (!mhlo.token) -> (tensor<3x3xi32>, !mhlo.token)
// CHECK:           return %[[VAL_1]]#0 : tensor<3x3xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->s32[3,3]{0,1}}

ENTRY %main.6 (Arg_0.1: token[]) -> s32[3,3] {
  %Arg_0.1 = token[] parameter(0)
  %infeed.2 = ((s32[3,3]), token[]) infeed(%Arg_0.1), infeed_config="foobar"
  %get-tuple-element.3 = (s32[3,3]) get-tuple-element(%infeed.2), index=0
  ROOT %get-tuple-element.4 = s32[3,3] get-tuple-element(%get-tuple-element.3), index=0
  %get-tuple-element.5 = token[] get-tuple-element(%infeed.2), index=1
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_1:.*]] = "mhlo.infeed"(%[[VAL_0]]) <{infeed_config = "foobar", layout = []}> : (!mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_1]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->token[]}

ENTRY %main.4 (Arg_0.1: token[]) -> token[] {
  %Arg_0.1 = token[] parameter(0)
  %infeed.2 = ((), token[]) infeed(%Arg_0.1), infeed_config="foobar"
  ROOT %get-tuple-element.3 = token[] get-tuple-element(%infeed.2), index=1
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main() -> tensor<1x10xf32> {
// CHECK:           %[[VAL_0:.*]] = stablehlo.iota dim = 0 : tensor<10xf32>
// CHECK:           %[[VAL_1:.*]] = stablehlo.reshape %[[VAL_0]] : (tensor<10xf32>) -> tensor<1x10xf32>
// CHECK:           return %[[VAL_1]] : tensor<1x10xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={()->f32[1,10]{1,0}}

ENTRY %main.3 () -> f32[1,10] {
  %iota.1 = f32[10] iota(), iota_dimension=0
  ROOT %reshape.2 = f32[1,10] reshape(%iota.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.3(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<4xf32>, %[[VAL_4:.*]]: tensor<4xf32>) -> tensor<4xf32> {
// CHECK:           %[[VAL_5:.*]] = "stablehlo.map"(%[[VAL_3]], %[[VAL_4]]) <{dimensions = array<i64: 0>}> ({
// CHECK:           ^bb0(%[[VAL_6:.*]]: tensor<f32>, %[[VAL_7:.*]]: tensor<f32>):
// CHECK:             %[[VAL_8:.*]] = stablehlo.add %[[VAL_6]], %[[VAL_7]] : tensor<f32>
// CHECK:             stablehlo.return %[[VAL_8]] : tensor<f32>
// CHECK:           }) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>
// CHECK:           return %[[VAL_5]] : tensor<4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4]{0}, f32[4]{0})->f32[4]{0}}

%region_0.3 (Arg_0.4: f32[], Arg_1.5: f32[]) -> f32[] {
  %Arg_0.4 = f32[] parameter(0)
  %Arg_1.5 = f32[] parameter(1)
  ROOT %add.6 = f32[] add(%Arg_0.4, %Arg_1.5)
}

ENTRY %main.8 (Arg_0.1: f32[4], Arg_1.2: f32[4]) -> f32[4] {
  %Arg_0.1 = f32[4] parameter(0)
  %Arg_1.2 = f32[4] parameter(1)
  ROOT %map.7 = f32[4] map(%Arg_0.1, %Arg_1.2), dimensions={0}, to_apply=%region_0.3
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xf32>, %[[VAL_1:.*]]: tensor<4xi32>) -> tensor<4xf32> {
// CHECK:           return %[[VAL_0]] : tensor<4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4]{0}, s32[4]{0})->f32[4]{0}}

ENTRY %main.3 (Arg_0.1: f32[4], Arg_1.2: s32[4]) -> f32[4] {
  ROOT %Arg_0.1 = f32[4] parameter(0)
  %Arg_1.2 = s32[4] parameter(1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3xi32>, %[[VAL_1:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_2:.*]] = "mhlo.outfeed"(%[[VAL_0]], %[[VAL_1]]) <{outfeed_config = "foobar"}> {xla_shape = "token[]"} : (tensor<3xi32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_2]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3]{0}, token[])->token[]}

ENTRY %main.5 (Arg_0.1: s32[3], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = s32[3] parameter(0)
  %tuple.3 = (s32[3]) tuple(%Arg_0.1)
  %Arg_1.2 = token[] parameter(1)
  ROOT %outfeed.4 = token[] outfeed(%tuple.3, %Arg_1.2), outfeed_shape=(s32[3]{0}), outfeed_config="foobar"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x2xi32>, %[[VAL_1:.*]]: !mhlo.token) -> (!mhlo.token {mhlo.sharding = "{{\{\{}}devices=[2,1]0,1}, {maximal device=0}}"}) {
// CHECK:           %[[VAL_2:.*]] = mhlo.custom_call @Sharding(%[[VAL_0]]) {backend_config = "", mhlo.sharding = "{devices=[1,2]0,1}"} : (tensor<3x2xi32>) -> tensor<3x2xi32>
// CHECK:           %[[VAL_3:.*]] = mhlo.custom_call @SPMDShardToFullShape(%[[VAL_2]]) {backend_config = "", mhlo.sharding = "{devices=[1,2]0,1}"} : (tensor<3x2xi32>) -> tensor<6x2xi32>
// CHECK:           %[[VAL_4:.*]] = "mhlo.outfeed"(%[[VAL_3]], %[[VAL_1]]) <{outfeed_config = "foobar"}> {mhlo.sharding = "{{\{\{}}devices=[2,1]0,1}, {maximal device=0}}", xla_shape = "token[]"} : (tensor<6x2xi32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_4]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3,2]{1,0}, token[])->token[]}

ENTRY %main.7 (Arg_0.1: s32[3,2], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = s32[3,2] parameter(0)
  %custom-call.3 = s32[3,2] custom-call(%Arg_0.1), custom_call_target="Sharding", sharding={devices=[1,2]0,1}
  %custom-call.4 = s32[6,2] custom-call(%custom-call.3), custom_call_target="SPMDShardToFullShape", sharding={devices=[1,2]0,1}
  %tuple.5 = (s32[6,2]) tuple(%custom-call.4)
  %Arg_1.2 = token[] parameter(1)
  ROOT %outfeed.6 = token[] outfeed(%tuple.5, %Arg_1.2), outfeed_shape=(s32[6,2]{1,0}), outfeed_config="foobar", sharding={{devices=[2,1]0,1}, {maximal device=0}}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3xi32>, %[[VAL_1:.*]]: tensor<3xi32>, %[[VAL_2:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_3:.*]] = "mhlo.outfeed"(%[[VAL_0]], %[[VAL_1]], %[[VAL_2]]) <{outfeed_config = "foobar"}> {xla_shape = "token[]"} : (tensor<3xi32>, tensor<3xi32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_3]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3]{0}, s32[3]{0}, token[])->token[]}

ENTRY %main.6 (Arg_0.1: s32[3], Arg_1.2: s32[3], Arg_2.3: token[]) -> token[] {
  %Arg_0.1 = s32[3] parameter(0)
  %Arg_1.2 = s32[3] parameter(1)
  %tuple.4 = (s32[3], s32[3]) tuple(%Arg_0.1, %Arg_1.2)
  %Arg_2.3 = token[] parameter(2)
  ROOT %outfeed.5 = token[] outfeed(%tuple.4, %Arg_2.3), outfeed_shape=(s32[3]{0}, s32[3]{0}), outfeed_config="foobar"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_1:.*]] = "mhlo.outfeed"(%[[VAL_0]]) <{outfeed_config = "foobar"}> {xla_shape = "token[]"} : (!mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_1]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->token[]}

ENTRY %main.4 (Arg_0.1: token[]) -> token[] {
  %tuple.2 = () tuple()
  %Arg_0.1 = token[] parameter(0)
  ROOT %outfeed.3 = token[] outfeed(%tuple.2, %Arg_0.1), outfeed_shape=(), outfeed_config="foobar"
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x6xf32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<13x19xf32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.pad %[[VAL_0]], %[[VAL_1]], low = [2, 3], high = [4, 5], interior = [1, 1] : (tensor<4x6xf32>, tensor<f32>) -> tensor<13x19xf32>
// CHECK:           return %[[VAL_2]] : tensor<13x19xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,6]{1,0}, f32[])->f32[13,19]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[4,6], Arg_1.2: f32[]) -> f32[13,19] {
  %Arg_0.1 = f32[4,6] parameter(0)
  %Arg_1.2 = f32[] parameter(1)
  ROOT %pad.3 = f32[13,19] pad(%Arg_0.1, %Arg_1.2), padding=2_4_1x3_5_1
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> tuple<tensor<3x4xi32>, !mhlo.token> {
// CHECK:           %[[VAL_1:.*]]:2 = "mhlo.recv"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 3>, is_host_transfer = true}> : (!mhlo.token) -> (tensor<3x4xi32>, !mhlo.token)
// CHECK:           %[[VAL_2:.*]] = mhlo.tuple %[[VAL_1]]#0, %[[VAL_1]]#1 {xla_shape = "(s32[3,4]{1,0}, token[])"} : tuple<tensor<3x4xi32>, !mhlo.token>
// CHECK:           return %[[VAL_2]] : tuple<tensor<3x4xi32>, !mhlo.token>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->(s32[3,4]{1,0}, token[])}

ENTRY %main.7 (Arg_0.1: token[]) -> (s32[3,4], token[]) {
  %Arg_0.1 = token[] parameter(0)
  %recv.2 = (s32[3,4], u32[], token[]) recv(%Arg_0.1), channel_id=5, is_host_transfer=true
  %recv-done.3 = (s32[3,4], token[]) recv-done(%recv.2), channel_id=5, is_host_transfer=true
  %get-tuple-element.4 = s32[3,4] get-tuple-element(%recv-done.3), index=0
  %get-tuple-element.5 = token[] get-tuple-element(%recv-done.3), index=1
  ROOT %tuple.6 = (s32[3,4], token[]) tuple(%get-tuple-element.4, %get-tuple-element.5)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> tuple<tensor<3x4xi32>, !mhlo.token> {
// CHECK:           %[[VAL_1:.*]]:2 = "mhlo.recv"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 1>, is_host_transfer = false}> : (!mhlo.token) -> (tensor<3x4xi32>, !mhlo.token)
// CHECK:           %[[VAL_2:.*]] = mhlo.tuple %[[VAL_1]]#0, %[[VAL_1]]#1 {xla_shape = "(s32[3,4]{1,0}, token[])"} : tuple<tensor<3x4xi32>, !mhlo.token>
// CHECK:           return %[[VAL_2]] : tuple<tensor<3x4xi32>, !mhlo.token>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->(s32[3,4]{1,0}, token[])}

ENTRY %main.7 (Arg_0.1: token[]) -> (s32[3,4], token[]) {
  %Arg_0.1 = token[] parameter(0)
  %recv.2 = (s32[3,4], u32[], token[]) recv(%Arg_0.1), channel_id=5
  %recv-done.3 = (s32[3,4], token[]) recv-done(%recv.2), channel_id=5
  %get-tuple-element.4 = s32[3,4] get-tuple-element(%recv-done.3), index=0
  %get-tuple-element.5 = token[] get-tuple-element(%recv-done.3), index=1
  ROOT %tuple.6 = (s32[3,4], token[]) tuple(%get-tuple-element.4, %get-tuple-element.5)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_1:.*]] = "mhlo.recv"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 1>, is_host_transfer = false}> : (!mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_1]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->token[]}

ENTRY %main.6 (Arg_0.1: token[]) -> token[] {
  %Arg_0.1 = token[] parameter(0)
  %recv.2 = ((), u32[], token[]) recv(%Arg_0.1), channel_id=5
  %recv-done.3 = ((), token[]) recv-done(%recv.2), channel_id=5
  %get-tuple-element.4 = () get-tuple-element(%recv-done.3), index=0
  ROOT %get-tuple-element.5 = token[] get-tuple-element(%recv-done.3), index=1
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.5(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<i32>, %[[VAL_2:.*]]: tensor<f32>, %[[VAL_3:.*]]: tensor<i32>) -> tuple<tensor<f32>, tensor<i32>> {
// CHECK:           %[[VAL_4:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_2]] : tensor<f32>
// CHECK:           %[[VAL_5:.*]] = stablehlo.maximum %[[VAL_1]], %[[VAL_3]] : tensor<i32>
// CHECK:           %[[VAL_6:.*]] = mhlo.tuple %[[VAL_4]], %[[VAL_5]] {xla_shape = "(f32[], s32[])"} : tuple<tensor<f32>, tensor<i32>>
// CHECK:           return %[[VAL_6]] : tuple<tensor<f32>, tensor<i32>>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_7:.*]]: tensor<1x10xf32>, %[[VAL_8:.*]]: tensor<1x10xi32>, %[[VAL_9:.*]]: tensor<f32>, %[[VAL_10:.*]]: tensor<i32>) -> tuple<tensor<1xf32>, tensor<1xi32>> {
// CHECK:           %[[VAL_11:.*]]:2 = mhlo.reduce(%[[VAL_7]] init: %[[VAL_9]]), (%[[VAL_8]] init: %[[VAL_10]]) across dimensions = [1] : (tensor<1x10xf32>, tensor<1x10xi32>, tensor<f32>, tensor<i32>) -> (tensor<1xf32>, tensor<1xi32>)
// CHECK:            reducer(%[[VAL_12:.*]]: tensor<f32>, %[[VAL_13:.*]]: tensor<f32>) (%[[VAL_14:.*]]: tensor<i32>, %[[VAL_15:.*]]: tensor<i32>)  {
// CHECK:             %[[VAL_16:.*]] = stablehlo.maximum %[[VAL_12]], %[[VAL_13]] : tensor<f32>
// CHECK:             %[[VAL_17:.*]] = stablehlo.maximum %[[VAL_14]], %[[VAL_15]] : tensor<i32>
// CHECK:             mhlo.return %[[VAL_16]], %[[VAL_17]] : tensor<f32>, tensor<i32>
// CHECK:           }
// CHECK:           %[[VAL_18:.*]] = mhlo.tuple %[[VAL_11]]#0, %[[VAL_11]]#1 {xla_shape = "(f32[1]{0}, s32[1]{0})"} : tuple<tensor<1xf32>, tensor<1xi32>>
// CHECK:           return %[[VAL_18]] : tuple<tensor<1xf32>, tensor<1xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[1,10]{1,0}, s32[1,10]{1,0}, f32[], s32[])->(f32[1]{0}, s32[1]{0})}

%region_0.5 (Arg_0.6: f32[], Arg_1.7: s32[], Arg_2.8: f32[], Arg_3.9: s32[]) -> (f32[], s32[]) {
  %Arg_0.6 = f32[] parameter(0)
  %Arg_2.8 = f32[] parameter(2)
  %maximum.10 = f32[] maximum(%Arg_0.6, %Arg_2.8)
  %Arg_1.7 = s32[] parameter(1)
  %Arg_3.9 = s32[] parameter(3)
  %maximum.11 = s32[] maximum(%Arg_1.7, %Arg_3.9)
  ROOT %tuple.12 = (f32[], s32[]) tuple(%maximum.10, %maximum.11)
}

ENTRY %main.17 (Arg_0.1: f32[1,10], Arg_1.2: s32[1,10], Arg_2.3: f32[], Arg_3.4: s32[]) -> (f32[1], s32[1]) {
  %Arg_0.1 = f32[1,10] parameter(0)
  %Arg_1.2 = s32[1,10] parameter(1)
  %Arg_2.3 = f32[] parameter(2)
  %Arg_3.4 = s32[] parameter(3)
  %reduce.13 = (f32[1], s32[1]) reduce(%Arg_0.1, %Arg_1.2, %Arg_2.3, %Arg_3.4), dimensions={1}, to_apply=%region_0.5
  %get-tuple-element.14 = f32[1] get-tuple-element(%reduce.13), index=0
  %get-tuple-element.15 = s32[1] get-tuple-element(%reduce.13), index=1
  ROOT %tuple.16 = (f32[1], s32[1]) tuple(%get-tuple-element.14, %get-tuple-element.15)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.3(%[[VAL_0:.*]]: tensor<i32>, %[[VAL_1:.*]]: tensor<i32>) -> tensor<i32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_1]] : tensor<i32>
// CHECK:           return %[[VAL_2]] : tensor<i32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<2x17x31x7xi32>) -> tensor<2x5x8x7xi32> {
// CHECK:           %[[VAL_4:.*]] = stablehlo.constant dense<-2147483648> : tensor<i32>
// CHECK:           %[[VAL_5:.*]] = "stablehlo.reduce_window"(%[[VAL_3]], %[[VAL_4]]) <{base_dilations = array<i64: 1, 1, 1, 1>, padding = dense<{{\[\[}}0, 0], [2, 0], [0, 2], [0, 0]]> : tensor<4x2xi64>, window_dilations = array<i64: 1, 2, 2, 1>, window_dimensions = array<i64: 1, 2, 2, 1>, window_strides = array<i64: 1, 4, 4, 1>}> ({
// CHECK:           ^bb0(%[[VAL_6:.*]]: tensor<i32>, %[[VAL_7:.*]]: tensor<i32>):
// CHECK:             %[[VAL_8:.*]] = stablehlo.maximum %[[VAL_6]], %[[VAL_7]] : tensor<i32>
// CHECK:             stablehlo.return %[[VAL_8]] : tensor<i32>
// CHECK:           }) : (tensor<2x17x31x7xi32>, tensor<i32>) -> tensor<2x5x8x7xi32>
// CHECK:           return %[[VAL_5]] : tensor<2x5x8x7xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[2,17,31,7]{3,2,1,0})->s32[2,5,8,7]{3,2,1,0}}

%region_0.3 (Arg_0.4: s32[], Arg_1.5: s32[]) -> s32[] {
  %Arg_0.4 = s32[] parameter(0)
  %Arg_1.5 = s32[] parameter(1)
  ROOT %maximum.6 = s32[] maximum(%Arg_0.4, %Arg_1.5)
}

ENTRY %main.8 (Arg_0.1: s32[2,17,31,7]) -> s32[2,5,8,7] {
  %Arg_0.1 = s32[2,17,31,7] parameter(0)
  %constant.2 = s32[] constant(-2147483648)
  ROOT %reduce-window.7 = s32[2,5,8,7] reduce-window(%Arg_0.1, %constant.2), window={size=1x2x2x1 stride=1x4x4x1 pad=0_0x2_0x0_2x0_0 rhs_dilate=1x2x2x1}, to_apply=%region_0.3
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xf32>) -> tensor<1x2xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.reshape %[[VAL_0]] : (tensor<2xf32>) -> tensor<1x2xf32>
// CHECK:           return %[[VAL_1]] : tensor<1x2xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2]{0})->f32[1,2]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[2]) -> f32[1,2] {
  %Arg_0.1 = f32[2] parameter(0)
  ROOT %reshape.2 = f32[1,2] reshape(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<10x11x12x13xf32>) -> tensor<10x11x12x13xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.reverse %[[VAL_0]], dims = [1, 2] : tensor<10x11x12x13xf32>
// CHECK:           return %[[VAL_1]] : tensor<10x11x12x13xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10,11,12,13]{3,2,1,0})->f32[10,11,12,13]{3,2,1,0}}

ENTRY %main.3 (Arg_0.1: f32[10,11,12,13]) -> f32[10,11,12,13] {
  %Arg_0.1 = f32[10,11,12,13] parameter(0)
  ROOT %reverse.2 = f32[10,11,12,13] reverse(%Arg_0.1), dimensions={1,2}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<2x3x5xf32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.constant dense<[2, 3, 5]> : tensor<3xi64>
// CHECK:           %[[VAL_3:.*]] = stablehlo.constant dense<[2, 3, 5]> : tensor<3xi64>
// CHECK:           %[[VAL_4:.*]] = stablehlo.rng %[[VAL_0]], %[[VAL_1]], %[[VAL_3]], distribution =  NORMAL : (tensor<f32>, tensor<f32>, tensor<3xi64>) -> tensor<2x3x5xf32>
// CHECK:           return %[[VAL_4]] : tensor<2x3x5xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[], f32[])->f32[2,3,5]{2,1,0}}

ENTRY %main.5 (Arg_0.1: f32[], Arg_1.2: f32[]) -> f32[2,3,5] {
  %constant.3 = s64[3] constant({2, 3, 5})
  %Arg_0.1 = f32[] parameter(0)
  %Arg_1.2 = f32[] parameter(1)
  ROOT %rng.4 = f32[2,3,5] rng(%Arg_0.1, %Arg_1.2), distribution=rng_normal
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main() -> tensor<2x3x5xf32> {
// CHECK:           %[[VAL_0:.*]] = stablehlo.constant dense<[2, 3, 5]> : tensor<3xi64>
// CHECK:           %[[VAL_1:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_2:.*]] = stablehlo.constant dense<1.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_3:.*]] = stablehlo.constant dense<[2, 3, 5]> : tensor<3xi64>
// CHECK:           %[[VAL_4:.*]] = stablehlo.rng %[[VAL_1]], %[[VAL_2]], %[[VAL_3]], distribution =  UNIFORM : (tensor<f32>, tensor<f32>, tensor<3xi64>) -> tensor<2x3x5xf32>
// CHECK:           return %[[VAL_4]] : tensor<2x3x5xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={()->f32[2,3,5]{2,1,0}}

ENTRY %main.5 () -> f32[2,3,5] {
  %constant.3 = s64[3] constant({2, 3, 5})
  %constant.1 = f32[] constant(0)
  %constant.2 = f32[] constant(1)
  ROOT %rng.4 = f32[2,3,5] rng(%constant.1, %constant.2), distribution=rng_uniform
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.4(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<200x100x300xf32>, %[[VAL_4:.*]]: tensor<10x2xi32>, %[[VAL_5:.*]]: tensor<10x300xf32>) -> tensor<200x100x300xf32> {
// CHECK:           %[[VAL_6:.*]] = "mhlo.scatter"(%[[VAL_3]], %[[VAL_4]], %[[VAL_5]]) <{indices_are_sorted = true, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1], inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 1>, unique_indices = true}> ({
// CHECK:           ^bb0(%[[VAL_7:.*]]: tensor<f32>, %[[VAL_8:.*]]: tensor<f32>):
// CHECK:             %[[VAL_9:.*]] = stablehlo.add %[[VAL_7]], %[[VAL_8]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_9]] : tensor<f32>
// CHECK:           }) : (tensor<200x100x300xf32>, tensor<10x2xi32>, tensor<10x300xf32>) -> tensor<200x100x300xf32>
// CHECK:           return %[[VAL_6]] : tensor<200x100x300xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[200,100,300]{2,1,0}, s32[10,2]{1,0}, f32[10,300]{1,0})->f32[200,100,300]{2,1,0}}

%region_0.4 (Arg_0.5: f32[], Arg_1.6: f32[]) -> f32[] {
  %Arg_0.5 = f32[] parameter(0)
  %Arg_1.6 = f32[] parameter(1)
  ROOT %add.7 = f32[] add(%Arg_0.5, %Arg_1.6)
}

ENTRY %main.9 (Arg_0.1: f32[200,100,300], Arg_1.2: s32[10,2], Arg_2.3: f32[10,300]) -> f32[200,100,300] {
  %Arg_0.1 = f32[200,100,300] parameter(0)
  %Arg_1.2 = s32[10,2] parameter(1)
  %Arg_2.3 = f32[10,300] parameter(2)
  ROOT %scatter.8 = f32[200,100,300] scatter(%Arg_0.1, %Arg_1.2, %Arg_2.3), update_window_dims={1}, inserted_window_dims={0,1}, scatter_dims_to_operand_dims={0,1}, index_vector_dim=1, indices_are_sorted=true, unique_indices=true, to_apply=%region_0.4
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.4(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           return %[[VAL_2]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_3:.*]]: tensor<200x100x300xf32>, %[[VAL_4:.*]]: tensor<100x200x1xi32>, %[[VAL_5:.*]]: tensor<100x200x300xf32>) -> tensor<200x100x300xf32> {
// CHECK:           %[[VAL_6:.*]] = "mhlo.scatter"(%[[VAL_3]], %[[VAL_4]], %[[VAL_5]]) <{indices_are_sorted = true, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [2], input_batching_dims = [0, 1], scatter_indices_batching_dims = [1, 0], scatter_dims_to_operand_dims = [2], index_vector_dim = 2>, unique_indices = true}> ({
// CHECK:           ^bb0(%[[VAL_7:.*]]: tensor<f32>, %[[VAL_8:.*]]: tensor<f32>):
// CHECK:             %[[VAL_9:.*]] = stablehlo.add %[[VAL_7]], %[[VAL_8]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_9]] : tensor<f32>
// CHECK:           }) : (tensor<200x100x300xf32>, tensor<100x200x1xi32>, tensor<100x200x300xf32>) -> tensor<200x100x300xf32>
// CHECK:           return %[[VAL_6]] : tensor<200x100x300xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[200,100,300]{2,1,0}, s32[100,200,1]{2,1,0}, f32[100,200,300]{2,1,0})->f32[200,100,300]{2,1,0}}

%region_0.4 (Arg_0.5: f32[], Arg_1.6: f32[]) -> f32[] {
  %Arg_0.5 = f32[] parameter(0)
  %Arg_1.6 = f32[] parameter(1)
  ROOT %add.7 = f32[] add(%Arg_0.5, %Arg_1.6)
}

ENTRY %main.9 (Arg_0.1: f32[200,100,300], Arg_1.2: s32[100,200,1], Arg_2.3: f32[100,200,300]) -> f32[200,100,300] {
  %Arg_0.1 = f32[200,100,300] parameter(0)
  %Arg_1.2 = s32[100,200,1] parameter(1)
  %Arg_2.3 = f32[100,200,300] parameter(2)
  ROOT %scatter.8 = f32[200,100,300] scatter(%Arg_0.1, %Arg_1.2, %Arg_2.3), update_window_dims={2}, inserted_window_dims={}, scatter_dims_to_operand_dims={2}, input_batching_dims={0,1}, scatter_indices_batching_dims={1,0}, index_vector_dim=2, indices_are_sorted=true, unique_indices=true, to_apply=%region_0.4
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.4(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>, %[[VAL_2:.*]]: tensor<f32>, %[[VAL_3:.*]]: tensor<f32>) -> tuple<tensor<f32>, tensor<f32>> {
// CHECK:           %[[VAL_4:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_1]] : tensor<f32>
// CHECK:           %[[VAL_5:.*]] = stablehlo.add %[[VAL_2]], %[[VAL_3]] : tensor<f32>
// CHECK:           %[[VAL_6:.*]] = mhlo.tuple %[[VAL_4]], %[[VAL_5]] {xla_shape = "(f32[], f32[])"} : tuple<tensor<f32>, tensor<f32>>
// CHECK:           return %[[VAL_6]] : tuple<tensor<f32>, tensor<f32>>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_7:.*]]: tensor<200x100x300xf32>, %[[VAL_8:.*]]: tensor<10x2xi64>, %[[VAL_9:.*]]: tensor<10x300xf32>) -> tuple<tensor<200x100x300xf32>, tensor<200x100x300xf32>> {
// CHECK:           %[[VAL_10:.*]]:2 = "mhlo.scatter"(%[[VAL_7]], %[[VAL_7]], %[[VAL_8]], %[[VAL_9]], %[[VAL_9]]) <{indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<update_window_dims = [1], inserted_window_dims = [0, 1], scatter_dims_to_operand_dims = [0, 1], index_vector_dim = 1>, unique_indices = false}> ({
// CHECK:           ^bb0(%[[VAL_11:.*]]: tensor<f32>, %[[VAL_12:.*]]: tensor<f32>, %[[VAL_13:.*]]: tensor<f32>, %[[VAL_14:.*]]: tensor<f32>):
// CHECK:             %[[VAL_15:.*]] = stablehlo.add %[[VAL_11]], %[[VAL_12]] : tensor<f32>
// CHECK:             %[[VAL_16:.*]] = stablehlo.add %[[VAL_13]], %[[VAL_14]] : tensor<f32>
// CHECK:             mhlo.return %[[VAL_15]], %[[VAL_16]] : tensor<f32>, tensor<f32>
// CHECK:           }) : (tensor<200x100x300xf32>, tensor<200x100x300xf32>, tensor<10x2xi64>, tensor<10x300xf32>, tensor<10x300xf32>) -> (tensor<200x100x300xf32>, tensor<200x100x300xf32>)
// CHECK:           %[[VAL_17:.*]] = mhlo.tuple %[[VAL_18:.*]]#0, %[[VAL_18]]#1 {xla_shape = "(f32[200,100,300]{2,1,0}, f32[200,100,300]{2,1,0})"} : tuple<tensor<200x100x300xf32>, tensor<200x100x300xf32>>
// CHECK:           return %[[VAL_17]] : tuple<tensor<200x100x300xf32>, tensor<200x100x300xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[200,100,300]{2,1,0}, s64[10,2]{1,0}, f32[10,300]{1,0})->(f32[200,100,300]{2,1,0}, f32[200,100,300]{2,1,0})}

%region_0.4 (Arg_0.5: f32[], Arg_1.6: f32[], Arg_2.7: f32[], Arg_3.8: f32[]) -> (f32[], f32[]) {
  %Arg_0.5 = f32[] parameter(0)
  %Arg_1.6 = f32[] parameter(1)
  %add.9 = f32[] add(%Arg_0.5, %Arg_1.6)
  %Arg_2.7 = f32[] parameter(2)
  %Arg_3.8 = f32[] parameter(3)
  %add.10 = f32[] add(%Arg_2.7, %Arg_3.8)
  ROOT %tuple.11 = (f32[], f32[]) tuple(%add.9, %add.10)
}

ENTRY %main.16 (Arg_0.1: f32[200,100,300], Arg_1.2: s64[10,2], Arg_2.3: f32[10,300]) -> (f32[200,100,300], f32[200,100,300]) {
  %Arg_0.1 = f32[200,100,300] parameter(0)
  %Arg_1.2 = s64[10,2] parameter(1)
  %Arg_2.3 = f32[10,300] parameter(2)
  %scatter.12 = (f32[200,100,300], f32[200,100,300]) scatter(%Arg_0.1, %Arg_0.1, %Arg_1.2, %Arg_2.3, %Arg_2.3), update_window_dims={1}, inserted_window_dims={0,1}, scatter_dims_to_operand_dims={0,1}, index_vector_dim=1, to_apply=%region_0.4
  %get-tuple-element.13 = f32[200,100,300] get-tuple-element(%scatter.12), index=0
  %get-tuple-element.14 = f32[200,100,300] get-tuple-element(%scatter.12), index=1
  ROOT %tuple.15 = (f32[200,100,300], f32[200,100,300]) tuple(%get-tuple-element.13, %get-tuple-element.14)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<i1>, %[[VAL_1:.*]]: tensor<2x3xi32>, %[[VAL_2:.*]]: tensor<2x3xi32>) -> tensor<2x3xi32> {
// CHECK:           %[[VAL_3:.*]] = stablehlo.broadcast_in_dim %[[VAL_0]], dims = [] : (tensor<i1>) -> tensor<2x3xi1>
// CHECK:           %[[VAL_4:.*]] = stablehlo.select %[[VAL_3]], %[[VAL_1]], %[[VAL_2]] : tensor<2x3xi1>, tensor<2x3xi32>
// CHECK:           return %[[VAL_4]] : tensor<2x3xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(pred[], s32[2,3]{1,0}, s32[2,3]{1,0})->s32[2,3]{1,0}}

ENTRY %main.6 (Arg_0.1: pred[], Arg_1.2: s32[2,3], Arg_2.3: s32[2,3]) -> s32[2,3] {
  %Arg_0.1 = pred[] parameter(0)
  %broadcast.4 = pred[2,3] broadcast(%Arg_0.1), dimensions={}
  %Arg_1.2 = s32[2,3] parameter(1)
  %Arg_2.3 = s32[2,3] parameter(2)
  ROOT %select.5 = s32[2,3] select(%broadcast.4, %Arg_1.2, %Arg_2.3)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.4(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<i1> {
// CHECK:           %[[VAL_2:.*]] = mhlo.compare  GE, %[[VAL_0]], %[[VAL_1]],  TOTALORDER : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:           return %[[VAL_2]] : tensor<i1>
// CHECK:         }
// CHECK:         func.func private @region_1.8(%[[VAL_3:.*]]: tensor<f32>, %[[VAL_4:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_5:.*]] = stablehlo.add %[[VAL_3]], %[[VAL_4]] : tensor<f32>
// CHECK:           return %[[VAL_5]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_6:.*]]: tensor<10x24x24x64xf32>, %[[VAL_7:.*]]: tensor<10x12x12x64xf32>) -> tensor<10x24x24x64xf32> {
// CHECK:           %[[VAL_8:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_9:.*]] = "stablehlo.select_and_scatter"(%[[VAL_6]], %[[VAL_7]], %[[VAL_8]]) <{padding = dense<0> : tensor<4x2xi64>, window_dimensions = array<i64: 1, 2, 2, 1>, window_strides = array<i64: 1, 2, 2, 1>}> ({
// CHECK:           ^bb0(%[[VAL_10:.*]]: tensor<f32>, %[[VAL_11:.*]]: tensor<f32>):
// CHECK:             %[[VAL_12:.*]] = mhlo.compare  GE, %[[VAL_10]], %[[VAL_11]],  TOTALORDER : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:             stablehlo.return %[[VAL_12]] : tensor<i1>
// CHECK:           }, {
// CHECK:           ^bb0(%[[VAL_13:.*]]: tensor<f32>, %[[VAL_14:.*]]: tensor<f32>):
// CHECK:             %[[VAL_15:.*]] = stablehlo.add %[[VAL_13]], %[[VAL_14]] : tensor<f32>
// CHECK:             stablehlo.return %[[VAL_15]] : tensor<f32>
// CHECK:           }) : (tensor<10x24x24x64xf32>, tensor<10x12x12x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
// CHECK:           return %[[VAL_9]] : tensor<10x24x24x64xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10,24,24,64]{3,2,1,0}, f32[10,12,12,64]{3,2,1,0})->f32[10,24,24,64]{3,2,1,0}}

%region_0.4 (Arg_0.5: f32[], Arg_1.6: f32[]) -> pred[] {
  %Arg_0.5 = f32[] parameter(0)
  %Arg_1.6 = f32[] parameter(1)
  ROOT %compare.7 = pred[] compare(%Arg_0.5, %Arg_1.6), direction=GE, type=TOTALORDER
}

%region_1.8 (Arg_0.9: f32[], Arg_1.10: f32[]) -> f32[] {
  %Arg_0.9 = f32[] parameter(0)
  %Arg_1.10 = f32[] parameter(1)
  ROOT %add.11 = f32[] add(%Arg_0.9, %Arg_1.10)
}

ENTRY %main.13 (Arg_0.1: f32[10,24,24,64], Arg_1.2: f32[10,12,12,64]) -> f32[10,24,24,64] {
  %Arg_0.1 = f32[10,24,24,64] parameter(0)
  %Arg_1.2 = f32[10,12,12,64] parameter(1)
  %constant.3 = f32[] constant(0)
  ROOT %select-and-scatter.12 = f32[10,24,24,64] select-and-scatter(%Arg_0.1, %Arg_1.2, %constant.3), window={size=1x2x2x1 stride=1x2x2x1}, select=%region_0.4, scatter=%region_1.8
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xi32>, %[[VAL_1:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_2:.*]] = "mhlo.send"(%[[VAL_0]], %[[VAL_1]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 2>, is_host_transfer = true}> {xla_shape = "token[]"} : (tensor<3x4xi32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_2]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3,4]{1,0}, token[])->token[]}

ENTRY %main.5 (Arg_0.1: s32[3,4], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = s32[3,4] parameter(0)
  %Arg_1.2 = token[] parameter(1)
  %send.3 = (s32[3,4], u32[], token[]) send(%Arg_0.1, %Arg_1.2), channel_id=5, is_host_transfer=true
  ROOT %send-done.4 = token[] send-done(%send.3), channel_id=5, is_host_transfer=true
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xi32>, %[[VAL_1:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_2:.*]] = "mhlo.send"(%[[VAL_0]], %[[VAL_1]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 1>, is_host_transfer = false}> {xla_shape = "token[]"} : (tensor<3x4xi32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_2]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3,4]{1,0}, token[])->token[]}

ENTRY %main.5 (Arg_0.1: s32[3,4], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = s32[3,4] parameter(0)
  %Arg_1.2 = token[] parameter(1)
  %send.3 = (s32[3,4], u32[], token[]) send(%Arg_0.1, %Arg_1.2), channel_id=5
  ROOT %send-done.4 = token[] send-done(%send.3), channel_id=5
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_1:.*]] = "mhlo.send"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 5, type = 1>, is_host_transfer = false}> {xla_shape = "token[]"} : (!mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_1]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(token[])->token[]}

ENTRY %main.5 (Arg_0.1: token[]) -> token[] {
  %tuple.2 = () tuple()
  %Arg_0.1 = token[] parameter(0)
  %send.3 = ((), u32[], token[]) send(%tuple.2, %Arg_0.1), channel_id=5
  ROOT %send-done.4 = token[] send-done(%send.3), channel_id=5
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x4xf32>, %[[VAL_1:.*]]: tensor<i32>) -> tensor<4x?xf32, #mhlo.type_extensions<bounds = [?, 4]>> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.set_dimension_size %[[VAL_0]], %[[VAL_1]], dim = 1 : (tensor<4x4xf32>, tensor<i32>) -> tensor<4x?xf32, #mhlo.type_extensions<bounds = [?, 4]>>
// CHECK:           return %[[VAL_2]] : tensor<4x?xf32, #mhlo.type_extensions<bounds = [?, 4]>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,4]{1,0}, s32[])->f32[4,<=4]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[4,4], Arg_1.2: s32[]) -> f32[4,<=4] {
  %Arg_0.1 = f32[4,4] parameter(0)
  %Arg_1.2 = s32[] parameter(1)
  ROOT %set-dimension-size.3 = f32[4,<=4] set-dimension-size(%Arg_0.1, %Arg_1.2), dimensions={1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xi32>) -> tensor<1x2xi32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.slice %[[VAL_0]] [1:2, 0:4:2] : (tensor<3x4xi32>) -> tensor<1x2xi32>
// CHECK:           return %[[VAL_1]] : tensor<1x2xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3,4]{1,0})->s32[1,2]{1,0}}

ENTRY %main.3 (Arg_0.1: s32[3,4]) -> s32[1,2] {
  %Arg_0.1 = s32[3,4] parameter(0)
  ROOT %slice.2 = s32[1,2] slice(%Arg_0.1), slice={[1:2:1], [0:4:2]}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xi32>, %[[VAL_1:.*]]: tensor<i64>, %[[VAL_2:.*]]: tensor<i64>) -> tensor<1x4xi32> {
// CHECK:           %[[VAL_3:.*]] = stablehlo.dynamic_slice %[[VAL_0]], %[[VAL_1]], %[[VAL_2]], sizes = [1, 4] : (tensor<3x4xi32>, tensor<i64>, tensor<i64>) -> tensor<1x4xi32>
// CHECK:           return %[[VAL_3]] : tensor<1x4xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3,4]{1,0}, s64[], s64[])->s32[1,4]{1,0}}

ENTRY %main.5 (Arg_0.1: s32[3,4], Arg_1.2: s64[], Arg_2.3: s64[]) -> s32[1,4] {
  %Arg_0.1 = s32[3,4] parameter(0)
  %Arg_1.2 = s64[] parameter(1)
  %Arg_2.3 = s64[] parameter(2)
  ROOT %dynamic-slice.4 = s32[1,4] dynamic-slice(%Arg_0.1, %Arg_1.2, %Arg_2.3), dynamic_slice_sizes={1,4}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, mhlo.xla_entry_computation_result_layout = [dense<[2, 3, 0, 1]> : tensor<4xindex>], mhlo.xla_entry_computation_result_tiles = {{\[\[}}]]} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<1x2x3x4xi32>) -> tensor<2x1x4x3xi32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.transpose %[[VAL_0]], dims = [1, 0, 3, 2] : (tensor<1x2x3x4xi32>) -> tensor<2x1x4x3xi32>
// CHECK:           return %[[VAL_1]] : tensor<2x1x4x3xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[1,2,3,4]{3,2,1,0})->s32[2,1,4,3]{2,3,0,1}}

ENTRY %main.3 (Arg_0.1: s32[1,2,3,4]) -> s32[2,1,4,3] {
  %Arg_0.1 = s32[1,2,3,4] parameter(0)
  ROOT %transpose.2 = s32[2,1,4,3] transpose(%Arg_0.1), dimensions={1,0,3,2}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x4xf32>, %[[VAL_1:.*]]: tensor<4x3xf32>) -> tensor<4x3xf32> {
// CHECK:           %[[VAL_2:.*]] = "stablehlo.triangular_solve"(%[[VAL_0]], %[[VAL_1]]) <{left_side = true, lower = true, transpose_a = #stablehlo<transpose NO_TRANSPOSE>, unit_diagonal = true}> : (tensor<4x4xf32>, tensor<4x3xf32>) -> tensor<4x3xf32>
// CHECK:           return %[[VAL_2]] : tensor<4x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,4]{1,0}, f32[4,3]{1,0})->f32[4,3]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[4,4], Arg_1.2: f32[4,3]) -> f32[4,3] {
  %Arg_0.1 = f32[4,4] parameter(0)
  %Arg_1.2 = f32[4,3] parameter(1)
  ROOT %triangular-solve.3 = f32[4,3] triangular-solve(%Arg_0.1, %Arg_1.2), left_side=true, lower=true, unit_diagonal=true, transpose_a=NO_TRANSPOSE
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<i32>) -> tuple<tensor<f32>, tensor<i32>> {
// CHECK:           %[[VAL_2:.*]] = mhlo.tuple %[[VAL_0]], %[[VAL_1]] {xla_shape = "(f32[], s32[])"} : tuple<tensor<f32>, tensor<i32>>
// CHECK:           return %[[VAL_2]] : tuple<tensor<f32>, tensor<i32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[], s32[])->(f32[], s32[])}

ENTRY %main.4 (Arg_0.1: f32[], Arg_1.2: s32[]) -> (f32[], s32[]) {
  %Arg_0.1 = f32[] parameter(0)
  %Arg_1.2 = s32[] parameter(1)
  ROOT %tuple.3 = (f32[], s32[]) tuple(%Arg_0.1, %Arg_1.2)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xf32>, %[[VAL_1:.*]]: tensor<4xi32>) -> tuple<tensor<4xf32>, tensor<4xf32>, tensor<4xi32>, tensor<4xi32>> {
// CHECK:           %[[VAL_2:.*]] = mhlo.exponential_minus_one %[[VAL_0]] : tensor<4xf32>
// CHECK:           %[[VAL_3:.*]] = mhlo.log_plus_one %[[VAL_0]] : tensor<4xf32>
// CHECK:           %[[VAL_4:.*]] = stablehlo.not %[[VAL_1]] : tensor<4xi32>
// CHECK:           %[[VAL_5:.*]] = stablehlo.popcnt %[[VAL_1]] : tensor<4xi32>
// CHECK:           %[[VAL_6:.*]] = mhlo.tuple %[[VAL_2]], %[[VAL_3]], %[[VAL_4]], %[[VAL_5]] {xla_shape = "(f32[4]{0}, f32[4]{0}, s32[4]{0}, s32[4]{0})"} : tuple<tensor<4xf32>, tensor<4xf32>, tensor<4xi32>, tensor<4xi32>>
// CHECK:           return %[[VAL_6]] : tuple<tensor<4xf32>, tensor<4xf32>, tensor<4xi32>, tensor<4xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4]{0}, s32[4]{0})->(f32[4]{0}, f32[4]{0}, s32[4]{0}, s32[4]{0})}

ENTRY %main.8 (Arg_0.1: f32[4], Arg_1.2: s32[4]) -> (f32[4], f32[4], s32[4], s32[4]) {
  %Arg_0.1 = f32[4] parameter(0)
  %exponential-minus-one.3 = f32[4] exponential-minus-one(%Arg_0.1)
  %log-plus-one.4 = f32[4] log-plus-one(%Arg_0.1)
  %Arg_1.2 = s32[4] parameter(1)
  %not.5 = s32[4] not(%Arg_1.2)
  %popcnt.6 = s32[4] popcnt(%Arg_1.2)
  ROOT %tuple.7 = (f32[4], f32[4], s32[4], s32[4]) tuple(%exponential-minus-one.3, %log-plus-one.4, %not.5, %popcnt.6)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xi1>, %[[VAL_1:.*]]: tensor<4xi1>) -> tensor<4xi1> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.xor %[[VAL_0]], %[[VAL_1]] : tensor<4xi1>
// CHECK:           return %[[VAL_2]] : tensor<4xi1>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(pred[4]{0}, pred[4]{0})->pred[4]{0}}

ENTRY %main.4 (Arg_0.1: pred[4], Arg_1.2: pred[4]) -> pred[4] {
  %Arg_0.1 = pred[4] parameter(0)
  %Arg_1.2 = pred[4] parameter(1)
  ROOT %xor.3 = pred[4] xor(%Arg_0.1, %Arg_1.2)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<16x16xf32>, %[[VAL_1:.*]]: tensor<16x16xi32>) -> tuple<> {
// CHECK:           %[[VAL_2:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_2]] : tuple<>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[16,16]{1,0}, s32[16,16]{1,0})->()}

ENTRY %main.4 (Arg_0.1: f32[16,16], Arg_1.2: s32[16,16]) -> () {
  %Arg_0.1 = f32[16,16] parameter(0)
  %Arg_1.2 = s32[16,16] parameter(1)
  ROOT %tuple.3 = () tuple()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<16x16xf32>) -> tuple<> {
// CHECK:           %[[VAL_1:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_1]] : tuple<>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[16,16]{1,0})->()}

ENTRY %main.3 (Arg_0.1: f32[16,16]) -> () {
  %Arg_0.1 = f32[16,16] parameter(0)
  ROOT %tuple.2 = () tuple()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<16x16xf32>) -> (tensor<16x16xf32> {mhlo.sharding = "{devices=[1,2]0,1}"}) {
// CHECK:           %[[VAL_1:.*]] = mhlo.custom_call @Sharding(%[[VAL_0]]) {backend_config = "", mhlo.sharding = "{devices=[1,2]0,1}"} : (tensor<16x16xf32>) -> tensor<16x16xf32>
// CHECK:           return %[[VAL_1]] : tensor<16x16xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[16,16]{1,0})->f32[16,16]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[16,16]) -> f32[16,16] {
  %Arg_0.1 = f32[16,16] parameter(0)
  ROOT %custom-call.2 = f32[16,16] custom-call(%Arg_0.1), custom_call_target="Sharding", sharding={devices=[1,2]0,1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @foo.3(%[[VAL_0:.*]]: tensor<2x3xf32>, %[[VAL_1:.*]]: tensor<5x5xf32>) -> tensor<2x3xf32> {
// CHECK:           return %[[VAL_0]] : tensor<2x3xf32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_2:.*]]: tensor<2x3xf32>, %[[VAL_3:.*]]: tensor<5x5xf32>) -> tensor<2x3xf32> {
// CHECK:           %[[VAL_4:.*]] = mhlo.custom_call @foo(%[[VAL_2]], %[[VAL_3]]) {backend_config = "", called_computations = [@foo.3]} : (tensor<2x3xf32>, tensor<5x5xf32>) -> tensor<2x3xf32>
// CHECK:           return %[[VAL_4]] : tensor<2x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0}, f32[5,5]{1,0})->f32[2,3]{1,0}}

%foo.3 (Arg_0.4: f32[2,3], Arg_1.5: f32[5,5]) -> f32[2,3] {
  ROOT %Arg_0.4 = f32[2,3] parameter(0)
  %Arg_1.5 = f32[5,5] parameter(1)
}

ENTRY %main.7 (Arg_0.1: f32[2,3], Arg_1.2: f32[5,5]) -> f32[2,3] {
  %Arg_0.1 = f32[2,3] parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  ROOT %custom-call.6 = f32[2,3] custom-call(%Arg_0.1, %Arg_1.2), custom_call_target="foo", called_computations={%foo.3}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xcomplex<f32>>, %[[VAL_1:.*]]: tensor<2xcomplex<f64>>) -> tuple<tensor<2xf32>, tensor<2xf64>> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.abs %[[VAL_0]] : (tensor<2xcomplex<f32>>) -> tensor<2xf32>
// CHECK:           %[[VAL_3:.*]] = stablehlo.abs %[[VAL_1]] : (tensor<2xcomplex<f64>>) -> tensor<2xf64>
// CHECK:           %[[VAL_4:.*]] = mhlo.tuple %[[VAL_2]], %[[VAL_3]] {xla_shape = "(f32[2]{0}, f64[2]{0})"} : tuple<tensor<2xf32>, tensor<2xf64>>
// CHECK:           return %[[VAL_4]] : tuple<tensor<2xf32>, tensor<2xf64>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(c64[2]{0}, c128[2]{0})->(f32[2]{0}, f64[2]{0})}

ENTRY %main.6 (Arg_0.1: c64[2], Arg_1.2: c128[2]) -> (f32[2], f64[2]) {
  %Arg_0.1 = c64[2] parameter(0)
  %abs.3 = f32[2] abs(%Arg_0.1)
  %Arg_1.2 = c128[2] parameter(1)
  %abs.4 = f64[2] abs(%Arg_1.2)
  ROOT %tuple.5 = (f32[2], f64[2]) tuple(%abs.3, %abs.4)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xui8>) -> tensor<4xui8> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.not %[[VAL_0]] : tensor<4xui8>
// CHECK:           return %[[VAL_1]] : tensor<4xui8>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(u8[4]{0})->u8[4]{0}}

ENTRY %main.3 (Arg_0.1: u8[4]) -> u8[4] {
  %Arg_0.1 = u8[4] parameter(0)
  ROOT %not.2 = u8[4] not(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xi32>) -> tensor<4xi32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.not %[[VAL_0]] : tensor<4xi32>
// CHECK:           return %[[VAL_1]] : tensor<4xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[4]{0})->s32[4]{0}}

ENTRY %main.3 (Arg_0.1: s32[4]) -> s32[4] {
  %Arg_0.1 = s32[4] parameter(0)
  ROOT %not.2 = s32[4] not(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4xf32>, %[[VAL_1:.*]]: tensor<i32>) -> tensor<?xf32, #mhlo.type_extensions<bounds = [4]>> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.set_dimension_size %[[VAL_0]], %[[VAL_1]], dim = 0 : (tensor<4xf32>, tensor<i32>) -> tensor<?xf32, #mhlo.type_extensions<bounds = [4]>>
// CHECK:           return %[[VAL_2]] : tensor<?xf32, #mhlo.type_extensions<bounds = [4]>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4]{0}, s32[])->f32[<=4]{0}}

ENTRY %main.4 (Arg_0.1: f32[4], Arg_1.2: s32[]) -> f32[<=4] {
  %Arg_0.1 = f32[4] parameter(0)
  %Arg_1.2 = s32[] parameter(1)
  ROOT %set-dimension-size.3 = f32[<=4] set-dimension-size(%Arg_0.1, %Arg_1.2), dimensions={0}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>, %[[VAL_1:.*]]: !mhlo.token) -> tuple<tensor<3x4xf32>, !mhlo.token> {
// CHECK:           %[[VAL_2:.*]] = "mhlo.send"(%[[VAL_0]], %[[VAL_1]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 2>, is_host_transfer = true}> {mhlo.frontend_attributes = {_xla_host_transfer_rendezvous = "channel_dtoh_0"}, xla_shape = "token[]"} : (tensor<3x4xf32>, !mhlo.token) -> !mhlo.token
// CHECK:           %[[VAL_3:.*]]:2 = "mhlo.recv"(%[[VAL_2]]) <{channel_handle = #mhlo.channel_handle<handle = 2, type = 3>, is_host_transfer = true}> {mhlo.frontend_attributes = {_xla_host_transfer_rendezvous = "channel_htod_0"}} : (!mhlo.token) -> (tensor<3x4xf32>, !mhlo.token)
// CHECK:           %[[VAL_4:.*]] = mhlo.tuple %[[VAL_3]]#0, %[[VAL_3]]#1 {xla_shape = "(f32[3,4]{1,0}, token[])"} : tuple<tensor<3x4xf32>, !mhlo.token>
// CHECK:           return %[[VAL_4]] : tuple<tensor<3x4xf32>, !mhlo.token>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0}, token[])->(f32[3,4]{1,0}, token[])}

ENTRY %main.10 (Arg_0.1: f32[3,4], Arg_1.2: token[]) -> (f32[3,4], token[]) {
  %Arg_0.1 = f32[3,4] parameter(0)
  %Arg_1.2 = token[] parameter(1)
  %send.3 = (f32[3,4], u32[], token[]) send(%Arg_0.1, %Arg_1.2), channel_id=1, is_host_transfer=true, frontend_attributes={_xla_host_transfer_rendezvous="channel_dtoh_0"}
  %send-done.4 = token[] send-done(%send.3), channel_id=1, is_host_transfer=true, frontend_attributes={_xla_host_transfer_rendezvous="channel_dtoh_0"}
  %recv.5 = (f32[3,4], u32[], token[]) recv(%send-done.4), channel_id=2, is_host_transfer=true, frontend_attributes={_xla_host_transfer_rendezvous="channel_htod_0"}
  %recv-done.6 = (f32[3,4], token[]) recv-done(%recv.5), channel_id=2, is_host_transfer=true, frontend_attributes={_xla_host_transfer_rendezvous="channel_htod_0"}
  %get-tuple-element.7 = f32[3,4] get-tuple-element(%recv-done.6), index=0, frontend_attributes={_xla_host_transfer_rendezvous="channel_htod_0"}
  %get-tuple-element.8 = token[] get-tuple-element(%recv-done.6), index=1, frontend_attributes={_xla_host_transfer_rendezvous="channel_htod_0"}
  ROOT %tuple.9 = (f32[3,4], token[]) tuple(%get-tuple-element.7, %get-tuple-element.8)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>, %[[VAL_1:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_2:.*]] = "mhlo.send"(%[[VAL_0]], %[[VAL_1]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 2>, is_host_transfer = true}> {xla_shape = "token[]"} : (tensor<3x4xf32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_2]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0}, token[])->token[]}

ENTRY %main.5 (Arg_0.1: f32[3,4], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = f32[3,4] parameter(0)
  %Arg_1.2 = token[] parameter(1)
  %send.3 = (f32[3,4], u32[], token[]) send(%Arg_0.1, %Arg_1.2), channel_id=1, is_host_transfer=true
  ROOT %send-done.4 = token[] send-done(%send.3), channel_id=1, is_host_transfer=true
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>, %[[VAL_1:.*]]: !mhlo.token) -> !mhlo.token {
// CHECK:           %[[VAL_2:.*]] = "mhlo.send"(%[[VAL_0]], %[[VAL_1]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 2>, is_host_transfer = true}> {xla_shape = "token[]"} : (tensor<3x4xf32>, !mhlo.token) -> !mhlo.token
// CHECK:           return %[[VAL_2]] : !mhlo.token
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0}, token[])->token[]}

ENTRY %main.5 (Arg_0.1: f32[3,4], Arg_1.2: token[]) -> token[] {
  %Arg_0.1 = f32[3,4] parameter(0)
  %Arg_1.2 = token[] parameter(1)
  %send.3 = (f32[3,4], u32[], token[]) send(%Arg_0.1, %Arg_1.2), channel_id=1, is_host_transfer=true
  ROOT %send-done.4 = token[] send-done(%send.3), channel_id=1, is_host_transfer=true
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3xui64>) -> tuple<tensor<3xui64>, tensor<2x2xui32>> {
// CHECK:           %[[VAL_1:.*]], %[[VAL_2:.*]] = stablehlo.rng_bit_generator %[[VAL_0]], algorithm =  PHILOX : (tensor<3xui64>) -> (tensor<3xui64>, tensor<2x2xui32>)
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple %[[VAL_1]], %[[VAL_2]] {xla_shape = "(u64[3]{0}, u32[2,2]{1,0})"} : tuple<tensor<3xui64>, tensor<2x2xui32>>
// CHECK:           return %[[VAL_3]] : tuple<tensor<3xui64>, tensor<2x2xui32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(u64[3]{0})->(u64[3]{0}, u32[2,2]{1,0})}

ENTRY %main.6 (Arg_0.1: u64[3]) -> (u64[3], u32[2,2]) {
  %Arg_0.1 = u64[3] parameter(0)
  %rng-bit-generator.2 = (u64[3], u32[2,2]) rng-bit-generator(%Arg_0.1), algorithm=rng_philox
  %get-tuple-element.3 = u64[3] get-tuple-element(%rng-bit-generator.2), index=0
  %get-tuple-element.4 = u32[2,2] get-tuple-element(%rng-bit-generator.2), index=1
  ROOT %tuple.5 = (u64[3], u32[2,2]) tuple(%get-tuple-element.3, %get-tuple-element.4)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>) -> tensor<3x4xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.cbrt %[[VAL_0]] : tensor<3x4xf32>
// CHECK:           return %[[VAL_1]] : tensor<3x4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0})->f32[3,4]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[3,4]) -> f32[3,4] {
  %Arg_0.1 = f32[3,4] parameter(0)
  ROOT %cbrt.2 = f32[3,4] cbrt(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>) -> tensor<3x4xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.reduce_precision %[[VAL_0]], format = e8m10 : tensor<3x4xf32>
// CHECK:           return %[[VAL_1]] : tensor<3x4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0})->f32[3,4]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[3,4]) -> f32[3,4] {
  %Arg_0.1 = f32[3,4] parameter(0)
  ROOT %reduce-precision.2 = f32[3,4] reduce-precision(%Arg_0.1), exponent_bits=8, mantissa_bits=10
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>) -> tensor<3x4x1xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.bitcast %[[VAL_0]] {result_layout = dense<[2, 1, 0]> : tensor<3xindex>, source_layout = dense<[1, 0]> : tensor<2xindex>} : (tensor<3x4xf32>) -> tensor<3x4x1xf32>
// CHECK:           return %[[VAL_1]] : tensor<3x4x1xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0})->f32[3,4,1]{2,1,0}}

ENTRY %main.3 (Arg_0.1: f32[3,4]) -> f32[3,4,1] {
  %Arg_0.1 = f32[3,4] parameter(0)
  ROOT %bitcast.2 = f32[3,4,1] bitcast(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x4xf32>, %[[VAL_1:.*]]: tensor<3x4xf32>) -> tuple<tensor<4x4xf32>, tensor<3x4xf32>> {
// CHECK:           %[[VAL_2:.*]]:2 = stablehlo.optimization_barrier %[[VAL_0]], %[[VAL_1]] : tensor<4x4xf32>, tensor<3x4xf32>
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple %[[VAL_2]]#0, %[[VAL_2]]#1 {xla_shape = "(f32[4,4]{1,0}, f32[3,4]{1,0})"} : tuple<tensor<4x4xf32>, tensor<3x4xf32>>
// CHECK:           return %[[VAL_3]] : tuple<tensor<4x4xf32>, tensor<3x4xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,4]{1,0}, f32[3,4]{1,0})->(f32[4,4]{1,0}, f32[3,4]{1,0})}

ENTRY %main.8 (Arg_0.1: f32[4,4], Arg_1.2: f32[3,4]) -> (f32[4,4], f32[3,4]) {
  %Arg_0.1 = f32[4,4] parameter(0)
  %Arg_1.2 = f32[3,4] parameter(1)
  %tuple.3 = (f32[4,4], f32[3,4]) tuple(%Arg_0.1, %Arg_1.2), sharding={{replicated}, {devices=[1,2]<=[2]}}
  %opt-barrier.4 = (f32[4,4], f32[3,4]) opt-barrier(%tuple.3), sharding={{replicated}, {devices=[1,2]<=[2]}}
  %get-tuple-element.5 = f32[4,4] get-tuple-element(%opt-barrier.4), index=0, sharding={replicated}
  %get-tuple-element.6 = f32[3,4] get-tuple-element(%opt-barrier.4), index=1, sharding={devices=[1,2]<=[2]}
  ROOT %tuple.7 = (f32[4,4], f32[3,4]) tuple(%get-tuple-element.5, %get-tuple-element.6)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main() -> tensor<ui32> {
// CHECK:           %[[VAL_0:.*]] = stablehlo.partition_id : tensor<ui32>
// CHECK:           return %[[VAL_0]] : tensor<ui32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={()->u32[]}

ENTRY %main.2 () -> u32[] {
  ROOT %partition-id.1 = u32[] partition-id()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<ui32>) -> tensor<ui32> {
// CHECK:           %[[VAL_1:.*]] = "mhlo.domain"(%[[VAL_0]]) <{entry_metadata = "{maximal device=1}", exit_metadata = "{}", kind = #mhlo<kind sharding>}> : (tensor<ui32>) -> tensor<ui32>
// CHECK:           return %[[VAL_1]] : tensor<ui32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(u32[])->u32[]}

ENTRY %main.3 (Arg_0.1: u32[]) -> u32[] {
  %Arg_0.1 = u32[] parameter(0)
  ROOT %domain.2 = u32[] domain(%Arg_0.1), domain={kind="sharding", entry={maximal device=1}, exit={}}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x4xf32>, %[[VAL_1:.*]]: tensor<3x4xf32>) -> tensor<3x4xf32> {
// CHECK:           %[[VAL_2:.*]] = "stablehlo.triangular_solve"(%[[VAL_0]], %[[VAL_1]]) <{left_side = false, lower = true, transpose_a = #stablehlo<transpose NO_TRANSPOSE>, unit_diagonal = false}> : (tensor<4x4xf32>, tensor<3x4xf32>) -> tensor<3x4xf32>
// CHECK:           return %[[VAL_2]] : tensor<3x4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,4]{1,0}, f32[3,4]{1,0})->f32[3,4]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[4,4], Arg_1.2: f32[3,4]) -> f32[3,4] {
  %Arg_0.1 = f32[4,4] parameter(0)
  %Arg_1.2 = f32[3,4] parameter(1)
  ROOT %triangular-solve.3 = f32[3,4] triangular-solve(%Arg_0.1, %Arg_1.2), lower=true, transpose_a=NO_TRANSPOSE
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.5(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<i32>, %[[VAL_2:.*]]: tensor<f32>, %[[VAL_3:.*]]: tensor<i32>) -> tuple<tensor<f32>, tensor<i32>> {
// CHECK:           %[[VAL_4:.*]] = stablehlo.add %[[VAL_0]], %[[VAL_2]] : tensor<f32>
// CHECK:           %[[VAL_5:.*]] = stablehlo.add %[[VAL_1]], %[[VAL_3]] : tensor<i32>
// CHECK:           %[[VAL_6:.*]] = mhlo.tuple %[[VAL_4]], %[[VAL_5]] {xla_shape = "(f32[], s32[])"} : tuple<tensor<f32>, tensor<i32>>
// CHECK:           return %[[VAL_6]] : tuple<tensor<f32>, tensor<i32>>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_7:.*]]: tensor<4x2xf32>, %[[VAL_8:.*]]: tensor<4x2xi32>, %[[VAL_9:.*]]: tensor<f32>, %[[VAL_10:.*]]: tensor<i32>) -> tuple<tensor<2x2xf32>, tensor<2x2xi32>> {
// CHECK:           %[[VAL_11:.*]]:2 = "stablehlo.reduce_window"(%[[VAL_7]], %[[VAL_8]], %[[VAL_9]], %[[VAL_10]]) <{base_dilations = array<i64: 1, 1>, padding = dense<{{\[\[}}2, 2], [0, 0]]> : tensor<2x2xi64>, window_dilations = array<i64: 1, 1>, window_dimensions = array<i64: 5, 1>, window_strides = array<i64: 3, 1>}> ({
// CHECK:           ^bb0(%[[VAL_12:.*]]: tensor<f32>, %[[VAL_13:.*]]: tensor<i32>, %[[VAL_14:.*]]: tensor<f32>, %[[VAL_15:.*]]: tensor<i32>):
// CHECK:             %[[VAL_16:.*]] = stablehlo.add %[[VAL_12]], %[[VAL_14]] : tensor<f32>
// CHECK:             %[[VAL_17:.*]] = stablehlo.add %[[VAL_13]], %[[VAL_15]] : tensor<i32>
// CHECK:             stablehlo.return %[[VAL_16]], %[[VAL_17]] : tensor<f32>, tensor<i32>
// CHECK:           }) : (tensor<4x2xf32>, tensor<4x2xi32>, tensor<f32>, tensor<i32>) -> (tensor<2x2xf32>, tensor<2x2xi32>)
// CHECK:           %[[VAL_18:.*]] = mhlo.tuple %[[VAL_19:.*]]#0, %[[VAL_19]]#1 {xla_shape = "(f32[2,2]{1,0}, s32[2,2]{1,0})"} : tuple<tensor<2x2xf32>, tensor<2x2xi32>>
// CHECK:           return %[[VAL_18]] : tuple<tensor<2x2xf32>, tensor<2x2xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,2]{1,0}, s32[4,2]{1,0}, f32[], s32[])->(f32[2,2]{1,0}, s32[2,2]{1,0})}

%region_0.5 (Arg_0.6: f32[], Arg_1.7: s32[], Arg_2.8: f32[], Arg_3.9: s32[]) -> (f32[], s32[]) {
  %Arg_0.6 = f32[] parameter(0)
  %Arg_2.8 = f32[] parameter(2)
  %add.10 = f32[] add(%Arg_0.6, %Arg_2.8)
  %Arg_1.7 = s32[] parameter(1)
  %Arg_3.9 = s32[] parameter(3)
  %add.11 = s32[] add(%Arg_1.7, %Arg_3.9)
  ROOT %tuple.12 = (f32[], s32[]) tuple(%add.10, %add.11)
}

ENTRY %main.17 (Arg_0.1: f32[4,2], Arg_1.2: s32[4,2], Arg_2.3: f32[], Arg_3.4: s32[]) -> (f32[2,2], s32[2,2]) {
  %Arg_0.1 = f32[4,2] parameter(0)
  %Arg_1.2 = s32[4,2] parameter(1)
  %Arg_2.3 = f32[] parameter(2)
  %Arg_3.4 = s32[] parameter(3)
  %reduce-window.13 = (f32[2,2], s32[2,2]) reduce-window(%Arg_0.1, %Arg_1.2, %Arg_2.3, %Arg_3.4), window={size=5x1 stride=3x1 pad=2_2x0_0}, to_apply=%region_0.5
  %get-tuple-element.14 = f32[2,2] get-tuple-element(%reduce-window.13), index=0
  %get-tuple-element.15 = s32[2,2] get-tuple-element(%reduce-window.13), index=1
  ROOT %tuple.16 = (f32[2,2], s32[2,2]) tuple(%get-tuple-element.14, %get-tuple-element.15)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xf32>) -> tensor<2xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.round_nearest_even %[[VAL_0]] : tensor<2xf32>
// CHECK:           return %[[VAL_1]] : tensor<2xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2]{0})->f32[2]{0}}

ENTRY %main.3 (Arg_0.1: f32[2]) -> f32[2] {
  %Arg_0.1 = f32[2] parameter(0)
  ROOT %round-nearest-even.2 = f32[2] round-nearest-even(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2xf32>) -> tensor<2xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.tan %[[VAL_0]] : tensor<2xf32>
// CHECK:           return %[[VAL_1]] : tensor<2xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2]{0})->f32[2]{0}}

ENTRY %main.3 (Arg_0.1: f32[2]) -> f32[2] {
  %Arg_0.1 = f32[2] parameter(0)
  ROOT %tan.2 = f32[2] tan(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<4x4xf32>) -> tuple<tensor<4x2xf32>, tensor<4x2xi32>> {
// CHECK:           %[[VAL_1:.*]], %[[VAL_2:.*]] = mhlo.topk(%[[VAL_0]], k = 2) : tensor<4x4xf32> -> (tensor<4x2xf32>, tensor<4x2xi32>)
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple %[[VAL_1]], %[[VAL_2]] {xla_shape = "(f32[4,2]{1,0}, s32[4,2]{1,0})"} : tuple<tensor<4x2xf32>, tensor<4x2xi32>>
// CHECK:           return %[[VAL_3]] : tuple<tensor<4x2xf32>, tensor<4x2xi32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,4]{1,0})->(f32[4,2]{1,0}, s32[4,2]{1,0})}

ENTRY %main.6 (Arg_0.1: f32[4,4]) -> (f32[4,2], s32[4,2]) {
  %Arg_0.1 = f32[4,4] parameter(0)
  %topk.2 = (f32[4,2], s32[4,2]) topk(%Arg_0.1), k=2, largest=true
  %get-tuple-element.3 = f32[4,2] get-tuple-element(%topk.2), index=0
  %get-tuple-element.4 = s32[4,2] get-tuple-element(%topk.2), index=1
  ROOT %tuple.5 = (f32[4,2], s32[4,2]) tuple(%get-tuple-element.3, %get-tuple-element.4)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tuple<tensor<1x1xf32>, tensor<2x3xf32>>, %[[VAL_1:.*]]: tensor<5x5xf32>) -> tuple<> {
// CHECK:           %[[VAL_2:.*]] = mhlo.custom_call @foo(%[[VAL_0]], %[[VAL_1]]) {backend_config = "", output_operand_aliases = [#mhlo.output_operand_alias<output_tuple_indices = [0], operand_index = 0, operand_tuple_indices = [1]>], xla_shape = "(f32[2,3]{1,0})"} : (tuple<tensor<1x1xf32>, tensor<2x3xf32>>, tensor<5x5xf32>) -> tuple<tensor<2x3xf32>>
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_3]] : tuple<>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={((f32[1,1]{1,0}, f32[2,3]{1,0}), f32[5,5]{1,0})->()}

ENTRY %main.5 (Arg_0.1: (f32[1,1], f32[2,3]), Arg_1.2: f32[5,5]) -> () {
  %Arg_0.1 = (f32[1,1], f32[2,3]) parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  %custom-call.3 = (f32[2,3]) custom-call(%Arg_0.1, %Arg_1.2), custom_call_target="foo", output_to_operand_aliasing={{0}: (0, {1})}
  ROOT %tuple.4 = () tuple()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tuple<tensor<1x1xf32>, tensor<2x3xf32>>, %[[VAL_1:.*]]: tensor<5x5xf32>) -> tuple<> {
// CHECK:           %[[VAL_2:.*]] = mhlo.custom_call @foo(%[[VAL_0]], %[[VAL_1]]) {backend_config = "", output_operand_aliases = [#mhlo.output_operand_alias<output_tuple_indices = [], operand_index = 0, operand_tuple_indices = [1]>]} : (tuple<tensor<1x1xf32>, tensor<2x3xf32>>, tensor<5x5xf32>) -> tensor<2x3xf32>
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_3]] : tuple<>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={((f32[1,1]{1,0}, f32[2,3]{1,0}), f32[5,5]{1,0})->()}

ENTRY %main.5 (Arg_0.1: (f32[1,1], f32[2,3]), Arg_1.2: f32[5,5]) -> () {
  %Arg_0.1 = (f32[1,1], f32[2,3]) parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  %custom-call.3 = f32[2,3] custom-call(%Arg_0.1, %Arg_1.2), custom_call_target="foo", output_to_operand_aliasing={{}: (0, {1})}
  ROOT %tuple.4 = () tuple()
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>) -> tensor<3x4xf32> {
// CHECK:           %[[VAL_1:.*]] = mhlo.create_token {xla_shape = "token[]"} : !mhlo.token
// CHECK:           %[[VAL_2:.*]] = mhlo.add_dependency %[[VAL_0]], %[[VAL_1]] : (tensor<3x4xf32>, !mhlo.token) -> tensor<3x4xf32>
// CHECK:           return %[[VAL_2]] : tensor<3x4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0})->f32[3,4]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[3,4]) -> f32[3,4] {
  %Arg_0.1 = f32[3,4] parameter(0)
  %after-all.2 = token[] after-all()
  ROOT %add-dependency.3 = f32[3,4] add-dependency(%Arg_0.1, %after-all.2)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<3x4xf32>) -> tensor<3x4xf32> attributes {execution_thread = "test_thread"} {
// CHECK:           %[[VAL_1:.*]] = mhlo.create_token {xla_shape = "token[]"} : !mhlo.token
// CHECK:           %[[VAL_2:.*]] = mhlo.add_dependency %[[VAL_0]], %[[VAL_1]] : (tensor<3x4xf32>, !mhlo.token) -> tensor<3x4xf32>
// CHECK:           return %[[VAL_2]] : tensor<3x4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[3,4]{1,0})->f32[3,4]{1,0}}

ENTRY %main.4 (Arg_0.1: f32[3,4]) -> f32[3,4] {
  %Arg_0.1 = f32[3,4] parameter(0)
  %after-all.2 = token[] after-all()
  ROOT %add-dependency.3 = f32[3,4] add-dependency(%Arg_0.1, %after-all.2)
}, execution_thread="test_thread"

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x2xi32>) -> tensor<2x2xi32> {
// CHECK:           %[[VAL_1:.*]] = "mhlo.all_to_all"(%[[VAL_0]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, concat_dimension = 1 : i64, replica_groups = dense<{{\[\[}}1, 2], [0, 3]]> : tensor<2x2xi64>, split_count = 2 : i64, split_dimension = 1 : i64}> : (tensor<2x2xi32>) -> tensor<2x2xi32>
// CHECK:           return %[[VAL_1]] : tensor<2x2xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}

ENTRY %main.3 (Arg_0.1: s32[2,2]) -> s32[2,2] {
  %Arg_0.1 = s32[2,2] parameter(0)
  ROOT %all-to-all.2 = s32[2,2] all-to-all(%Arg_0.1), channel_id=1, replica_groups={{1,2},{0,3}}, dimensions={1}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<128x4xf32>, %[[VAL_1:.*]]: tensor<128x4xf32>) -> tuple<tensor<128x4xf32>, tensor<128x4xf32>> {
// CHECK:           %[[VAL_2:.*]]:2 = "mhlo.all_to_all"(%[[VAL_0]], %[[VAL_1]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\[\[}}0, 1]]> : tensor<1x2xi64>}> : (tensor<128x4xf32>, tensor<128x4xf32>) -> (tensor<128x4xf32>, tensor<128x4xf32>)
// CHECK:           %[[VAL_3:.*]] = mhlo.tuple %[[VAL_2]]#0, %[[VAL_2]]#1 {xla_shape = "(f32[128,4]{1,0}, f32[128,4]{1,0})"} : tuple<tensor<128x4xf32>, tensor<128x4xf32>>
// CHECK:           return %[[VAL_3]] : tuple<tensor<128x4xf32>, tensor<128x4xf32>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[128,4]{1,0}, f32[128,4]{1,0})->(f32[128,4]{1,0}, f32[128,4]{1,0})}

ENTRY %main.7 (Arg_0.1: f32[128,4], Arg_1.2: f32[128,4]) -> (f32[128,4], f32[128,4]) {
  %Arg_0.1 = f32[128,4] parameter(0)
  %Arg_1.2 = f32[128,4] parameter(1)
  %all-to-all.3 = (f32[128,4], f32[128,4]) all-to-all(%Arg_0.1, %Arg_1.2), channel_id=1, replica_groups={{0,1}}
  %get-tuple-element.4 = f32[128,4] get-tuple-element(%all-to-all.3), index=0
  %get-tuple-element.5 = f32[128,4] get-tuple-element(%all-to-all.3), index=1
  ROOT %tuple.6 = (f32[128,4], f32[128,4]) tuple(%get-tuple-element.4, %get-tuple-element.5)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<2x3xf32>, %[[VAL_1:.*]]: tensor<5x5xf32>) -> tensor<1x2x3xf32> {
// CHECK:           %[[VAL_2:.*]] = mhlo.custom_call @foo(%[[VAL_0]], %[[VAL_1]]) {api_version = 4 : i32, backend_config = {user_attr0 = 123 : i32, user_attr1 = dense<42> : tensor<i32>}, has_side_effect = true} : (tensor<2x3xf32>, tensor<5x5xf32>) -> tensor<1x2x3xf32>
// CHECK:           return %[[VAL_2]] : tensor<1x2x3xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,3]{1,0}, f32[5,5]{1,0})->f32[1,2,3]{2,1,0}}

ENTRY %main.4 (Arg_0.1: f32[2,3], Arg_1.2: f32[5,5]) -> f32[1,2,3] {
  %Arg_0.1 = f32[2,3] parameter(0)
  %Arg_1.2 = f32[5,5] parameter(1)
  ROOT %custom-call.3 = f32[1,2,3] custom-call(%Arg_0.1, %Arg_1.2), custom_call_target="foo", custom_call_has_side_effect=true, api_version=API_VERSION_TYPED_FFI, backend_config={user_attr0 = 123 : i32, user_attr1 = dense<42> : tensor<i32>}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<f32> {mhlo.parameter_replication = [true]}, %[[VAL_1:.*]]: tuple<tensor<2x4xf32>, tuple<tensor<2x4xf32>>> {mhlo.parameter_replication = [false, true]}) -> tensor<f32> {
// CHECK:           return %[[VAL_0]] : tensor<f32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[], (f32[2,4]{1,0}, (f32[2,4]{1,0})))->f32[]}

ENTRY %main.3 (Arg_0.1: f32[], Arg_1.2: (f32[2,4], (f32[2,4]))) -> f32[] {
  ROOT %Arg_0.1 = f32[] parameter(0), parameter_replication={true}
  %Arg_1.2 = (f32[2,4], (f32[2,4])) parameter(1), parameter_replication={false,true}
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<?x784xf32>) -> tensor<?x784xf32> {
// CHECK:           %[[VAL_1:.*]] = stablehlo.abs %[[VAL_0]] : tensor<?x784xf32>
// CHECK:           return %[[VAL_1]] : tensor<?x784xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[?,784]{1,0})->f32[?,784]{1,0}}

ENTRY %main.3 (Arg_0.1: f32[?,784]) -> f32[?,784] {
  %Arg_0.1 = f32[?,784] parameter(0)
  ROOT %abs.2 = f32[?,784] abs(%Arg_0.1)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.3(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_3:.*]] = mhlo.compare  NE, %[[VAL_0]], %[[VAL_2]] : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:           %[[VAL_4:.*]] = mhlo.compare  NE, %[[VAL_1]], %[[VAL_2]] : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:           %[[VAL_5:.*]] = stablehlo.or %[[VAL_3]], %[[VAL_4]] : tensor<i1>
// CHECK:           %[[VAL_6:.*]] = stablehlo.constant dense<1.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_7:.*]] = stablehlo.select %[[VAL_5]], %[[VAL_6]], %[[VAL_2]] : tensor<i1>, tensor<f32>
// CHECK:           return %[[VAL_7]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_8:.*]]: tensor<2x2xf32>) -> tuple<tensor<i1>> {
// CHECK:           %[[VAL_9:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_10:.*]] = mhlo.reduce(%[[VAL_8]] init: %[[VAL_9]]) across dimensions = [0, 1] : (tensor<2x2xf32>, tensor<f32>) -> tensor<f32>
// CHECK:            reducer(%[[VAL_11:.*]]: tensor<f32>, %[[VAL_12:.*]]: tensor<f32>)  {
// CHECK:             %[[VAL_13:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:             %[[VAL_14:.*]] = mhlo.compare  NE, %[[VAL_11]], %[[VAL_13]] : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:             %[[VAL_15:.*]] = mhlo.compare  NE, %[[VAL_12]], %[[VAL_13]] : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:             %[[VAL_16:.*]] = stablehlo.or %[[VAL_14]], %[[VAL_15]] : tensor<i1>
// CHECK:             %[[VAL_17:.*]] = stablehlo.constant dense<1.000000e+00> : tensor<f32>
// CHECK:             %[[VAL_18:.*]] = stablehlo.select %[[VAL_16]], %[[VAL_17]], %[[VAL_13]] : tensor<i1>, tensor<f32>
// CHECK:             mhlo.return %[[VAL_18]] : tensor<f32>
// CHECK:           }
// CHECK:           %[[VAL_19:.*]] = mhlo.compare  NE, %[[VAL_10]], %[[VAL_9]] : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:           %[[VAL_20:.*]] = mhlo.tuple %[[VAL_19]] {xla_shape = "(pred[])"} : tuple<tensor<i1>>
// CHECK:           return %[[VAL_20]] : tuple<tensor<i1>>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,2]{1,0})->(pred[])}

%region_0.3 (Arg_0.4: f32[], Arg_1.5: f32[]) -> f32[] {
  %Arg_0.4 = f32[] parameter(0)
  %constant.7 = f32[] constant(0)
  %compare.8 = pred[] compare(%Arg_0.4, %constant.7), direction=NE
  %Arg_1.5 = f32[] parameter(1)
  %compare.9 = pred[] compare(%Arg_1.5, %constant.7), direction=NE
  %or.10 = pred[] or(%compare.8, %compare.9)
  %constant.6 = f32[] constant(1)
  ROOT %select.11 = f32[] select(%or.10, %constant.6, %constant.7)
}

ENTRY %main.15 (Arg_0.1: f32[2,2]) -> (pred[]) {
  %Arg_0.1 = f32[2,2] parameter(0)
  %constant.2 = f32[] constant(0)
  %reduce.12 = f32[] reduce(%Arg_0.1, %constant.2), dimensions={0,1}, to_apply=%region_0.3
  %compare.13 = pred[] compare(%reduce.12, %constant.2), direction=NE
  ROOT %tuple.14 = (pred[]) tuple(%compare.13)
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           return %[[VAL_0]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_2:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_3:.*]] = "mhlo.all_reduce"(%[[VAL_2]]) <{replica_groups = dense<{{\[\[}}0], [1]]> : tensor<2x1xi64>}> ({
// CHECK:           ^bb0(%[[VAL_4:.*]]: tensor<f32>, %[[VAL_5:.*]]: tensor<f32>):
// CHECK:             mhlo.return %[[VAL_4]] : tensor<f32>
// CHECK:           }) : (tensor<f32>) -> tensor<f32>
// CHECK:           return %[[VAL_3]] : tensor<f32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[])->f32[]}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  ROOT %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
}

ENTRY %main.6 (Arg_0.1: f32[]) -> f32[] {
  %Arg_0.1 = f32[] parameter(0)
  ROOT %all-reduce.5 = f32[] all-reduce(%Arg_0.1), replica_groups={{0},{1}}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.2(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           return %[[VAL_0]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_2:.*]]: tensor<4x16xf32>) -> tensor<4x4xf32> {
// CHECK:           %[[VAL_3:.*]] = "mhlo.reduce_scatter"(%[[VAL_2]]) <{channel_handle = #mhlo.channel_handle<handle = 1, type = 0>, replica_groups = dense<{{\[\[}}0, 1, 2, 3]]> : tensor<1x4xi64>, scatter_dimension = 1 : i64, use_global_device_ids}> ({
// CHECK:           ^bb0(%[[VAL_4:.*]]: tensor<f32>, %[[VAL_5:.*]]: tensor<f32>):
// CHECK:             mhlo.return %[[VAL_4]] : tensor<f32>
// CHECK:           }) : (tensor<4x16xf32>) -> tensor<4x4xf32>
// CHECK:           return %[[VAL_3]] : tensor<4x4xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[4,16]{1,0})->f32[4,4]{1,0}}

%region_0.2 (Arg_0.3: f32[], Arg_1.4: f32[]) -> f32[] {
  ROOT %Arg_0.3 = f32[] parameter(0)
  %Arg_1.4 = f32[] parameter(1)
}

ENTRY %main.6 (Arg_0.1: f32[4,16]) -> f32[4,4] {
  %Arg_0.1 = f32[4,16] parameter(0)
  ROOT %reduce-scatter.5 = f32[4,4] reduce-scatter(%Arg_0.1), channel_id=1, replica_groups={{0,1,2,3}}, use_global_device_ids=true, dimensions={1}, to_apply=%region_0.2
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.3(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_3:.*]] = stablehlo.maximum %[[VAL_0]], %[[VAL_2]] : tensor<f32>
// CHECK:           return %[[VAL_3]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_4:.*]]: tensor<2x17x31x7xf32>, %[[VAL_5:.*]]: tensor<f32>) -> tensor<2x16x30x7xf32> {
// CHECK:           %[[VAL_6:.*]] = "stablehlo.reduce_window"(%[[VAL_4]], %[[VAL_5]]) <{base_dilations = array<i64: 1, 1, 1, 1>, padding = dense<0> : tensor<4x2xi64>, window_dilations = array<i64: 1, 1, 1, 1>, window_dimensions = array<i64: 1, 2, 2, 1>, window_strides = array<i64: 1, 1, 1, 1>}> ({
// CHECK:           ^bb0(%[[VAL_7:.*]]: tensor<f32>, %[[VAL_8:.*]]: tensor<f32>):
// CHECK:             %[[VAL_9:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:             %[[VAL_10:.*]] = stablehlo.maximum %[[VAL_7]], %[[VAL_9]] : tensor<f32>
// CHECK:             stablehlo.return %[[VAL_10]] : tensor<f32>
// CHECK:           }) : (tensor<2x17x31x7xf32>, tensor<f32>) -> tensor<2x16x30x7xf32>
// CHECK:           return %[[VAL_6]] : tensor<2x16x30x7xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[2,17,31,7]{3,2,1,0}, f32[])->f32[2,16,30,7]{3,2,1,0}}

%region_0.3 (Arg_0.4: f32[], Arg_1.5: f32[]) -> f32[] {
  %Arg_1.5 = f32[] parameter(1)
  %Arg_0.4 = f32[] parameter(0)
  %constant.6 = f32[] constant(0)
  ROOT %maximum.7 = f32[] maximum(%Arg_0.4, %constant.6)
}

ENTRY %main.9 (Arg_0.1: f32[2,17,31,7], Arg_1.2: f32[]) -> f32[2,16,30,7] {
  %Arg_0.1 = f32[2,17,31,7] parameter(0)
  %Arg_1.2 = f32[] parameter(1)
  ROOT %reduce-window.8 = f32[2,16,30,7] reduce-window(%Arg_0.1, %Arg_1.2), window={size=1x2x2x1}, to_apply=%region_0.3
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.4(%[[VAL_0:.*]]: tensor<i32>, %[[VAL_1:.*]]: tensor<i32>) -> tensor<i32> {
// CHECK:           return %[[VAL_1]] : tensor<i32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_2:.*]]: tensor<3xi32>, %[[VAL_3:.*]]: tensor<1x1xi32>, %[[VAL_4:.*]]: tensor<1xi32>) -> tensor<3xi32> {
// CHECK:           %[[VAL_5:.*]] = "mhlo.scatter"(%[[VAL_2]], %[[VAL_3]], %[[VAL_4]]) <{indices_are_sorted = false, scatter_dimension_numbers = #mhlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>, unique_indices = false}> ({
// CHECK:           ^bb0(%[[VAL_6:.*]]: tensor<i32>, %[[VAL_7:.*]]: tensor<i32>):
// CHECK:             mhlo.return %[[VAL_7]] : tensor<i32>
// CHECK:           }) : (tensor<3xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<3xi32>
// CHECK:           return %[[VAL_5]] : tensor<3xi32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(s32[3]{0}, s32[1,1]{1,0}, s32[1]{0})->s32[3]{0}}

%region_0.4 (Arg_0.5: s32[], Arg_1.6: s32[]) -> s32[] {
  %Arg_0.5 = s32[] parameter(0)
  ROOT %Arg_1.6 = s32[] parameter(1)
}

ENTRY %main.8 (Arg_0.1: s32[3], Arg_1.2: s32[1,1], Arg_2.3: s32[1]) -> s32[3] {
  %Arg_0.1 = s32[3] parameter(0)
  %Arg_1.2 = s32[1,1] parameter(1)
  %Arg_2.3 = s32[1] parameter(2)
  ROOT %scatter.7 = s32[3] scatter(%Arg_0.1, %Arg_1.2, %Arg_2.3), update_window_dims={}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%region_0.4
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func private @region_0.4(%[[VAL_0:.*]]: tensor<f32>, %[[VAL_1:.*]]: tensor<f32>) -> tensor<i1> {
// CHECK:           %[[VAL_2:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:           %[[VAL_3:.*]] = mhlo.compare  GE, %[[VAL_0]], %[[VAL_2]],  TOTALORDER : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:           return %[[VAL_3]] : tensor<i1>
// CHECK:         }
// CHECK:         func.func private @region_1.9(%[[VAL_4:.*]]: tensor<f32>, %[[VAL_5:.*]]: tensor<f32>) -> tensor<f32> {
// CHECK:           return %[[VAL_5]] : tensor<f32>
// CHECK:         }
// CHECK:         func.func @main(%[[VAL_6:.*]]: tensor<10x24x24x64xf32>, %[[VAL_7:.*]]: tensor<10x23x23x64xf32>, %[[VAL_8:.*]]: tensor<f32>) -> tensor<10x24x24x64xf32> {
// CHECK:           %[[VAL_9:.*]] = "stablehlo.select_and_scatter"(%[[VAL_6]], %[[VAL_7]], %[[VAL_8]]) <{padding = dense<0> : tensor<4x2xi64>, window_dimensions = array<i64: 1, 2, 2, 1>, window_strides = array<i64: 1, 1, 1, 1>}> ({
// CHECK:           ^bb0(%[[VAL_10:.*]]: tensor<f32>, %[[VAL_11:.*]]: tensor<f32>):
// CHECK:             %[[VAL_12:.*]] = stablehlo.constant dense<0.000000e+00> : tensor<f32>
// CHECK:             %[[VAL_13:.*]] = mhlo.compare  GE, %[[VAL_10]], %[[VAL_12]],  TOTALORDER : (tensor<f32>, tensor<f32>) -> tensor<i1>
// CHECK:             stablehlo.return %[[VAL_13]] : tensor<i1>
// CHECK:           }, {
// CHECK:           ^bb0(%[[VAL_14:.*]]: tensor<f32>, %[[VAL_15:.*]]: tensor<f32>):
// CHECK:             stablehlo.return %[[VAL_15]] : tensor<f32>
// CHECK:           }) : (tensor<10x24x24x64xf32>, tensor<10x23x23x64xf32>, tensor<f32>) -> tensor<10x24x24x64xf32>
// CHECK:           return %[[VAL_9]] : tensor<10x24x24x64xf32>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[10,24,24,64]{3,2,1,0}, f32[10,23,23,64]{3,2,1,0}, f32[])->f32[10,24,24,64]{3,2,1,0}}

%region_0.4 (Arg_0.5: f32[], Arg_1.6: f32[]) -> pred[] {
  %Arg_1.6 = f32[] parameter(1)
  %Arg_0.5 = f32[] parameter(0)
  %constant.7 = f32[] constant(0)
  ROOT %compare.8 = pred[] compare(%Arg_0.5, %constant.7), direction=GE, type=TOTALORDER
}

%region_1.9 (Arg_0.10: f32[], Arg_1.11: f32[]) -> f32[] {
  %Arg_0.10 = f32[] parameter(0)
  ROOT %Arg_1.11 = f32[] parameter(1)
}

ENTRY %main.13 (Arg_0.1: f32[10,24,24,64], Arg_1.2: f32[10,23,23,64], Arg_2.3: f32[]) -> f32[10,24,24,64] {
  %Arg_0.1 = f32[10,24,24,64] parameter(0)
  %Arg_1.2 = f32[10,23,23,64] parameter(1)
  %Arg_2.3 = f32[] parameter(2)
  ROOT %select-and-scatter.12 = f32[10,24,24,64] select-and-scatter(%Arg_0.1, %Arg_1.2, %Arg_2.3), window={size=1x2x2x1}, select=%region_0.4, scatter=%region_1.9
}

// -----

// CHECK-LABEL: module @main attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
// CHECK:         func.func @main(%[[VAL_0:.*]]: tensor<16x16xf32>, %[[VAL_1:.*]]: tensor<16x16xi32>) -> tuple<> {
// CHECK:           %[[VAL_2:.*]] = mhlo.tuple  {xla_shape = "()"} : tuple<>
// CHECK:           return %[[VAL_2]] : tuple<>
// CHECK:         }
// CHECK:       }
HloModule main, entry_computation_layout={(f32[16,16]{1,0}, s32[16,16]{1,0})->()}

ENTRY %main.4 (Arg_0.1: f32[16,16], Arg_1.2: s32[16,16]) -> () {
  %Arg_0.1 = f32[16,16] parameter(0)
  %Arg_1.2 = s32[16,16] parameter(1)
  ROOT %tuple.3 = () tuple()
}
