# Copyright 2025 The OpenXLA Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
name: Presubmit Benchmarks
permissions:
  contents: read
on:
  workflow_dispatch:
    inputs:
      halt-for-connection:
        description: 'Should this workflow run wait for a remote connection?'
        type: choice
        required: true
        default: 'no'
        options:
        - 'yes'
        - 'no'
  pull_request:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}
  cancel-in-progress: ${{ github.ref != 'main' }}

jobs:
  Tests:
    strategy:
      # Don't fail fast - want to see results for all builds even if one fails.
      fail-fast: false
      matrix:
        job_info: [
          {
            pool: "linux-x86-n2-128",
            container: "us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest",
            pretty_name: "XLA Linux x86 CPU 128 vcpu Presubmit",
            bazel_arch_dir: "k8-opt",
            platform: "CPU"
          },
          {
            pool: "linux-x86-g2-16-l4-1gpu",
            container: "us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest",
            pretty_name: "XLA Linux x86 GPU L4 16 vcpu Presubmit",
            bazel_arch_dir: "k8-opt",
            platform: "GPU"
          },
        ]
    name: ${{ matrix.job_info.pretty_name }}
    runs-on: ${{ matrix.job_info.pool }}
    container: ${{ matrix.job_info.container }}
    defaults:
      run:
        shell: bash
    timeout-minutes: 15
    env:
      OUTPUT_DIR: ${{ github.workspace }}/output
    steps:
      - name: Print GitHub Context
        run: |
          echo "GitHub SHA: ${{ github.sha }}"
          echo "GitHub Ref: ${{ github.ref }}"
          echo "GitHub Ref Name: ${{ github.ref_name }}"
          echo "GitHub Head Ref: ${{ github.head_ref }})"
          echo "GitHub Base Ref: ${{ github.base_ref }})"
          echo "GitHub Repository: ${{ github.repository }}"
          echo "GitHub Run ID: ${{ github.run_id }}"
          echo "GitHub Run Number: ${{ github.run_number }}"
          echo "GitHub Workflow: ${{ github.workflow }}"
          echo "GitHub Actor: ${{ github.actor }}"
          echo "GitHub Event Name: ${{ github.event_name }}"
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "Pull Request Number: ${{ github.event.pull_request.number }}"
            echo "Pull Request Head Ref: ${{ github.event.pull_request.head.ref }}"
            echo "Pull Request Base Ref: ${{ github.event.pull_request.base.ref }}"
          fi

      - name: Checkout OpenXLA
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Configure GPU backend
        if: ${{ matrix.job_info.platform == 'GPU' }}
        run: |
          ./configure.py --backend=CUDA --cuda_compiler=nvcc

      - name: "Run build.py"
        run: |
          ./build_tools/ci/build.py --build="${{ matrix.job_info.pretty_name }}_github_actions"

      - name: Download Gemma Hlo Files
        run: |
          mkdir -p "$OUTPUT_DIR/tmp_hlo"
          cd "$OUTPUT_DIR/tmp_hlo"
          wget https://storage.googleapis.com/xla-benchmarking-temp/gemma3_1b_flax_call.hlo
          cd ..

      # Run the corresponding HLO tests based on platform
      - name: Run HLO tests
        run: |
          BAZEL_ARCH_DIR="${{ matrix.job_info.bazel_arch_dir }}"
          BINARY_DIR="./bazel-out/${BAZEL_ARCH_DIR}/bin/xla/tools"
          OUTPUT_FILE_PATH="$OUTPUT_DIR/gemma3_1b_flax_call_output.txt"
          TEST_HLO_FILE="$OUTPUT_DIR/tmp_hlo/gemma3_1b_flax_call.hlo"
          XSPACE_FILE_PATH="$OUTPUT_DIR/gemma3_1b_flax_call_xspace.pb"
          DEVICE_TYPE=""

          if [[ ${{ matrix.job_info.platform }} == "CPU" ]]; then
            DEVICE_TYPE="host"
            HLO_RUNNER_BINARY="$BINARY_DIR/multihost_hlo_runner/hlo_runner_main"
            COMPUTE_XSPACE_STATS_BINARY="$BINARY_DIR/compute_xspace_stats_main"
          elif [[ ${{ matrix.job_info.platform }} == "GPU" ]]; then
            DEVICE_TYPE="gpu"
            HLO_RUNNER_BINARY="$BINARY_DIR/multihost_hlo_runner/hlo_runner_main_gpu"
            COMPUTE_XSPACE_STATS_BINARY="$BINARY_DIR/compute_xspace_stats_main_gpu"
          else
            echo "Unsupported platform: ${{ matrix.job_info.platform }}"
            exit 1
          fi

          echo "Running test with binary: $HLO_RUNNER_BINARY"
          pwd #print working directory
          $HLO_RUNNER_BINARY --device_type=$DEVICE_TYPE --use_spmd_partitioning --num_repeats=5 --profile_execution=True --xla_gpu_dump_xspace_to="$XSPACE_FILE_PATH" "$TEST_HLO_FILE" > "$OUTPUT_FILE_PATH"
          $COMPUTE_XSPACE_STATS_BINARY --input="$XSPACE_FILE_PATH" --device_type="${{ matrix.job_info.platform }}" >> "$OUTPUT_FILE_PATH"
          cat "$OUTPUT_FILE_PATH"
