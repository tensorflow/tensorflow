diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 2856e5f..5382a9b 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,34 +1,13 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/mlir/test/Target/SPIRV/function-decorations-asserts.mlir b/mlir/test/Target/SPIRV/function-decorations-asserts.mlir
---- a/mlir/test/Target/SPIRV/function-decorations-asserts.mlir
-+++ b/mlir/test/Target/SPIRV/function-decorations-asserts.mlir
-@@ -0,0 +1,20 @@
-+// REQUIRES: asserts
-+// RUN: mlir-translate --no-implicit-module --test-spirv-roundtrip --split-input-file --debug %s | FileCheck %s
-+
-+spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader, Linkage], []> {
-+    spirv.func @linkage_attr_test_kernel()  "DontInline"  attributes {}  {
-+        %uchar_0 = spirv.Constant 0 : i8
-+        %ushort_1 = spirv.Constant 1 : i16
-+        %uint_0 = spirv.Constant 0 : i32
-+        spirv.FunctionCall @outside.func.with.linkage(%uchar_0):(i8) -> ()
-+        spirv.Return
-+    }
-+    // CHECK: linkage_attributes = #spirv.linkage_attributes<linkage_name = "outside.func", linkage_type = <Import>>
-+    spirv.func @outside.func.with.linkage(%arg0 : i8) -> () "Pure" attributes {
-+      linkage_attributes=#spirv.linkage_attributes<
-+        linkage_name="outside.func",
-+        linkage_type=<Import>
-+      >
-+    }
-+    spirv.func @inside.func() -> () "Pure" attributes {} {spirv.Return}
-+}
-diff -ruN --strip-trailing-cr a/mlir/test/Target/SPIRV/function-decorations.mlir b/mlir/test/Target/SPIRV/function-decorations.mlir
---- a/mlir/test/Target/SPIRV/function-decorations.mlir
-+++ b/mlir/test/Target/SPIRV/function-decorations.mlir
-@@ -1,5 +1,4 @@
- // RUN: mlir-translate --no-implicit-module --test-spirv-roundtrip --split-input-file %s | FileCheck %s
--// RUN: mlir-translate --no-implicit-module --test-spirv-roundtrip --split-input-file --debug %s | FileCheck %s
+diff -ruN --strip-trailing-cr a/compiler-rt/lib/builtins/assembly.h b/compiler-rt/lib/builtins/assembly.h
+--- a/compiler-rt/lib/builtins/assembly.h
++++ b/compiler-rt/lib/builtins/assembly.h
+@@ -337,7 +337,7 @@
+ #endif
+ #endif
+ 
+-#if defined(__i386__) || defined(__amd64__)
++#if defined(__ASSEMBLER__) && (defined(__i386__) || defined(__amd64__))
+ .att_syntax
+ #endif
  
- spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader, Linkage], []> {
-     spirv.func @linkage_attr_test_kernel()  "DontInline"  attributes {}  {
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index b1675f1..70d4e52 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "32de3b9ef9e7e8debc14416e968456ca13b48bea"
-    LLVM_SHA256 = "e048b05e1fb9366e224ea3c06f8473714114039bfad00e81db4ecb6409f23efa"
+    LLVM_COMMIT = "073335d056a39d273df4f395dd9cbad58657e207"
+    LLVM_SHA256 = "e8c822210a8428f982c7e840093b306640147cfe40baf1f37bfd5b495121ea74"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index f2a8787..f2e3e59 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -812,39 +812,7 @@ diff --ruN a/stablehlo/stablehlo/reference/InterpreterOps.cpp b/stablehlo/stable
 diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
 --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
 +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
-@@ -22,21 +22,24 @@
- // CHECK-LABEL: func.func @broadcast_in_dim_fold_splat
- // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x3xi32>)
- func.func @broadcast_in_dim_fold_splat(%arg0: tensor<3x3xi32>)
--  -> (tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>) {
-+  -> (tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>) {
-   %c0 = stablehlo.constant dense<5> : tensor<i32>
-   %c1 = stablehlo.constant dense<3.0> : tensor<f32>
--  %c2 = stablehlo.constant dense<1> : tensor<1x3xi32>
-+  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<complex<f32>>
-+  %c3 = stablehlo.constant dense<1> : tensor<1x3xi32>
- 
-   %0 = stablehlo.broadcast_in_dim %c0, dims = [] : (tensor<i32>) -> tensor<6xi32>
-   %1 = stablehlo.broadcast_in_dim %c1, dims = [] : (tensor<f32>) -> tensor<3xf32>
--  %2 = stablehlo.broadcast_in_dim %c2, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>
-+  %2 = stablehlo.broadcast_in_dim %c2, dims = [] : (tensor<complex<f32>>) -> tensor<5xcomplex<f32>>
-+  %3 = stablehlo.broadcast_in_dim %c3, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>
- 
-   // CHECK-DAG:  [[R0:%.+]] = stablehlo.constant dense<5> : tensor<6xi32>
-   // CHECK-DAG:  [[R1:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<3xf32>
--  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>
--
--  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]]
--  return %0, %1, %2 : tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>
-+  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<5xcomplex<f32>>
-+  // CHECK-DAG:  [[R3:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>
-+
-+  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]], [[R3]]
-+  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>
- }
- 
- // -----
-@@ -529,28 +532,15 @@
+@@ -529,28 +529,15 @@
  // IotaOp
  
  // CHECK-LABEL: func @eval_iota
@@ -879,7 +847,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml
  }
  
  // -----
-@@ -596,6 +586,37 @@
+@@ -596,6 +583,37 @@
    // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\[\[1, 2\], \[3, 4\]\]}}> : tensor<2x2xi32>
    // CHECK-NEXT: return [[CST1]], [[CST2]]
    return %0, %1 : tensor<1xi32>, tensor<2x2xi32>
@@ -920,62 +888,7 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml
 diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
 --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
 +++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
-@@ -132,6 +132,35 @@
- 
-   // CHECK-NEXT: return [[R0]], [[R5]]
-   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>
-+}
-+
-+// CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape
-+// CHECK-SAME:   ([[ARG0:%[^ ]+]]: tensor<3x4xi32>)
-+func.func @broadcast_in_dim_prefer_nested_reshape(%arg0: tensor<3x4xi32>) -> (tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>) {
-+  // When `broadcast_in_dim(broadcast_in_dim(x))` could be optimized into either
-+  // `broadcast_in_dim(reshape(x))` or `broadcast_in_dim(x)`, we want to select
-+  // the former pattern.
-+  //
-+  // (We accomplish this by blocking the merge-composition pattern if the inner
-+  // op can be replaced with a `reshape`. Simply adding benefit to the
-+  // replace-with-reshape pattern isn't sufficient here because the outermost
-+  // op, which only matches the merge-composition pattern, is traversed first.)
-+
-+  // CHECK-DAG: [[INNER_RESHAPE:%[^ ]+]] = stablehlo.reshape [[ARG0]] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>
-+  // CHECK-DAG: [[BROADCAST_OF_RESHAPE:%[^ ]+]] = stablehlo.broadcast_in_dim [[INNER_RESHAPE]], dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>
-+  %0 = stablehlo.broadcast_in_dim %arg0, dims = [0, 2] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>
-+  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>
-+
-+  // When the inner op doesn't qualify for replacement with a `reshape` op,
-+  // however (particularly when it meets some conditions but not others), ensure
-+  // that we allow the merge-composition pattern to match.
-+
-+  // CHECK-DAG: [[MERGED_BROADCAST:%[^ ]+]] = stablehlo.broadcast_in_dim [[ARG0]], dims = [3, 2] : (tensor<3x4xi32>) -> tensor<2x3x4x3xi32>
-+  %2 = stablehlo.broadcast_in_dim %arg0, dims = [2, 1] : (tensor<3x4xi32>) -> tensor<1x4x3xi32>
-+  %3 = stablehlo.broadcast_in_dim %2, dims = [0, 2, 3] : (tensor<1x4x3xi32>) -> tensor<2x3x4x3xi32>
-+
-+  // CHECK-DAG: return [[BROADCAST_OF_RESHAPE]], [[MERGED_BROADCAST]]
-+  return %1, %3 : tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>
- }
- 
- // CHECK-LABEL: func.func @broadcast_in_dim_not_identity_broadcasts
-@@ -1021,6 +1050,18 @@
-   // CHECK-NOT: stablehlo.pad
-   %1 = stablehlo.pad %arg0, %0, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<256x1024xbf16>, tensor<bf16>) -> tensor<256x1024xbf16>
-   return %1 : tensor<256x1024xbf16>
-+}
-+
-+// We don't want to delete `pad` ops that move a tensor's values around without
-+// affecting its dimensions.
-+//
-+// CHECK-LABEL: @pad_rotate_tensor_no_dim_change
-+func.func @pad_rotate_tensor_no_dim_change(%arg0: tensor<50x50xf32>) -> tensor<50x50xf32> {
-+  // CHECK: %[[RES:.+]] = stablehlo.pad
-+  // CHECK: return %[[RES]]
-+  %cst = stablehlo.constant dense<0.0> : tensor<f32>
-+  %0 = stablehlo.pad %arg0, %cst, low = [0, -1], high = [0, 1], interior = [0, 0] : (tensor<50x50xf32>, tensor<f32>) -> tensor<50x50xf32>
-+  return %0 : tensor<50x50xf32>
- }
- 
- // -----
-@@ -1810,6 +1851,15 @@
+@@ -1810,6 +1810,15 @@
    return %0 : tensor<2x4x1x5xf32>
  }
  
@@ -1624,44 +1537,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
  }
  
  LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,
-@@ -530,10 +557,15 @@
-   using FoldOpRewritePattern<OpType>::matchAndRewrite;
-   using FoldOpRewritePattern<OpType>::options;
- 
-+  // TODO: Generalize all relevant folder patterns to support complex data
-+  // types, then hard-code `allowComplex` to `true`.
-   LogicalResult validateShapeFoldDtype(PatternRewriter& rewriter, OpType op,
--                                       ShapedType resultType) const {
-+                                       ShapedType resultType,
-+                                       bool allowComplex = false) const {
-     if (resultType.getElementType().isInteger()) return success();
--    if (options.optimizeFloat && isa<FloatType>(resultType.getElementType()))
-+    if (options.optimizeFloat &&
-+        (allowComplex ? isa<FloatType, ComplexType>(resultType.getElementType())
-+                      : isa<FloatType>(resultType.getElementType())))
-       return success();
-     return rewriter.notifyMatchFailure(op, "skipping fold of shape op dtype");
-   }
-@@ -605,7 +637,8 @@
-                                 PatternRewriter& rewriter) const override {
-     auto resultType = op.getType();
-     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
--        failed(validateShapeFoldDtype(rewriter, op, resultType)))
-+        failed(validateShapeFoldDtype(rewriter, op, resultType,
-+                                      /*allowComplex=*/true)))
-       return failure();
- 
-     SplatElementsAttr cstAttr;
-@@ -1104,7 +1137,7 @@
-         failed(validateShapeFoldDtype(rewriter, op, resultType)))
-       return failure();
- 
--    DenseIntOrFPElementsAttr attr;
-+    DenseElementsAttr attr;
-     if (!matchPattern(op.getOperand(), m_Constant(&attr)))
-       return rewriter.notifyMatchFailure(op, "expected constant operand");
-     rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));
-@@ -1256,21 +1289,48 @@
+@@ -1256,21 +1283,48 @@
        return rewriter.notifyMatchFailure(
            op, "expected operand with static ranked tensor type");
  
@@ -1713,7 +1589,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
      return success();
    }
  };
-@@ -1482,6 +1542,14 @@
+@@ -1482,6 +1536,14 @@
        rewriter.replaceOpWithNewOp<ConstantOp>(
            op, DenseIntElementsAttr::get(resultType, values));
        return success();
@@ -1728,7 +1604,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
      }
  
      int64_t sequences = 1;
-@@ -1881,6 +1949,7 @@
+@@ -1881,6 +1943,7 @@
    patterns->add<FoldConcatenateOpPattern>(context, options, benefit);
    patterns->add<FoldConvertOpPattern>(context, options, benefit);
    patterns->add<FoldDivOpPattern>(context, options, benefit);
@@ -1769,83 +1645,16 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp
 diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
 --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
 +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
-@@ -43,6 +43,14 @@
-     CPred<"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()">,
-     "same number of elements">;
- 
-+def BroadcastNotReducibleToReshape : Constraint<
-+    CPred<"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && "
-+          "!("
-+            "llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && "
-+            "llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()"
-+          ")">,
-+    "is a broadcast_in_dim op that cannot be simplified to a reshape op">;
-+
- def OperandsEqual : Constraint<CPred<"$0 == $1">, "operands are equal">;
- 
- def RankEqual : Constraint<
-@@ -61,6 +69,10 @@
- def AnyZero : AttrConstraint<
-     CPred<"::mlir::matchPattern($_self, m_AnyAttrOf(m_Zero(), m_AnyZeroFloat()))">,
-     "is int or float zero">;
-+
-+def ZeroArrayI64 : AttrConstraint<
-+    CPred<"::llvm::all_of(::llvm::cast<DenseI64ArrayAttr>($_self).asArrayRef(), [](int64_t val) { return val == 0; })">,
-+    "is an array of zeros">;
- 
- def DenseIntElementsAttr : AttrConstraint<
-     CPred<"llvm::isa<DenseIntElementsAttr>($_self)">,
-@@ -120,6 +132,8 @@
+@@ -119,6 +119,8 @@
+ def InvertBroadcastDims : NativeCodeCall<"getInvertedBroadcastDimensions($_builder, $0)">;
  
  def MergeBroadcastDims : NativeCodeCall<"getMergedBroadcastDimensions($_builder, $0, $1)">;
- 
-+def MergePermutations : NativeCodeCall<"getMergedTransposePermutation($_builder, $0, $1)">;
 +
++def MergePermutations : NativeCodeCall<"getMergedTransposePermutation($_builder, $0, $1)">;
+ 
  def StableHLO_ConvertOpWithShape : NativeCodeCall<
      "$_builder.create<stablehlo::ConvertOp>($_loc, $0.getType(), $1)">;
- 
-@@ -178,18 +192,23 @@
- 
- // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])
- //       -> broadcast_in_dim(X, merge(dimsA, dimsB))
-+//          [if the nested broadcast can't be simplified to a reshape]
- def BroadcastInDimOp_MergeComposition
--  : Pat<(StableHLO_BroadcastInDimOp
--            (StableHLO_BroadcastInDimOp $operand, $dims_parent), $dims),
-+  : Pat<(StableHLO_BroadcastInDimOp:$outer_op
-+            (StableHLO_BroadcastInDimOp:$inner_op $operand, $inner_dims),
-+            $outer_dims),
-         (StableHLO_BroadcastInDimOp
--            $operand, (MergeBroadcastDims $dims, $dims_parent))>;
-+            $operand, (MergeBroadcastDims $outer_dims, $inner_dims)),
-+        [(BroadcastNotReducibleToReshape $inner_op, $operand)]>;
- 
- // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])
- //          [if same numel]
- def BroadcastInDimOp_ReplaceWithReshape
-   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),
-         (StableHLO_ReshapeOpWithShape $op, $operand),
--        [(NumberOfElementsEqual $op, $operand)]>;
-+        [(NumberOfElementsEqual $op, $operand)],
-+        [],
-+        (addBenefit 1)>;
- 
- // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])
- //          [if same numel & rank]
-@@ -424,9 +443,9 @@
-   : Pat<(StableHLO_PadOp:$pad
-             $operand,
-             $padding_value,
--            $edge_padding_low,
--            $edge_padding_high,
--            $interior_padding),
-+            ZeroArrayI64:$edge_padding_low,
-+            ZeroArrayI64:$edge_padding_high,
-+            ZeroArrayI64:$interior_padding),
-         (replaceWithValue $operand),
-         [(TypesEqual $pad, $operand)]>;
- 
-@@ -539,6 +558,12 @@
+@@ -539,6 +541,12 @@
    : Pat<(StableHLO_TransposeOp $lhs, IotaDims:$dims),
          (replaceWithValue $lhs)>;
  
