diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index fedf09a..f91b8d5 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,12 +1,1237 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/lldb/unittests/Core/MangledTest.cpp b/lldb/unittests/Core/MangledTest.cpp
---- a/lldb/unittests/Core/MangledTest.cpp
-+++ b/lldb/unittests/Core/MangledTest.cpp
-@@ -605,6 +605,7 @@
-   EXPECT_EQ(get_part(OB.NameInfo.BasenameRange), basename);
-   EXPECT_EQ(get_part(OB.NameInfo.ScopeRange), scope);
-   EXPECT_EQ(get_part(OB.NameInfo.QualifiersRange), qualifiers);
-+  std::free(OB.getBuffer());
+diff -ruN --strip-trailing-cr a/clang/include/clang/AST/Type.h b/clang/include/clang/AST/Type.h
+--- a/clang/include/clang/AST/Type.h
++++ b/clang/include/clang/AST/Type.h
+@@ -3602,6 +3602,9 @@
+   }
+ 
+   NestedNameSpecifier *getQualifier() const { return Qualifier; }
++  /// Note: this can trigger extra deserialization when external AST sources are
++  /// used. Prefer `getCXXRecordDecl()` unless you really need the most recent
++  /// decl.
+   CXXRecordDecl *getMostRecentCXXRecordDecl() const;
+ 
+   bool isSugared() const;
+@@ -3610,7 +3613,10 @@
+   }
+ 
+   void Profile(llvm::FoldingSetNodeID &ID) {
+-    Profile(ID, getPointeeType(), getQualifier(), getMostRecentCXXRecordDecl());
++    // FIXME: `getMostRecentCXXRecordDecl()` should be possible to use here,
++    // however when external AST sources are used it causes nondeterminism
++    // issues (see https://github.com/llvm/llvm-project/pull/137910).
++    Profile(ID, getPointeeType(), getQualifier(), getCXXRecordDecl());
+   }
+ 
+   static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee,
+@@ -3620,6 +3626,9 @@
+   static bool classof(const Type *T) {
+     return T->getTypeClass() == MemberPointer;
+   }
++
++private:
++  CXXRecordDecl *getCXXRecordDecl() const;
+ };
+ 
+ /// Capture whether this is a normal array (e.g. int X[4])
+diff -ruN --strip-trailing-cr a/clang/include/clang/Sema/Overload.h b/clang/include/clang/Sema/Overload.h
+--- a/clang/include/clang/Sema/Overload.h
++++ b/clang/include/clang/Sema/Overload.h
+@@ -360,6 +360,13 @@
+     LLVM_PREFERRED_TYPE(bool)
+     unsigned ObjCLifetimeConversionBinding : 1;
+ 
++    /// Whether the source expression was originally a single element
++    /// braced-init-list. Such a conversion is not a perfect match,
++    /// as we prefer a std::list_initializer constructor over an exact match
++    /// constructor.
++    LLVM_PREFERRED_TYPE(bool)
++    unsigned FromBracedInitList : 1;
++
+     /// FromType - The type that this conversion is converting
+     /// from. This is an opaque pointer that can be translated into a
+     /// QualType.
+@@ -412,6 +419,12 @@
+     bool isPerfect(const ASTContext &C) const {
+       if (!isIdentityConversion())
+         return false;
++
++      // We might prefer a std::initializer constructor,
++      // so this sequence cannot be perfect
++      if (FromBracedInitList)
++        return false;
++
+       // If we are not performing a reference binding, we can skip comparing
+       // the types, which has a noticeable performance impact.
+       if (!ReferenceBinding) {
+diff -ruN --strip-trailing-cr a/clang/lib/AST/Type.cpp b/clang/lib/AST/Type.cpp
+--- a/clang/lib/AST/Type.cpp
++++ b/clang/lib/AST/Type.cpp
+@@ -5305,10 +5305,14 @@
+     ID.AddPointer(Cls->getCanonicalDecl());
+ }
+ 
++CXXRecordDecl *MemberPointerType::getCXXRecordDecl() const {
++  return dyn_cast<MemberPointerType>(getCanonicalTypeInternal())
++      ->getQualifier()
++      ->getAsRecordDecl();
++}
++
+ CXXRecordDecl *MemberPointerType::getMostRecentCXXRecordDecl() const {
+-  auto *RD = dyn_cast<MemberPointerType>(getCanonicalTypeInternal())
+-                 ->getQualifier()
+-                 ->getAsRecordDecl();
++  auto *RD = getCXXRecordDecl();
+   if (!RD)
+     return nullptr;
+   return RD->getMostRecentNonInjectedDecl();
+diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaOverload.cpp b/clang/lib/Sema/SemaOverload.cpp
+--- a/clang/lib/Sema/SemaOverload.cpp
++++ b/clang/lib/Sema/SemaOverload.cpp
+@@ -246,6 +246,7 @@
+   BindsToRvalue = false;
+   BindsImplicitObjectArgumentWithoutRefQualifier = false;
+   ObjCLifetimeConversionBinding = false;
++  FromBracedInitList = false;
+   CopyConstructor = nullptr;
+ }
+ 
+@@ -1692,12 +1693,14 @@
+       //   has a single element of type cv U, where U is X or a class derived
+       //   from X, the implicit conversion sequence has Exact Match rank if U is
+       //   X, or Conversion rank if U is derived from X.
++      bool FromListInit = false;
+       if (const auto *InitList = dyn_cast<InitListExpr>(From);
+           InitList && InitList->getNumInits() == 1 &&
+           !S.isInitListConstructor(Constructor)) {
+         const Expr *SingleInit = InitList->getInit(0);
+         FromType = SingleInit->getType();
+         FromLoc = SingleInit->getBeginLoc();
++        FromListInit = true;
+       } else {
+         FromType = From->getType();
+         FromLoc = From->getBeginLoc();
+@@ -1715,6 +1718,7 @@
+         ICS.Standard.setAsIdentityConversion();
+         ICS.Standard.setFromType(FromType);
+         ICS.Standard.setAllToTypes(ToType);
++        ICS.Standard.FromBracedInitList = FromListInit;
+         ICS.Standard.CopyConstructor = Constructor;
+         ICS.Standard.FoundCopyConstructor = Found;
+         if (ToCanon != FromCanon)
+@@ -4061,6 +4065,7 @@
+       if (isa<InitListExpr>(From)) {
+         // Initializer lists don't have conversions as such.
+         User.Before.setAsIdentityConversion();
++        User.Before.FromBracedInitList = true;
+       } else {
+         if (Best->Conversions[0].isEllipsis())
+           User.EllipsisConversion = true;
+@@ -5275,6 +5280,7 @@
+     ICS.Standard.BindsImplicitObjectArgumentWithoutRefQualifier = false;
+     ICS.Standard.ObjCLifetimeConversionBinding =
+         (RefConv & Sema::ReferenceConversions::ObjCLifetime) != 0;
++    ICS.Standard.FromBracedInitList = false;
+     ICS.Standard.CopyConstructor = nullptr;
+     ICS.Standard.DeprecatedStringLiteralToCharPtr = false;
+   };
+@@ -5473,6 +5479,7 @@
+     ICS.UserDefined.After.BindsToRvalue = !LValRefType;
+     ICS.UserDefined.After.BindsImplicitObjectArgumentWithoutRefQualifier = false;
+     ICS.UserDefined.After.ObjCLifetimeConversionBinding = false;
++    ICS.UserDefined.After.FromBracedInitList = false;
+   }
+ 
+   return ICS;
+@@ -5759,6 +5766,8 @@
+       SCS.BindsToFunctionLvalue = false;
+       SCS.BindsImplicitObjectArgumentWithoutRefQualifier = false;
+       SCS.ObjCLifetimeConversionBinding = false;
++      SCS.FromBracedInitList = false;
++
+     } else
+       Result.setBad(BadConversionSequence::lvalue_ref_to_rvalue,
+                     From, ToType);
+@@ -5776,10 +5785,13 @@
+     // single integer.
+     unsigned NumInits = From->getNumInits();
+     if (NumInits == 1 && !isa<InitListExpr>(From->getInit(0)) &&
+-        !isa<EmbedExpr>(From->getInit(0)))
++        !isa<EmbedExpr>(From->getInit(0))) {
+       Result = TryCopyInitialization(
+           S, From->getInit(0), ToType, SuppressUserConversions,
+           InOverloadResolution, AllowObjCWritebackConversion);
++      if (Result.isStandard())
++        Result.Standard.FromBracedInitList = true;
++    }
+     //    - if the initializer list has no elements, the implicit conversion
+     //      sequence is the identity conversion.
+     else if (NumInits == 0) {
+@@ -5992,6 +6004,7 @@
+   ICS.Standard.IsLvalueReference = Method->getRefQualifier() != RQ_RValue;
+   ICS.Standard.BindsToFunctionLvalue = false;
+   ICS.Standard.BindsToRvalue = FromClassification.isRValue();
++  ICS.Standard.FromBracedInitList = false;
+   ICS.Standard.BindsImplicitObjectArgumentWithoutRefQualifier
+     = (Method->getRefQualifier() == RQ_None);
+   return ICS;
+diff -ruN --strip-trailing-cr a/clang/test/SemaCXX/overload-resolution-deferred-templates.cpp b/clang/test/SemaCXX/overload-resolution-deferred-templates.cpp
+--- a/clang/test/SemaCXX/overload-resolution-deferred-templates.cpp
++++ b/clang/test/SemaCXX/overload-resolution-deferred-templates.cpp
+@@ -2,6 +2,20 @@
+ // RUN: %clang_cc1 -triple=x86_64-unknown-unknown -fsyntax-only -verify -std=c++20 %s
+ // RUN: %clang_cc1 -triple=x86_64-unknown-unknown -fsyntax-only -verify -std=c++2c %s
+ 
++namespace std {
++  typedef decltype(sizeof(int)) size_t;
++  template <class _E> class initializer_list {
++    const _E *__begin_;
++    size_t __size_;
++
++    constexpr initializer_list(const _E *__b, size_t __s)
++        : __begin_(__b), __size_(__s) {}
++
++  public:
++    constexpr initializer_list() : __begin_(nullptr), __size_(0) {}
++  };
++} // namespace std
++
+ template <typename T>
+ struct Invalid { static_assert(false, "instantiated Invalid"); }; // #err-invalid
+ 
+@@ -204,3 +218,17 @@
+ template <typename c> void d(c &);
+ void f(a);
+ template <class> void f(bool j) { f(&d<int>); }
++
++struct InitListAreNotPerfect {
++  InitListAreNotPerfect(int) = delete;
++  template<class T>
++  InitListAreNotPerfect(std::initializer_list<T>);
++};
++InitListAreNotPerfect InitListAreNotPerfect_test({0});
++struct InitListAreNotPerfectCpy {
++  InitListAreNotPerfectCpy();
++  InitListAreNotPerfectCpy(const InitListAreNotPerfectCpy&);
++  template <typename T> InitListAreNotPerfectCpy(std::initializer_list<T>);
++};
++
++InitListAreNotPerfectCpy InitListAreNotPerfectCpy_test({InitListAreNotPerfectCpy{}});
+diff -ruN --strip-trailing-cr a/llvm/lib/Analysis/InstructionSimplify.cpp b/llvm/lib/Analysis/InstructionSimplify.cpp
+--- a/llvm/lib/Analysis/InstructionSimplify.cpp
++++ b/llvm/lib/Analysis/InstructionSimplify.cpp
+@@ -5042,14 +5042,6 @@
+   if (Q.isUndefValue(Ptr))
+     return UndefValue::get(GEPTy);
+ 
+-  // getelementptr inbounds null, idx -> null
+-  if (NW.isInBounds() && Q.IIQ.UseInstrInfo && Q.CxtI) {
+-    if (auto *BaseC = dyn_cast<Constant>(Ptr))
+-      if (BaseC->isNullValue() &&
+-          !NullPointerIsDefined(Q.CxtI->getFunction(), AS))
+-        return Constant::getNullValue(GEPTy);
+-  }
+-
+   bool IsScalableVec =
+       SrcTy->isScalableTy() || any_of(Indices, [](const Value *V) {
+         return isa<ScalableVectorType>(V->getType());
+diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/CMakeLists.txt b/llvm/lib/Target/NVPTX/CMakeLists.txt
+--- a/llvm/lib/Target/NVPTX/CMakeLists.txt
++++ b/llvm/lib/Target/NVPTX/CMakeLists.txt
+@@ -13,34 +13,35 @@
+ set(NVPTXCodeGen_sources
+   NVPTXAliasAnalysis.cpp
+   NVPTXAllocaHoisting.cpp
+-  NVPTXAtomicLower.cpp
+   NVPTXAsmPrinter.cpp
+   NVPTXAssignValidGlobalNames.cpp
++  NVPTXAtomicLower.cpp
++  NVPTXCtorDtorLowering.cpp
+   NVPTXForwardParams.cpp
+   NVPTXFrameLowering.cpp
+   NVPTXGenericToNVVM.cpp
+-  NVPTXISelDAGToDAG.cpp
+-  NVPTXISelLowering.cpp
+   NVPTXImageOptimizer.cpp
+   NVPTXInstrInfo.cpp
++  NVPTXISelDAGToDAG.cpp
++  NVPTXISelLowering.cpp
+   NVPTXLowerAggrCopies.cpp
+-  NVPTXLowerArgs.cpp
+   NVPTXLowerAlloca.cpp
++  NVPTXLowerArgs.cpp
+   NVPTXLowerUnreachable.cpp
+-  NVPTXPeephole.cpp
+   NVPTXMCExpr.cpp
++  NVPTXPeephole.cpp
+   NVPTXPrologEpilogPass.cpp
++  NVPTXProxyRegErasure.cpp
+   NVPTXRegisterInfo.cpp
+   NVPTXReplaceImageHandles.cpp
+   NVPTXSelectionDAGInfo.cpp
+   NVPTXSubtarget.cpp
++  NVPTXTagInvariantLoads.cpp
+   NVPTXTargetMachine.cpp
+   NVPTXTargetTransformInfo.cpp
+   NVPTXUtilities.cpp
+   NVVMIntrRange.cpp
+   NVVMReflect.cpp
+-  NVPTXProxyRegErasure.cpp
+-  NVPTXCtorDtorLowering.cpp
+   )
+ 
+ add_llvm_target(NVPTXCodeGen
+diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/NVPTX.h b/llvm/lib/Target/NVPTX/NVPTX.h
+--- a/llvm/lib/Target/NVPTX/NVPTX.h
++++ b/llvm/lib/Target/NVPTX/NVPTX.h
+@@ -51,6 +51,7 @@
+ FunctionPass *createNVPTXLowerAllocaPass();
+ FunctionPass *createNVPTXLowerUnreachablePass(bool TrapUnreachable,
+                                               bool NoTrapAfterNoreturn);
++FunctionPass *createNVPTXTagInvariantLoadsPass();
+ MachineFunctionPass *createNVPTXPeephole();
+ MachineFunctionPass *createNVPTXProxyRegErasurePass();
+ MachineFunctionPass *createNVPTXForwardParamsPass();
+@@ -73,6 +74,7 @@
+ void initializeNVPTXAAWrapperPassPass(PassRegistry &);
+ void initializeNVPTXExternalAAWrapperPass(PassRegistry &);
+ void initializeNVPTXPeepholePass(PassRegistry &);
++void initializeNVPTXTagInvariantLoadLegacyPassPass(PassRegistry &);
+ 
+ struct NVVMIntrRangePass : PassInfoMixin<NVVMIntrRangePass> {
+   PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);
+@@ -104,6 +106,10 @@
+   PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);
+ };
+ 
++struct NVPTXTagInvariantLoadsPass : PassInfoMixin<NVPTXTagInvariantLoadsPass> {
++  PreservedAnalyses run(Function &F, FunctionAnalysisManager &AM);
++};
++
+ namespace NVPTX {
+ enum DrvInterface {
+   NVCL,
+diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp b/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp
+--- a/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp
++++ b/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp
+@@ -767,46 +767,12 @@
+   llvm_unreachable("unhandled ordering");
+ }
+ 
+-static bool canLowerToLDG(MemSDNode *N, const NVPTXSubtarget &Subtarget,
+-                          unsigned CodeAddrSpace, MachineFunction *F) {
++static bool canLowerToLDG(const MemSDNode &N, const NVPTXSubtarget &Subtarget,
++                          unsigned CodeAddrSpace) {
+   // We use ldg (i.e. ld.global.nc) for invariant loads from the global address
+   // space.
+-  //
+-  // We have two ways of identifying invariant loads: Loads may be explicitly
+-  // marked as invariant, or we may infer them to be invariant.
+-  //
+-  // We currently infer invariance for loads from
+-  //  - constant global variables, and
+-  //  - kernel function pointer params that are noalias (i.e. __restrict) and
+-  //    never written to.
+-  //
+-  // TODO: Perform a more powerful invariance analysis (ideally IPO, and ideally
+-  // not during the SelectionDAG phase).
+-  //
+-  // TODO: Infer invariance only at -O2.  We still want to use ldg at -O0 for
+-  // explicitly invariant loads because these are how clang tells us to use ldg
+-  // when the user uses a builtin.
+-  if (!Subtarget.hasLDG() || CodeAddrSpace != NVPTX::AddressSpace::Global)
+-    return false;
+-
+-  if (N->isInvariant())
+-    return true;
+-
+-  bool IsKernelFn = isKernelFunction(F->getFunction());
+-
+-  // We use getUnderlyingObjects() here instead of getUnderlyingObject() mainly
+-  // because the former looks through phi nodes while the latter does not. We
+-  // need to look through phi nodes to handle pointer induction variables.
+-  SmallVector<const Value *, 8> Objs;
+-  getUnderlyingObjects(N->getMemOperand()->getValue(), Objs);
+-
+-  return all_of(Objs, [&](const Value *V) {
+-    if (auto *A = dyn_cast<const Argument>(V))
+-      return IsKernelFn && A->onlyReadsMemory() && A->hasNoAliasAttr();
+-    if (auto *GV = dyn_cast<const GlobalVariable>(V))
+-      return GV->isConstant();
+-    return false;
+-  });
++  return Subtarget.hasLDG() && CodeAddrSpace == NVPTX::AddressSpace::Global &&
++         N.isInvariant();
+ }
+ 
+ static unsigned int getFenceOp(NVPTX::Ordering O, NVPTX::Scope S,
+@@ -1107,10 +1073,9 @@
+     return false;
+ 
+   // Address Space Setting
+-  unsigned int CodeAddrSpace = getCodeAddrSpace(LD);
+-  if (canLowerToLDG(LD, *Subtarget, CodeAddrSpace, MF)) {
++  const unsigned CodeAddrSpace = getCodeAddrSpace(LD);
++  if (canLowerToLDG(*LD, *Subtarget, CodeAddrSpace))
+     return tryLDGLDU(N);
+-  }
+ 
+   SDLoc DL(N);
+   SDValue Chain = N->getOperand(0);
+@@ -1196,10 +1161,9 @@
+   const MVT MemVT = MemEVT.getSimpleVT();
+ 
+   // Address Space Setting
+-  unsigned int CodeAddrSpace = getCodeAddrSpace(MemSD);
+-  if (canLowerToLDG(MemSD, *Subtarget, CodeAddrSpace, MF)) {
++  const unsigned CodeAddrSpace = getCodeAddrSpace(MemSD);
++  if (canLowerToLDG(*MemSD, *Subtarget, CodeAddrSpace))
+     return tryLDGLDU(N);
+-  }
+ 
+   EVT EltVT = N->getValueType(0);
+   SDLoc DL(N);
+diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/NVPTXPassRegistry.def b/llvm/lib/Target/NVPTX/NVPTXPassRegistry.def
+--- a/llvm/lib/Target/NVPTX/NVPTXPassRegistry.def
++++ b/llvm/lib/Target/NVPTX/NVPTXPassRegistry.def
+@@ -38,5 +38,6 @@
+ #endif
+ FUNCTION_PASS("nvvm-intr-range", NVVMIntrRangePass())
+ FUNCTION_PASS("nvptx-copy-byval-args", NVPTXCopyByValArgsPass())
+-FUNCTION_PASS("nvptx-lower-args", NVPTXLowerArgsPass(*this));
++FUNCTION_PASS("nvptx-lower-args", NVPTXLowerArgsPass(*this))
++FUNCTION_PASS("nvptx-tag-invariant-loads", NVPTXTagInvariantLoadsPass())
+ #undef FUNCTION_PASS
+diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/NVPTXTagInvariantLoads.cpp b/llvm/lib/Target/NVPTX/NVPTXTagInvariantLoads.cpp
+--- a/llvm/lib/Target/NVPTX/NVPTXTagInvariantLoads.cpp
++++ b/llvm/lib/Target/NVPTX/NVPTXTagInvariantLoads.cpp
+@@ -0,0 +1,104 @@
++//===------ NVPTXTagInvariantLoads.cpp - Tag invariant loads --------------===//
++//
++// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
++// See https://llvm.org/LICENSE.txt for license information.
++// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
++//
++//===----------------------------------------------------------------------===//
++//
++// This file implements invaraint load tagging. It traverses load instructions
++// in a function, and determines if each load can be tagged as invariant.
++//
++// We currently infer invariance for loads from
++//  - constant global variables, and
++//  - kernel function pointer params that are noalias (i.e. __restrict) and
++//    never written to.
++//
++// TODO: Perform a more powerful invariance analysis (ideally IPO).
++//
++//===----------------------------------------------------------------------===//
++
++#include "NVPTXUtilities.h"
++#include "llvm/Analysis/ValueTracking.h"
++#include "llvm/IR/InstIterator.h"
++#include "llvm/IR/Instructions.h"
++#include "llvm/IR/Metadata.h"
++#include "llvm/Support/NVPTXAddrSpace.h"
++
++using namespace llvm;
++
++static bool isInvariantLoad(const LoadInst *LI, const bool IsKernelFn) {
++  // Don't bother with non-global loads
++  if (LI->getPointerAddressSpace() != NVPTXAS::ADDRESS_SPACE_GLOBAL)
++    return false;
++
++  // If the load is already marked as invariant, we don't need to do anything
++  if (LI->getMetadata(LLVMContext::MD_invariant_load))
++    return false;
++
++  // We use getUnderlyingObjects() here instead of getUnderlyingObject()
++  // mainly because the former looks through phi nodes while the latter does
++  // not. We need to look through phi nodes to handle pointer induction
++  // variables.
++  SmallVector<const Value *, 8> Objs;
++  getUnderlyingObjects(LI->getPointerOperand(), Objs);
++
++  return all_of(Objs, [&](const Value *V) {
++    if (const auto *A = dyn_cast<const Argument>(V))
++      return IsKernelFn && ((A->onlyReadsMemory() && A->hasNoAliasAttr()) ||
++                            isParamGridConstant(*A));
++    if (const auto *GV = dyn_cast<const GlobalVariable>(V))
++      return GV->isConstant();
++    return false;
++  });
++}
++
++static void markLoadsAsInvariant(LoadInst *LI) {
++  LI->setMetadata(LLVMContext::MD_invariant_load,
++                  MDNode::get(LI->getContext(), {}));
++}
++
++static bool tagInvariantLoads(Function &F) {
++  const bool IsKernelFn = isKernelFunction(F);
++
++  bool Changed = false;
++  for (auto &I : instructions(F)) {
++    if (auto *LI = dyn_cast<LoadInst>(&I)) {
++      if (isInvariantLoad(LI, IsKernelFn)) {
++        markLoadsAsInvariant(LI);
++        Changed = true;
++      }
++    }
++  }
++  return Changed;
++}
++
++namespace {
++
++struct NVPTXTagInvariantLoadLegacyPass : public FunctionPass {
++  static char ID;
++
++  NVPTXTagInvariantLoadLegacyPass() : FunctionPass(ID) {}
++  bool runOnFunction(Function &F) override;
++};
++
++} // namespace
++
++INITIALIZE_PASS(NVPTXTagInvariantLoadLegacyPass, "nvptx-tag-invariant-loads",
++                "NVPTX Tag Invariant Loads", false, false)
++
++bool NVPTXTagInvariantLoadLegacyPass::runOnFunction(Function &F) {
++  return tagInvariantLoads(F);
++}
++
++char NVPTXTagInvariantLoadLegacyPass::ID = 0;
++
++FunctionPass *llvm::createNVPTXTagInvariantLoadsPass() {
++  return new NVPTXTagInvariantLoadLegacyPass();
++}
++
++PreservedAnalyses NVPTXTagInvariantLoadsPass::run(Function &F,
++                                                  FunctionAnalysisManager &) {
++  return tagInvariantLoads(F) ? PreservedAnalyses::none()
++                              : PreservedAnalyses::all();
++}
+diff -ruN --strip-trailing-cr a/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp b/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp
+--- a/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp
++++ b/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp
+@@ -112,6 +112,7 @@
+   initializeNVPTXAAWrapperPassPass(PR);
+   initializeNVPTXExternalAAWrapperPass(PR);
+   initializeNVPTXPeepholePass(PR);
++  initializeNVPTXTagInvariantLoadLegacyPassPass(PR);
  }
  
- INSTANTIATE_TEST_SUITE_P(DemanglingPartsTests, DemanglingPartsTestFixture,
+ static std::string computeDataLayout(bool is64Bit, bool UseShortPointers) {
+@@ -395,6 +396,7 @@
+     if (!DisableLoadStoreVectorizer)
+       addPass(createLoadStoreVectorizerPass());
+     addPass(createSROAPass());
++    addPass(createNVPTXTagInvariantLoadsPass());
+   }
+ 
+   if (ST.hasPTXASUnreachableBug()) {
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+--- a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
++++ b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+@@ -24069,10 +24069,23 @@
+             const auto *V1 = dyn_cast<Instruction>(E1->getVectorOperand());
+             const auto *V2 = dyn_cast<Instruction>(E2->getVectorOperand());
+             if (V1 != V2) {
+-              if (!V1 || !V2)
+-                continue;
+-              if (V1->getParent() != V2->getParent())
+-                continue;
++              if (V1 && !V2)
++                return true;
++              if (!V1 && V2)
++                return false;
++              DomTreeNodeBase<BasicBlock> *NodeI1 =
++                  DT->getNode(V1->getParent());
++              DomTreeNodeBase<BasicBlock> *NodeI2 =
++                  DT->getNode(V2->getParent());
++              if (!NodeI1)
++                return NodeI2 != nullptr;
++              if (!NodeI2)
++                return false;
++              assert((NodeI1 == NodeI2) ==
++                         (NodeI1->getDFSNumIn() == NodeI2->getDFSNumIn()) &&
++                     "Different nodes should have different DFS numbers");
++              if (NodeI1 != NodeI2)
++                return NodeI1->getDFSNumIn() < NodeI2->getDFSNumIn();
+               return V1->comesBefore(V2);
+             }
+             // If we have the same vector operand, try to sort by constant
+diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/AMDGPU/memcpy-crash-issue63986.ll b/llvm/test/CodeGen/AMDGPU/memcpy-crash-issue63986.ll
+--- a/llvm/test/CodeGen/AMDGPU/memcpy-crash-issue63986.ll
++++ b/llvm/test/CodeGen/AMDGPU/memcpy-crash-issue63986.ll
+@@ -7,141 +7,138 @@
+ ; CHECK-LABEL: issue63986:
+ ; CHECK:       ; %bb.0: ; %entry
+ ; CHECK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
+-; CHECK-NEXT:    v_lshlrev_b64 v[8:9], 6, v[2:3]
+-; CHECK-NEXT:    v_mov_b32_e32 v4, s17
+-; CHECK-NEXT:    v_add_co_u32_e32 v10, vcc, s16, v8
+-; CHECK-NEXT:    v_addc_co_u32_e32 v11, vcc, v4, v9, vcc
+-; CHECK-NEXT:  ; %bb.1: ; %entry.loop-memcpy-expansion_crit_edge
+-; CHECK-NEXT:    v_mov_b32_e32 v4, 0
+-; CHECK-NEXT:    v_mov_b32_e32 v5, 0
+-; CHECK-NEXT:    flat_load_dwordx4 v[4:7], v[4:5]
++; CHECK-NEXT:    v_lshlrev_b64 v[4:5], 6, v[2:3]
++; CHECK-NEXT:    v_mov_b32_e32 v6, s17
++; CHECK-NEXT:    v_add_co_u32_e32 v8, vcc, s16, v4
++; CHECK-NEXT:    v_addc_co_u32_e32 v9, vcc, v6, v5, vcc
+ ; CHECK-NEXT:    s_mov_b64 s[4:5], 0
+-; CHECK-NEXT:    s_waitcnt vmcnt(0)
+-; CHECK-NEXT:  .LBB0_2: ; %loop-memcpy-expansion
++; CHECK-NEXT:  .LBB0_1: ; %loop-memcpy-expansion
+ ; CHECK-NEXT:    ; =>This Inner Loop Header: Depth=1
+-; CHECK-NEXT:    v_add_co_u32_e32 v12, vcc, s4, v10
++; CHECK-NEXT:    v_mov_b32_e32 v7, s5
++; CHECK-NEXT:    v_mov_b32_e32 v6, s4
++; CHECK-NEXT:    flat_load_dwordx4 v[10:13], v[6:7]
++; CHECK-NEXT:    v_add_co_u32_e32 v6, vcc, s4, v8
+ ; CHECK-NEXT:    s_add_u32 s4, s4, 16
+-; CHECK-NEXT:    v_mov_b32_e32 v13, s5
+ ; CHECK-NEXT:    s_addc_u32 s5, s5, 0
+ ; CHECK-NEXT:    v_cmp_ge_u64_e64 s[6:7], s[4:5], 32
+-; CHECK-NEXT:    v_addc_co_u32_e32 v13, vcc, v11, v13, vcc
++; CHECK-NEXT:    v_addc_co_u32_e32 v7, vcc, v9, v7, vcc
+ ; CHECK-NEXT:    s_and_b64 vcc, exec, s[6:7]
+-; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
+-; CHECK-NEXT:    flat_store_dwordx4 v[12:13], v[4:7]
+-; CHECK-NEXT:    s_cbranch_vccz .LBB0_2
+-; CHECK-NEXT:  ; %bb.3: ; %loop-memcpy-residual-header
++; CHECK-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
++; CHECK-NEXT:    flat_store_dwordx4 v[6:7], v[10:13]
++; CHECK-NEXT:    s_cbranch_vccz .LBB0_1
++; CHECK-NEXT:  ; %bb.2: ; %loop-memcpy-residual-header
++; CHECK-NEXT:    s_branch .LBB0_4
++; CHECK-NEXT:  ; %bb.3:
++; CHECK-NEXT:    ; implicit-def: $vgpr6_vgpr7
+ ; CHECK-NEXT:    s_branch .LBB0_5
+-; CHECK-NEXT:  ; %bb.4:
+-; CHECK-NEXT:    ; implicit-def: $vgpr2_vgpr3
+-; CHECK-NEXT:    s_branch .LBB0_6
+-; CHECK-NEXT:  .LBB0_5: ; %loop-memcpy-residual-header.post-loop-memcpy-expansion_crit_edge
+-; CHECK-NEXT:    v_lshlrev_b64 v[2:3], 6, v[2:3]
+-; CHECK-NEXT:    s_cbranch_execnz .LBB0_9
+-; CHECK-NEXT:  .LBB0_6: ; %loop-memcpy-residual-header.loop-memcpy-residual_crit_edge
+-; CHECK-NEXT:    v_mov_b32_e32 v2, 0
+-; CHECK-NEXT:    v_mov_b32_e32 v3, 0
+-; CHECK-NEXT:    flat_load_ubyte v2, v[2:3]
+-; CHECK-NEXT:    s_add_u32 s6, s16, 32
+-; CHECK-NEXT:    s_addc_u32 s4, s17, 0
+-; CHECK-NEXT:    v_mov_b32_e32 v4, s4
+-; CHECK-NEXT:    v_add_co_u32_e32 v3, vcc, s6, v8
++; CHECK-NEXT:  .LBB0_4: ; %loop-memcpy-residual-header.post-loop-memcpy-expansion_crit_edge
++; CHECK-NEXT:    v_lshlrev_b64 v[6:7], 6, v[2:3]
++; CHECK-NEXT:    s_cbranch_execnz .LBB0_8
++; CHECK-NEXT:  .LBB0_5: ; %loop-memcpy-residual.preheader
++; CHECK-NEXT:    s_add_u32 s4, s16, 32
++; CHECK-NEXT:    s_addc_u32 s5, s17, 0
++; CHECK-NEXT:    v_mov_b32_e32 v3, s5
++; CHECK-NEXT:    v_add_co_u32_e32 v2, vcc, s4, v4
++; CHECK-NEXT:    v_addc_co_u32_e32 v3, vcc, v3, v5, vcc
+ ; CHECK-NEXT:    s_mov_b64 s[4:5], 0
+-; CHECK-NEXT:    v_addc_co_u32_e32 v4, vcc, v4, v9, vcc
+-; CHECK-NEXT:    s_waitcnt vmcnt(0)
+-; CHECK-NEXT:  ; %bb.7: ; %loop-memcpy-residual
+-; CHECK-NEXT:    v_mov_b32_e32 v6, s5
+-; CHECK-NEXT:    v_add_co_u32_e32 v5, vcc, s4, v3
++; CHECK-NEXT:  ; %bb.6: ; %loop-memcpy-residual
++; CHECK-NEXT:    s_add_u32 s6, 32, s4
++; CHECK-NEXT:    s_addc_u32 s7, 0, s5
++; CHECK-NEXT:    v_mov_b32_e32 v6, s6
++; CHECK-NEXT:    v_mov_b32_e32 v7, s7
++; CHECK-NEXT:    flat_load_ubyte v10, v[6:7]
++; CHECK-NEXT:    v_mov_b32_e32 v7, s5
++; CHECK-NEXT:    v_add_co_u32_e32 v6, vcc, s4, v2
++; CHECK-NEXT:    v_addc_co_u32_e32 v7, vcc, v3, v7, vcc
+ ; CHECK-NEXT:    s_add_u32 s4, s4, 1
+-; CHECK-NEXT:    v_addc_co_u32_e32 v6, vcc, v4, v6, vcc
+ ; CHECK-NEXT:    s_addc_u32 s5, s5, 0
+-; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
+-; CHECK-NEXT:    flat_store_byte v[5:6], v2
+-; CHECK-NEXT:  ; %bb.8:
+-; CHECK-NEXT:    v_mov_b32_e32 v2, v8
+-; CHECK-NEXT:    v_mov_b32_e32 v3, v9
+-; CHECK-NEXT:  .LBB0_9: ; %post-loop-memcpy-expansion
+-; CHECK-NEXT:    v_and_b32_e32 v6, 15, v0
++; CHECK-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
++; CHECK-NEXT:    flat_store_byte v[6:7], v10
++; CHECK-NEXT:  ; %bb.7:
++; CHECK-NEXT:    v_mov_b32_e32 v7, v5
++; CHECK-NEXT:    v_mov_b32_e32 v6, v4
++; CHECK-NEXT:  .LBB0_8: ; %post-loop-memcpy-expansion
++; CHECK-NEXT:    v_and_b32_e32 v2, 15, v0
+ ; CHECK-NEXT:    v_and_b32_e32 v0, -16, v0
+-; CHECK-NEXT:    v_add_co_u32_e32 v2, vcc, v2, v0
+-; CHECK-NEXT:    v_mov_b32_e32 v7, 0
+-; CHECK-NEXT:    v_addc_co_u32_e32 v3, vcc, v3, v1, vcc
++; CHECK-NEXT:    v_add_co_u32_e32 v4, vcc, v6, v0
++; CHECK-NEXT:    v_mov_b32_e32 v3, 0
++; CHECK-NEXT:    v_addc_co_u32_e32 v5, vcc, v7, v1, vcc
+ ; CHECK-NEXT:    v_cmp_ne_u64_e64 s[4:5], 0, v[0:1]
+-; CHECK-NEXT:    v_cmp_ne_u64_e64 s[6:7], 0, v[6:7]
+-; CHECK-NEXT:    v_mov_b32_e32 v4, s17
+-; CHECK-NEXT:    v_mov_b32_e32 v8, 0
+-; CHECK-NEXT:    v_add_co_u32_e32 v12, vcc, s16, v2
+-; CHECK-NEXT:    v_mov_b32_e32 v9, 0
+-; CHECK-NEXT:    v_addc_co_u32_e32 v13, vcc, v4, v3, vcc
+-; CHECK-NEXT:    s_branch .LBB0_12
+-; CHECK-NEXT:  .LBB0_10: ; %Flow14
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
++; CHECK-NEXT:    v_cmp_ne_u64_e64 s[6:7], 0, v[2:3]
++; CHECK-NEXT:    v_mov_b32_e32 v6, s17
++; CHECK-NEXT:    v_add_co_u32_e32 v4, vcc, s16, v4
++; CHECK-NEXT:    v_addc_co_u32_e32 v5, vcc, v6, v5, vcc
++; CHECK-NEXT:    s_branch .LBB0_11
++; CHECK-NEXT:  .LBB0_9: ; %Flow14
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_or_b64 exec, exec, s[10:11]
+ ; CHECK-NEXT:    s_mov_b64 s[8:9], 0
+-; CHECK-NEXT:  .LBB0_11: ; %Flow16
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
++; CHECK-NEXT:  .LBB0_10: ; %Flow16
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_andn2_b64 vcc, exec, s[8:9]
+-; CHECK-NEXT:    s_cbranch_vccz .LBB0_20
+-; CHECK-NEXT:  .LBB0_12: ; %while.cond
++; CHECK-NEXT:    s_cbranch_vccz .LBB0_19
++; CHECK-NEXT:  .LBB0_11: ; %while.cond
+ ; CHECK-NEXT:    ; =>This Loop Header: Depth=1
+-; CHECK-NEXT:    ; Child Loop BB0_14 Depth 2
+-; CHECK-NEXT:    ; Child Loop BB0_18 Depth 2
++; CHECK-NEXT:    ; Child Loop BB0_13 Depth 2
++; CHECK-NEXT:    ; Child Loop BB0_17 Depth 2
+ ; CHECK-NEXT:    s_and_saveexec_b64 s[8:9], s[4:5]
+-; CHECK-NEXT:    s_cbranch_execz .LBB0_15
+-; CHECK-NEXT:  ; %bb.13: ; %while.cond.loop-memcpy-expansion2_crit_edge
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
+-; CHECK-NEXT:    flat_load_dwordx4 v[2:5], v[8:9]
++; CHECK-NEXT:    s_cbranch_execz .LBB0_14
++; CHECK-NEXT:  ; %bb.12: ; %loop-memcpy-expansion2.preheader
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_mov_b64 s[10:11], 0
+ ; CHECK-NEXT:    s_mov_b64 s[12:13], 0
+-; CHECK-NEXT:    s_waitcnt vmcnt(0)
+-; CHECK-NEXT:  .LBB0_14: ; %loop-memcpy-expansion2
+-; CHECK-NEXT:    ; Parent Loop BB0_12 Depth=1
++; CHECK-NEXT:  .LBB0_13: ; %loop-memcpy-expansion2
++; CHECK-NEXT:    ; Parent Loop BB0_11 Depth=1
+ ; CHECK-NEXT:    ; => This Inner Loop Header: Depth=2
+-; CHECK-NEXT:    v_mov_b32_e32 v15, s13
+-; CHECK-NEXT:    v_add_co_u32_e32 v14, vcc, s12, v10
++; CHECK-NEXT:    v_mov_b32_e32 v6, s12
++; CHECK-NEXT:    v_mov_b32_e32 v7, s13
++; CHECK-NEXT:    flat_load_dwordx4 v[10:13], v[6:7]
++; CHECK-NEXT:    v_add_co_u32_e32 v6, vcc, s12, v8
+ ; CHECK-NEXT:    s_add_u32 s12, s12, 16
+-; CHECK-NEXT:    v_addc_co_u32_e32 v15, vcc, v11, v15, vcc
++; CHECK-NEXT:    v_addc_co_u32_e32 v7, vcc, v9, v7, vcc
+ ; CHECK-NEXT:    s_addc_u32 s13, s13, 0
+ ; CHECK-NEXT:    v_cmp_ge_u64_e32 vcc, s[12:13], v[0:1]
+-; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
+-; CHECK-NEXT:    flat_store_dwordx4 v[14:15], v[2:5]
+ ; CHECK-NEXT:    s_or_b64 s[10:11], vcc, s[10:11]
++; CHECK-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
++; CHECK-NEXT:    flat_store_dwordx4 v[6:7], v[10:13]
+ ; CHECK-NEXT:    s_andn2_b64 exec, exec, s[10:11]
+-; CHECK-NEXT:    s_cbranch_execnz .LBB0_14
+-; CHECK-NEXT:  .LBB0_15: ; %Flow15
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
++; CHECK-NEXT:    s_cbranch_execnz .LBB0_13
++; CHECK-NEXT:  .LBB0_14: ; %Flow15
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_or_b64 exec, exec, s[8:9]
+ ; CHECK-NEXT:    s_mov_b64 s[8:9], -1
+-; CHECK-NEXT:    s_cbranch_execz .LBB0_11
+-; CHECK-NEXT:  ; %bb.16: ; %loop-memcpy-residual-header5
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
++; CHECK-NEXT:    s_cbranch_execz .LBB0_10
++; CHECK-NEXT:  ; %bb.15: ; %loop-memcpy-residual-header5
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_and_saveexec_b64 s[8:9], s[6:7]
+ ; CHECK-NEXT:    s_xor_b64 s[10:11], exec, s[8:9]
+-; CHECK-NEXT:    s_cbranch_execz .LBB0_10
+-; CHECK-NEXT:  ; %bb.17: ; %loop-memcpy-residual-header5.loop-memcpy-residual4_crit_edge
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
+-; CHECK-NEXT:    flat_load_ubyte v2, v[8:9]
++; CHECK-NEXT:    s_cbranch_execz .LBB0_9
++; CHECK-NEXT:  ; %bb.16: ; %loop-memcpy-residual4.preheader
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_mov_b64 s[12:13], 0
+ ; CHECK-NEXT:    s_mov_b64 s[14:15], 0
+-; CHECK-NEXT:    s_waitcnt vmcnt(0)
+-; CHECK-NEXT:  .LBB0_18: ; %loop-memcpy-residual4
+-; CHECK-NEXT:    ; Parent Loop BB0_12 Depth=1
++; CHECK-NEXT:  .LBB0_17: ; %loop-memcpy-residual4
++; CHECK-NEXT:    ; Parent Loop BB0_11 Depth=1
+ ; CHECK-NEXT:    ; => This Inner Loop Header: Depth=2
+-; CHECK-NEXT:    v_add_co_u32_e32 v3, vcc, s14, v12
++; CHECK-NEXT:    v_mov_b32_e32 v10, s15
++; CHECK-NEXT:    v_add_co_u32_e32 v6, vcc, s14, v0
++; CHECK-NEXT:    v_addc_co_u32_e32 v7, vcc, v1, v10, vcc
++; CHECK-NEXT:    flat_load_ubyte v11, v[6:7]
++; CHECK-NEXT:    v_add_co_u32_e32 v6, vcc, s14, v4
+ ; CHECK-NEXT:    s_add_u32 s14, s14, 1
+-; CHECK-NEXT:    v_mov_b32_e32 v4, s15
+ ; CHECK-NEXT:    s_addc_u32 s15, s15, 0
+-; CHECK-NEXT:    v_cmp_ge_u64_e64 s[8:9], s[14:15], v[6:7]
+-; CHECK-NEXT:    v_addc_co_u32_e32 v4, vcc, v13, v4, vcc
++; CHECK-NEXT:    v_cmp_ge_u64_e64 s[8:9], s[14:15], v[2:3]
++; CHECK-NEXT:    v_addc_co_u32_e32 v7, vcc, v5, v10, vcc
+ ; CHECK-NEXT:    s_or_b64 s[12:13], s[8:9], s[12:13]
+-; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
+-; CHECK-NEXT:    flat_store_byte v[3:4], v2
++; CHECK-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
++; CHECK-NEXT:    flat_store_byte v[6:7], v11
+ ; CHECK-NEXT:    s_andn2_b64 exec, exec, s[12:13]
+-; CHECK-NEXT:    s_cbranch_execnz .LBB0_18
+-; CHECK-NEXT:  ; %bb.19: ; %Flow
+-; CHECK-NEXT:    ; in Loop: Header=BB0_12 Depth=1
++; CHECK-NEXT:    s_cbranch_execnz .LBB0_17
++; CHECK-NEXT:  ; %bb.18: ; %Flow
++; CHECK-NEXT:    ; in Loop: Header=BB0_11 Depth=1
+ ; CHECK-NEXT:    s_or_b64 exec, exec, s[12:13]
+-; CHECK-NEXT:    s_branch .LBB0_10
+-; CHECK-NEXT:  .LBB0_20: ; %DummyReturnBlock
++; CHECK-NEXT:    s_branch .LBB0_9
++; CHECK-NEXT:  .LBB0_19: ; %DummyReturnBlock
+ ; CHECK-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
+ ; CHECK-NEXT:    s_setpc_b64 s[30:31]
+ entry:
+diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/NVPTX/byval-const-global.ll b/llvm/test/CodeGen/NVPTX/byval-const-global.ll
+--- a/llvm/test/CodeGen/NVPTX/byval-const-global.ll
++++ b/llvm/test/CodeGen/NVPTX/byval-const-global.ll
+@@ -0,0 +1,33 @@
++; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
++; RUN: llc < %s -mcpu=sm_70 | FileCheck %s
++; RUN: %if ptxas %{ llc < %s -mcpu=sm_70 | %ptxas-verify %}
++
++target triple = "nvptx64-nvidia-cuda"
++
++%struct = type { [2 x i64] }
++@G = external constant %struct
++
++define void @foo() {
++; CHECK-LABEL: foo(
++; CHECK:       {
++; CHECK-NEXT:    .reg .b64 %rd<3>;
++; CHECK-EMPTY:
++; CHECK-NEXT:  // %bb.0:
++; CHECK-NEXT:    ld.global.u64 %rd1, [G];
++; CHECK-NEXT:    ld.global.u64 %rd2, [G+8];
++; CHECK-NEXT:    { // callseq 0, 0
++; CHECK-NEXT:    .param .align 8 .b8 param0[16];
++; CHECK-NEXT:    st.param.b64 [param0], %rd1;
++; CHECK-NEXT:    st.param.b64 [param0+8], %rd2;
++; CHECK-NEXT:    call.uni
++; CHECK-NEXT:    bar,
++; CHECK-NEXT:    (
++; CHECK-NEXT:    param0
++; CHECK-NEXT:    );
++; CHECK-NEXT:    } // callseq 0
++; CHECK-NEXT:    ret;
++  call void @bar(ptr byval(%struct) @G)
++  ret void
++}
++
++declare void @bar(ptr)
+diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/NVPTX/tag-invariant-loads.ll b/llvm/test/CodeGen/NVPTX/tag-invariant-loads.ll
+--- a/llvm/test/CodeGen/NVPTX/tag-invariant-loads.ll
++++ b/llvm/test/CodeGen/NVPTX/tag-invariant-loads.ll
+@@ -0,0 +1,138 @@
++; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
++; RUN: opt -S -passes=nvptx-tag-invariant-loads < %s -mcpu=sm_80 | FileCheck %s --check-prefix=OPT
++; RUN: llc -o - < %s -mcpu=sm_80 | FileCheck %s --check-prefix=PTX
++
++target triple = "nvptx-unknown-cuda"
++
++define ptx_kernel void @basic(ptr noalias readonly %a, ptr %out) {
++; OPT-LABEL: define ptx_kernel void @basic(
++; OPT-SAME: ptr noalias readonly [[A:%.*]], ptr [[OUT:%.*]]) #[[ATTR0:[0-9]+]] {
++; OPT-NEXT:    [[A_GLOBAL:%.*]] = addrspacecast ptr [[A]] to ptr addrspace(1)
++; OPT-NEXT:    [[VAL:%.*]] = load float, ptr addrspace(1) [[A_GLOBAL]], align 4, !invariant.load [[META0:![0-9]+]]
++; OPT-NEXT:    store float [[VAL]], ptr [[OUT]], align 4
++; OPT-NEXT:    ret void
++;
++; PTX-LABEL: basic(
++; PTX:       {
++; PTX-NEXT:    .reg .b32 %r<5>;
++; PTX-NEXT:    .reg .b32 %f<2>;
++; PTX-EMPTY:
++; PTX-NEXT:  // %bb.0:
++; PTX-NEXT:    ld.param.u32 %r1, [basic_param_0];
++; PTX-NEXT:    cvta.to.global.u32 %r2, %r1;
++; PTX-NEXT:    ld.param.u32 %r3, [basic_param_1];
++; PTX-NEXT:    cvta.to.global.u32 %r4, %r3;
++; PTX-NEXT:    ld.global.nc.f32 %f1, [%r2];
++; PTX-NEXT:    st.global.f32 [%r4], %f1;
++; PTX-NEXT:    ret;
++  %a_global = addrspacecast ptr %a to ptr addrspace(1)
++  %val = load float, ptr addrspace(1) %a_global
++  store float %val, ptr %out
++  ret void
++}
++
++define ptx_kernel void @select(ptr noalias readonly %a, ptr noalias readonly %b, i1 %c, ptr %out) {
++; OPT-LABEL: define ptx_kernel void @select(
++; OPT-SAME: ptr noalias readonly [[A:%.*]], ptr noalias readonly [[B:%.*]], i1 [[C:%.*]], ptr [[OUT:%.*]]) #[[ATTR0]] {
++; OPT-NEXT:    [[SELECT:%.*]] = select i1 [[C]], ptr [[A]], ptr [[B]]
++; OPT-NEXT:    [[SELECT_GLOBAL:%.*]] = addrspacecast ptr [[SELECT]] to ptr addrspace(1)
++; OPT-NEXT:    [[VAL:%.*]] = load i32, ptr addrspace(1) [[SELECT_GLOBAL]], align 4, !invariant.load [[META0]]
++; OPT-NEXT:    store i32 [[VAL]], ptr [[OUT]], align 4
++; OPT-NEXT:    ret void
++;
++; PTX-LABEL: select(
++; PTX:       {
++; PTX-NEXT:    .reg .pred %p<2>;
++; PTX-NEXT:    .reg .b16 %rs<3>;
++; PTX-NEXT:    .reg .b32 %r<9>;
++; PTX-EMPTY:
++; PTX-NEXT:  // %bb.0:
++; PTX-NEXT:    ld.param.u8 %rs1, [select_param_2];
++; PTX-NEXT:    and.b16 %rs2, %rs1, 1;
++; PTX-NEXT:    setp.ne.b16 %p1, %rs2, 0;
++; PTX-NEXT:    ld.param.u32 %r1, [select_param_0];
++; PTX-NEXT:    cvta.to.global.u32 %r2, %r1;
++; PTX-NEXT:    ld.param.u32 %r3, [select_param_1];
++; PTX-NEXT:    cvta.to.global.u32 %r4, %r3;
++; PTX-NEXT:    ld.param.u32 %r5, [select_param_3];
++; PTX-NEXT:    cvta.to.global.u32 %r6, %r5;
++; PTX-NEXT:    selp.b32 %r7, %r2, %r4, %p1;
++; PTX-NEXT:    ld.global.nc.u32 %r8, [%r7];
++; PTX-NEXT:    st.global.u32 [%r6], %r8;
++; PTX-NEXT:    ret;
++  %select = select i1 %c, ptr %a, ptr %b
++  %select_global = addrspacecast ptr %select to ptr addrspace(1)
++  %val = load i32, ptr addrspace(1) %select_global
++  store i32 %val, ptr %out
++  ret void
++}
++
++define void @not_kernel(ptr noalias readonly %a, ptr %out) {
++; OPT-LABEL: define void @not_kernel(
++; OPT-SAME: ptr noalias readonly [[A:%.*]], ptr [[OUT:%.*]]) #[[ATTR0]] {
++; OPT-NEXT:    [[A_GLOBAL:%.*]] = addrspacecast ptr [[A]] to ptr addrspace(1)
++; OPT-NEXT:    [[VAL:%.*]] = load float, ptr addrspace(1) [[A_GLOBAL]], align 4
++; OPT-NEXT:    store float [[VAL]], ptr [[OUT]], align 4
++; OPT-NEXT:    ret void
++;
++; PTX-LABEL: not_kernel(
++; PTX:       {
++; PTX-NEXT:    .reg .b32 %r<4>;
++; PTX-NEXT:    .reg .b32 %f<2>;
++; PTX-EMPTY:
++; PTX-NEXT:  // %bb.0:
++; PTX-NEXT:    ld.param.u32 %r1, [not_kernel_param_0];
++; PTX-NEXT:    cvta.to.global.u32 %r2, %r1;
++; PTX-NEXT:    ld.param.u32 %r3, [not_kernel_param_1];
++; PTX-NEXT:    ld.global.f32 %f1, [%r2];
++; PTX-NEXT:    st.f32 [%r3], %f1;
++; PTX-NEXT:    ret;
++  %a_global = addrspacecast ptr %a to ptr addrspace(1)
++  %val = load float, ptr addrspace(1) %a_global
++  store float %val, ptr %out
++  ret void
++}
++
++%struct.S2 = type { i64, i64 }
++@G = private unnamed_addr constant %struct.S2 { i64 1, i64 1 }, align 8
++
++define ptx_kernel void @global_load(ptr noalias readonly %a, i1 %c, ptr %out) {
++; OPT-LABEL: define ptx_kernel void @global_load(
++; OPT-SAME: ptr noalias readonly [[A:%.*]], i1 [[C:%.*]], ptr [[OUT:%.*]]) #[[ATTR0]] {
++; OPT-NEXT:    [[G_GLOBAL:%.*]] = addrspacecast ptr @G to ptr addrspace(1)
++; OPT-NEXT:    [[A_GLOBAL:%.*]] = addrspacecast ptr [[A]] to ptr addrspace(1)
++; OPT-NEXT:    [[SELECT:%.*]] = select i1 [[C]], ptr addrspace(1) [[G_GLOBAL]], ptr addrspace(1) [[A_GLOBAL]]
++; OPT-NEXT:    [[VAL:%.*]] = load i64, ptr addrspace(1) [[SELECT]], align 8, !invariant.load [[META0]]
++; OPT-NEXT:    store i64 [[VAL]], ptr [[OUT]], align 8
++; OPT-NEXT:    ret void
++;
++; PTX-LABEL: global_load(
++; PTX:       {
++; PTX-NEXT:    .reg .pred %p<2>;
++; PTX-NEXT:    .reg .b16 %rs<3>;
++; PTX-NEXT:    .reg .b32 %r<7>;
++; PTX-NEXT:    .reg .b64 %rd<2>;
++; PTX-EMPTY:
++; PTX-NEXT:  // %bb.0:
++; PTX-NEXT:    ld.param.u8 %rs1, [global_load_param_1];
++; PTX-NEXT:    and.b16 %rs2, %rs1, 1;
++; PTX-NEXT:    setp.ne.b16 %p1, %rs2, 0;
++; PTX-NEXT:    ld.param.u32 %r1, [global_load_param_0];
++; PTX-NEXT:    cvta.to.global.u32 %r2, %r1;
++; PTX-NEXT:    ld.param.u32 %r3, [global_load_param_2];
++; PTX-NEXT:    cvta.to.global.u32 %r4, %r3;
++; PTX-NEXT:    mov.b32 %r5, G;
++; PTX-NEXT:    selp.b32 %r6, %r5, %r2, %p1;
++; PTX-NEXT:    ld.global.nc.u64 %rd1, [%r6];
++; PTX-NEXT:    st.global.u64 [%r4], %rd1;
++; PTX-NEXT:    ret;
++  %g_global = addrspacecast ptr @G to ptr addrspace(1)
++  %a_global = addrspacecast ptr %a to ptr addrspace(1)
++  %select = select i1 %c, ptr addrspace(1) %g_global, ptr addrspace(1) %a_global
++  %val = load i64, ptr addrspace(1) %select
++  store i64 %val, ptr %out
++  ret void
++}
++;.
++; OPT: [[META0]] = !{}
++;.
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/InstCombine/store.ll b/llvm/test/Transforms/InstCombine/store.ll
+--- a/llvm/test/Transforms/InstCombine/store.ll
++++ b/llvm/test/Transforms/InstCombine/store.ll
+@@ -49,7 +49,8 @@
+ 
+ define void @store_at_gep_off_null_inbounds(i64 %offset) {
+ ; CHECK-LABEL: @store_at_gep_off_null_inbounds(
+-; CHECK-NEXT:    store i32 poison, ptr null, align 4
++; CHECK-NEXT:    [[PTR:%.*]] = getelementptr inbounds i32, ptr null, i64 [[OFFSET:%.*]]
++; CHECK-NEXT:    store i32 poison, ptr [[PTR]], align 4
+ ; CHECK-NEXT:    ret void
+ ;
+   %ptr = getelementptr inbounds i32, ptr null, i64 %offset
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/InstSimplify/gep.ll b/llvm/test/Transforms/InstSimplify/gep.ll
+--- a/llvm/test/Transforms/InstSimplify/gep.ll
++++ b/llvm/test/Transforms/InstSimplify/gep.ll
+@@ -389,7 +389,8 @@
+ 
+ define ptr @gep_null_inbounds(i64 %idx) {
+ ; CHECK-LABEL: @gep_null_inbounds(
+-; CHECK-NEXT:    ret ptr null
++; CHECK-NEXT:    [[GEP:%.*]] = getelementptr inbounds i8, ptr null, i64 [[IDX:%.*]]
++; CHECK-NEXT:    ret ptr [[GEP]]
+ ;
+   %gep = getelementptr inbounds i8, ptr null, i64 %idx
+   ret ptr %gep
+@@ -415,7 +416,8 @@
+ 
+ define ptr @gep_null_inbounds_different_type(i64 %idx1, i64 %idx2) {
+ ; CHECK-LABEL: @gep_null_inbounds_different_type(
+-; CHECK-NEXT:    ret ptr null
++; CHECK-NEXT:    [[GEP:%.*]] = getelementptr inbounds [0 x i8], ptr null, i64 [[IDX1:%.*]], i64 [[IDX2:%.*]]
++; CHECK-NEXT:    ret ptr [[GEP]]
+ ;
+   %gep = getelementptr inbounds [0 x i8], ptr null, i64 %idx1, i64 %idx2
+   ret ptr %gep
+@@ -423,7 +425,8 @@
+ 
+ define <2 x ptr> @gep_inbounds_null_vec(i64 %idx) {
+ ; CHECK-LABEL: @gep_inbounds_null_vec(
+-; CHECK-NEXT:    ret <2 x ptr> zeroinitializer
++; CHECK-NEXT:    [[P:%.*]] = getelementptr inbounds i8, <2 x ptr> zeroinitializer, i64 [[IDX:%.*]]
++; CHECK-NEXT:    ret <2 x ptr> [[P]]
+ ;
+   %p = getelementptr inbounds i8, <2 x ptr> zeroinitializer, i64 %idx
+   ret <2 x ptr> %p
+@@ -431,7 +434,8 @@
+ 
+ define <2 x ptr> @gep_inbounds_null_vec_broadcast(<2 x i64> %idx) {
+ ; CHECK-LABEL: @gep_inbounds_null_vec_broadcast(
+-; CHECK-NEXT:    ret <2 x ptr> zeroinitializer
++; CHECK-NEXT:    [[P:%.*]] = getelementptr inbounds i8, ptr null, <2 x i64> [[IDX:%.*]]
++; CHECK-NEXT:    ret <2 x ptr> [[P]]
+ ;
+   %p = getelementptr inbounds i8, ptr null, <2 x i64> %idx
+   ret <2 x ptr> %p
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/X86/phi-comparator-fix-vec-ops-compare.ll b/llvm/test/Transforms/SLPVectorizer/X86/phi-comparator-fix-vec-ops-compare.ll
+--- a/llvm/test/Transforms/SLPVectorizer/X86/phi-comparator-fix-vec-ops-compare.ll
++++ b/llvm/test/Transforms/SLPVectorizer/X86/phi-comparator-fix-vec-ops-compare.ll
+@@ -0,0 +1,46 @@
++; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
++; RUN: opt -S --passes=slp-vectorizer -mtriple=x86_64-unknown-linux-gnu < %s | FileCheck %s
++
++define void @test({ <2 x float>, float } %0, <2 x float> %1, i1 %2) {
++; CHECK-LABEL: define void @test(
++; CHECK-SAME: { <2 x float>, float } [[TMP0:%.*]], <2 x float> [[TMP1:%.*]], i1 [[TMP2:%.*]]) {
++; CHECK-NEXT:    [[TMP4:%.*]] = extractvalue { <2 x float>, float } [[TMP0]], 0
++; CHECK-NEXT:    [[TMP5:%.*]] = extractelement <2 x float> [[TMP4]], i64 0
++; CHECK-NEXT:    [[TMP6:%.*]] = extractelement <2 x float> [[TMP1]], i64 1
++; CHECK-NEXT:    [[TMP7:%.*]] = extractelement <2 x float> [[TMP1]], i64 0
++; CHECK-NEXT:    br i1 [[TMP2]], label %[[BB9:.*]], label %[[BB8:.*]]
++; CHECK:       [[BB8]]:
++; CHECK-NEXT:    br label %[[BB9]]
++; CHECK:       [[BB9]]:
++; CHECK-NEXT:    [[TMP10:%.*]] = phi float [ 0.000000e+00, %[[BB8]] ], [ [[TMP7]], [[TMP3:%.*]] ]
++; CHECK-NEXT:    [[TMP11:%.*]] = phi float [ 0.000000e+00, %[[BB8]] ], [ [[TMP6]], [[TMP3]] ]
++; CHECK-NEXT:    [[TMP12:%.*]] = phi float [ 0.000000e+00, %[[BB8]] ], [ [[TMP5]], [[TMP3]] ]
++; CHECK-NEXT:    [[TMP13:%.*]] = fpext float [[TMP12]] to double
++; CHECK-NEXT:    [[TMP14:%.*]] = fpext float [[TMP11]] to double
++; CHECK-NEXT:    [[TMP15:%.*]] = tail call i32 (ptr, ptr, ...) @fprintf(ptr null, ptr null, double [[TMP13]], double [[TMP14]], double 0.000000e+00)
++; CHECK-NEXT:    [[TMP16:%.*]] = fpext float [[TMP10]] to double
++; CHECK-NEXT:    [[TMP17:%.*]] = tail call i32 (ptr, ptr, ...) @fprintf(ptr null, ptr null, double 0.000000e+00, double [[TMP16]], double 0.000000e+00)
++; CHECK-NEXT:    ret void
++;
++  %4 = extractvalue { <2 x float>, float } %0, 0
++  %5 = extractelement <2 x float> %4, i64 0
++  %6 = extractelement <2 x float> %1, i64 1
++  %7 = extractelement <2 x float> %1, i64 0
++  br i1 %2, label %9, label %8
++
++8:
++  br label %9
++
++9:
++  %10 = phi float [ 0.000000e+00, %8 ], [ %7, %3 ]
++  %11 = phi float [ 0.000000e+00, %8 ], [ %6, %3 ]
++  %12 = phi float [ 0.000000e+00, %8 ], [ %5, %3 ]
++  %13 = fpext float %12 to double
++  %14 = fpext float %11 to double
++  %15 = tail call i32 (ptr, ptr, ...) @fprintf(ptr null, ptr null, double %13, double %14, double 0.000000e+00)
++  %16 = fpext float %10 to double
++  %17 = tail call i32 (ptr, ptr, ...) @fprintf(ptr null, ptr null, double 0.000000e+00, double %16, double 0.000000e+00)
++  ret void
++}
++
++declare i32 @fprintf(ptr, ptr, ...)
+diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel
+--- a/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel
++++ b/utils/bazel/llvm-project-overlay/mlir/BUILD.bazel
+@@ -3801,6 +3801,7 @@
+         ":ControlFlowToSCF",
+         ":ControlFlowToSPIRV",
+         ":ConversionPassIncGen",
++        ":ConvertToEmitC",
+         ":ConvertToLLVM",
+         ":FuncToEmitC",
+         ":FuncToLLVM",
+@@ -3919,6 +3920,7 @@
+         ":BufferizationInterfaces",
+         ":ControlFlowDialect",
+         ":ControlFlowInterfaces",
++        ":ConvertToEmitCInterface",
+         ":DestinationStyleOpInterface",
+         ":FunctionInterfaces",
+         ":IR",
+@@ -4332,6 +4334,7 @@
+         ":BytecodeOpInterface",
+         ":CallOpInterfaces",
+         ":ControlFlowInterfaces",
++        ":ConvertToEmitCInterface",
+         ":ConvertToLLVMInterface",
+         ":FuncIncGen",
+         ":FunctionInterfaces",
+@@ -4411,6 +4414,7 @@
+     deps = [
+         ":AMXTransforms",
+         ":AffineTransformOps",
++        ":ArithToEmitC",
+         ":ArithToLLVM",
+         ":BufferizationTransformOps",
+         ":BuiltinToLLVMIRTranslation",
+@@ -4418,6 +4422,7 @@
+         ":ControlFlowToLLVM",
+         ":DLTITransformOps",
+         ":FuncExtensions",
++        ":FuncToEmitC",
+         ":FuncToLLVM",
+         ":FuncTransformOps",
+         ":GPUToGPURuntimeTransforms",
+@@ -4429,6 +4434,7 @@
+         ":LinalgTransformOps",
+         ":MPIToLLVM",
+         ":MathToLLVM",
++        ":MemRefToEmitC",
+         ":MemRefToLLVM",
+         ":MemRefTransformOps",
+         ":MeshDialect",
+@@ -4438,6 +4444,7 @@
+         ":OpenMPToLLVM",
+         ":ROCDLTarget",
+         ":ROCDLToLLVMIRTranslation",
++        ":SCFToEmitC",
+         ":SCFTransformOps",
+         ":SparseTensorTransformOps",
+         ":TensorExtensions",
+@@ -6755,6 +6762,7 @@
+     ],
+     deps = [
+         ":ConversionPassIncGen",
++        ":ConvertToEmitCInterface",
+         ":EmitCDialect",
+         ":FuncDialect",
+         ":Pass",
+@@ -7506,6 +7514,7 @@
+     deps = [
+         ":ArithDialect",
+         ":ConversionPassIncGen",
++        ":ConvertToEmitCInterface",
+         ":EmitCDialect",
+         ":EmitCTransforms",
+         ":IR",
+@@ -7658,6 +7667,34 @@
+ )
+ 
+ cc_library(
++    name = "ConvertToEmitCInterface",
++    hdrs = ["include/mlir/Conversion/ConvertToEmitC/ToEmitCInterface.h"],
++    includes = ["include"],
++    deps = [
++        ":ConversionPassIncGen",
++        ":IR",
++        "//llvm:Support",
++    ],
++)
++
++cc_library(
++    name = "ConvertToEmitC",
++    srcs = ["lib/Conversion/ConvertToEmitC/ConvertToEmitCPass.cpp"],
++    hdrs = ["include/mlir/Conversion/ConvertToEmitC/ConvertToEmitCPass.h"],
++    includes = ["include"],
++    deps = [
++        ":ConversionPassIncGen",
++        ":ConvertToEmitCInterface",
++        ":EmitCDialect",
++        ":IR",
++        ":LLVMCommonConversion",
++        ":Pass",
++        ":TransformUtils",
++        "//llvm:Support",
++    ],
++)
++
++cc_library(
+     name = "FuncToLLVM",
+     srcs = [
+         "lib/Conversion/FuncToLLVM/FuncToLLVM.cpp",
+@@ -7789,6 +7826,7 @@
+     ],
+     deps = [
+         ":ConversionPassIncGen",
++        ":ConvertToEmitCInterface",
+         ":EmitCDialect",
+         ":IR",
+         ":MemRefDialect",
+@@ -7908,6 +7946,7 @@
+     deps = [
+         ":ArithDialect",
+         ":ConversionPassIncGen",
++        ":ConvertToEmitCInterface",
+         ":EmitCDialect",
+         ":EmitCTransforms",
+         ":IR",
+@@ -8764,6 +8803,7 @@
+         ":ControlFlowDialect",
+         ":ControlFlowTransforms",
+         ":ConversionPasses",
++        ":ConvertToEmitC",
+         ":ConvertToLLVM",
+         ":DLTIDialect",
+         ":EmitCDialect",
+@@ -11768,6 +11808,7 @@
+         ":CastInterfaces",
+         ":CommonFolders",
+         ":ControlFlowInterfaces",
++        ":ConvertToEmitCInterface",
+         ":ConvertToLLVMInterface",
+         ":DestinationStyleOpInterface",
+         ":IR",
+@@ -12044,6 +12085,7 @@
+         ":CallOpInterfaces",
+         ":CastInterfaces",
+         ":ControlFlowInterfaces",
++        ":ConvertToEmitCInterface",
+         ":ConvertToLLVMInterface",
+         ":CopyOpInterface",
+         ":DialectUtils",
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index 9851b18..a8b5ea7 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "7752e0a10b25da2f2eadbed10606bd5454dbca05"
-    LLVM_SHA256 = "1e67e67854bf00c07e5f876083cf7482d2ed4719b8d6595179a945f9c9f7ffe7"
+    LLVM_COMMIT = "0009a1783490a8ff69251a0ec7df1891a427cfb0"
+    LLVM_SHA256 = "68bc21389501bb2662f6b7d9385cd2d8419863e2f492b1bb35e6ff1cb12e3f7b"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index bdca7e0..6164236 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -250,6 +250,15 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehl
  
    return success();
  }
+@@ -1094,7 +1094,7 @@
+   size_t firstFunctionalArgument =
+       leadingTokenOperands + key.getGlobalConstants().size();
+   argIndices.set(leadingTokenOperands, firstFunctionalArgument);
+-  func.eraseArguments(argIndices);
++  if (failed(func.eraseArguments(argIndices))) return failure();
+ 
+   // Refine the remaining argument types, wrap with shape buffer custom calls.
+   SmallVector<Type> refinedTypes =
 diff --ruN a/stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp b/stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp
 --- stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp
 +++ stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp
