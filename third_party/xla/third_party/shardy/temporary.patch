diff --git a/docs/sdy_dialect.md b/docs/sdy_dialect.md
index 58ffa07..dc5857b 100755
--- a/docs/sdy_dialect.md
+++ b/docs/sdy_dialect.md
@@ -466,9 +466,6 @@ communication.
 The body is local wrt the manual_axes. Propagation will occur through
 the body on any free axes - those not in the manual_axes list.
 
-Note that any unranked tensors are expected to have a sharding with rank 0,
-i.e. fully replicated.
-
 **Constraints:**
 - Elements in `in_shardings` and `out_shardings` must satisfy the constraints listed in `TensorShardingAttr`.
 - The number of global and local tensor inputs/outputs of the op region must match.
@@ -494,13 +491,13 @@ Interfaces: `ShardableDataFlowOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensors` | variadic of any type |
+| `tensors` | variadic of ranked tensor of any type values |
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `results` | variadic of any type |
+| `results` | variadic of ranked tensor of any type values |
 
 
 
@@ -1277,10 +1274,9 @@ name, referencing a corresponding `MeshOp` symbol, or an inlined `MeshAttr`.
 - Elements in `dim_shardings` must satisfy the constraints listed in `DimensionShardingAttr`.
 - Elements in `replicated_axes` must satisfy the constraints listed in `AxisRefListAttr`.
 - If the corresponding tensor type isn't a `ShapedType`, the sharding must have rank 0 and no replicated axes.
-- If it is a `ShapedType`, then:
-  - The tensor should have a rank.
-  - The number of dimension shardings is equal to the rank of the tensor.
-  - Dimensions of size 0 aren't sharded.
+- The tensor should have a rank.
+- The number of dimension shardings is equal to the rank of the tensor.
+- Dimensions of size 0 aren't sharded.
 - Items in `replicated_axes` are ordered w.r.t. `mesh_or_ref` (see `AxisRefAttr::getMeshComparator`).
 
 #### Parameters:
diff --git a/shardy/dialect/sdy/ir/attrs.td b/shardy/dialect/sdy/ir/attrs.td
index a1d21f4..27aa8cd 100644
--- a/shardy/dialect/sdy/ir/attrs.td
+++ b/shardy/dialect/sdy/ir/attrs.td
@@ -576,10 +576,9 @@ def Sdy_TensorSharding : AttrDef<Sdy_Dialect, "TensorSharding"> {
     - Elements in `dim_shardings` must satisfy the constraints listed in `DimensionShardingAttr`.
     - Elements in `replicated_axes` must satisfy the constraints listed in `AxisRefListAttr`.
     - If the corresponding tensor type isn't a `ShapedType`, the sharding must have rank 0 and no replicated axes.
-    - If it is a `ShapedType`, then:
-      - The tensor should have a rank.
-      - The number of dimension shardings is equal to the rank of the tensor.
-      - Dimensions of size 0 aren't sharded.
+    - The tensor should have a rank.
+    - The number of dimension shardings is equal to the rank of the tensor.
+    - Dimensions of size 0 aren't sharded.
     - Items in `replicated_axes` are ordered w.r.t. `mesh_or_ref` (see `AxisRefAttr::getMeshComparator`).
   }];
   let parameters = (ins
diff --git a/shardy/dialect/sdy/ir/ops.td b/shardy/dialect/sdy/ir/ops.td
index 2fca247..61ed12b 100644
--- a/shardy/dialect/sdy/ir/ops.td
+++ b/shardy/dialect/sdy/ir/ops.td
@@ -17,7 +17,6 @@ limitations under the License.
 #define SDY_OPS
 
 include "mlir/Interfaces/SideEffectInterfaces.td"
-include "mlir/IR/AttrTypeBase.td"
 include "mlir/IR/BuiltinAttributeInterfaces.td"
 include "mlir/IR/OpBase.td"
 include "mlir/IR/SymbolInterfaces.td"
@@ -143,9 +142,6 @@ def Sdy_ManualComputationOp : Sdy_Op<"manual_computation",
     The body is local wrt the manual_axes. Propagation will occur through
     the body on any free axes - those not in the manual_axes list.
 
-    Note that any unranked tensors are expected to have a sharding with rank 0,
-    i.e. fully replicated.
-
     **Constraints:**
     - Elements in `in_shardings` and `out_shardings` must satisfy the constraints listed in `TensorShardingAttr`.
     - The number of global and local tensor inputs/outputs of the op region must match.
@@ -156,12 +152,12 @@ def Sdy_ManualComputationOp : Sdy_Op<"manual_computation",
   }];
 
   let arguments = (ins
-    Variadic<AnyType>:$tensors,
+    Variadic<AnyRankedTensor>:$tensors,
     Sdy_TensorShardingPerValue:$in_shardings,
     Sdy_TensorShardingPerValue:$out_shardings,
     Sdy_ManualAxes:$manual_axes
   );
-  let results = (outs Variadic<AnyType>:$results);
+  let results = (outs Variadic<AnyRankedTensor>:$results);
   let regions = (region SizedRegion<1>:$body);
 
   let assemblyFormat = [{
diff --git a/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir b/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir
index 22bd93f..e07c803 100644
--- a/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir
+++ b/shardy/dialect/sdy/ir/test/manual_computation_verification.mlir
@@ -328,18 +328,3 @@ func.func @correct_dynamic_dim_static_dim_mismatch(%arg0: tensor<?x32xf32>) -> t
   } : (tensor<?x32xf32>) -> tensor<?x64xf32>
   func.return %0: tensor<?x64xf32>
 }
-
-// -----
-
-sdy.mesh @mesh = <["a"=2]>
-
-func.func @ranked_sharding_on_token(%arg0: !stablehlo.token) -> !stablehlo.token {
-  // expected-error @+1 {{'sdy.manual_computation' op operand - non-shaped tensors can only have a sharding with rank 0 and no replicated axes.}}
-  %0 = sdy.manual_computation(%arg0)
-      in_shardings=[<@mesh, [{"a"}]>]
-      out_shardings=[<@mesh, [{"a"}]>]
-      manual_axes={"b"} (%arg1: !stablehlo.token) {
-    sdy.return %arg1 : !stablehlo.token
-  } : (!stablehlo.token) -> !stablehlo.token
-  return %0 : !stablehlo.token
-}
diff --git a/shardy/dialect/sdy/ir/test/tensor_sharding_parse_print.mlir b/shardy/dialect/sdy/ir/test/tensor_sharding_parse_print.mlir
index c087a23..2708cc3 100644
--- a/shardy/dialect/sdy/ir/test/tensor_sharding_parse_print.mlir
+++ b/shardy/dialect/sdy/ir/test/tensor_sharding_parse_print.mlir
@@ -166,12 +166,3 @@ func.func @maximal_sharding_no_results(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32
   stablehlo.custom_call @foo(%arg0) {has_side_effect = true, sdy.sharding = #sdy.sharding_per_value<[<@maximal_mesh, []>]>} : (tensor<8x8xf32>) -> ()
   return %arg0 : tensor<8x8xf32>
 }
-
-// CHECK-LABEL: func @replicated_sharding_no_results
-// CHECK-SAME:      (%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
-func.func @replicated_sharding_no_results(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
-  // CHECK-NEXT: stablehlo.custom_call @foo(%arg0) {has_side_effect = true, sdy.sharding = #sdy.sharding_per_value<[<@foo, []>]>} : (tensor<8x8xf32>) -> ()
-  // CHECK-NEXT: return %arg0 : tensor<8x8xf32>
-  stablehlo.custom_call @foo(%arg0) {has_side_effect = true, sdy.sharding = #sdy.sharding_per_value<[<@foo, []>]>} : (tensor<8x8xf32>) -> ()
-  return %arg0 : tensor<8x8xf32>
-}
diff --git a/shardy/dialect/sdy/ir/verifiers.cc b/shardy/dialect/sdy/ir/verifiers.cc
index 3c38d5f..9bcab5f 100644
--- a/shardy/dialect/sdy/ir/verifiers.cc
+++ b/shardy/dialect/sdy/ir/verifiers.cc
@@ -230,7 +230,7 @@ LogicalResult verifyTensorShardingAttr(TensorShardingAttr shardingAttr,
                                        EmitErrorFn emitError,
                                        bool checkDivisibility,
                                        ManualAxisToOwner alreadyManualAxes) {
-  if (mesh.isMaximal() || (!type && shardingAttr.isFullyReplicated())) {
+  if (mesh.isMaximal()) {
     // A maximal sharding says that this op should be executed on a single
     // device. Skip checking against the type of the op. Just make sure there
     // are no dimension shardings and replicated axes.
@@ -397,14 +397,11 @@ LogicalResult verifyTensorShardingPerValueAttr(
     bool verifyCommonMesh = true) {
   ArrayRef<TensorShardingAttr> shardingsPerValue =
       shardingPerValueAttr.getShardings();
-  if (types.empty() && shardingsPerValue.size() == 1) {
-    TensorShardingAttr firstSharding = shardingsPerValue.front();
-    if (firstSharding.getMesh(symbolTable).isMaximal() ||
-        firstSharding.isFullyReplicated()) {
-      return verifyTensorShardingAttr(
-          firstSharding, Type(), op, symbolTable,
-          getEmitValueInRangeErrorFn(emitError, types.size(), /*index=*/0));
-    }
+  if (types.empty() && shardingsPerValue.size() == 1 &&
+      shardingsPerValue.front().getMesh(symbolTable).isMaximal()) {
+    return verifyTensorShardingAttr(
+        shardingsPerValue.front(), Type(), op, symbolTable,
+        getEmitValueInRangeErrorFn(emitError, types.size(), /*index=*/0));
   }
   if (shardingsPerValue.size() != types.size()) {
     return emitError("shardings don't match number of values: ")
@@ -862,14 +859,9 @@ LogicalResult verifyManualComputationValue(
     }
 
     SmallVector<int64_t> newDimSizes;
-    auto globalShapedType = mlir::dyn_cast<ShapedType>(globalType);
-    if (!globalShapedType) {
-      // Skipping verification for non-shaped types. This could for example be
-      // a token type.
-      continue;
-    }
+    auto globalRankedType = mlir::cast<RankedTensorType>(globalType);
     for (auto [dimensionSize, dimSharding] : llvm::zip_equal(
-             globalShapedType.getShape(), sharding.getDimShardings())) {
+             globalRankedType.getShape(), sharding.getDimShardings())) {
       if (dimensionSize == ShapedType::kDynamic) {
         newDimSizes.push_back(ShapedType::kDynamic);
       } else {
@@ -892,7 +884,7 @@ LogicalResult verifyManualComputationValue(
     // 6. Verify the global shape and local shapes of the op regions
     //    arguments/results match.
     auto expectedLocalRankedType =
-        RankedTensorType::get(newDimSizes, globalShapedType.getElementType());
+        RankedTensorType::get(newDimSizes, globalRankedType.getElementType());
     auto localRankedType = mlir::cast<RankedTensorType>(localType);
     if (expectedLocalRankedType != localRankedType) {
       return op->emitOpError(valueKindStr)
diff --git a/shardy/dialect/sdy/transforms/import/manual_axes_cleanup.cc b/shardy/dialect/sdy/transforms/import/manual_axes_cleanup.cc
index 4bf5d06..67e2042 100644
--- a/shardy/dialect/sdy/transforms/import/manual_axes_cleanup.cc
+++ b/shardy/dialect/sdy/transforms/import/manual_axes_cleanup.cc
@@ -25,11 +25,9 @@ limitations under the License.
 #include "mlir/IR/Attributes.h"
 #include "mlir/IR/Builders.h"
 #include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinTypeInterfaces.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/Operation.h"
 #include "mlir/IR/SymbolTable.h"
-#include "mlir/IR/TypeRange.h"
 #include "mlir/Pass/Pass.h"  // IWYU pragma: keep
 #include "mlir/Rewrite/FrozenRewritePatternSet.h"
 #include "mlir/Support/LLVM.h"
@@ -51,17 +49,12 @@ namespace {
 // its mesh with `commonMeshOrRef`.
 std::optional<SmallVector<TensorShardingAttr>>
 addUnusedManualAxesToReplicatedAxes(
-    ArrayRef<TensorShardingAttr> shardings, TypeRange types,
-    ArrayRef<StringAttr> manualAxes, Attribute commonMeshOrRef,
-    const SymbolTable& symbolTable,
+    ArrayRef<TensorShardingAttr> shardings, ArrayRef<StringAttr> manualAxes,
+    Attribute commonMeshOrRef, const SymbolTable& symbolTable,
     std::function<bool(AxisRefAttr lhs, AxisRefAttr rhs)> meshComparator) {
   SmallVector<TensorShardingAttr> newShardings;
   bool modified = false;
-  for (auto [sharding, type] : llvm::zip_equal(shardings, types)) {
-    if (!isa<ShapedType>(type)) {
-      newShardings.push_back(sharding);
-      continue;
-    }
+  for (TensorShardingAttr sharding : shardings) {
     llvm::SmallSet<StringRef, 2> unusedManualAxes;
     unusedManualAxes.insert(manualAxes.begin(), manualAxes.end());
     sharding.forEachAxisRef([&unusedManualAxes](AxisRefAttr axis) {
@@ -104,15 +97,15 @@ void addUnusedManualAxesToReplicatedAxes(ManualComputationOp op, MeshAttr mesh,
 
   if (std::optional<SmallVector<TensorShardingAttr>> newShardings =
           addUnusedManualAxesToReplicatedAxes(
-              op.getInShardings().getShardings(), op->getOperandTypes(),
-              manualAxes, commonMeshOrRef, symbolTable, meshComparator)) {
+              op.getInShardings().getShardings(), manualAxes, commonMeshOrRef,
+              symbolTable, meshComparator)) {
     op.setInShardings(*newShardings);
   }
 
   if (std::optional<SmallVector<TensorShardingAttr>> newShardings =
           addUnusedManualAxesToReplicatedAxes(
-              op.getOutShardings().getShardings(), op->getResultTypes(),
-              manualAxes, commonMeshOrRef, symbolTable, meshComparator)) {
+              op.getOutShardings().getShardings(), manualAxes, commonMeshOrRef,
+              symbolTable, meshComparator)) {
     op.setOutShardings(*newShardings);
   }
 }
diff --git a/shardy/dialect/sdy/transforms/import/test/add_data_flow_edges.mlir b/shardy/dialect/sdy/transforms/import/test/add_data_flow_edges.mlir
index 8379926..bcdd692 100644
--- a/shardy/dialect/sdy/transforms/import/test/add_data_flow_edges.mlir
+++ b/shardy/dialect/sdy/transforms/import/test/add_data_flow_edges.mlir
@@ -180,23 +180,6 @@ func.func @named_computation_skip_tokens(%arg0: tensor<8x2xi32>, %arg1: !stableh
   return %0#0, %0#1 : tensor<8x2xi32>, !stablehlo.token
 }
 
-// CHECK-LABEL: func @manual_computation_skip_tokens
-func.func @manual_computation_skip_tokens(%arg0: tensor<8x2xi32>, %arg1: !stablehlo.token) -> (tensor<8x2xi32>, !stablehlo.token) {
-  // CHECK-NEXT: %[[MC:.*]]:2 = sdy.manual_computation(%arg0, %arg1)
-  // CHECK-NEXT:   %[[EDGE_1:.*]] = sdy.data_flow_edge %arg2 sharding=<@mesh, [{"a", ?}, {?}]> : tensor<8x2xi32>
-  // CHECK-NEXT:   sdy.return %[[EDGE_1]], %arg3 : tensor<8x2xi32>, !stablehlo.token
-  // CHECK-NEXT: } : (tensor<8x2xi32>, !stablehlo.token) -> (tensor<8x2xi32>, !stablehlo.token)
-  // CHECK-NEXT: %[[EDGE_2:.*]] = sdy.data_flow_edge %[[MC]]#0 sharding=<@mesh, [{"a", ?}, {?}], replicated={"b"}> : tensor<8x2xi32>
-  // CHECK-NEXT: return %[[EDGE_2]], %[[MC]]#1 : tensor<8x2xi32>, !stablehlo.token
-  %0:2 = sdy.manual_computation(%arg0, %arg1)
-      in_shardings=[<@mesh, [{"a", ?}, {?}], replicated={"b"}>, <@mesh, []>]
-      out_shardings=[<@mesh, [{"a", ?}, {?}], replicated={"b"}>, <@mesh, []>]
-      manual_axes={"b"}  (%arg2: tensor<8x2xi32>, %arg3: !stablehlo.token) {
-    sdy.return %arg2, %arg3 : tensor<8x2xi32>, !stablehlo.token
-  } : (tensor<8x2xi32>, !stablehlo.token) -> (tensor<8x2xi32>, !stablehlo.token)
-  return %0#0, %0#1 : tensor<8x2xi32>, !stablehlo.token
-}
-
 // CHECK-LABEL: func @manual_computation_multiple_inputs_outputs
 func.func @manual_computation_multiple_inputs_outputs(%arg0: tensor<8x2xi32>, %arg1: tensor<4x2xi32>) -> (tensor<8x2xi32>, tensor<4x2xi32>) {
   // CHECK-NEXT: %[[MC:.*]]:2 = sdy.manual_computation(%arg0, %arg1)
diff --git a/shardy/dialect/sdy/transforms/import/test/manual_axes_cleanup.mlir b/shardy/dialect/sdy/transforms/import/test/manual_axes_cleanup.mlir
index b37708b..e2f9d1d 100644
--- a/shardy/dialect/sdy/transforms/import/test/manual_axes_cleanup.mlir
+++ b/shardy/dialect/sdy/transforms/import/test/manual_axes_cleanup.mlir
@@ -126,18 +126,3 @@ func.func @empty_mesh_operand(%arg0: tensor<8xf32>) -> tensor<8xf32> {
   } : (tensor<8xf32>) -> tensor<8xf32>
   return %0 : tensor<8xf32>
 }
-
-// CHECK-LABEL: @dont_add_manual_axes_to_non_shaped_types
-func.func @dont_add_manual_axes_to_non_shaped_types(%arg0: !stablehlo.token, %arg1: tensor<8xf32>) -> (!stablehlo.token, tensor<8xf32>) {
-  // CHECK-NEXT: sdy.manual_computation(%arg0, %arg1)
-  // CHECK-SAME{LITERAL}: in_shardings=[<@mesh, []>, <@mesh, [{?}], replicated={"c"}>]
-  // CHECK-SAME{LITERAL}: out_shardings=[<@mesh, []>, <@mesh, [{?}], replicated={"c"}>]
-  // CHECK-SAME{LITERAL}: manual_axes={"c"} (%arg2: !stablehlo.token, %arg3: tensor<8xf32>) {
-  %0:2 = sdy.manual_computation(%arg0, %arg1)
-      in_shardings=[<@mesh, []>, <@mesh, [{?}]>]
-      out_shardings=[<@mesh, []>, <@mesh, [{?}]>]
-      manual_axes={"c"} (%arg2: !stablehlo.token, %arg3: tensor<8xf32>) {
-    sdy.return %arg2, %arg3 : !stablehlo.token, tensor<8xf32>
-  } : (!stablehlo.token, tensor<8xf32>) -> (!stablehlo.token, tensor<8xf32>)
-  return %0#0, %0#1 : !stablehlo.token, tensor<8xf32>
-}
diff --git a/shardy/dialect/sdy/transforms/propagation/test/basic_propagation_manual_computation.mlir b/shardy/dialect/sdy/transforms/propagation/test/basic_propagation_manual_computation.mlir
index f79adf5..f48fcac 100644
--- a/shardy/dialect/sdy/transforms/propagation/test/basic_propagation_manual_computation.mlir
+++ b/shardy/dialect/sdy/transforms/propagation/test/basic_propagation_manual_computation.mlir
@@ -488,31 +488,3 @@ func.func @dont_propagate_into_in_sharding_closed_dim_from_outside(%arg0: tensor
   } : (tensor<32x32xf32>) -> tensor<32x32xf32>
   func.return %1: tensor<32x32xf32>
 }
-
-// CHECK-LABEL: func @manual_computation_with_tokens
-// CHECK-SAME:      %arg0: !stablehlo.token {sdy.sharding = #sdy.sharding<@mesh, []>},
-// CHECK-SAME:      %arg1: tensor<4x4xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {"b", ?}]>})
-// CHECK-SAME:      -> (!stablehlo.token, tensor<4x4xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"a", ?}, {"b", ?}]>}) {
-func.func @manual_computation_with_tokens(
-    %arg0: !stablehlo.token {sdy.sharding = #sdy.sharding<@mesh, []>},
-    %arg1: tensor<4x4xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {?}]>}
-) -> (!stablehlo.token, tensor<4x4xi64>) {
-  // CHECK-NEXT: %[[MAN_COMP:.*]]:2 = sdy.manual_computation(%arg0, %arg1)
-  // CHECK-SAME{LITERAL}:   in_shardings=[<@mesh, []>, <@mesh, [{"a", ?}, {"b"}]>]
-  // CHECK-SAME{LITERAL}:   out_shardings=[<@mesh, []>, <@mesh, [{"a", ?}, {"b"}]>]
-  // CHECK-SAME{LITERAL}:   manual_axes={"b"} (%arg2: !stablehlo.token, %arg3: tensor<4x2xi64>) {
-  // CHECK-NEXT:   %[[TOK:.*]] = stablehlo.custom_call @sdy_testonly(%arg2) : (!stablehlo.token) -> !stablehlo.token
-  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg3, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a", ?}, {?}]>]>} : tensor<4x2xi64>
-  // CHECK-NEXT:   sdy.return %[[TOK]], %[[ADD]] : !stablehlo.token, tensor<4x2xi64>
-  // CHECK-NEXT: } : (!stablehlo.token, tensor<4x4xi64>) -> (!stablehlo.token, tensor<4x4xi64>)
-  // CHECK-NEXT: return %[[MAN_COMP]]#0, %[[MAN_COMP]]#1 : !stablehlo.token, tensor<4x4xi64>
-  %0:2 = sdy.manual_computation(%arg0, %arg1)
-      in_shardings=[<@mesh, []>, <@mesh, [{?}, {"b"}]>]
-      out_shardings=[<@mesh, []>, <@mesh, [{?}, {"b"}]>]
-      manual_axes={"b"} (%arg2: !stablehlo.token, %arg3: tensor<4x2xi64>) {
-    %1 = stablehlo.custom_call @sdy_testonly(%arg2) : (!stablehlo.token) -> (!stablehlo.token)
-    %2 = stablehlo.add %arg3, %arg3 : tensor<4x2xi64>
-    sdy.return %1, %2 : !stablehlo.token, tensor<4x2xi64>
-  } : (!stablehlo.token, tensor<4x4xi64>) -> (!stablehlo.token, tensor<4x4xi64>)
-  return %0#0, %0#1 : !stablehlo.token, tensor<4x4xi64>
-}
diff --git a/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir b/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir
index 8dfa809..9790325 100644
--- a/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir
+++ b/shardy/dialect/sdy/transforms/propagation/test/propagation_pipeline.mlir
@@ -124,51 +124,3 @@ func.func @maximal_sharding_no_results(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32
   stablehlo.custom_call @xla_python_cpu_callback(%arg0) {has_side_effect = true, sdy.sharding = #sdy.sharding_per_value<[<@maximal_mesh, []>]>} : (tensor<8x8xf32>) -> ()
   return %arg0 : tensor<8x8xf32>
 }
-
-// -----
-
-sdy.mesh @mesh = <["a"=2, "b"=2]>
-
-// Nothing should be propagated, but this verifies the `transformShardings`
-// sharding walker is able to handle a replicated sharding with no returned
-// values.
-// CHECK-LABEL: func @replicated_sharding_no_results
-// CHECK-SAME:      (%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
-func.func @replicated_sharding_no_results(%arg0: tensor<8x8xf32>) -> tensor<8x8xf32> {
-  // CHECK-NEXT: stablehlo.custom_call @sdy_testonly(%arg0) {has_side_effect = true, sdy.sharding = #sdy.sharding_per_value<[<@mesh, []>]>} : (tensor<8x8xf32>) -> ()
-  // CHECK-NEXT: return %arg0 : tensor<8x8xf32>
-  stablehlo.custom_call @sdy_testonly(%arg0) {has_side_effect = true, sdy.sharding = #sdy.sharding_per_value<[<@mesh, []>]>} : (tensor<8x8xf32>) -> ()
-  return %arg0 : tensor<8x8xf32>
-}
-
-// -----
-
-sdy.mesh @mesh = <["a"=2, "b"=2]>
-
-// CHECK-LABEL: func @manual_computation_with_tokens
-// CHECK-SAME:      %arg0: !stablehlo.token {sdy.sharding = #sdy.sharding<@mesh, []>},
-// CHECK-SAME:      %arg1: tensor<4x4xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {"b", ?}]>})
-// CHECK-SAME:      -> (!stablehlo.token, tensor<4x4xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"a", ?}, {"b", ?}]>}) {
-func.func @manual_computation_with_tokens(
-    %arg0: !stablehlo.token {sdy.sharding = #sdy.sharding<@mesh, []>},
-    %arg1: tensor<4x4xi64> {sdy.sharding = #sdy.sharding<@mesh, [{"a"}, {?}]>}
-) -> (!stablehlo.token, tensor<4x4xi64>) {
-  // CHECK-NEXT: %[[MAN_COMP:.*]]:2 = sdy.manual_computation(%arg0, %arg1)
-  // CHECK-SAME{LITERAL}:   in_shardings=[<@mesh, []>, <@mesh, [{"a", ?}, {"b"}]>]
-  // CHECK-SAME{LITERAL}:   out_shardings=[<@mesh, []>, <@mesh, [{"a", ?}, {"b"}]>]
-  // CHECK-SAME{LITERAL}:   manual_axes={"b"} (%arg2: !stablehlo.token, %arg3: tensor<4x2xi64>) {
-  // CHECK-NEXT:   %[[TOK:.*]] = stablehlo.custom_call @sdy_testonly(%arg2) : (!stablehlo.token) -> !stablehlo.token
-  // CHECK-NEXT:   %[[ADD:.*]] = stablehlo.add %arg3, %arg3 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"a", ?}, {?}]>]>} : tensor<4x2xi64>
-  // CHECK-NEXT:   sdy.return %[[TOK]], %[[ADD]] : !stablehlo.token, tensor<4x2xi64>
-  // CHECK-NEXT: } : (!stablehlo.token, tensor<4x4xi64>) -> (!stablehlo.token, tensor<4x4xi64>)
-  // CHECK-NEXT: return %[[MAN_COMP]]#0, %[[MAN_COMP]]#1 : !stablehlo.token, tensor<4x4xi64>
-  %0:2 = sdy.manual_computation(%arg0, %arg1)
-      in_shardings=[<@mesh, []>, <@mesh, [{?}, {"b"}]>]
-      out_shardings=[<@mesh, []>, <@mesh, [{?}, {"b"}]>]
-      manual_axes={"b"} (%arg2: !stablehlo.token, %arg3: tensor<4x2xi64>) {
-    %1 = stablehlo.custom_call @sdy_testonly(%arg2) : (!stablehlo.token) -> (!stablehlo.token)
-    %2 = stablehlo.add %arg3, %arg3 : tensor<4x2xi64>
-    sdy.return %1, %2 : !stablehlo.token, tensor<4x2xi64>
-  } : (!stablehlo.token, tensor<4x4xi64>) -> (!stablehlo.token, tensor<4x4xi64>)
-  return %0#0, %0#1 : !stablehlo.token, tensor<4x4xi64>
-}
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 442e692..509398d 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,154 +1 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/clang/include/clang/AST/Type.h b/clang/include/clang/AST/Type.h
---- a/clang/include/clang/AST/Type.h
-+++ b/clang/include/clang/AST/Type.h
-@@ -3602,6 +3602,9 @@
-   }
- 
-   NestedNameSpecifier *getQualifier() const { return Qualifier; }
-+  /// Note: this can trigger extra deserialization when external AST sources are
-+  /// used. Prefer `getCXXRecordDecl()` unless you really need the most recent
-+  /// decl.
-   CXXRecordDecl *getMostRecentCXXRecordDecl() const;
- 
-   bool isSugared() const;
-@@ -3610,7 +3613,10 @@
-   }
- 
-   void Profile(llvm::FoldingSetNodeID &ID) {
--    Profile(ID, getPointeeType(), getQualifier(), getMostRecentCXXRecordDecl());
-+    // FIXME: `getMostRecentCXXRecordDecl()` should be possible to use here,
-+    // however when external AST sources are used it causes nondeterminism
-+    // issues (see https://github.com/llvm/llvm-project/pull/137910).
-+    Profile(ID, getPointeeType(), getQualifier(), getCXXRecordDecl());
-   }
- 
-   static void Profile(llvm::FoldingSetNodeID &ID, QualType Pointee,
-@@ -3620,6 +3626,9 @@
-   static bool classof(const Type *T) {
-     return T->getTypeClass() == MemberPointer;
-   }
-+
-+private:
-+  CXXRecordDecl *getCXXRecordDecl() const;
- };
- 
- /// Capture whether this is a normal array (e.g. int X[4])
-diff -ruN --strip-trailing-cr a/clang/lib/AST/Type.cpp b/clang/lib/AST/Type.cpp
---- a/clang/lib/AST/Type.cpp
-+++ b/clang/lib/AST/Type.cpp
-@@ -5305,10 +5305,14 @@
-     ID.AddPointer(Cls->getCanonicalDecl());
- }
- 
-+CXXRecordDecl *MemberPointerType::getCXXRecordDecl() const {
-+  return dyn_cast<MemberPointerType>(getCanonicalTypeInternal())
-+      ->getQualifier()
-+      ->getAsRecordDecl();
-+}
-+
- CXXRecordDecl *MemberPointerType::getMostRecentCXXRecordDecl() const {
--  auto *RD = dyn_cast<MemberPointerType>(getCanonicalTypeInternal())
--                 ->getQualifier()
--                 ->getAsRecordDecl();
-+  auto *RD = getCXXRecordDecl();
-   if (!RD)
-     return nullptr;
-   return RD->getMostRecentNonInjectedDecl();
-diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaChecking.cpp b/clang/lib/Sema/SemaChecking.cpp
---- a/clang/lib/Sema/SemaChecking.cpp
-+++ b/clang/lib/Sema/SemaChecking.cpp
-@@ -11596,6 +11596,15 @@
-   }
- }
- 
-+static void CheckCommaOperand(Sema &S, Expr *E, QualType T, SourceLocation CC,
-+                              bool ExtraCheckForImplicitConversion) {
-+  E = E->IgnoreParenImpCasts();
-+  AnalyzeImplicitConversions(S, E, CC);
-+
-+  if (ExtraCheckForImplicitConversion && E->getType() != T)
-+    S.CheckImplicitConversion(E, T, CC);
-+}
-+
- /// Analyze the given compound assignment for the possible losing of
- /// floating-point precision.
- static void AnalyzeCompoundAssignment(Sema &S, BinaryOperator *E) {
-@@ -12413,7 +12422,7 @@
-           << OrigE->getSourceRange() << T->isBooleanType()
-           << FixItHint::CreateReplacement(UO->getBeginLoc(), "!");
- 
--  if (const auto *BO = dyn_cast<BinaryOperator>(SourceExpr))
-+  if (auto *BO = dyn_cast<BinaryOperator>(SourceExpr)) {
-     if ((BO->getOpcode() == BO_And || BO->getOpcode() == BO_Or) &&
-         BO->getLHS()->isKnownToHaveBooleanValue() &&
-         BO->getRHS()->isKnownToHaveBooleanValue() &&
-@@ -12439,7 +12448,21 @@
-                    (BO->getOpcode() == BO_And ? "&&" : "||"));
-         S.Diag(BO->getBeginLoc(), diag::note_cast_operand_to_int);
-       }
-+    } else if (BO->isCommaOp() && !S.getLangOpts().CPlusPlus) {
-+      /// Analyze the given comma operator. The basic idea behind the analysis
-+      /// is to analyze the left and right operands slightly differently. The
-+      /// left operand needs to check whether the operand itself has an implicit
-+      /// conversion, but not whether the left operand induces an implicit
-+      /// conversion for the entire comma expression itself. This is similar to
-+      /// how CheckConditionalOperand behaves; it's as-if the correct operand
-+      /// were directly used for the implicit conversion check.
-+      CheckCommaOperand(S, BO->getLHS(), T, BO->getOperatorLoc(),
-+                        /*ExtraCheckForImplicitConversion=*/false);
-+      CheckCommaOperand(S, BO->getRHS(), T, BO->getOperatorLoc(),
-+                        /*ExtraCheckForImplicitConversion=*/true);
-+      return;
-     }
-+  }
- 
-   // For conditional operators, we analyze the arguments as if they
-   // were being fed directly into the output.
-diff -ruN --strip-trailing-cr a/clang/test/Sema/implicit-cast.c b/clang/test/Sema/implicit-cast.c
---- a/clang/test/Sema/implicit-cast.c
-+++ b/clang/test/Sema/implicit-cast.c
-@@ -1,4 +1,4 @@
--// RUN: %clang_cc1 -fsyntax-only %s
-+// RUN: %clang_cc1 -fsyntax-only -verify %s
- 
- static char *test1(int cf) {
-   return cf ? "abc" : 0;
-@@ -6,3 +6,8 @@
- static char *test2(int cf) {
-   return cf ? 0 : "abc";
- }
-+
-+int baz(void) {
-+  int f;
-+  return ((void)0, f = 1.4f); // expected-warning {{implicit conversion from 'float' to 'int' changes value from 1.4 to 1}}
-+}
-diff -ruN --strip-trailing-cr a/clang/test/Sema/implicit-int-enum-conversion.c b/clang/test/Sema/implicit-int-enum-conversion.c
---- a/clang/test/Sema/implicit-int-enum-conversion.c
-+++ b/clang/test/Sema/implicit-int-enum-conversion.c
-@@ -50,3 +50,25 @@
-   return E2_Zero;       // expected-warning {{implicit conversion from enumeration type 'enum E2' to different enumeration type 'enum E1'}} \
-                            cxx-error {{cannot initialize return object of type 'enum E1' with an rvalue of type 'E2'}}
- }
-+
-+enum E1 comma1(void) {
-+  return ((void)0, E1_One);
-+}
-+
-+enum E1 comma2(void) {
-+  enum E1 x;
-+  return
-+    (x = 12,  // expected-warning {{implicit conversion from 'int' to enumeration type 'enum E1' is invalid in C++}} \
-+                 cxx-error {{assigning to 'enum E1' from incompatible type 'int'}}
-+    E1_One);
-+}
-+
-+enum E1 comma3(void) {
-+  enum E1 x;
-+  return ((void)0, foo()); // Okay, no conversion in C++
-+}
-+
-+enum E1 comma4(void) {
-+  return ((void)1, 2); // expected-warning {{implicit conversion from 'int' to enumeration type 'enum E1' is invalid in C++}} \
-+                          cxx-error {{cannot initialize return object of type 'enum E1' with an rvalue of type 'int'}}
-+}
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index b41439e..2573c25 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "2d287f51eff2a5fbf84458a33f7fb2493cf67965"
-    LLVM_SHA256 = "e06d0a35b0e0570b2f54dfd23d0e9fe6f084e032c14bb7ab194b06cb8c9cb86c"
+    LLVM_COMMIT = "741fef3a445339523500f614e0f752b9a74517a6"
+    LLVM_SHA256 = "ae542233c385388cb5f8ce04bf2085ed108b1a592c530c1746e8c92d00bd33fb"
 
     tf_http_archive(
         name = name,
