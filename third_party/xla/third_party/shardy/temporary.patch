diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
index aceb4d7..8752484 100644
--- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
+++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
@@ -908,8 +908,8 @@ void insertAllReducesForReductionFactors(
   }
 }
 
-bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,
-                                          const SymbolTable& symbolTable) {
+bool convertReshardToShardedToUnreduced(Operation* op, IRRewriter& rewriter,
+                                        const SymbolTable& symbolTable) {
   ReshardOp reshardOp = dyn_cast<ReshardOp>(op);
   if (!reshardOp) {
     return false;
@@ -934,12 +934,7 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,
       << "Reshard op has different meshes for input and output. The result has "
          "non-empty unreduced axes.";
 
-  // The relationship of the unreduced axes is "out = in + r2u + s2u", where
-  // "r2u" is the replicated-to-unreduced axes and "s2u" is the
-  // sharded-to-unreduced axes.
-  SmallVector<AxisRefAttr> r2uAnds2uAxes =
-      getAxisSetDiff(outUnreducedAxes, inUnreducedAxes, inMesh);
-  if (r2uAnds2uAxes.empty()) {
+  if (getAxisSetDiff(outUnreducedAxes, inUnreducedAxes, inMesh).empty()) {
     return false;
   }
 
@@ -950,7 +945,7 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,
       << "Input of sharded-to-unreduced reshard must be a block argument or a "
          "reshard op.";
 
-  SmallVector<AxisRefAttr> s2uAxes;
+  SmallVector<AxisRefAttr> newUnreducedAxes = llvm::to_vector(inUnreducedAxes);
   SmallVector<AxisRefListAttr> axesPerDim(inSharding.getRank());
   for (auto [inDimSharding, outDimSharding, axes] :
        llvm::zip_equal(inSharding.getDimShardings(),
@@ -971,7 +966,7 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,
       }
       diff.append(inAxes.begin() + outAxes.size(), inAxes.end());
       axes = AxisRefListAttr::get(rewriter.getContext(), diff);
-      s2uAxes.append(diff);
+      newUnreducedAxes.append(diff);
     } else {
       SDY_LOG(FATAL)
           << "The reshard op needs to be decomposed to a sharded-to-unreduced "
@@ -979,27 +974,17 @@ bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,
     }
   }
 
+  sortAndMergeAxes(newUnreducedAxes, inMesh);
+
   rewriter.setInsertionPoint(reshardOp);
-  Value result = input;
-
-  SmallVector<AxisRefAttr> r2uAxes =
-      getAxisSetDiff(r2uAnds2uAxes, s2uAxes, inMesh);
-  if (!r2uAxes.empty()) {
-    SmallVector<AxisRefAttr> inPlusR2uAxes = llvm::to_vector(inUnreducedAxes);
-    inPlusR2uAxes.append(r2uAxes.begin(), r2uAxes.end());
-    sortAndMergeAxes(inPlusR2uAxes, inMesh);
-    TensorShardingAttr r2uSharding =
-        TensorShardingAttr::get(rewriter.getContext(), inSharding.getMeshName(),
-                                inSharding.getDimShardings(),
-                                outSharding.getReplicatedAxes(), inPlusR2uAxes);
-    result = ReplicatedToUnreducedOp::create(rewriter, reshardOp.getLoc(),
-                                             result, r2uAxes, r2uSharding);
-  }
-  if (!s2uAxes.empty()) {
-    result = ShardedToUnreducedOp::create(rewriter, reshardOp.getLoc(), result,
-                                          axesPerDim, outSharding);
+  Operation* result = ShardedToUnreducedOp::create(
+      rewriter, reshardOp.getLoc(), input, axesPerDim,
+      outSharding.replaceUnreducedAxes(newUnreducedAxes));
+  if (newUnreducedAxes != outUnreducedAxes) {
+    SDY_LOG(WARNING) << "need repliaced-to-unreduced";
+    result = ReshardOp::create(rewriter, reshardOp.getLoc(),
+                               result->getResult(0), outSharding);
   }
-
   rewriter.replaceOp(reshardOp, result);
   return true;
 }
diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h
index 0a5563f..c183216 100644
--- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h
+++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.h
@@ -164,19 +164,15 @@ AxesPerFactor findCommonAxes(const ShardingProjection& shardingProjection,
                              OpShardingRuleAttr shardingRule,
                              ArrayRef<int64_t> tensorSizes, const Mesh& mesh);
 
-// Converts a `sdy.reshard` op to an `sdy.replicated-to-unreduced` op and/or an
-// `sdy.sharded-to-unreduced` op. Returns true if the conversion is successful.
-//
-// `r2u` keeps the sharded size, while `s2u` increases the sharded size. Hence,
-// we do `r2u` first and then `s2u`.
+// Converts a `sdy.reshard` op to an `sdy.sharded-to-unreduced` op. Returns true
+// if the conversion is successful.
 //
 // The requirements are:
 // 1. `op` is a `sdy.reshard` op.
-// 2. The input and output shardings have the same mesh.
-// 3. The input of `op` is another `sdy.reshard` op or a block argument.
-// 4. The input unreduced axes is a strict subset of the output unreduced axes.
-bool convertReshardToUnreducedCollectives(Operation* op, IRRewriter& rewriter,
-                                          const SymbolTable& symbolTable);
+// 2. The input of `op` is another `sdy.reshard` op or a block argument.
+// 3. The `op` can be converted to a single `sdy.sharded-to-unreduced` op.
+bool convertReshardToShardedToUnreduced(Operation* op, IRRewriter& rewriter,
+                                        const SymbolTable& symbolTable);
 
 }  // namespace sdy
 }  // namespace mlir
diff --git a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
index 85d048e..7f96c9b 100644
--- a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
+++ b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
@@ -486,7 +486,7 @@ struct InsertExplicitReshardsPass
         return;
       }
 
-      if (convertReshardToUnreducedCollectives(op, rewriter, symbolTable)) {
+      if (convertReshardToShardedToUnreduced(op, rewriter, symbolTable)) {
         return;
       }
 
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir
index f30109e..f3868a9 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards.mlir
@@ -2,7 +2,6 @@
 
 sdy.mesh @mesh = <["x"=2, "y"=2, "z"=4]>
 sdy.mesh @other_mesh = <["x"=2, "y"=2]>
-sdy.mesh @mesh_x16 = <["x"=16]>
 sdy.mesh @mesh_abcd = <["a"=2, "b"=2, "c"=2, "d"=2]>
 
 //===----------------------------------------------------------------------===//
@@ -521,17 +520,17 @@ func.func @different_arguments_to_multiple_named_computations_with_same_input_ou
 }
 
 //===----------------------------------------------------------------------===//
-// Replicated and sharded to unreduced tests
+// Sharded to unreduced tests
 //===----------------------------------------------------------------------===//
 
-// CHECK-LABEL: func @sharded_to_unreduced
-func.func @sharded_to_unreduced(
-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x"}>}) {
+// CHECK-LABEL: func @sharded_to_unreduced_1
+func.func @sharded_to_unreduced_1(
+    %arg0 : tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
+    -> (tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x"}>}) {
   // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{"x"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={"x"}>
   // CHECK-NEXT: return %0
-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x"}> : tensor<16x8xf32>
-  return %0 : tensor<16x8xf32>
+  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x"}> : tensor<24x8xf32>
+  return %0 : tensor<24x8xf32>
 }
 
 // CHECK-LABEL: func @sharded_to_unreduced_single_axis
@@ -574,44 +573,13 @@ func.func @sharded_to_unreduced_with_subaxis(
  return %0 : tensor<16x8xf32>
 }
 
-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_full_axis
-func.func @implicitly_and_explicitly_replicated_to_unreduced_full_axis(
-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], replicated={"z"}, unreduced={"y"}>})
-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x", "y", "z"}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"x", "z"} %arg0 out_sharding=<@mesh, [{}, {}], unreduced={"x", "y", "z"}>
-  // CHECK-NEXT: return %0
-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x", "y", "z"}> : tensor<16x8xf32>
-  return %0 : tensor<16x8xf32>
-}
-
-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis
-func.func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis(
-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{"x":(1)2}, {}], replicated={"x":(8)2}, unreduced={"x":(4)2}>})
-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{"x":(1)2}, {}], unreduced={"x":(2)8}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"x":(2)2, "x":(8)2} %arg0 out_sharding=<@mesh_x16, [{"x":(1)2}, {}], unreduced={"x":(2)8}>
-  // CHECK-NEXT: return %0
-  %0 = sdy.reshard %arg0 <@mesh_x16, [{"x":(1)2}, {}], unreduced={"x":(2)8}> : tensor<16x8xf32>
-  return %0 : tensor<16x8xf32>
-}
-
-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_full_axis
-func.func @replicated_and_sharded_to_unreduced_full_axis(
+// CHECK-LABEL: func @sharded_to_unreduced_and_replicated_to_unreduced
+func.func @sharded_to_unreduced_and_replicated_to_unreduced(
     %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}], unreduced={"y"}>})
     -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x", "y", "z"}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"z"} %arg0 out_sharding=<@mesh, [{"x"}, {}], unreduced={"y", "z"}> : tensor<16x8xf32>
-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{"x"}, {}] %0 out_sharding=<@mesh, [{}, {}], unreduced={"x", "y", "z"}> : tensor<16x8xf32>
+  // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{"x"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={"x", "y"}>
+  // CHECK-NEXT: %1 = sdy.reshard %0 <@mesh, [{}, {}], unreduced={"x", "y", "z"}>
   // CHECK-NEXT: return %1
  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x", "y", "z"}> :  tensor<16x8xf32>
  return %0 : tensor<16x8xf32>
 }
-
-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_sub_axis
-func.func @replicated_and_sharded_to_unreduced_sub_axis(
-    %arg0 : tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {"z":(1)2}], unreduced={"y"}>})
-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}], unreduced={"y", "z"}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"z":(2)2} %arg0 out_sharding=<@mesh, [{"x"}, {"z":(1)2}], unreduced={"y", "z":(2)2}> : tensor<16x8xf32>
-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{}, {"z":(1)2}] %0 out_sharding=<@mesh, [{"x"}, {}], unreduced={"y", "z"}> : tensor<16x8xf32>
-  // CHECK-NEXT: return %1
- %0 = sdy.reshard %arg0 <@mesh, [{"x"}, {}], unreduced={"y", "z"}> :  tensor<16x8xf32>
- return %0 : tensor<16x8xf32>
-}
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir
index 5b1973a..5dea360 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/unreduced.mlir
@@ -1,7 +1,6 @@
 // RUN: sdy_opt %s -sdy-insert-explicit-reshards='enable-full-version=true' | FileCheck %s
 
 sdy.mesh @mesh = <["x"=4, "y"=2, "z"=4]>
-sdy.mesh @mesh_x16 = <["x"=16]>
 
 // CHECK-LABEL: func @all_reduce_on_func_input
 func.func @all_reduce_on_func_input(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"y"}>}, %arg1: tensor<4x8xf32>) -> tensor<4x8xf32> {
@@ -307,17 +306,17 @@ func.func @all_reduce_source_and_target_fully_replicated_shardings_and_different
 }
 
 //===----------------------------------------------------------------------===//
-// Replicated and sharded to unreduced tests
+// Sharded to unreduced tests
 //===----------------------------------------------------------------------===//
 
-// CHECK-LABEL: func @sharded_to_unreduced
-func.func @sharded_to_unreduced(
-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x"}>}) {
+// CHECK-LABEL: func @sharded_to_unreduced_1
+func.func @sharded_to_unreduced_1(
+    %arg0 : tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>})
+    -> (tensor<24x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x"}>}) {
   // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{"x"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={"x"}>
   // CHECK-NEXT: return %0
-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x"}> : tensor<32x32xf32>
-  return %0 : tensor<32x32xf32>
+  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x"}> : tensor<24x8xf32>
+  return %0 : tensor<24x8xf32>
 }
 
 // CHECK-LABEL: func @sharded_to_unreduced_single_axis
@@ -360,44 +359,13 @@ func.func @sharded_to_unreduced_with_subaxis(
  return %0 : tensor<32x32xf32>
 }
 
-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_full_axis
-func.func @implicitly_and_explicitly_replicated_to_unreduced_full_axis(
-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], replicated={"z"}, unreduced={"y"}>})
-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x", "y", "z"}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"x", "z"} %arg0 out_sharding=<@mesh, [{}, {}], unreduced={"x", "y", "z"}>
-  // CHECK-NEXT: return %0
-  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x", "y", "z"}> : tensor<32x32xf32>
-  return %0 : tensor<32x32xf32>
-}
-
-// CHECK-LABEL: func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis
-func.func @implicitly_and_explicitly_replicated_to_unreduced_sub_axis(
-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{"x":(1)2}, {}], replicated={"x":(8)2}, unreduced={"x":(4)2}>})
-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh_x16, [{"x":(1)2}, {}], unreduced={"x":(2)8}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"x":(2)2, "x":(8)2} %arg0 out_sharding=<@mesh_x16, [{"x":(1)2}, {}], unreduced={"x":(2)8}>
-  // CHECK-NEXT: return %0
-  %0 = sdy.reshard %arg0 <@mesh_x16, [{"x":(1)2}, {}], unreduced={"x":(2)8}> : tensor<32x32xf32>
-  return %0 : tensor<32x32xf32>
-}
-
-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_full_axis
-func.func @replicated_and_sharded_to_unreduced_full_axis(
+// CHECK-LABEL: func @sharded_to_unreduced_and_replicated_to_unreduced
+func.func @sharded_to_unreduced_and_replicated_to_unreduced(
     %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}], unreduced={"y"}>})
     -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}], unreduced={"x", "y", "z"}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"z"} %arg0 out_sharding=<@mesh, [{"x"}, {}], unreduced={"y", "z"}> : tensor<32x32xf32>
-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{"x"}, {}] %0 out_sharding=<@mesh, [{}, {}], unreduced={"x", "y", "z"}> : tensor<32x32xf32>
+  // CHECK-NEXT: %0 = sdy.sharded_to_unreduced [{"x"}, {}] %arg0 out_sharding=<@mesh, [{}, {}], unreduced={"x", "y"}>
+  // CHECK-NEXT: %1 = sdy.reshard %0 <@mesh, [{}, {}], unreduced={"x", "y", "z"}>
   // CHECK-NEXT: return %1
  %0 = sdy.reshard %arg0 <@mesh, [{}, {}], unreduced={"x", "y", "z"}> :  tensor<32x32xf32>
  return %0 : tensor<32x32xf32>
 }
-
-// CHECK-LABEL: func @replicated_and_sharded_to_unreduced_sub_axis
-func.func @replicated_and_sharded_to_unreduced_sub_axis(
-    %arg0 : tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {"z":(1)2}], unreduced={"y"}>})
-    -> (tensor<32x32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}], unreduced={"y", "z"}>}) {
-  // CHECK-NEXT: %0 = sdy.replicated_to_unreduced {"z":(2)2} %arg0 out_sharding=<@mesh, [{"x"}, {"z":(1)2}], unreduced={"y", "z":(2)2}> : tensor<32x32xf32>
-  // CHECK-NEXT: %1 = sdy.sharded_to_unreduced [{}, {"z":(1)2}] %0 out_sharding=<@mesh, [{"x"}, {}], unreduced={"y", "z"}> : tensor<32x32xf32>
-  // CHECK-NEXT: return %1
- %0 = sdy.reshard %arg0 <@mesh, [{"x"}, {}], unreduced={"y", "z"}> :  tensor<32x32xf32>
- return %0 : tensor<32x32xf32>
-}
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 509398d..f82404c 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1 +1,152 @@
 Auto generated patch. Do not edit or delete it, even if empty.
+diff -ruN --strip-trailing-cr a/clang/lib/Serialization/ASTReaderDecl.cpp b/clang/lib/Serialization/ASTReaderDecl.cpp
+--- a/clang/lib/Serialization/ASTReaderDecl.cpp
++++ b/clang/lib/Serialization/ASTReaderDecl.cpp
+@@ -2107,8 +2107,9 @@
+     auto *Def = DD.Definition;
+     DD = std::move(MergeDD);
+     DD.Definition = Def;
+-    for (auto *D : Def->redecls())
+-      cast<CXXRecordDecl>(D)->DefinitionData = &DD;
++    for (auto *R = Reader.getMostRecentExistingDecl(Def); R;
++         R = R->getPreviousDecl())
++      cast<CXXRecordDecl>(R)->DefinitionData = &DD;
+     return;
+   }
+ 
+diff -ruN --strip-trailing-cr a/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h b/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h
+--- a/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h
++++ b/libc/src/__support/FPUtil/x86_64/fenv_mxcsr_utils.h
+@@ -61,14 +61,14 @@
+ LIBC_INLINE static void write_mxcsr(uint32_t w) { _mm_setcsr(w); }
+ 
+ LIBC_INLINE static void clear_except(uint16_t excepts) {
+-  uint32_t mxcsr = _MM_GET_EXCEPTION_STATE();
++  uint32_t mxcsr = get_mxcsr();
+   mxcsr &= ~static_cast<uint32_t>(excepts);
+-  _MM_SET_EXCEPTION_STATE(mxcsr);
++  write_mxcsr(mxcsr);
+ }
+ 
+ LIBC_INLINE static uint16_t test_except(uint16_t excepts) {
+   uint32_t mxcsr = get_mxcsr();
+-  return static_cast<uint16_t>(excepts & mxcsr);
++  return static_cast<uint16_t>(excepts & ExceptionFlags::ALL_F & mxcsr);
+ }
+ 
+ LIBC_INLINE static uint16_t get_except() {
+@@ -83,9 +83,9 @@
+ }
+ 
+ LIBC_INLINE static void raise_except(uint16_t excepts) {
+-  uint32_t mxcsr = _MM_GET_EXCEPTION_STATE();
+-  mxcsr |= excepts;
+-  _MM_SET_EXCEPTION_STATE(mxcsr);
++  uint32_t mxcsr = get_mxcsr();
++  mxcsr |= excepts & ExceptionFlags::ALL_F;
++  write_mxcsr(mxcsr);
+ #ifdef LIBC_TRAP_ON_RAISE_FP_EXCEPT
+   // We will try to trigger the SIGFPE if floating point exceptions are not
+   // masked.  Since we already set all the floating point exception flags, we
+diff -ruN --strip-trailing-cr a/libcxx/include/__flat_map/flat_map.h b/libcxx/include/__flat_map/flat_map.h
+--- a/libcxx/include/__flat_map/flat_map.h
++++ b/libcxx/include/__flat_map/flat_map.h
+@@ -465,13 +465,13 @@
+   }
+ 
+   // [flat.map.access], element access
+-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](const key_type& __x)
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](const key_type& __x)
+     requires is_constructible_v<mapped_type>
+   {
+     return try_emplace(__x).first->second;
+   }
+ 
+-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](key_type&& __x)
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](key_type&& __x)
+     requires is_constructible_v<mapped_type>
+   {
+     return try_emplace(std::move(__x)).first->second;
+@@ -480,7 +480,7 @@
+   template <class _Kp>
+     requires(__is_compare_transparent && is_constructible_v<key_type, _Kp> && is_constructible_v<mapped_type> &&
+              !is_convertible_v<_Kp &&, const_iterator> && !is_convertible_v<_Kp &&, iterator>)
+-  [[nodiscard]] _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](_Kp&& __x) {
++  _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX26 mapped_type& operator[](_Kp&& __x) {
+     return try_emplace(std::forward<_Kp>(__x)).first->second;
+   }
+ 
+diff -ruN --strip-trailing-cr a/libcxx/include/map b/libcxx/include/map
+--- a/libcxx/include/map
++++ b/libcxx/include/map
+@@ -1092,9 +1092,9 @@
+   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI size_type size() const _NOEXCEPT { return __tree_.size(); }
+   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI size_type max_size() const _NOEXCEPT { return __tree_.max_size(); }
+ 
+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);
++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);
+ #  ifndef _LIBCPP_CXX03_LANG
+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);
++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);
+ #  endif
+ 
+   template <class _Arg,
+diff -ruN --strip-trailing-cr a/libcxx/include/unordered_map b/libcxx/include/unordered_map
+--- a/libcxx/include/unordered_map
++++ b/libcxx/include/unordered_map
+@@ -1262,9 +1262,9 @@
+   }
+ #  endif // _LIBCPP_STD_VER >= 20
+ 
+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);
++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](const key_type& __k);
+ #  ifndef _LIBCPP_CXX03_LANG
+-  [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);
++  _LIBCPP_HIDE_FROM_ABI mapped_type& operator[](key_type&& __k);
+ #  endif
+ 
+   [[__nodiscard__]] _LIBCPP_HIDE_FROM_ABI mapped_type& at(const key_type& __k);
+diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp
+--- a/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp
++++ b/libcxx/test/libcxx/diagnostics/flat_map.nodiscard.verify.cpp
+@@ -66,9 +66,9 @@
+   TransparentKey<int> tkey;
+ 
+   std::flat_map<int, int> nfm;
+-  nfm[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+-  fm[std::move(key)];  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+-  fm[std::move(tkey)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
++  nfm[key];            // no-warning
++  fm[std::move(key)];  // no-warning
++  fm[std::move(tkey)]; // no-warning
+ 
+   fm.at(key);   // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+   cfm.at(key);  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp
+--- a/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp
++++ b/libcxx/test/libcxx/diagnostics/map.nodiscard.verify.cpp
+@@ -55,8 +55,8 @@
+ 
+   int key = 0;
+ 
+-  m[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+-  m[std::move(key)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
++  m[key];            // no-warning
++  m[std::move(key)]; // no-warning
+ 
+ #if TEST_STD_VER >= 14
+   std::map<std::string, int, std::less<>> strMap;
+diff -ruN --strip-trailing-cr a/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp b/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp
+--- a/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp
++++ b/libcxx/test/libcxx/diagnostics/unordered_map.nodiscard.verify.cpp
+@@ -81,8 +81,8 @@
+   ctm.equal_range(tkey); // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+ #endif
+ 
+-  m[key];            // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+-  m[std::move(key)]; // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
++  m[key];            // no-warning
++  m[std::move(key)]; // no-warning
+ 
+   m.at(key);  // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
+   cm.at(key); // expected-warning {{ignoring return value of function declared with 'nodiscard' attribute}}
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index f2c3289..29af0ff 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "8f264586d7521b0e305ca7bb78825aa3382ffef7"
-    LLVM_SHA256 = "5784c4af94caba66bc8c460e07e222f751e4f4c9db9c45b3a68ff55379cf587d"
+    LLVM_COMMIT = "7d381f2a5634d1e41b61299839d652cc4a021898"
+    LLVM_SHA256 = "f1641918fd3f5e1667d39afb9c261da39ed9f74e30f1c2f98031d6d609a8de15"
 
     tf_http_archive(
         name = name,
