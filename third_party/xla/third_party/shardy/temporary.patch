diff --git a/docs/sdy_export_passes.md b/docs/sdy_export_passes.md
index 9e14eae..165377e 100755
--- a/docs/sdy_export_passes.md
+++ b/docs/sdy_export_passes.md
@@ -72,8 +72,7 @@ operation has compatible shardings.
 #### Options
 
 ```
--enable-full-version                  : Enable full version.
--avoid-reshards-on-named-computations : Avoid explicit reshards/collectives on named computations.
+-enable-full-version : Enable full version.
 ```
 
 ### `-sdy-remove-all-gather-reduce-scatter-for-cmv1`
diff --git a/shardy/dialect/sdy/transforms/common/propagation_options.h b/shardy/dialect/sdy/transforms/common/propagation_options.h
index 3700cd4..09ed107 100644
--- a/shardy/dialect/sdy/transforms/common/propagation_options.h
+++ b/shardy/dialect/sdy/transforms/common/propagation_options.h
@@ -50,8 +50,6 @@ struct PropagationOptions {
   // auto-partitioner will be invoked after propagation of user-specified
   // shardings.
   bool enableAutoPartitioning = false;
-  // Whether to avoid explicit reshards/collectives on named computations.
-  bool avoidReshardsOnNamedComputations = false;
 };
 
 }  // namespace sdy
diff --git a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
index 2d5cc0a..50d584a 100644
--- a/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
+++ b/shardy/dialect/sdy/transforms/export/explicit_reshards_util.cc
@@ -258,22 +258,18 @@ struct FactorAxesCandidate {
   // sharding is size(B)/size(A), and where A is a strict prefix of B.
   int64_t shardingSize = 0;
   int64_t factorTypePrecedence = 0;
-  int64_t communicationCost = INT64_MAX;
 
   FactorAxesCandidate(FactorAxesPair factorAxes, int64_t sourceTensorSize,
-                      int64_t shardingSize, FactorType factorType,
-                      int64_t communicationCost)
+                      int64_t shardingSize, FactorType factorType)
       : factorAxes(factorAxes),
         totalSourceTensorSize(sourceTensorSize),
         largestSourceTensorSize(sourceTensorSize),
         shardingSize(shardingSize),
-        factorTypePrecedence(precedence(factorType)),
-        communicationCost(communicationCost) {}
+        factorTypePrecedence(precedence(factorType)) {}
 
   FactorAxesCandidate() = default;
 
   // Multi-level comparison.
-  // 0. communicationCost
   // 1. totalSourceTensorSize
   // 2. factorTypePrecedence
   // 3. largestSourceTensorSize
@@ -281,10 +277,10 @@ struct FactorAxesCandidate {
   // 5. factorAxes: If A is a strict prefix of B, then A is smaller than B.
   bool operator<(const FactorAxesCandidate& rhs) const {
     auto makeComparisonTuple = [](const FactorAxesCandidate& candidate) {
-      return std::make_tuple(
-          -candidate.communicationCost, candidate.totalSourceTensorSize,
-          candidate.factorTypePrecedence, candidate.largestSourceTensorSize,
-          candidate.shardingSize, candidate.factorAxes);
+      return std::forward_as_tuple(
+          candidate.totalSourceTensorSize, candidate.factorTypePrecedence,
+          candidate.largestSourceTensorSize, candidate.shardingSize,
+          candidate.factorAxes);
     };
     return makeComparisonTuple(*this) < makeComparisonTuple(rhs);
   }
@@ -317,8 +313,7 @@ using FactorAxesCandidatesMap =
 void updateFactorAxesCandidate(FactorAxesCandidatesMap& factorAxesCandidatesMap,
                                const FactorAxesPair& factorAxes,
                                int64_t sourceTensorSize, const Mesh& mesh,
-                               const FactorType factorType,
-                               int64_t communicationCost) {
+                               const FactorType factorType) {
   if (auto it = factorAxesCandidatesMap.find(factorAxes);
       it != factorAxesCandidatesMap.end()) {
     FactorAxesCandidate& candidate = it->second;
@@ -329,8 +324,7 @@ void updateFactorAxesCandidate(FactorAxesCandidatesMap& factorAxesCandidatesMap,
   }
   factorAxesCandidatesMap.try_emplace(
       factorAxes, factorAxes, sourceTensorSize,
-      factorAxes.axes.getShardingSize(mesh.attr()), factorType,
-      communicationCost);
+      factorAxes.axes.getShardingSize(mesh.attr()), factorType);
 }
 
 // A container for FactorAxesCandidates where the order of iteration does not
@@ -479,150 +473,6 @@ class FactorAxesCandidateBag {
   MeshAttr mesh;
 };
 
-int64_t getShardingSize(ArrayRef<AxisRefAttr> axisRefs, MeshAttr mesh) {
-  int64_t shardingSize = 1;
-  for (AxisRefAttr axisRef : axisRefs) {
-    shardingSize *= axisRef.getSize(mesh);
-  }
-  return shardingSize;
-}
-
-std::pair<SmallVector<AxisRefAttr>, SmallVector<AxisRefAttr>>
-getShardingAxesInOtherAndThisFactor(
-    const TensorFactorShardings& tensorFactorSharding,
-    const int64_t factorIndex) {
-  SmallVector<AxisRefAttr> axesInOtherFactor;
-  SmallVector<AxisRefAttr> axesInThisFactor;
-  for (const auto& [i, factorSharding] :
-       tensorFactorSharding.factorIndexToSharding) {
-    if (i == factorIndex) {
-      axesInThisFactor = factorSharding.axisRefs;
-    } else {
-      axesInOtherFactor.append(factorSharding.axisRefs.begin(),
-                               factorSharding.axisRefs.end());
-    }
-  }
-  return {axesInOtherFactor, axesInThisFactor};
-}
-
-int64_t getCommunicationCost(const ShardingProjection& shardingProjection,
-                             OpShardingRuleAttr shardingRule,
-                             ArrayRef<int64_t> tensorSizes, const Mesh& mesh,
-                             const FactorAxesPair& factorAxesPair) {
-  // The relative cost of collective operations.
-  const int64_t allToAllCost = 1;
-  const int64_t collectivePermuteCost = 2;
-  const int64_t allGatherCost = 4;
-  const int64_t reduceScatterCost = 4;
-  const int64_t allReduceCost = 8;
-
-  int64_t communicationCost = 0;
-
-  // For each tensor (operand or result), we use the following notations.
-  //
-  // `factorAxesPair` is the candidate factor-axes pair.
-  // * X = factorAxesPair.axes.
-  // * A = sharding axes in other factors in the original sharding.
-  // * B = sharding axes in this factor in the original sharding.
-  // * AX = the intersection (overlap) of A and X.
-  // * B-X = the difference of B and X.
-
-  SmallVector<AxisRefAttr> axesX = factorAxesPair.axes.toVector();
-  int64_t axesXSize = factorAxesPair.axes.getShardingSize(mesh.attr());
-
-  // For each operand, estimate the cost of reshard from original sharding to
-  // the candidate sharding axes.
-  //
-  // If the operand does not contain this factor, we need an all-gather on AX.
-  //
-  // If the operand contains this factor, we need
-  // 1. all-to-all to move AX from other factors to this factor.
-  // 2. collective-permute to handle B-X.
-  // 3. all-gather to shrink the sharding size if needed.
-  for (const auto& [tensorSize, tensorFactorSharding] : llvm::zip_equal(
-           tensorSizes.drop_back(shardingProjection.getNumResults()),
-           shardingProjection.getOperands())) {
-    bool operandContainsFactor =
-        tensorFactorSharding.factorIndexToSharding.contains(
-            factorAxesPair.factorIndex);
-    int64_t shardedTensorSize =
-        tensorSize / tensorFactorSharding.getShardingSize(mesh.attr());
-    auto [axesA, axesB] = getShardingAxesInOtherAndThisFactor(
-        tensorFactorSharding, factorAxesPair.factorIndex);
-
-    SmallVector<AxisRefAttr> diffXA = getAxisSetDiff(axesX, axesA, mesh.attr());
-    int64_t diffXASize = getShardingSize(diffXA, mesh.attr());
-
-    if (axesXSize > diffXASize) {
-      // all-to-all on AX.
-      communicationCost +=
-          (operandContainsFactor ? allToAllCost : allGatherCost) *
-          shardedTensorSize;
-    }
-
-    if (operandContainsFactor) {
-      if (!getAxisSetDiff(axesB, axesX, mesh.attr()).empty()) {
-        communicationCost += collectivePermuteCost * shardedTensorSize;
-      }
-      if (getShardingSize(axesB, mesh.attr()) > diffXASize) {
-        // The operand is over-sharded than the candidate. We need all-gather to
-        // shrink the sharding size.
-        communicationCost += allGatherCost * shardedTensorSize;
-      }
-    }
-  }
-
-  // For each result, estimate the cost of reshard from the candidate sharding
-  // axes to original sharding.
-  //
-  // We use the same notations as above.
-  //
-  // If the candidate factor is a reduction factor, we need all-reduce or
-  // reduce-scatter on the result.
-  //
-  // If the result does not contain this factor, there is no additional cost.
-  //
-  // If the result contains this factor, we need
-  // 1. all-to-all to move AX from this factor to other factors.
-  // 2. all-gather to shrink the sharding size after the all-to-all above.
-  for (const auto& [tensorSize, tensorFactorSharding] : llvm::zip_equal(
-           tensorSizes.drop_front(shardingProjection.getNumOperands()),
-           shardingProjection.getResults())) {
-    int64_t shardedTensorSize = tensorSize / axesXSize;
-    auto [axesA, axesB] = getShardingAxesInOtherAndThisFactor(
-        tensorFactorSharding, factorAxesPair.factorIndex);
-
-    SmallVector<AxisRefAttr> diffXA = getAxisSetDiff(axesX, axesA, mesh.attr());
-    int64_t diffXASize = getShardingSize(diffXA, mesh.attr());
-
-    if (shardingRule.isReductionFactor(factorAxesPair.factorIndex)) {
-      communicationCost +=
-          (diffXASize > 1 ? allReduceCost : reduceScatterCost) *
-          shardedTensorSize;
-    }
-
-    if (!tensorFactorSharding.factorIndexToSharding.contains(
-            factorAxesPair.factorIndex)) {
-      continue;
-    }
-    if (axesXSize > diffXASize) {
-      // all-to-all on AX.
-      communicationCost += allToAllCost * shardedTensorSize;
-    }
-
-    if (!getAxisSetDiff(axesB, axesX, mesh.attr()).empty()) {
-      communicationCost += collectivePermuteCost * shardedTensorSize;
-    }
-    if (getShardingSize(axesB, mesh.attr()) < diffXASize) {
-      // The result is less-sharded than the candidate. We need all-gather to
-      // shrink the sharding size.
-      communicationCost += allGatherCost * shardedTensorSize;
-    }
-  }
-
-  return communicationCost;
-}
-
 FactorAxesCandidateBag findFactorAxesCandidates(
     const ShardingProjection& shardingProjection,
     OpShardingRuleAttr shardingRule, ArrayRef<int64_t> tensorSizes,
@@ -644,13 +494,10 @@ FactorAxesCandidateBag findFactorAxesCandidates(
       }
       ArrayRef<AxisRefAttr> axisRefs = factorSharding.axisRefs;
       while (!axisRefs.empty()) {
-        FactorAxesPair factorAxesPair(factorIndex, AxisListRef(axisRefs));
-        int64_t communicationCost =
-            getCommunicationCost(shardingProjection, shardingRule, tensorSizes,
-                                 mesh, factorAxesPair);
         updateFactorAxesCandidate(
-            factorAxesCandidatesMap, factorAxesPair, tensorSize, mesh,
-            shardingRule.getFactorType(factorIndex), communicationCost);
+            factorAxesCandidatesMap,
+            FactorAxesPair(factorIndex, AxisListRef(axisRefs)), tensorSize,
+            mesh, shardingRule.getFactorType(factorIndex));
         axisRefs = axisRefs.drop_back();
       }
     }
diff --git a/shardy/dialect/sdy/transforms/export/export_pipeline.cc b/shardy/dialect/sdy/transforms/export/export_pipeline.cc
index e0557b1..90a6afc 100644
--- a/shardy/dialect/sdy/transforms/export/export_pipeline.cc
+++ b/shardy/dialect/sdy/transforms/export/export_pipeline.cc
@@ -40,8 +40,6 @@ void runShardyPartitioner(OpPassManager& pm, int& dumpIndex,
                           const ExportOptions& options) {
   InsertExplicitReshardsPassOptions passOptions;
   passOptions.enableFullVersion = options.enableInsertExplicitCollectives;
-  passOptions.avoidReshardsOnNamedComputations =
-      options.avoidReshardsOnNamedComputations;
   pm.addNestedPass<func::FuncOp>(createInsertExplicitReshardsPass(passOptions));
 
   if (options.enableInsertExplicitCollectives) {
diff --git a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
index fb95adc..dcec262 100644
--- a/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
+++ b/shardy/dialect/sdy/transforms/export/insert_explicit_reshards.cc
@@ -97,22 +97,10 @@ void insertExplicitReshardsOnFuncReturn(Operation* op, func::FuncOp& funcOp,
   }
 }
 
-void insertExplicitReshardsOnDataFlowOp(
-    ShardableDataFlowOpInterface& op, IRRewriter& rewriter,
-    const SymbolTable& symbolTable, const bool onFullVersion,
-    const bool avoidReshardsOnNamedComputations) {
-  if (isa<NamedComputationOp>(op) && avoidReshardsOnNamedComputations) {
-    for (Value owner : op.getOpResultEdgeOwners()) {
-      for (OpOperand* sourceOpOperand : op.getEdgeSources(owner)) {
-        insertExplicitReshardsToTargetSharding(
-            *sourceOpOperand,
-            /*targetSharding=*/op.getEdgeOwnerSharding(owner), rewriter,
-            symbolTable,
-            /*insertAfterOperand=*/true, onFullVersion);
-      }
-    }
-    return;
-  }
+void insertExplicitReshardsOnDataFlowOp(ShardableDataFlowOpInterface& op,
+                                        IRRewriter& rewriter,
+                                        const SymbolTable& symbolTable,
+                                        const bool onFullVersion) {
   for (Value owner : llvm::concat<Value>(op.getOpResultEdgeOwners(),
                                          op.getBlockArgumentEdgeOwners())) {
     TensorShardingAttr ownerSharding = op.transformTargetSharding(
@@ -487,8 +475,7 @@ struct InsertExplicitReshardsPass
         // TODO(enver): Prefer resharding the owner when multiple sources are
         // sharded in the same way.
         insertExplicitReshardsOnDataFlowOp(shardableDataFlowOp, rewriter,
-                                           symbolTable, onFullVersion,
-                                           avoidReshardsOnNamedComputations);
+                                           symbolTable, onFullVersion);
         return;
       }
 
diff --git a/shardy/dialect/sdy/transforms/export/passes.h b/shardy/dialect/sdy/transforms/export/passes.h
index 36f4de9..07b5d9d 100644
--- a/shardy/dialect/sdy/transforms/export/passes.h
+++ b/shardy/dialect/sdy/transforms/export/passes.h
@@ -75,12 +75,6 @@ struct ExportOptions : public PassPipelineOptions<ExportOptions> {
       *this, "dump-propagation-edges",
       llvm::cl::desc("Sink sdy.propagation_edges attr."),
       llvm::cl::init(false)};
-
-  Option<bool> avoidReshardsOnNamedComputations{
-      *this, "avoid-reshards-on-named-computations",
-      llvm::cl::desc("Avoid inserting explicit reshards/collectives for named "
-                     "computations."),
-      llvm::cl::init(false)};
 };
 
 // Adds a sequence of export passes needed as a post-processing step for SDY
diff --git a/shardy/dialect/sdy/transforms/export/passes.td b/shardy/dialect/sdy/transforms/export/passes.td
index f6c8d86..f810ecb 100644
--- a/shardy/dialect/sdy/transforms/export/passes.td
+++ b/shardy/dialect/sdy/transforms/export/passes.td
@@ -110,11 +110,7 @@ def InsertExplicitReshardsPass : Pass<"sdy-insert-explicit-reshards", "func::Fun
   let options = [
       Option<"enableFullVersion", "enable-full-version",
             "bool", /*default=*/"false",
-            "Enable full version.">,
-      Option<"avoidReshardsOnNamedComputations",
-            "avoid-reshards-on-named-computations",
-            "bool", /*default=*/"false",
-            "Avoid explicit reshards/collectives on named computations.">
+            "Enable full version.">
     ];
 }
 
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/cholesky_triangular_solve.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/cholesky_triangular_solve.mlir
index bb14074..3d6f236 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/cholesky_triangular_solve.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/cholesky_triangular_solve.mlir
@@ -147,12 +147,13 @@ func.func @cholesky_cholesky_dims_shardings_can_merge(%arg0: tensor<16x8x8x8xf32
   return %0 :  tensor<16x8x8x8xf32>
 }
 
-// TODO(zixuanjiang). We may want to keep 'x' due to its larger size.
+
 // CHECK-LABEL: func @cholesky_sharded_cholesky_dim_input_only_batch_dim_both_but_input_sharding_larger
 func.func @cholesky_sharded_cholesky_dim_input_only_batch_dim_both_but_input_sharding_larger(%arg0: tensor<8x4x8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"x"}, {}, {}, {"z"}]>}) -> (tensor<8x4x8x8xf32> {sdy.sharding = #sdy.sharding<@mesh_xyz, [{"y"}, {}, {}, {}]>}){
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyz, [{"y"}, {}, {}, {}]> : tensor<8x4x8x8xf32>
-  // CHECK-NEXT: %[[CHOLESKY:.*]] = stablehlo.cholesky %[[RESHARD1]], lower = true {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"y"}, {}, {}, {}]>]>} : tensor<8x4x8x8xf32>
-  // CHECK-NEXT: return %[[CHOLESKY]] : tensor<8x4x8x8xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyz, [{"x"}, {}, {}, {}]> : tensor<8x4x8x8xf32>
+  // CHECK-NEXT: %[[CHOLESKY:.*]] = stablehlo.cholesky %[[RESHARD1]], lower = true {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"x"}, {}, {}, {}]>]>} : tensor<8x4x8x8xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[CHOLESKY]] <@mesh_xyz, [{"y"}, {}, {}, {}]> : tensor<8x4x8x8xf32>
+  // CHECK-NEXT: return %[[RESHARD2]] : tensor<8x4x8x8xf32>
   %0 = stablehlo.cholesky %arg0, lower = true {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyz, [{"y"}, {}, {}, {}]>]>} : (tensor<8x4x8x8xf32>) -> tensor<8x4x8x8xf32>
   return %0 :  tensor<8x4x8x8xf32>
 }
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir
index 0b10f3f..f1123a1 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/concatenate.mlir
@@ -13,8 +13,8 @@ func.func @concatenate_single_input(%arg0: tensor<4x32x256xf32> {sdy.sharding =
 
 // CHECK-LABEL: func @concatenate
 func.func @concatenate(%arg0: tensor<4x32x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>}, %arg1: tensor<4x48x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}, {}]>}) -> tensor<4x80x256xf32> {
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg1 <@mesh, [{"x"}, {}, {}]> : tensor<4x48x256xf32>
-  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %arg0, %[[RESHARD1]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{"y"}, {}, {}]> : tensor<4x32x256xf32>
+  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %arg1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}, {}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[CONCATENATE]] <@mesh, [{}, {}, {}]> : tensor<4x80x256xf32>
   // CHECK-NEXT: return %[[RESHARD2]] : tensor<4x80x256xf32>
   %0 = stablehlo.concatenate %arg0, %arg1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
@@ -64,11 +64,10 @@ func.func @concatenate_operands_are_from_slices_of_the_same_tensor(%arg0: tensor
 func.func @concatenate_operands_are_results_of_slices_different_shardings_on_permutation_dim_with_equal_counts(%arg0: tensor<4x40x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>}, %arg1: tensor<4x60x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}, {}]>}) -> (tensor<4x80x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {}]>}) {
   %0 = stablehlo.slice %arg0 [0:4, 0:32, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}, {}]>]>} : (tensor<4x40x256xf32>) -> tensor<4x32x256xf32>
   %1 = stablehlo.slice %arg1 [0:4, 0:48, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}, {}]>]>} : (tensor<4x60x256xf32>) -> tensor<4x48x256xf32>
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{"x"}, {"y"}, {}]> : tensor<4x32x256xf32>
-  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{"x"}, {"y"}, {}]> : tensor<4x48x256xf32>
-  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {"y"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
-  // CHECK-NEXT: %[[RESHARD3:.*]] = sdy.reshard %[[CONCATENATE]] <@mesh, [{}, {"x"}, {}]> : tensor<4x80x256xf32>
-  // CHECK-NEXT: return %[[RESHARD3]] : tensor<4x80x256xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{}, {"x"}, {}]> : tensor<4x32x256xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{}, {"x"}, {}]> : tensor<4x48x256xf32>
+  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
+  // CHECK-NEXT: return %[[CONCATENATE]] : tensor<4x80x256xf32>
   %2 = stablehlo.concatenate %0, %1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [
 {}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
   return %2 : tensor<4x80x256xf32>
@@ -78,11 +77,10 @@ func.func @concatenate_operands_are_results_of_slices_different_shardings_on_per
 func.func @concatenate_operands_are_results_of_slices_different_shardings_on_permutation_dim_with_equal_counts_but_conflicting_on_batching_dim(%arg0: tensor<4x40x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x":(2)2}, {}, {}]>}, %arg1: tensor<4x60x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}, {}]>}) -> (tensor<4x80x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {}]>}) {
   %0 = stablehlo.slice %arg0 [0:4, 0:32, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x":(2)2}, {}, {}]>]>} : (tensor<4x40x256xf32>) -> tensor<4x32x256xf32>
   %1 = stablehlo.slice %arg1 [0:4, 0:48, 0:256] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}, {}]>]>} : (tensor<4x60x256xf32>) -> tensor<4x48x256xf32>
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{"x":(2)2}, {"x":(1)2}, {}]> : tensor<4x32x256xf32>
-  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{"x":(2)2}, {"x":(1)2}, {}]> : tensor<4x48x256xf32>
-  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x":(2)2}, {"x":(1)2}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
-  // CHECK-NEXT: %[[RESHARD3:.*]] = sdy.reshard %[[CONCATENATE]] <@mesh, [{}, {"x"}, {}]> : tensor<4x80x256xf32>
-  // CHECK-NEXT: return %[[RESHARD3]] : tensor<4x80x256xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %0 <@mesh, [{}, {"x"}, {}]> : tensor<4x32x256xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %1 <@mesh, [{}, {"x"}, {}]> : tensor<4x48x256xf32>
+  // CHECK-NEXT: %[[CONCATENATE:.*]] = stablehlo.concatenate %[[RESHARD1]], %[[RESHARD2]], dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
+  // CHECK-NEXT: return %[[CONCATENATE]] : tensor<4x80x256xf32>
   %2 = stablehlo.concatenate %0, %1, dim = 1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {}]>]>} : (tensor<4x32x256xf32>, tensor<4x48x256xf32>) -> tensor<4x80x256xf32>
   return %2 : tensor<4x80x256xf32>
 }
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir
index e6162c4..c408590 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dot_dot_general.mlir
@@ -250,10 +250,11 @@ func.func @dot_incompatible_in_out_mismatch_same_axis_on_different_factors_lhs_n
 
 // CHECK-LABEL: func @dot_incompatible_in_out_mismatch_same_axis_on_different_factors_lhs_non_contracting_dim_is_sharded_smaller_local_contracting_dim
 func.func @dot_incompatible_in_out_mismatch_same_axis_on_different_factors_lhs_non_contracting_dim_is_sharded_smaller_local_contracting_dim(%arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {"y"}]>}, %arg1: tensor<16x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}, {}]>}) -> (tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>}) {
-  // CHECK-NEXT: %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<8x16xf32>, tensor<16x16xf32>) -> tensor<8x16xf32>
-  // CHECK-NEXT: %1 = sdy.all_reduce {"y"} %0 out_sharding=<@mesh, [{"x"}, {}]> : tensor<8x16xf32>
-  // CHECK-NEXT: %2 = sdy.reshard %1 <@mesh, [{}, {"x"}]> : tensor<8x16xf32>
-  // CHECK-NEXT: return %2 : tensor<8x16xf32>
+  // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{}, {"y"}]> : tensor<8x16xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %arg1 <@mesh, [{"y"}, {"x"}]> : tensor<16x16xf32>
+  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[RESHARD1]], %[[RESHARD2]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} : (tensor<8x16xf32>, tensor<16x16xf32>) -> tensor<8x16xf32>
+  // CHECK-NEXT: %[[ALL_REDUCE:.*]] = sdy.all_reduce {"y"} %[[DOT]] out_sharding=<@mesh, [{}, {"x"}]> : tensor<8x16xf32>
+  // CHECK-NEXT: return %[[ALL_REDUCE]] : tensor<8x16xf32>
   %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} : (tensor<8x16xf32>, tensor<16x16xf32>) -> tensor<8x16xf32>
   return %0 : tensor<8x16xf32>
 }
@@ -523,12 +524,10 @@ func.func @dot_genaral_one_suffix_has_larger_count_on_another_factor(%arg0: tens
   return %0 : tensor<4x8x16xf32>
 }
 
-// TODO(zixuanjiang). We may want to keep {"y", "x":(1)2, "t":(2)2} for the batch dimension.
 // CHECK-LABEL: func @dot_genaral_batching_dimension_shardings_have_common_prefix
 func.func @dot_genaral_batching_dimension_shardings_have_common_prefix(%arg0: tensor<64x8x32xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y", "x":(1)2, "t":(1)2}, {"t":(2)2}, {}]>}, %arg1: tensor<64x32x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {"t":(1)2}]>}) ->(tensor<64x8x16xf32> {sdy.sharding = #sdy.sharding<@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]>}) {
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y", "x":(1)2}, {"t":(2)2}, {}]> : tensor<64x8x32xf32>
-  // CHECK: %[[RESHARD2:.*]] = sdy.reshard %arg1 <@mesh_xyzt, [{"y", "x":(1)2}, {}, {"t":(1)2}]> : tensor<64x32x16xf32>
-  // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %[[RESHARD2]], batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y", "x":(1)2}, {"t":(2)2}, {"t":(1)2}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {}]> : tensor<64x8x32xf32>
+  // CHECK-NEXT: %[[DOTGENERAL:.*]] = stablehlo.dot_general %[[RESHARD1]], %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{"y", "x":(1)2, "t":(2)2}, {}, {"t":(1)2}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
   // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOTGENERAL]] <@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]> : tensor<64x8x16xf32>
   // CHECK-NEXT: return %[[RESHARD2]] : tensor<64x8x16xf32>
   %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1] {sdy.sharding = #sdy.sharding_per_value<[<@mesh_xyzt, [{}, {"t":(2)2}, {"t":(1)2}]>]>} : (tensor<64x8x32xf32>, tensor<64x32x16xf32>) -> tensor<64x8x16xf32>
@@ -586,62 +585,6 @@ func.func @dot_only_contracting_dims_sharded_and_has_same_shardings(
   return %0 : tensor<8x16xf32>
 }
 
-// The following 4 test targets are analyzed quantitlively in b/448376870#comment6.
-// In short, keep the largest factor sharded.
-
-// CHECK-LABEL: func @dot_ij_jk_ik_i_is_largset
-func.func @dot_ij_jk_ik_i_is_largset(
-    %arg0: tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
-    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
-    -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
-  // CHECK-NEXT: %[[RESHARD_LHS:.*]] = sdy.reshard %arg0 <@mesh, [{"x"}, {}]> : tensor<16x8xf32>
-  // CHECK-NEXT: %[[RESHARD_RHS:.*]] = sdy.reshard %arg1 <@mesh, [{}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[RESHARD_LHS]], %[[RESHARD_RHS]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<16x8xf32>, tensor<8x8xf32>) -> tensor<16x8xf32>
-  // CHECK-NEXT: return %[[DOT]] : tensor<16x8xf32>
-  %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<16x8xf32>, tensor<8x8xf32>) -> tensor<16x8xf32>
-  return %0 : tensor<16x8xf32>
-}
-
-// CHECK-LABEL: func @dot_ij_jk_ik_j_is_largset
-func.func @dot_ij_jk_ik_j_is_largset(
-    %arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
-    %arg1: tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
-    -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
-  // CHECK-NEXT: %[[RESHARD_RHS:.*]] = sdy.reshard %arg1 <@mesh, [{"x"}, {}]> : tensor<16x8xf32>
-  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %arg0, %[[RESHARD_RHS]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {}]>]>} : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
-  // CHECK-NEXT: %[[ALL_REDUCE:.*]] = sdy.all_reduce {"x"} %[[DOT]] out_sharding=<@mesh, [{}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[RESHARD_OUT:.*]] = sdy.reshard %[[ALL_REDUCE]] <@mesh, [{"x"}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: return %[[RESHARD_OUT]] : tensor<8x8xf32>
-  %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
-  return %0 : tensor<8x8xf32>
-}
-
-// CHECK-LABEL: func @dot_ij_jk_ik_k_is_largset
-func.func @dot_ij_jk_ik_k_is_largset(
-    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
-    %arg1: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
-    -> (tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
-  // CHECK-NEXT: %[[RESHARD_LHS:.*]] = sdy.reshard %arg0 <@mesh, [{}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[RESHARD_LHS]], %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} : (tensor<8x8xf32>, tensor<8x16xf32>) -> tensor<8x16xf32>
-  // CHECK-NEXT: %[[RESHARD_OUT:.*]] = sdy.reshard %[[DOT]] <@mesh, [{"x"}, {}]> : tensor<8x16xf32>
-  // CHECK-NEXT: return %[[RESHARD_OUT]] : tensor<8x16xf32>
-  %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<8x8xf32>, tensor<8x16xf32>) -> tensor<8x16xf32>
-  return %0 : tensor<8x16xf32>
-}
-
-// CHECK-LABEL: func @dot_ij_jk_ik_same_ijk
-func.func @dot_ij_jk_ik_same_ijk(
-    %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>},
-    %arg1: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}]>})
-    -> (tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}]>}) {
-  // CHECK-NEXT: %[[RESHARD_LHS:.*]] = sdy.reshard %arg0 <@mesh, [{}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[RESHARD_LHS]], %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}]>]>} : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
-  // CHECK-NEXT: %[[RESHARD_OUT:.*]] = sdy.reshard %[[DOT]] <@mesh, [{"x"}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: return %[[RESHARD_OUT]] : tensor<8x8xf32>
-  %0 = stablehlo.dot %arg0, %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {}]>]>} : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<8x8xf32>
-  return %0 : tensor<8x8xf32>
-}
-
 // CHECK-LABEL: func @dot_on_square_matrices_lhs_2nd_dim_rhs_2nd_dim_sharded_the_same_way
 func.func @dot_on_square_matrices_lhs_2nd_dim_rhs_2nd_dim_sharded_the_same_way(
     %arg0: tensor<8x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}]>},
@@ -673,10 +616,10 @@ func.func @dot_on_rectangular_inputs_square_output_large_contracting_dim_lhs_2nd
     %arg0: tensor<8x16xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}]>},
     %arg1: tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}]>})
     -> tensor<8x8xf32> {
-  // CHECK-NEXT: %0 = sdy.reshard %arg1 <@mesh, [{"y"}, {}]> : tensor<16x8xf32>
-  // CHECK-NEXT: %1 = stablehlo.dot %arg0, %0 : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
-  // CHECK-NEXT: %2 = sdy.all_reduce {"y"} %1 out_sharding=<@mesh, [{}, {}]> : tensor<8x8xf32>
-  // CHECK-NEXT: return %2 : tensor<8x8xf32>
+  // CHECK-NEXT: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{}, {}]>
+  // CHECK-NEXT: %[[DOT:.*]] = stablehlo.dot %[[RESHARD1]], %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}]>]>}
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[DOT]] <@mesh, [{}, {}]>
+  // CHECK-NEXT: return %[[RESHARD2]]
   %0 = stablehlo.dot %arg0, %arg1 : (tensor<8x16xf32>, tensor<16x8xf32>) -> tensor<8x8xf32>
   return %0 : tensor<8x8xf32>
 }
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dynamic_slice_dynamic_update_slice.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dynamic_slice_dynamic_update_slice.mlir
index c12086a..3c5e4c7 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dynamic_slice_dynamic_update_slice.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/dynamic_slice_dynamic_update_slice.mlir
@@ -42,10 +42,10 @@ func.func @dynamic_update_slice(%arg0: tensor<32x4x8xf32> {sdy.sharding = #sdy.s
 
 // CHECK-LABEL: func @dynamic_update_slice_different_input_and_output_sharding
 func.func @dynamic_update_slice_different_input_and_output_sharding(%arg0: tensor<32x4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"x"}, {"y"}]>}, %arg1: tensor<32x1x2xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {"y"}]>}, %arg2: tensor<i32>, %arg3: tensor<i32>, %arg4: tensor<i32>) -> (tensor<32x4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y"}, {"x"}]>}){
-  // CHECK-NEXT: %[[REPLICATED_UPDATE:.*]] = sdy.reshard %arg1 <@mesh, [{}, {}, {}]> : tensor<32x1x2xf32>
-  // CHECK-NEXT: %[[DYNAMIC_UPDATE_SLICE:.*]] = stablehlo.dynamic_update_slice %arg0, %[[REPLICATED_UPDATE]], %arg2, %arg3, %arg4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"x"}, {"y"}]>]>} : (tensor<32x4x8xf32>, tensor<32x1x2xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<32x4x8xf32>
-  // CHECK-NEXT: %[[RESHARD:.*]] = sdy.reshard %[[DYNAMIC_UPDATE_SLICE]] <@mesh, [{}, {"y"}, {"x"}]> : tensor<32x4x8xf32>
-  // CHECK-NEXT: return %[[RESHARD]] : tensor<32x4x8xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{}, {"y"}, {"x"}]> : tensor<32x4x8xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %arg1 <@mesh, [{}, {}, {}]> : tensor<32x1x2xf32>
+  // CHECK-NEXT: %[[DYNAMIC_UPDATE_SLICE:.*]] = stablehlo.dynamic_update_slice %[[RESHARD1]], %[[RESHARD2]], %arg2, %arg3, %arg4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}, {"x"}]>]>} : (tensor<32x4x8xf32>, tensor<32x1x2xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<32x4x8xf32>
+  // CHECK-NEXT: return %[[DYNAMIC_UPDATE_SLICE]] : tensor<32x4x8xf32>
   %0 = stablehlo.dynamic_update_slice %arg0, %arg1, %arg2, %arg3, %arg4 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y"}, {"x"}]>]>} : (tensor<32x4x8xf32>, tensor<32x1x2xf32>, tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<32x4x8xf32>
   return %0 : tensor<32x4x8xf32>
 }
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/reshape.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/reshape.mlir
index 81c0b6e..d9f743c 100644
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/reshape.mlir
+++ b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards/reshape.mlir
@@ -5,9 +5,10 @@ sdy.mesh @mesh_xyz = <["x"=4, "y"=2, "z"=4]>
 
 // CHECK-LABEL: func @reshape
 func.func @reshape(%arg0: tensor<16x2x4xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {}, {}]>}) -> (tensor<16x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {"y", "x"}]>}) {
-  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{}, {"y"}, {"x"}]> : tensor<16x2x4xf32>
-  // CHECK-NEXT: %[[RESHAPE:.*]] = stablehlo.reshape %[[RESHARD1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y", "x"}]>]>} : (tensor<16x2x4xf32>) -> tensor<16x8xf32>
-  // CHECK-NEXT: return %[[RESHAPE]] : tensor<16x8xf32>
+  // CHECK: %[[RESHARD1:.*]] = sdy.reshard %arg0 <@mesh, [{"x"}, {"y"}, {}]> : tensor<16x2x4xf32>
+  // CHECK-NEXT: %[[RESHAPE:.*]] = stablehlo.reshape %[[RESHARD1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x"}, {"y"}]>]>} : (tensor<16x2x4xf32>) -> tensor<16x8xf32>
+  // CHECK-NEXT: %[[RESHARD2:.*]] = sdy.reshard %[[RESHAPE]] <@mesh, [{}, {"y", "x"}]> : tensor<16x8xf32>
+  // CHECK-NEXT: return %[[RESHARD2]] : tensor<16x8xf32>
   %0 = stablehlo.reshape %arg0  {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{}, {"y", "x"}]>]>} : (tensor<16x2x4xf32>) -> tensor<16x8xf32>
   return %0 : tensor<16x8xf32>
 }
@@ -54,9 +55,9 @@ func.func @reshape_simple_merge_sharding_is_from_xy_to_xy_and_x_fits_exactly_to_
 // CHECK-LABEL: func.func @reshape_simple_merge_sharding_is_from_xy_to_yx_and_x_fits_exactly_to_first_dim
 // NOTE: It reshards this way because the dependencies are dropped as factors are fully-sharded.
 func.func @reshape_simple_merge_sharding_is_from_xy_to_yx_and_x_fits_exactly_to_first_dim(%arg0: tensor<4x8xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}, {"y"}]>}) -> (tensor<32xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"y", "x"}]>}) {
-  // CHECK-NEXT: %[[RESHAPE:.*]] = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"x", "y"}]>]>} : (tensor<4x8xf32>) -> tensor<32xf32>
-  // CHECK-NEXT: %[[RESHARD:.*]] = sdy.reshard %[[RESHAPE]] <@mesh, [{"y", "x"}]> : tensor<32xf32>
-  // CHECK-NEXT: return %[[RESHARD]] : tensor<32xf32>
+  // CHECK: %[[RESHARD:.*]] = sdy.reshard %arg0 <@mesh, [{"y", "x":(1)2}, {"x":(2)2}]> : tensor<4x8xf32>
+  // CHECK-NEXT: %[[RESHAPE:.*]] = stablehlo.reshape %[[RESHARD]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y", "x"}]>]>} : (tensor<4x8xf32>) -> tensor<32xf32>
+  // CHECK-NEXT: return %[[RESHAPE]] : tensor<32xf32>
   %0 = stablehlo.reshape %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y", "x"}]>]>} : (tensor<4x8xf32>) -> tensor<32xf32>
   return %0 : tensor<32xf32>
 }
diff --git a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards_avoid_reshards_on_named_computations.mlir b/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards_avoid_reshards_on_named_computations.mlir
deleted file mode 100644
index 15acf58..0000000
--- a/shardy/dialect/sdy/transforms/export/test/insert_explicit_reshards_avoid_reshards_on_named_computations.mlir
+++ /dev/null
@@ -1,62 +0,0 @@
-// RUN: sdy_opt %s -sdy-insert-explicit-reshards='avoid-reshards-on-named-computations=true' -sdy-insert-explicit-reshards='avoid-reshards-on-named-computations=true' | FileCheck %s
-
-sdy.mesh @mesh = <["x"=2, "y"=2, "z"=4]>
-
-//===----------------------------------------------------------------------===//
-// Named computations tests
-// More tests are in insert_explicit_reshards/data_flow_ops.mlir
-//===----------------------------------------------------------------------===//
-
-// CHECK-LABEL: func @named_computation
-func.func @named_computation(%arg0: tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}]>}) -> (tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"z"}]>}) {
-  // CHECK-NEXT: sdy.named_computation<"foo">(%arg0)
-  // CHECK-SAME: in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"z"}]>
-  %0 = sdy.named_computation<"foo">(%arg0) in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"z"}]>] (%arg1: tensor<210xf32>) {
-    %2 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>} : tensor<210xf32>
-    // CHECK: %[[RESHARD:.*]] = sdy.reshard %{{.*}} <@mesh, [{"z"}]> : tensor<210xf32>
-    // CHECK-NEXT: sdy.return %[[RESHARD]] : tensor<210xf32>
-    sdy.return %2 : tensor<210xf32>
-  } : (tensor<210xf32>) -> (tensor<210xf32>)
-  %1 = stablehlo.negate %0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"z"}]>]>} : tensor<210xf32>
-  return %1 : tensor<210xf32>
-}
-
-// CHECK-LABEL: func @one_argument_to_multiple_named_computations(
-func.func @one_argument_to_multiple_named_computations(%arg0: tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}]>}) -> (tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"z"}]>}) {
-  // CHECK-NEXT: %[[NC0:.*]] = sdy.named_computation<"foo">(%arg0)
-  // CHECK-SAME: in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"y"}]>
-  %0 = sdy.named_computation<"foo">(%arg0) in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"y"}]>] (%arg1: tensor<210xf32>) {
-    %2 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>} : tensor<210xf32>
-    sdy.return %2 : tensor<210xf32>
-  } : (tensor<210xf32>) -> (tensor<210xf32>)
-  // CHECK: %[[NC1:.*]] = sdy.named_computation<"foo">(%arg0)
-  // CHECK-SAME: in_shardings=[<@mesh, [{"z"}]>] out_shardings=[<@mesh, [{"z"}]>
-  %1 = sdy.named_computation<"foo">(%arg0) in_shardings=[<@mesh, [{"z"}]>] out_shardings=[<@mesh, [{"z"}]>] (%arg1: tensor<210xf32>) {
-    %2 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"z"}]>]>} : tensor<210xf32>
-    sdy.return %2 : tensor<210xf32>
-  } : (tensor<210xf32>) -> (tensor<210xf32>)
-  // CHECK: %[[ADD:.*]] = stablehlo.add %[[NC0]], %[[NC1]] {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"z"}]>]>}
-  // CHECK-NEXT: return %[[ADD]]
-  %3 = stablehlo.add %0, %1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"z"}]>]>} : tensor<210xf32>
-  return %3 : tensor<210xf32>
-}
-
-// CHECK-LABEL: func @different_arguments_to_multiple_named_computations_with_same_input_output_shardings
-func.func @different_arguments_to_multiple_named_computations_with_same_input_output_shardings(%arg0: tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"x"}]>}) -> (tensor<210xf32> {sdy.sharding = #sdy.sharding<@mesh, [{"y"}]>}) {
-  // CHECK-NEXT: %[[NC0:.*]] = sdy.named_computation<"foo">(%arg0)
-  // CHECK-SAME: in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"y"}]>
-  %0 = sdy.named_computation<"foo">(%arg0) in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"y"}]>] (%arg1: tensor<210xf32>) {
-    %3 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>} : tensor<210xf32>
-    sdy.return %3 : tensor<210xf32>
-  } : (tensor<210xf32>) -> (tensor<210xf32>)
-  // CHECK: %[[NEGATE:.*]] = stablehlo.negate %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>}
-  // CHECK-NEXT: %[[NC1:.*]] = sdy.named_computation<"foo">(%[[NEGATE]])
-  // CHECK-SAME: in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"y"}]>
-  %1 = stablehlo.negate %arg0 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>} : tensor<210xf32>
-  %2 = sdy.named_computation<"foo">(%1) in_shardings=[<@mesh, [{"y"}]>] out_shardings=[<@mesh, [{"y"}]>] (%arg1: tensor<210xf32>) {
-    %3 = stablehlo.abs %arg1 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>} : tensor<210xf32>
-    sdy.return %3 : tensor<210xf32>
-  } : (tensor<210xf32>) -> (tensor<210xf32>)
-  %4 = stablehlo.add %0, %2 {sdy.sharding = #sdy.sharding_per_value<[<@mesh, [{"y"}]>]>} : tensor<210xf32>
-  return %4 : tensor<210xf32>
-}
diff --git a/shardy/dialect/sdy/transforms/propagation/propagation_pipeline.cc b/shardy/dialect/sdy/transforms/propagation/propagation_pipeline.cc
index 6ccbb0c..bd56418 100644
--- a/shardy/dialect/sdy/transforms/propagation/propagation_pipeline.cc
+++ b/shardy/dialect/sdy/transforms/propagation/propagation_pipeline.cc
@@ -40,8 +40,6 @@ void populateExportOptions(ExportOptions& options,
       propOptions.removeAllGatherReduceScatterForCMV1;
   options.dumpShardingOrigins = propOptions.debugShardingOrigins;
   options.dumpPropagationEdges = propOptions.debugPropagationEdgeSharding;
-  options.avoidReshardsOnNamedComputations =
-      propOptions.avoidReshardsOnNamedComputations;
 }
 
 }  // namespace
diff --git a/third_party/llvm/generated.patch b/third_party/llvm/generated.patch
index 2856e5f..6b6973d 100644
--- a/third_party/llvm/generated.patch
+++ b/third_party/llvm/generated.patch
@@ -1,34 +1,1588 @@
 Auto generated patch. Do not edit or delete it, even if empty.
-diff -ruN --strip-trailing-cr a/mlir/test/Target/SPIRV/function-decorations-asserts.mlir b/mlir/test/Target/SPIRV/function-decorations-asserts.mlir
---- a/mlir/test/Target/SPIRV/function-decorations-asserts.mlir
-+++ b/mlir/test/Target/SPIRV/function-decorations-asserts.mlir
-@@ -0,0 +1,20 @@
-+// REQUIRES: asserts
-+// RUN: mlir-translate --no-implicit-module --test-spirv-roundtrip --split-input-file --debug %s | FileCheck %s
-+
-+spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader, Linkage], []> {
-+    spirv.func @linkage_attr_test_kernel()  "DontInline"  attributes {}  {
-+        %uchar_0 = spirv.Constant 0 : i8
-+        %ushort_1 = spirv.Constant 1 : i16
-+        %uint_0 = spirv.Constant 0 : i32
-+        spirv.FunctionCall @outside.func.with.linkage(%uchar_0):(i8) -> ()
-+        spirv.Return
-+    }
-+    // CHECK: linkage_attributes = #spirv.linkage_attributes<linkage_name = "outside.func", linkage_type = <Import>>
-+    spirv.func @outside.func.with.linkage(%arg0 : i8) -> () "Pure" attributes {
-+      linkage_attributes=#spirv.linkage_attributes<
-+        linkage_name="outside.func",
-+        linkage_type=<Import>
-+      >
+diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaConcept.cpp b/clang/lib/Sema/SemaConcept.cpp
+--- a/clang/lib/Sema/SemaConcept.cpp
++++ b/clang/lib/Sema/SemaConcept.cpp
+@@ -385,6 +385,28 @@
+     return inherited::TraverseStmt(E->getReplacement());
+   }
+ 
++  bool TraverseTemplateName(TemplateName Template) {
++    if (auto *TTP = dyn_cast_if_present<TemplateTemplateParmDecl>(
++            Template.getAsTemplateDecl());
++        TTP && TTP->getDepth() < TemplateArgs.getNumLevels()) {
++      if (!TemplateArgs.hasTemplateArgument(TTP->getDepth(),
++                                            TTP->getPosition()))
++        return true;
++
++      TemplateArgument Arg = TemplateArgs(TTP->getDepth(), TTP->getPosition());
++      if (TTP->isParameterPack() && SemaRef.ArgPackSubstIndex) {
++        assert(Arg.getKind() == TemplateArgument::Pack &&
++               "Missing argument pack");
++        Arg = SemaRef.getPackSubstitutedTemplateArgument(Arg);
++      }
++      assert(!Arg.getAsTemplate().isNull() &&
++             "Null template template argument");
++      UsedTemplateArgs.push_back(
++          SemaRef.Context.getCanonicalTemplateArgument(Arg));
 +    }
-+    spirv.func @inside.func() -> () "Pure" attributes {} {spirv.Return}
++    return inherited::TraverseTemplateName(Template);
++  }
++
+   void VisitConstraint(const NormalizedConstraintWithParamMapping &Constraint) {
+     if (!Constraint.hasParameterMapping()) {
+       for (const auto &List : TemplateArgs)
+diff -ruN --strip-trailing-cr a/clang/test/SemaTemplate/concepts.cpp b/clang/test/SemaTemplate/concepts.cpp
+--- a/clang/test/SemaTemplate/concepts.cpp
++++ b/clang/test/SemaTemplate/concepts.cpp
+@@ -1573,3 +1573,62 @@
+   template<typename... Ts> auto comma = (..., Ts());
+   auto b = comma<check<e{}>>;
+ } // namespace GH162770
++
++namespace GH164750 {
++
++template <typename>
++struct a;
++template <typename>
++struct b;
++
++template <template <typename> typename c, typename d, typename>
++concept e = !__is_convertible_to(c<d>*, b<d>*);
++
++template <typename...>
++struct f;
++template <typename g, typename... h>
++struct f<g, h...> {
++    g i;
++};
++
++template <typename, typename>
++struct u;
++template <typename j, template <typename> typename k, typename l>
++    requires e<k, j, l>
++struct u<const k<j>*, l> {
++    u(const a<j>*);
++};
++template <typename j, template <typename> typename k, typename l>
++struct u<const k<j>*, l> {
++    u(const b<j>*);
++};
++
++template <typename>
++struct m;
++template <typename n, typename... o>
++struct m<n (*)(o...)> {
++    template <template <typename> typename j>
++    using p = j<o...>;
++};
++
++template <typename q, typename r>
++struct s {
++    template <typename... p>
++    struct D {
++        using v = f<u<r, p>...>;
++    };
++    template <typename... t>
++    s(t... p1) : x(p1...) {}
++    m<q>::template p<D>::v x;
++};
++template <typename w, typename... t>
++void fn1(w, t... p2) {
++    s<w, t...>(p2...);
++}
++int* fn2(int) { return nullptr; }
++void fn3() {
++    fn1(fn2, static_cast<const a<int>*>(nullptr));
++    fn1(fn2, static_cast<const b<int>*>(nullptr));
++}
++
++}
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
+--- a/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
++++ b/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp
+@@ -7231,6 +7231,9 @@
+     return DenseMap<const SCEV *, Value *>();
+   }
+ 
++  VPlanTransforms::narrowInterleaveGroups(
++      BestVPlan, BestVF,
++      TTI.getRegisterBitWidth(TargetTransformInfo::RGK_FixedWidthVector));
+   VPlanTransforms::removeDeadRecipes(BestVPlan);
+ 
+   VPlanTransforms::convertToConcreteRecipes(BestVPlan);
+@@ -8199,10 +8202,6 @@
+       if (CM.foldTailWithEVL())
+         VPlanTransforms::runPass(VPlanTransforms::addExplicitVectorLength,
+                                  *Plan, CM.getMaxSafeElements());
+-
+-      if (auto P = VPlanTransforms::narrowInterleaveGroups(*Plan, TTI))
+-        VPlans.push_back(std::move(P));
+-
+       assert(verifyVPlanIsValid(*Plan) && "VPlan is invalid");
+       VPlans.push_back(std::move(Plan));
+     }
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/VPlan.cpp b/llvm/lib/Transforms/Vectorize/VPlan.cpp
+--- a/llvm/lib/Transforms/Vectorize/VPlan.cpp
++++ b/llvm/lib/Transforms/Vectorize/VPlan.cpp
+@@ -1191,7 +1191,6 @@
+   }
+   Old2NewVPValues[&VectorTripCount] = &NewPlan->VectorTripCount;
+   Old2NewVPValues[&VF] = &NewPlan->VF;
+-  Old2NewVPValues[&UF] = &NewPlan->UF;
+   Old2NewVPValues[&VFxUF] = &NewPlan->VFxUF;
+   if (BackedgeTakenCount) {
+     NewPlan->BackedgeTakenCount = new VPValue();
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/VPlan.h b/llvm/lib/Transforms/Vectorize/VPlan.h
+--- a/llvm/lib/Transforms/Vectorize/VPlan.h
++++ b/llvm/lib/Transforms/Vectorize/VPlan.h
+@@ -4152,9 +4152,6 @@
+   /// Represents the vectorization factor of the loop.
+   VPValue VF;
+ 
+-  /// Represents the symbolic unroll factor of the loop.
+-  VPValue UF;
+-
+   /// Represents the loop-invariant VF * UF of the vector loop region.
+   VPValue VFxUF;
+ 
+@@ -4308,9 +4305,6 @@
+   VPValue &getVF() { return VF; };
+   const VPValue &getVF() const { return VF; };
+ 
+-  /// Returns the symbolic UF of the vector loop region.
+-  VPValue &getSymbolicUF() { return UF; };
+-
+   /// Returns VF * UF of the vector loop region.
+   VPValue &getVFxUF() { return VFxUF; }
+ 
+@@ -4320,12 +4314,6 @@
+ 
+   void addVF(ElementCount VF) { VFs.insert(VF); }
+ 
+-  /// Remove \p VF from the plan.
+-  void removeVF(ElementCount VF) {
+-    assert(hasVF(VF) && "tried to remove VF not present in plan");
+-    VFs.remove(VF);
+-  }
+-
+   void setVF(ElementCount VF) {
+     assert(hasVF(VF) && "Cannot set VF not already in plan");
+     VFs.clear();
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp b/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp
+--- a/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp
++++ b/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp
+@@ -3956,9 +3956,6 @@
+   // used.
+   // TODO: Assert that they aren't used.
+ 
+-  VPValue *UF = Plan.getOrAddLiveIn(ConstantInt::get(TCTy, Plan.getUF()));
+-  Plan.getSymbolicUF().replaceAllUsesWith(UF);
+-
+   // If there are no users of the runtime VF, compute VFxUF by constant folding
+   // the multiplication of VF and UF.
+   if (VF.getNumUsers() == 0) {
+@@ -3978,6 +3975,7 @@
+   }
+   VF.replaceAllUsesWith(RuntimeVF);
+ 
++  VPValue *UF = Plan.getOrAddLiveIn(ConstantInt::get(TCTy, Plan.getUF()));
+   VPValue *MulByUF = Builder.createNaryOp(Instruction::Mul, {RuntimeVF, UF});
+   VFxUF.replaceAllUsesWith(MulByUF);
+ }
+@@ -4045,14 +4043,14 @@
+   return false;
+ }
+ 
+-/// Returns VF from \p VFs if \p IR is a full interleave group with factor and
+-/// number of members both equal to VF. The interleave group must also access
+-/// the full vector width.
+-static std::optional<ElementCount> isConsecutiveInterleaveGroup(
+-    VPInterleaveRecipe *InterleaveR, ArrayRef<ElementCount> VFs,
+-    VPTypeAnalysis &TypeInfo, const TargetTransformInfo &TTI) {
++/// Returns true if \p IR is a full interleave group with factor and number of
++/// members both equal to \p VF. The interleave group must also access the full
++/// vector width \p VectorRegWidth.
++static bool isConsecutiveInterleaveGroup(VPInterleaveRecipe *InterleaveR,
++                                         unsigned VF, VPTypeAnalysis &TypeInfo,
++                                         unsigned VectorRegWidth) {
+   if (!InterleaveR || InterleaveR->getMask())
+-    return std::nullopt;
++    return false;
+ 
+   Type *GroupElementTy = nullptr;
+   if (InterleaveR->getStoredValues().empty()) {
+@@ -4061,7 +4059,7 @@
+                 [&TypeInfo, GroupElementTy](VPValue *Op) {
+                   return TypeInfo.inferScalarType(Op) == GroupElementTy;
+                 }))
+-      return std::nullopt;
++      return false;
+   } else {
+     GroupElementTy =
+         TypeInfo.inferScalarType(InterleaveR->getStoredValues()[0]);
+@@ -4069,27 +4067,13 @@
+                 [&TypeInfo, GroupElementTy](VPValue *Op) {
+                   return TypeInfo.inferScalarType(Op) == GroupElementTy;
+                 }))
+-      return std::nullopt;
++      return false;
+   }
+ 
+-  auto GetVectorWidthForVF = [&TTI](ElementCount VF) {
+-    TypeSize Size = TTI.getRegisterBitWidth(
+-        VF.isFixed() ? TargetTransformInfo::RGK_FixedWidthVector
+-                     : TargetTransformInfo::RGK_ScalableVector);
+-    assert(Size.isScalable() == VF.isScalable() &&
+-           "if Size is scalable, VF must to and vice versa");
+-    return Size.getKnownMinValue();
+-  };
+-
+-  for (ElementCount VF : VFs) {
+-    unsigned MinVal = VF.getKnownMinValue();
+-    unsigned GroupSize = GroupElementTy->getScalarSizeInBits() * MinVal;
+-    auto IG = InterleaveR->getInterleaveGroup();
+-    if (IG->getFactor() == MinVal && IG->getNumMembers() == MinVal &&
+-        GroupSize == GetVectorWidthForVF(VF))
+-      return {VF};
+-  }
+-  return std::nullopt;
++  unsigned GroupSize = GroupElementTy->getScalarSizeInBits() * VF;
++  auto IG = InterleaveR->getInterleaveGroup();
++  return IG->getFactor() == VF && IG->getNumMembers() == VF &&
++         GroupSize == VectorRegWidth;
+ }
+ 
+ /// Returns true if \p VPValue is a narrow VPValue.
+@@ -4100,18 +4084,16 @@
+   return RepR && RepR->isSingleScalar();
+ }
+ 
+-std::unique_ptr<VPlan>
+-VPlanTransforms::narrowInterleaveGroups(VPlan &Plan,
+-                                        const TargetTransformInfo &TTI) {
+-  using namespace llvm::VPlanPatternMatch;
++void VPlanTransforms::narrowInterleaveGroups(VPlan &Plan, ElementCount VF,
++                                             unsigned VectorRegWidth) {
+   VPRegionBlock *VectorLoop = Plan.getVectorLoopRegion();
+-
+   if (!VectorLoop)
+-    return nullptr;
++    return;
+ 
+   VPTypeAnalysis TypeInfo(Plan);
++
++  unsigned VFMinVal = VF.getKnownMinValue();
+   SmallVector<VPInterleaveRecipe *> StoreGroups;
+-  std::optional<ElementCount> VFToOptimize;
+   for (auto &R : *VectorLoop->getEntryBasicBlock()) {
+     if (isa<VPCanonicalIVPHIRecipe>(&R) || match(&R, m_BranchOnCount()))
+       continue;
+@@ -4125,33 +4107,30 @@
+     //  * recipes writing to memory except interleave groups
+     // Only support plans with a canonical induction phi.
+     if (R.isPhi())
+-      return nullptr;
++      return;
+ 
+     auto *InterleaveR = dyn_cast<VPInterleaveRecipe>(&R);
+     if (R.mayWriteToMemory() && !InterleaveR)
+-      return nullptr;
++      return;
++
++    // Do not narrow interleave groups if there are VectorPointer recipes and
++    // the plan was unrolled. The recipe implicitly uses VF from
++    // VPTransformState.
++    // TODO: Remove restriction once the VF for the VectorPointer offset is
++    // modeled explicitly as operand.
++    if (isa<VPVectorPointerRecipe>(&R) && Plan.getUF() > 1)
++      return;
+ 
+     // All other ops are allowed, but we reject uses that cannot be converted
+     // when checking all allowed consumers (store interleave groups) below.
+     if (!InterleaveR)
+       continue;
+ 
+-    // Try to find a single VF, where all interleave groups are consecutive and
+-    // saturate the full vector width. If we already have a candidate VF, check
+-    // if it is applicable for the current InterleaveR, otherwise look for a
+-    // suitable VF across the Plans VFs.
+-    //
+-    if (VFToOptimize) {
+-      if (!isConsecutiveInterleaveGroup(InterleaveR, {*VFToOptimize}, TypeInfo,
+-                                        TTI))
+-        return nullptr;
+-    } else {
+-      if (auto VF = isConsecutiveInterleaveGroup(
+-              InterleaveR, to_vector(Plan.vectorFactors()), TypeInfo, TTI))
+-        VFToOptimize = *VF;
+-      else
+-        return nullptr;
+-    }
++    // Bail out on non-consecutive interleave groups.
++    if (!isConsecutiveInterleaveGroup(InterleaveR, VFMinVal, TypeInfo,
++                                      VectorRegWidth))
++      return;
++
+     // Skip read interleave groups.
+     if (InterleaveR->getStoredValues().empty())
+       continue;
+@@ -4185,34 +4164,24 @@
+     auto *WideMember0 = dyn_cast_or_null<VPWidenRecipe>(
+         InterleaveR->getStoredValues()[0]->getDefiningRecipe());
+     if (!WideMember0)
+-      return nullptr;
++      return;
+     for (const auto &[I, V] : enumerate(InterleaveR->getStoredValues())) {
+       auto *R = dyn_cast_or_null<VPWidenRecipe>(V->getDefiningRecipe());
+       if (!R || R->getOpcode() != WideMember0->getOpcode() ||
+           R->getNumOperands() > 2)
+-        return nullptr;
++        return;
+       if (any_of(enumerate(R->operands()),
+                  [WideMember0, Idx = I](const auto &P) {
+                    const auto &[OpIdx, OpV] = P;
+                    return !canNarrowLoad(WideMember0, OpIdx, OpV, Idx);
+                  }))
+-        return nullptr;
++        return;
+     }
+     StoreGroups.push_back(InterleaveR);
+   }
+ 
+   if (StoreGroups.empty())
+-    return nullptr;
+-
+-  // All interleave groups in Plan can be narrowed for VFToOptimize. Split the
+-  // original Plan into 2: a) a new clone which contains all VFs of Plan, except
+-  // VFToOptimize, and b) the original Plan with VFToOptimize as single VF.
+-  std::unique_ptr<VPlan> NewPlan;
+-  if (size(Plan.vectorFactors()) != 1) {
+-    NewPlan = std::unique_ptr<VPlan>(Plan.duplicate());
+-    Plan.setVF(*VFToOptimize);
+-    NewPlan->removeVF(*VFToOptimize);
+-  }
++    return;
+ 
+   // Convert InterleaveGroup \p R to a single VPWidenLoadRecipe.
+   SmallPtrSet<VPValue *, 4> NarrowedOps;
+@@ -4283,8 +4252,9 @@
+   auto *Inc = cast<VPInstruction>(CanIV->getBackedgeValue());
+   VPBuilder PHBuilder(Plan.getVectorPreheader());
+ 
+-  VPValue *UF = &Plan.getSymbolicUF();
+-  if (VFToOptimize->isScalable()) {
++  VPValue *UF = Plan.getOrAddLiveIn(
++      ConstantInt::get(CanIV->getScalarType(), 1 * Plan.getUF()));
++  if (VF.isScalable()) {
+     VPValue *VScale = PHBuilder.createElementCount(
+         CanIV->getScalarType(), ElementCount::getScalable(1));
+     VPValue *VScaleUF = PHBuilder.createNaryOp(Instruction::Mul, {VScale, UF});
+@@ -4296,10 +4266,6 @@
+         Plan.getOrAddLiveIn(ConstantInt::get(CanIV->getScalarType(), 1)));
+   }
+   removeDeadRecipes(Plan);
+-  assert(none_of(*VectorLoop->getEntryBasicBlock(),
+-                 IsaPred<VPVectorPointerRecipe>) &&
+-         "All VPVectorPointerRecipes should have been removed");
+-  return NewPlan;
+ }
+ 
+ /// Add branch weight metadata, if the \p Plan's middle block is terminated by a
+diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/VPlanTransforms.h b/llvm/lib/Transforms/Vectorize/VPlanTransforms.h
+--- a/llvm/lib/Transforms/Vectorize/VPlanTransforms.h
++++ b/llvm/lib/Transforms/Vectorize/VPlanTransforms.h
+@@ -341,20 +341,14 @@
+   static DenseMap<const SCEV *, Value *> expandSCEVs(VPlan &Plan,
+                                                      ScalarEvolution &SE);
+ 
+-  /// Try to find a single VF among \p Plan's VFs for which all interleave
+-  /// groups (with known minimum VF elements) can be replaced by wide loads and
+-  /// stores processing VF elements, if all transformed interleave groups access
+-  /// the full vector width (checked via the maximum vector register width). If
+-  /// the transformation can be applied, the original \p Plan will be split in
+-  /// 2:
+-  ///  1. The original Plan with the single VF containing the optimized recipes
+-  ///  using wide loads instead of interleave groups.
+-  ///  2. A new clone which contains all VFs of Plan except the optimized VF.
+-  ///
+-  /// This effectively is a very simple form of loop-aware SLP, where we use
+-  /// interleave groups to identify candidates.
+-  static std::unique_ptr<VPlan>
+-  narrowInterleaveGroups(VPlan &Plan, const TargetTransformInfo &TTI);
++  /// Try to convert a plan with interleave groups with VF elements to a plan
++  /// with the interleave groups replaced by wide loads and stores processing VF
++  /// elements, if all transformed interleave groups access the full vector
++  /// width (checked via \o VectorRegWidth). This effectively is a very simple
++  /// form of loop-aware SLP, where we use interleave groups to identify
++  /// candidates.
++  static void narrowInterleaveGroups(VPlan &Plan, ElementCount VF,
++                                     unsigned VectorRegWidth);
+ 
+   /// Predicate and linearize the control-flow in the only loop region of
+   /// \p Plan. If \p FoldTail is true, create a mask guarding the loop
+diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/WebAssembly/memory-interleave.ll b/llvm/test/CodeGen/WebAssembly/memory-interleave.ll
+--- a/llvm/test/CodeGen/WebAssembly/memory-interleave.ll
++++ b/llvm/test/CodeGen/WebAssembly/memory-interleave.ll
+@@ -2103,10 +2103,7 @@
+ 
+ ; CHECK-LABEL: four_floats_same_op:
+ ; CHECK: loop
+-; CHECK: v128.load
+-; CHECK: v128.load
+-; CHECK: f32x4.mul
+-; CHECK: v128.store
++; CHECK-NOT: v128.load
+ define hidden void @four_floats_same_op(ptr noundef readonly captures(none) %a, ptr noundef readonly captures(none) %b, ptr noundef writeonly captures(none) %res, i32 noundef %N) {
+ entry:
+   %cmp45.not = icmp eq i32 %N, 0
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/Inputs/bar.ll b/llvm/test/LTO/AArch64/Inputs/bar.ll
+--- a/llvm/test/LTO/AArch64/Inputs/bar.ll
++++ b/llvm/test/LTO/AArch64/Inputs/bar.ll
+@@ -0,0 +1,35 @@
++;; This file contains the new semantic of the branch-target-enforcement, sign-return-address.
++;; Used for test mixing a mixed link case and also verify the import too in llc.
++
++; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
++
++target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
++target triple = "aarch64-unknown-linux-gnu"
++
++define dso_local void @bar() #0 {
++entry:
++  ret void
++}
++; CHECK-LABEL: bar:
++; CHECK-NOT:       hint
++; CHECK-NOT:       bti
++; CHECK:           ret
++
++define dso_local void @baz() #1 {
++entry:
++  ret void
 +}
-diff -ruN --strip-trailing-cr a/mlir/test/Target/SPIRV/function-decorations.mlir b/mlir/test/Target/SPIRV/function-decorations.mlir
---- a/mlir/test/Target/SPIRV/function-decorations.mlir
-+++ b/mlir/test/Target/SPIRV/function-decorations.mlir
-@@ -1,5 +1,4 @@
- // RUN: mlir-translate --no-implicit-module --test-spirv-roundtrip --split-input-file %s | FileCheck %s
--// RUN: mlir-translate --no-implicit-module --test-spirv-roundtrip --split-input-file --debug %s | FileCheck %s
++
++; CHECK-LABEL: baz:
++; CHECK:           bti c
++; CHECK:           ret
++
++attributes #0 = { noinline nounwind optnone uwtable }
++attributes #1 = { noinline nounwind optnone uwtable "branch-target-enforcement" }
++
++!llvm.module.flags = !{!0, !1, !2, !3}
++
++!0 = !{i32 8, !"branch-target-enforcement", i32 2}
++!1 = !{i32 8, !"sign-return-address", i32 2}
++!2 = !{i32 8, !"sign-return-address-all", i32 2}
++!3 = !{i32 8, !"sign-return-address-with-bkey", i32 2}
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/Inputs/fiz.ll b/llvm/test/LTO/AArch64/Inputs/fiz.ll
+--- a/llvm/test/LTO/AArch64/Inputs/fiz.ll
++++ b/llvm/test/LTO/AArch64/Inputs/fiz.ll
+@@ -0,0 +1,41 @@
++;; This file contains the previous semantic of the branch-target-enforcement, sign-return-address.
++;; Used for test mixing a mixed link case and also verify the import too in llc.
++
++; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
++
++target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
++target triple = "aarch64-unknown-linux-gnu"
++
++declare void @func()
++
++define i32 @fiz_on() #0 {
++entry:
++  call void @func()
++  ret i32 42
++}
++
++; CHECK-LABEL: fiz_on:
++; CHECK:           paciasp
++; CHECK:           bl func
++; CHECK:           retaa
++
++define i32 @fiz_off() #1 {
++entry:
++  ret i32 43
++}
++
++; CHECK-LABEL: fiz_off:
++; CHECK-NOT:       pac
++; CHECK-NOT:       hint
++; CHECK-NOT:       bti
++; CHECK:           ret
++
++attributes #0 = { noinline nounwind optnone uwtable }
++attributes #1 = { noinline nounwind optnone uwtable "branch-target-enforcement"="false" "sign-return-address"="none" }
++
++!llvm.module.flags = !{!0, !1, !2, !3}
++
++!0 = !{i32 8, !"branch-target-enforcement", i32 1}
++!1 = !{i32 8, !"sign-return-address", i32 1}
++!2 = !{i32 8, !"sign-return-address-all", i32 0}
++!3 = !{i32 8, !"sign-return-address-with-bkey", i32 0}
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/Inputs/foo.ll b/llvm/test/LTO/AArch64/Inputs/foo.ll
+--- a/llvm/test/LTO/AArch64/Inputs/foo.ll
++++ b/llvm/test/LTO/AArch64/Inputs/foo.ll
+@@ -0,0 +1,38 @@
++;; This file contains the previous semantic of the branch-target-enforcement, sign-return-address.
++;; Used for test mixing a mixed link case and also verify the import too in llc.
++
++; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
++
++target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
++target triple = "aarch64-unknown-linux-gnu"
++
++define i32 @foo_on() #0 {
++entry:
++  ret i32 42
++}
++
++; CHECK-LABEL: foo_on:
++; CHECK:           pacibsp
++; CHECK:           mov
++; CHECK:           retab
++
++define i32 @foo_off() #1 {
++entry:
++  ret i32 43
++}
++
++; CHECK-LABEL: foo_off:
++; CHECK-NOT:       pac
++; CHECK-NOT:       hint
++; CHECK-NOT:       bti
++; CHECK:           ret
++
++attributes #0 = { noinline nounwind optnone uwtable }
++attributes #1 = { noinline nounwind optnone uwtable "branch-target-enforcement"="false" "sign-return-address"="none" }
++
++!llvm.module.flags = !{!0, !1, !2, !3}
++
++!0 = !{i32 8, !"branch-target-enforcement", i32 1}
++!1 = !{i32 8, !"sign-return-address", i32 1}
++!2 = !{i32 8, !"sign-return-address-all", i32 1}
++!3 = !{i32 8, !"sign-return-address-with-bkey", i32 1}
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/Inputs/old.ll b/llvm/test/LTO/AArch64/Inputs/old.ll
+--- a/llvm/test/LTO/AArch64/Inputs/old.ll
++++ b/llvm/test/LTO/AArch64/Inputs/old.ll
+@@ -0,0 +1,59 @@
++;; This file contains the previous semantic of the branch-target-enforcement, sign-return-address.
++;; Used for test mixing a mixed link case and also verify the import too in llc.
++
++; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
++
++target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
++target triple = "aarch64-unknown-linux-gnu"
++
++define i32 @old_bti() #0 {
++entry:
++  ret i32 2
++}
++
++; CHECK-LABEL: old_bti:
++; CHECK:           bti c
++; CHECK:           mov
++; CHECK:           ret
++
++define i32 @old_pac() #1 {
++entry:
++  ret i32 2
++}
++
++; CHECK-LABEL: old_pac:
++; CHECK:           paciasp
++; CHECK:           mov
++; CHECK:           retaa
++
++
++define i32 @old_none() #2 {
++entry:
++  ret i32 3
++}
++
++; CHECK-LABEL: old_none:
++; CHECK-NOT:           hint
++; CHECK-NOT:           paci
++; CHECK-NOT:           bti
++; CHECK:           ret
++
++declare i32 @func(i32)
++
++define i32 @old_none_leaf() #3 {
++entry:
++  %0 = call i32 @func()
++  ret i32 %0
++}
++
++; CHECK-LABEL: old_none_leaf:
++; CHECK:           paciasp
++; CHECK:           bl      func
++; CHECK:           retaa
++
++attributes #0 = { noinline nounwind optnone "branch-target-enforcement"="true" }
++attributes #1 = { noinline nounwind optnone "branch-target-enforcement"="false" "sign-return-address"="all" "sign-return-address-key"="a_key" }
++attributes #2 = { noinline nounwind optnone "branch-target-enforcement"="false" "sign-return-address"="none" }
++attributes #3 = { noinline nounwind optnone "branch-target-enforcement"="false" "sign-return-address"="non-leaf" "sign-return-address-key"="a_key" }
++
++;; Intentionally no module flags
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/TestInputs/bar.ll b/llvm/test/LTO/AArch64/TestInputs/bar.ll
+--- a/llvm/test/LTO/AArch64/TestInputs/bar.ll
++++ b/llvm/test/LTO/AArch64/TestInputs/bar.ll
+@@ -1,35 +0,0 @@
+-;; This file contains the new semantic of the branch-target-enforcement, sign-return-address.
+-;; Used for test mixing a mixed link case and also verify the import too in llc.
+-
+-; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
+-
+-target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
+-target triple = "aarch64-unknown-linux-gnu"
+-
+-define dso_local void @bar() #0 {
+-entry:
+-  ret void
+-}
+-; CHECK-LABEL: bar:
+-; CHECK-NOT:       hint
+-; CHECK-NOT:       bti
+-; CHECK:           ret
+-
+-define dso_local void @baz() #1 {
+-entry:
+-  ret void
+-}
+-
+-; CHECK-LABEL: baz:
+-; CHECK:           bti c
+-; CHECK:           ret
+-
+-attributes #0 = { noinline nounwind optnone uwtable }
+-attributes #1 = { noinline nounwind optnone uwtable "branch-target-enforcement" }
+-
+-!llvm.module.flags = !{!0, !1, !2, !3}
+-
+-!0 = !{i32 8, !"branch-target-enforcement", i32 2}
+-!1 = !{i32 8, !"sign-return-address", i32 2}
+-!2 = !{i32 8, !"sign-return-address-all", i32 2}
+-!3 = !{i32 8, !"sign-return-address-with-bkey", i32 2}
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/TestInputs/fiz.ll b/llvm/test/LTO/AArch64/TestInputs/fiz.ll
+--- a/llvm/test/LTO/AArch64/TestInputs/fiz.ll
++++ b/llvm/test/LTO/AArch64/TestInputs/fiz.ll
+@@ -1,41 +0,0 @@
+-;; This file contains the previous semantic of the branch-target-enforcement, sign-return-address.
+-;; Used for test mixing a mixed link case and also verify the import too in llc.
+-
+-; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
+-
+-target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
+-target triple = "aarch64-unknown-linux-gnu"
+-
+-declare void @func()
+-
+-define i32 @fiz_on() #0 {
+-entry:
+-  call void @func()
+-  ret i32 42
+-}
+-
+-; CHECK-LABEL: fiz_on:
+-; CHECK:           paciasp
+-; CHECK:           bl func
+-; CHECK:           retaa
+-
+-define i32 @fiz_off() #1 {
+-entry:
+-  ret i32 43
+-}
+-
+-; CHECK-LABEL: fiz_off:
+-; CHECK-NOT:       pac
+-; CHECK-NOT:       hint
+-; CHECK-NOT:       bti
+-; CHECK:           ret
+-
+-attributes #0 = { noinline nounwind optnone uwtable }
+-attributes #1 = { noinline nounwind optnone uwtable "branch-target-enforcement"="false" "sign-return-address"="none" }
+-
+-!llvm.module.flags = !{!0, !1, !2, !3}
+-
+-!0 = !{i32 8, !"branch-target-enforcement", i32 1}
+-!1 = !{i32 8, !"sign-return-address", i32 1}
+-!2 = !{i32 8, !"sign-return-address-all", i32 0}
+-!3 = !{i32 8, !"sign-return-address-with-bkey", i32 0}
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/TestInputs/foo.ll b/llvm/test/LTO/AArch64/TestInputs/foo.ll
+--- a/llvm/test/LTO/AArch64/TestInputs/foo.ll
++++ b/llvm/test/LTO/AArch64/TestInputs/foo.ll
+@@ -1,38 +0,0 @@
+-;; This file contains the previous semantic of the branch-target-enforcement, sign-return-address.
+-;; Used for test mixing a mixed link case and also verify the import too in llc.
+-
+-; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
+-
+-target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
+-target triple = "aarch64-unknown-linux-gnu"
+-
+-define i32 @foo_on() #0 {
+-entry:
+-  ret i32 42
+-}
+-
+-; CHECK-LABEL: foo_on:
+-; CHECK:           pacibsp
+-; CHECK:           mov
+-; CHECK:           retab
+-
+-define i32 @foo_off() #1 {
+-entry:
+-  ret i32 43
+-}
+-
+-; CHECK-LABEL: foo_off:
+-; CHECK-NOT:       pac
+-; CHECK-NOT:       hint
+-; CHECK-NOT:       bti
+-; CHECK:           ret
+-
+-attributes #0 = { noinline nounwind optnone uwtable }
+-attributes #1 = { noinline nounwind optnone uwtable "branch-target-enforcement"="false" "sign-return-address"="none" }
+-
+-!llvm.module.flags = !{!0, !1, !2, !3}
+-
+-!0 = !{i32 8, !"branch-target-enforcement", i32 1}
+-!1 = !{i32 8, !"sign-return-address", i32 1}
+-!2 = !{i32 8, !"sign-return-address-all", i32 1}
+-!3 = !{i32 8, !"sign-return-address-with-bkey", i32 1}
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/TestInputs/old.ll b/llvm/test/LTO/AArch64/TestInputs/old.ll
+--- a/llvm/test/LTO/AArch64/TestInputs/old.ll
++++ b/llvm/test/LTO/AArch64/TestInputs/old.ll
+@@ -1,59 +0,0 @@
+-;; This file contains the previous semantic of the branch-target-enforcement, sign-return-address.
+-;; Used for test mixing a mixed link case and also verify the import too in llc.
+-
+-; RUN: llc -mattr=+pauth -mattr=+bti %s -o - | FileCheck %s
+-
+-target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
+-target triple = "aarch64-unknown-linux-gnu"
+-
+-define i32 @old_bti() #0 {
+-entry:
+-  ret i32 2
+-}
+-
+-; CHECK-LABEL: old_bti:
+-; CHECK:           bti c
+-; CHECK:           mov
+-; CHECK:           ret
+-
+-define i32 @old_pac() #1 {
+-entry:
+-  ret i32 2
+-}
+-
+-; CHECK-LABEL: old_pac:
+-; CHECK:           paciasp
+-; CHECK:           mov
+-; CHECK:           retaa
+-
+-
+-define i32 @old_none() #2 {
+-entry:
+-  ret i32 3
+-}
+-
+-; CHECK-LABEL: old_none:
+-; CHECK-NOT:           hint
+-; CHECK-NOT:           paci
+-; CHECK-NOT:           bti
+-; CHECK:           ret
+-
+-declare i32 @func(i32)
+-
+-define i32 @old_none_leaf() #3 {
+-entry:
+-  %0 = call i32 @func()
+-  ret i32 %0
+-}
+-
+-; CHECK-LABEL: old_none_leaf:
+-; CHECK:           paciasp
+-; CHECK:           bl      func
+-; CHECK:           retaa
+-
+-attributes #0 = { noinline nounwind optnone "branch-target-enforcement"="true" }
+-attributes #1 = { noinline nounwind optnone "branch-target-enforcement"="false" "sign-return-address"="all" "sign-return-address-key"="a_key" }
+-attributes #2 = { noinline nounwind optnone "branch-target-enforcement"="false" "sign-return-address"="none" }
+-attributes #3 = { noinline nounwind optnone "branch-target-enforcement"="false" "sign-return-address"="non-leaf" "sign-return-address-key"="a_key" }
+-
+-;; Intentionally no module flags
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/link-branch-target-enforcement.ll b/llvm/test/LTO/AArch64/link-branch-target-enforcement.ll
+--- a/llvm/test/LTO/AArch64/link-branch-target-enforcement.ll
++++ b/llvm/test/LTO/AArch64/link-branch-target-enforcement.ll
+@@ -2,7 +2,7 @@
+ ;; be mixed.
+ ;;
+ ; RUN: llvm-as %s -o %t1.bc
+-; RUN: llvm-as %p/TestInputs/foo.ll -o %t2.bc
++; RUN: llvm-as %p/Inputs/foo.ll -o %t2.bc
+ ; RUN: llvm-lto -exported-symbol main \
+ ; RUN:          -exported-symbol foo_on \
+ ; RUN:          -filetype=obj \
+diff -ruN --strip-trailing-cr a/llvm/test/LTO/AArch64/link-sign-return-address.ll b/llvm/test/LTO/AArch64/link-sign-return-address.ll
+--- a/llvm/test/LTO/AArch64/link-sign-return-address.ll
++++ b/llvm/test/LTO/AArch64/link-sign-return-address.ll
+@@ -2,10 +2,10 @@
+ ;; be mixed.
+ ;
+ ; RUN: llvm-as %s -o %t1.bc
+-; RUN: llvm-as %p/TestInputs/foo.ll -o %t2.bc
+-; RUN: llvm-as %p/TestInputs/fiz.ll -o %t3.bc
+-; RUN: llvm-as %p/TestInputs/bar.ll -o %t4.bc
+-; RUN: llvm-as %p/TestInputs/old.ll -o %t5.bc
++; RUN: llvm-as %p/Inputs/foo.ll -o %t2.bc
++; RUN: llvm-as %p/Inputs/fiz.ll -o %t3.bc
++; RUN: llvm-as %p/Inputs/bar.ll -o %t4.bc
++; RUN: llvm-as %p/Inputs/old.ll -o %t5.bc
+ ; RUN: llvm-lto -exported-symbol main \
+ ; RUN:          -exported-symbol foo_on \
+ ; RUN:          -exported-symbol foo_off \
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-constant-ops.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-constant-ops.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-constant-ops.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-constant-ops.ll
+@@ -175,18 +175,28 @@
+ ; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 1
++; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 2
+ ; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[A]], i64 [[INDEX]]
+ ; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[A]], i64 [[TMP0]]
+-; CHECK-NEXT:    [[STRIDED_VEC1:%.*]] = load <2 x double>, ptr [[TMP1]], align 4
+-; CHECK-NEXT:    [[STRIDED_VEC4:%.*]] = load <2 x double>, ptr [[TMP2]], align 4
++; CHECK-NEXT:    [[WIDE_VEC:%.*]] = load <4 x double>, ptr [[TMP1]], align 4
++; CHECK-NEXT:    [[STRIDED_VEC:%.*]] = shufflevector <4 x double> [[WIDE_VEC]], <4 x double> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC1:%.*]] = shufflevector <4 x double> [[WIDE_VEC]], <4 x double> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[WIDE_VEC2:%.*]] = load <4 x double>, ptr [[TMP2]], align 4
++; CHECK-NEXT:    [[STRIDED_VEC3:%.*]] = shufflevector <4 x double> [[WIDE_VEC2]], <4 x double> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC4:%.*]] = shufflevector <4 x double> [[WIDE_VEC2]], <4 x double> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[TMP3:%.*]] = fadd <2 x double> [[STRIDED_VEC]], [[BROADCAST_SPLAT]]
++; CHECK-NEXT:    [[TMP4:%.*]] = fadd <2 x double> [[STRIDED_VEC3]], [[BROADCAST_SPLAT]]
+ ; CHECK-NEXT:    [[TMP5:%.*]] = fadd <2 x double> [[STRIDED_VEC1]], [[BROADCAST_SPLAT]]
+ ; CHECK-NEXT:    [[TMP6:%.*]] = fadd <2 x double> [[STRIDED_VEC4]], [[BROADCAST_SPLAT]]
+ ; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[RES]], i64 [[INDEX]]
+ ; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[RES]], i64 [[TMP0]]
+-; CHECK-NEXT:    store <2 x double> [[TMP5]], ptr [[TMP7]], align 4
+-; CHECK-NEXT:    store <2 x double> [[TMP6]], ptr [[TMP8]], align 4
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
++; CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <2 x double> [[TMP3]], <2 x double> [[TMP5]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x double> [[TMP9]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC]], ptr [[TMP7]], align 4
++; CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <2 x double> [[TMP4]], <2 x double> [[TMP6]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC5:%.*]] = shufflevector <4 x double> [[TMP10]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC5]], ptr [[TMP8]], align 4
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
+ ; CHECK-NEXT:    [[TMP11:%.*]] = icmp eq i64 [[INDEX_NEXT]], 100
+ ; CHECK-NEXT:    br i1 [[TMP11]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+@@ -227,18 +237,28 @@
+ ; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 1
++; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 2
+ ; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[A]], i64 [[INDEX]]
+ ; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[A]], i64 [[TMP0]]
+-; CHECK-NEXT:    [[STRIDED_VEC1:%.*]] = load <2 x double>, ptr [[TMP1]], align 4
+-; CHECK-NEXT:    [[STRIDED_VEC4:%.*]] = load <2 x double>, ptr [[TMP2]], align 4
++; CHECK-NEXT:    [[WIDE_VEC:%.*]] = load <4 x double>, ptr [[TMP1]], align 4
++; CHECK-NEXT:    [[STRIDED_VEC:%.*]] = shufflevector <4 x double> [[WIDE_VEC]], <4 x double> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC1:%.*]] = shufflevector <4 x double> [[WIDE_VEC]], <4 x double> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[WIDE_VEC2:%.*]] = load <4 x double>, ptr [[TMP2]], align 4
++; CHECK-NEXT:    [[STRIDED_VEC3:%.*]] = shufflevector <4 x double> [[WIDE_VEC2]], <4 x double> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC4:%.*]] = shufflevector <4 x double> [[WIDE_VEC2]], <4 x double> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[TMP3:%.*]] = fadd <2 x double> [[BROADCAST_SPLAT]], [[STRIDED_VEC]]
++; CHECK-NEXT:    [[TMP4:%.*]] = fadd <2 x double> [[BROADCAST_SPLAT]], [[STRIDED_VEC3]]
+ ; CHECK-NEXT:    [[TMP5:%.*]] = fadd <2 x double> [[BROADCAST_SPLAT]], [[STRIDED_VEC1]]
+ ; CHECK-NEXT:    [[TMP6:%.*]] = fadd <2 x double> [[BROADCAST_SPLAT]], [[STRIDED_VEC4]]
+ ; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[RES]], i64 [[INDEX]]
+ ; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds nuw { double, double }, ptr [[RES]], i64 [[TMP0]]
+-; CHECK-NEXT:    store <2 x double> [[TMP5]], ptr [[TMP7]], align 4
+-; CHECK-NEXT:    store <2 x double> [[TMP6]], ptr [[TMP8]], align 4
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
++; CHECK-NEXT:    [[TMP9:%.*]] = shufflevector <2 x double> [[TMP3]], <2 x double> [[TMP5]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x double> [[TMP9]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC]], ptr [[TMP7]], align 4
++; CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <2 x double> [[TMP4]], <2 x double> [[TMP6]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC5:%.*]] = shufflevector <4 x double> [[TMP10]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC5]], ptr [[TMP8]], align 4
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
+ ; CHECK-NEXT:    [[TMP11:%.*]] = icmp eq i64 [[INDEX_NEXT]], 100
+ ; CHECK-NEXT:    br i1 [[TMP11]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-cost.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-cost.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-cost.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-cost.ll
+@@ -319,46 +319,46 @@
+ ; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP21:%.*]] = add i64 [[INDEX]], 1
+ ; CHECK-NEXT:    [[TMP20:%.*]] = add i64 [[INDEX]], 2
+-; CHECK-NEXT:    [[TMP22:%.*]] = add i64 [[INDEX]], 3
++; CHECK-NEXT:    [[TMP21:%.*]] = add i64 [[INDEX]], 4
++; CHECK-NEXT:    [[TMP22:%.*]] = add i64 [[INDEX]], 6
+ ; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr double, ptr [[A]], i64 [[INDEX]]
+-; CHECK-NEXT:    [[TMP33:%.*]] = getelementptr double, ptr [[A]], i64 [[TMP21]]
+-; CHECK-NEXT:    [[TMP37:%.*]] = getelementptr double, ptr [[A]], i64 [[TMP20]]
+-; CHECK-NEXT:    [[TMP39:%.*]] = getelementptr double, ptr [[A]], i64 [[TMP22]]
+-; CHECK-NEXT:    [[TMP24:%.*]] = load double, ptr [[TMP23]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT1:%.*]] = insertelement <2 x double> poison, double [[TMP24]], i64 0
+-; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = shufflevector <2 x double> [[BROADCAST_SPLATINSERT1]], <2 x double> poison, <2 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP25:%.*]] = load double, ptr [[TMP33]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT12:%.*]] = insertelement <2 x double> poison, double [[TMP25]], i64 0
+-; CHECK-NEXT:    [[WIDE_LOAD12:%.*]] = shufflevector <2 x double> [[BROADCAST_SPLATINSERT12]], <2 x double> poison, <2 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP26:%.*]] = load double, ptr [[TMP37]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT14:%.*]] = insertelement <2 x double> poison, double [[TMP26]], i64 0
+-; CHECK-NEXT:    [[WIDE_LOAD13:%.*]] = shufflevector <2 x double> [[BROADCAST_SPLATINSERT14]], <2 x double> poison, <2 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP27:%.*]] = load double, ptr [[TMP39]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT16:%.*]] = insertelement <2 x double> poison, double [[TMP27]], i64 0
+-; CHECK-NEXT:    [[WIDE_LOAD14:%.*]] = shufflevector <2 x double> [[BROADCAST_SPLATINSERT16]], <2 x double> poison, <2 x i32> zeroinitializer
++; CHECK-NEXT:    [[TMP25:%.*]] = getelementptr double, ptr [[TMP23]], i32 2
++; CHECK-NEXT:    [[TMP26:%.*]] = getelementptr double, ptr [[TMP23]], i32 4
++; CHECK-NEXT:    [[TMP27:%.*]] = getelementptr double, ptr [[TMP23]], i32 6
++; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, ptr [[TMP23]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD12:%.*]] = load <2 x double>, ptr [[TMP25]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD13:%.*]] = load <2 x double>, ptr [[TMP26]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD14:%.*]] = load <2 x double>, ptr [[TMP27]], align 8
+ ; CHECK-NEXT:    [[TMP28:%.*]] = fmul <2 x double> [[WIDE_LOAD]], splat (double 5.000000e+00)
+ ; CHECK-NEXT:    [[TMP29:%.*]] = fmul <2 x double> [[WIDE_LOAD12]], splat (double 5.000000e+00)
+ ; CHECK-NEXT:    [[TMP30:%.*]] = fmul <2 x double> [[WIDE_LOAD13]], splat (double 5.000000e+00)
+ ; CHECK-NEXT:    [[TMP31:%.*]] = fmul <2 x double> [[WIDE_LOAD14]], splat (double 5.000000e+00)
+ ; CHECK-NEXT:    [[TMP32:%.*]] = getelementptr { double, double }, ptr [[B]], i64 [[INDEX]]
++; CHECK-NEXT:    [[TMP33:%.*]] = getelementptr { double, double }, ptr [[B]], i64 [[TMP20]]
+ ; CHECK-NEXT:    [[TMP34:%.*]] = getelementptr { double, double }, ptr [[B]], i64 [[TMP21]]
+-; CHECK-NEXT:    [[TMP38:%.*]] = getelementptr { double, double }, ptr [[B]], i64 [[TMP20]]
+ ; CHECK-NEXT:    [[TMP35:%.*]] = getelementptr { double, double }, ptr [[B]], i64 [[TMP22]]
+-; CHECK-NEXT:    store <2 x double> [[TMP28]], ptr [[TMP32]], align 8
+-; CHECK-NEXT:    store <2 x double> [[TMP29]], ptr [[TMP34]], align 8
+-; CHECK-NEXT:    store <2 x double> [[TMP30]], ptr [[TMP38]], align 8
+-; CHECK-NEXT:    store <2 x double> [[TMP31]], ptr [[TMP35]], align 8
++; CHECK-NEXT:    [[TMP36:%.*]] = shufflevector <2 x double> [[TMP28]], <2 x double> [[TMP28]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x double> [[TMP36]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC]], ptr [[TMP32]], align 8
++; CHECK-NEXT:    [[TMP37:%.*]] = shufflevector <2 x double> [[TMP29]], <2 x double> [[TMP29]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC15:%.*]] = shufflevector <4 x double> [[TMP37]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC15]], ptr [[TMP33]], align 8
++; CHECK-NEXT:    [[TMP38:%.*]] = shufflevector <2 x double> [[TMP30]], <2 x double> [[TMP30]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC16:%.*]] = shufflevector <4 x double> [[TMP38]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC16]], ptr [[TMP34]], align 8
++; CHECK-NEXT:    [[TMP39:%.*]] = shufflevector <2 x double> [[TMP31]], <2 x double> [[TMP31]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC17:%.*]] = shufflevector <4 x double> [[TMP39]], <4 x double> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC17]], ptr [[TMP35]], align 8
+ ; CHECK-NEXT:    [[TMP40:%.*]] = getelementptr { double, double }, ptr [[C]], i64 [[INDEX]]
+-; CHECK-NEXT:    [[TMP42:%.*]] = getelementptr { double, double }, ptr [[C]], i64 [[TMP21]]
+ ; CHECK-NEXT:    [[TMP41:%.*]] = getelementptr { double, double }, ptr [[C]], i64 [[TMP20]]
++; CHECK-NEXT:    [[TMP42:%.*]] = getelementptr { double, double }, ptr [[C]], i64 [[TMP21]]
+ ; CHECK-NEXT:    [[TMP43:%.*]] = getelementptr { double, double }, ptr [[C]], i64 [[TMP22]]
+-; CHECK-NEXT:    store <2 x double> [[TMP28]], ptr [[TMP40]], align 8
+-; CHECK-NEXT:    store <2 x double> [[TMP29]], ptr [[TMP42]], align 8
+-; CHECK-NEXT:    store <2 x double> [[TMP30]], ptr [[TMP41]], align 8
+-; CHECK-NEXT:    store <2 x double> [[TMP31]], ptr [[TMP43]], align 8
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC]], ptr [[TMP40]], align 8
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC15]], ptr [[TMP41]], align 8
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC16]], ptr [[TMP42]], align 8
++; CHECK-NEXT:    store <4 x double> [[INTERLEAVED_VEC17]], ptr [[TMP43]], align 8
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
+ ; CHECK-NEXT:    [[TMP44:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[TMP44]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+@@ -435,7 +435,7 @@
+   ret void
+ }
+ 
+-; We should interleave by 2 after narrowing interleave groups to saturate
++; FIXME: We should interleave by 2 after narrowing interleave groups to saturate
+ ; load/store units.
+ define void @test_interleave_after_narrowing(i32 %n, ptr %x, ptr noalias %y) {
+ ; CHECK-LABEL: define void @test_interleave_after_narrowing(
+@@ -447,18 +447,12 @@
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+ ; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = mul i64 [[INDEX]], 4
+-; CHECK-NEXT:    [[TMP5:%.*]] = add i64 [[OFFSET_IDX]], 4
+ ; CHECK-NEXT:    [[TMP0:%.*]] = getelementptr inbounds nuw float, ptr [[X]], i64 [[OFFSET_IDX]]
+-; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds nuw float, ptr [[X]], i64 [[TMP5]]
+ ; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, ptr [[TMP0]], align 4
+-; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, ptr [[TMP7]], align 4
+ ; CHECK-NEXT:    [[TMP1:%.*]] = fneg <4 x float> [[WIDE_LOAD]]
+-; CHECK-NEXT:    [[TMP4:%.*]] = fneg <4 x float> [[WIDE_LOAD1]]
+ ; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds nuw float, ptr [[Y]], i64 [[OFFSET_IDX]]
+-; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds nuw float, ptr [[Y]], i64 [[TMP5]]
+ ; CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[TMP2]], align 4
+-; CHECK-NEXT:    store <4 x float> [[TMP4]], ptr [[TMP6]], align 4
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 1
+ ; CHECK-NEXT:    [[TMP3:%.*]] = icmp eq i64 [[INDEX_NEXT]], 256
+ ; CHECK-NEXT:    br i1 [[TMP3]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP13:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-remove-loop-region.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-remove-loop-region.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-remove-loop-region.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-remove-loop-region.ll
+@@ -13,8 +13,12 @@
+ ; VF2:       [[VECTOR_PH]]:
+ ; VF2-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; VF2:       [[VECTOR_BODY]]:
+-; VF2-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x i64>, ptr [[DATA]], align 8
+-; VF2-NEXT:    store <2 x i64> [[WIDE_LOAD]], ptr [[DATA]], align 8
++; VF2-NEXT:    [[WIDE_VEC:%.*]] = load <4 x i64>, ptr [[DATA]], align 8
++; VF2-NEXT:    [[STRIDED_VEC:%.*]] = shufflevector <4 x i64> [[WIDE_VEC]], <4 x i64> poison, <2 x i32> <i32 0, i32 2>
++; VF2-NEXT:    [[STRIDED_VEC1:%.*]] = shufflevector <4 x i64> [[WIDE_VEC]], <4 x i64> poison, <2 x i32> <i32 1, i32 3>
++; VF2-NEXT:    [[TMP2:%.*]] = shufflevector <2 x i64> [[STRIDED_VEC]], <2 x i64> [[STRIDED_VEC1]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; VF2-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x i64> [[TMP2]], <4 x i64> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; VF2-NEXT:    store <4 x i64> [[INTERLEAVED_VEC]], ptr [[DATA]], align 8
+ ; VF2-NEXT:    br label %[[MIDDLE_BLOCK:.*]]
+ ; VF2:       [[MIDDLE_BLOCK]]:
+ ; VF2-NEXT:    br label %[[EXIT:.*]]
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-scalable.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-scalable.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-scalable.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-scalable.ll
+@@ -1,81 +1,29 @@
+ ; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --check-globals none --filter-out-after "^scalar.ph:" --version 5
+-; RUN: opt -p loop-vectorize -force-vector-interleave=1 -S -mcpu=neoverse-512tvb %s | FileCheck --check-prefixes=IC1 %s
+-; RUN: opt -p loop-vectorize -S -mcpu=neoverse-512tvb %s | FileCheck --check-prefixes=CHECK %s
++; RUN: opt -p loop-vectorize -force-vector-interleave=1 -S -mcpu=neoverse-512tvb %s | FileCheck --check-prefixes=CHECK %s
+ 
+ target triple = "aarch64-unknown-linux"
+ 
+ define void @load_store_interleave_group(ptr noalias %data) {
+-; IC1-LABEL: define void @load_store_interleave_group(
+-; IC1-SAME: ptr noalias [[DATA:%.*]]) #[[ATTR0:[0-9]+]] {
+-; IC1-NEXT:  [[ENTRY:.*:]]
+-; IC1-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
+-; IC1-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP0]], 1
+-; IC1-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 100, [[TMP1]]
+-; IC1-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+-; IC1:       [[VECTOR_PH]]:
+-; IC1-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
+-; IC1-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP2]], 2
+-; IC1-NEXT:    [[N_MOD_VF:%.*]] = urem i64 100, [[TMP3]]
+-; IC1-NEXT:    [[N_VEC:%.*]] = sub i64 100, [[N_MOD_VF]]
+-; IC1-NEXT:    br label %[[VECTOR_BODY:.*]]
+-; IC1:       [[VECTOR_BODY]]:
+-; IC1-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; IC1-NEXT:    [[TMP4:%.*]] = shl nsw i64 [[INDEX]], 1
+-; IC1-NEXT:    [[TMP5:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP4]]
+-; IC1-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 2 x i64>, ptr [[TMP5]], align 8
+-; IC1-NEXT:    store <vscale x 2 x i64> [[WIDE_LOAD]], ptr [[TMP5]], align 8
+-; IC1-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP2]]
+-; IC1-NEXT:    [[TMP6:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; IC1-NEXT:    br i1 [[TMP6]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
+-; IC1:       [[MIDDLE_BLOCK]]:
+-; IC1-NEXT:    [[CMP_N:%.*]] = icmp eq i64 100, [[N_VEC]]
+-; IC1-NEXT:    br i1 [[CMP_N]], [[EXIT:label %.*]], label %[[SCALAR_PH]]
+-; IC1:       [[SCALAR_PH]]:
+-;
+ ; CHECK-LABEL: define void @load_store_interleave_group(
+ ; CHECK-SAME: ptr noalias [[DATA:%.*]]) #[[ATTR0:[0-9]+]] {
+ ; CHECK-NEXT:  [[ENTRY:.*:]]
+ ; CHECK-NEXT:    [[TMP4:%.*]] = call i64 @llvm.vscale.i64()
+-; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw i64 [[TMP4]], 3
++; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw i64 [[TMP4]], 1
+ ; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 100, [[TMP5]]
+ ; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+ ; CHECK:       [[VECTOR_PH]]:
+ ; CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
+-; CHECK-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP2]], 8
++; CHECK-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP2]], 2
+ ; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 100, [[TMP3]]
+ ; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 100, [[N_MOD_VF]]
+-; CHECK-NEXT:    [[TMP16:%.*]] = mul i64 [[TMP2]], 4
+ ; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP20:%.*]] = add i64 [[TMP2]], 0
+-; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP20]], 1
+-; CHECK-NEXT:    [[TMP7:%.*]] = add i64 [[INDEX]], [[TMP6]]
+-; CHECK-NEXT:    [[TMP24:%.*]] = mul i64 [[TMP2]], 2
+-; CHECK-NEXT:    [[TMP9:%.*]] = add i64 [[TMP24]], 0
+-; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP9]], 1
+-; CHECK-NEXT:    [[TMP11:%.*]] = add i64 [[INDEX]], [[TMP10]]
+-; CHECK-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP2]], 3
+-; CHECK-NEXT:    [[TMP13:%.*]] = add i64 [[TMP12]], 0
+-; CHECK-NEXT:    [[TMP14:%.*]] = mul i64 [[TMP13]], 1
+-; CHECK-NEXT:    [[TMP15:%.*]] = add i64 [[INDEX]], [[TMP14]]
+ ; CHECK-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[INDEX]], 1
+-; CHECK-NEXT:    [[TMP17:%.*]] = shl nsw i64 [[TMP7]], 1
+-; CHECK-NEXT:    [[TMP18:%.*]] = shl nsw i64 [[TMP11]], 1
+-; CHECK-NEXT:    [[TMP19:%.*]] = shl nsw i64 [[TMP15]], 1
+ ; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP0]]
+-; CHECK-NEXT:    [[TMP21:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP17]]
+-; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP18]]
+-; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP19]]
+ ; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 2 x i64>, ptr [[TMP1]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <vscale x 2 x i64>, ptr [[TMP21]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD2:%.*]] = load <vscale x 2 x i64>, ptr [[TMP22]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD3:%.*]] = load <vscale x 2 x i64>, ptr [[TMP23]], align 8
+ ; CHECK-NEXT:    store <vscale x 2 x i64> [[WIDE_LOAD]], ptr [[TMP1]], align 8
+-; CHECK-NEXT:    store <vscale x 2 x i64> [[WIDE_LOAD1]], ptr [[TMP21]], align 8
+-; CHECK-NEXT:    store <vscale x 2 x i64> [[WIDE_LOAD2]], ptr [[TMP22]], align 8
+-; CHECK-NEXT:    store <vscale x 2 x i64> [[WIDE_LOAD3]], ptr [[TMP23]], align 8
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP16]]
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP2]]
+ ; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[TMP8]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+@@ -105,82 +53,27 @@
+ }
  
- spirv.module Logical GLSL450 requires #spirv.vce<v1.0, [Shader, Linkage], []> {
-     spirv.func @linkage_attr_test_kernel()  "DontInline"  attributes {}  {
+ define void @test_2xi64_unary_op_load_interleave_group(ptr noalias %data, ptr noalias %factor) {
+-; IC1-LABEL: define void @test_2xi64_unary_op_load_interleave_group(
+-; IC1-SAME: ptr noalias [[DATA:%.*]], ptr noalias [[FACTOR:%.*]]) #[[ATTR0]] {
+-; IC1-NEXT:  [[ENTRY:.*:]]
+-; IC1-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
+-; IC1-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP0]], 1
+-; IC1-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 1111, [[TMP1]]
+-; IC1-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+-; IC1:       [[VECTOR_PH]]:
+-; IC1-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
+-; IC1-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP2]], 2
+-; IC1-NEXT:    [[N_MOD_VF:%.*]] = urem i64 1111, [[TMP3]]
+-; IC1-NEXT:    [[N_VEC:%.*]] = sub i64 1111, [[N_MOD_VF]]
+-; IC1-NEXT:    br label %[[VECTOR_BODY:.*]]
+-; IC1:       [[VECTOR_BODY]]:
+-; IC1-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; IC1-NEXT:    [[TMP4:%.*]] = shl nsw i64 [[INDEX]], 1
+-; IC1-NEXT:    [[TMP5:%.*]] = getelementptr inbounds double, ptr [[DATA]], i64 [[TMP4]]
+-; IC1-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 2 x double>, ptr [[TMP5]], align 8
+-; IC1-NEXT:    [[TMP6:%.*]] = fneg <vscale x 2 x double> [[WIDE_LOAD]]
+-; IC1-NEXT:    store <vscale x 2 x double> [[TMP6]], ptr [[TMP5]], align 8
+-; IC1-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP2]]
+-; IC1-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; IC1-NEXT:    br i1 [[TMP7]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
+-; IC1:       [[MIDDLE_BLOCK]]:
+-; IC1-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1111, [[N_VEC]]
+-; IC1-NEXT:    br i1 [[CMP_N]], [[EXIT:label %.*]], label %[[SCALAR_PH]]
+-; IC1:       [[SCALAR_PH]]:
+-;
+ ; CHECK-LABEL: define void @test_2xi64_unary_op_load_interleave_group(
+ ; CHECK-SAME: ptr noalias [[DATA:%.*]], ptr noalias [[FACTOR:%.*]]) #[[ATTR0]] {
+ ; CHECK-NEXT:  [[ENTRY:.*:]]
+ ; CHECK-NEXT:    [[TMP4:%.*]] = call i64 @llvm.vscale.i64()
+-; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw i64 [[TMP4]], 3
++; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw i64 [[TMP4]], 1
+ ; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 1111, [[TMP5]]
+ ; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+ ; CHECK:       [[VECTOR_PH]]:
+ ; CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
+-; CHECK-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP2]], 8
++; CHECK-NEXT:    [[TMP3:%.*]] = mul nuw i64 [[TMP2]], 2
+ ; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 1111, [[TMP3]]
+ ; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 1111, [[N_MOD_VF]]
+-; CHECK-NEXT:    [[TMP16:%.*]] = mul i64 [[TMP2]], 4
+ ; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP20:%.*]] = add i64 [[TMP2]], 0
+-; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[TMP20]], 1
+-; CHECK-NEXT:    [[TMP24:%.*]] = add i64 [[INDEX]], [[TMP6]]
+-; CHECK-NEXT:    [[TMP8:%.*]] = mul i64 [[TMP2]], 2
+-; CHECK-NEXT:    [[TMP28:%.*]] = add i64 [[TMP8]], 0
+-; CHECK-NEXT:    [[TMP29:%.*]] = mul i64 [[TMP28]], 1
+-; CHECK-NEXT:    [[TMP11:%.*]] = add i64 [[INDEX]], [[TMP29]]
+-; CHECK-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP2]], 3
+-; CHECK-NEXT:    [[TMP13:%.*]] = add i64 [[TMP12]], 0
+-; CHECK-NEXT:    [[TMP14:%.*]] = mul i64 [[TMP13]], 1
+-; CHECK-NEXT:    [[TMP15:%.*]] = add i64 [[INDEX]], [[TMP14]]
+ ; CHECK-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[INDEX]], 1
+-; CHECK-NEXT:    [[TMP17:%.*]] = shl nsw i64 [[TMP24]], 1
+-; CHECK-NEXT:    [[TMP18:%.*]] = shl nsw i64 [[TMP11]], 1
+-; CHECK-NEXT:    [[TMP19:%.*]] = shl nsw i64 [[TMP15]], 1
+ ; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds double, ptr [[DATA]], i64 [[TMP0]]
+-; CHECK-NEXT:    [[TMP21:%.*]] = getelementptr inbounds double, ptr [[DATA]], i64 [[TMP17]]
+-; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds double, ptr [[DATA]], i64 [[TMP18]]
+-; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds double, ptr [[DATA]], i64 [[TMP19]]
+ ; CHECK-NEXT:    [[TMP7:%.*]] = load <vscale x 2 x double>, ptr [[TMP1]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <vscale x 2 x double>, ptr [[TMP21]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD2:%.*]] = load <vscale x 2 x double>, ptr [[TMP22]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD3:%.*]] = load <vscale x 2 x double>, ptr [[TMP23]], align 8
+ ; CHECK-NEXT:    [[TMP9:%.*]] = fneg <vscale x 2 x double> [[TMP7]]
+-; CHECK-NEXT:    [[TMP25:%.*]] = fneg <vscale x 2 x double> [[WIDE_LOAD1]]
+-; CHECK-NEXT:    [[TMP26:%.*]] = fneg <vscale x 2 x double> [[WIDE_LOAD2]]
+-; CHECK-NEXT:    [[TMP27:%.*]] = fneg <vscale x 2 x double> [[WIDE_LOAD3]]
+ ; CHECK-NEXT:    store <vscale x 2 x double> [[TMP9]], ptr [[TMP1]], align 8
+-; CHECK-NEXT:    store <vscale x 2 x double> [[TMP25]], ptr [[TMP21]], align 8
+-; CHECK-NEXT:    store <vscale x 2 x double> [[TMP26]], ptr [[TMP22]], align 8
+-; CHECK-NEXT:    store <vscale x 2 x double> [[TMP27]], ptr [[TMP23]], align 8
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP16]]
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP2]]
+ ; CHECK-NEXT:    [[TMP10:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[TMP10]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory-unroll.ll
+@@ -60,26 +60,32 @@
+ ; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+ ; CHECK:       [[VECTOR_BODY]]:
+ ; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 1
++; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 2
+ ; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[INDEX]]
+-; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[TMP0]]
+-; CHECK-NEXT:    [[TMP2:%.*]] = load i64, ptr [[TMP1]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i64 0
+-; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT]], <2 x i64> poison, <2 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP3:%.*]] = load i64, ptr [[TMP4]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT2:%.*]] = insertelement <2 x i64> poison, i64 [[TMP3]], i64 0
+-; CHECK-NEXT:    [[BROADCAST_SPLAT3:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLATINSERT2]], <2 x i64> poison, <2 x i32> zeroinitializer
++; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds i64, ptr [[TMP1]], i32 2
++; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = load <2 x i64>, ptr [[TMP1]], align 8
++; CHECK-NEXT:    [[BROADCAST_SPLAT3:%.*]] = load <2 x i64>, ptr [[TMP3]], align 8
+ ; CHECK-NEXT:    [[TMP6:%.*]] = shl nsw i64 [[INDEX]], 1
+ ; CHECK-NEXT:    [[TMP7:%.*]] = shl nsw i64 [[TMP0]], 1
+ ; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP6]]
+ ; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i64, ptr [[DATA]], i64 [[TMP7]]
+-; CHECK-NEXT:    [[STRIDED_VEC2:%.*]] = load <2 x i64>, ptr [[TMP8]], align 8
+-; CHECK-NEXT:    [[STRIDED_VEC5:%.*]] = load <2 x i64>, ptr [[TMP9]], align 8
++; CHECK-NEXT:    [[WIDE_VEC:%.*]] = load <4 x i64>, ptr [[TMP8]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = shufflevector <4 x i64> [[WIDE_VEC]], <4 x i64> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC2:%.*]] = shufflevector <4 x i64> [[WIDE_VEC]], <4 x i64> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[WIDE_VEC3:%.*]] = load <4 x i64>, ptr [[TMP9]], align 8
++; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = shufflevector <4 x i64> [[WIDE_VEC3]], <4 x i64> poison, <2 x i32> <i32 0, i32 2>
++; CHECK-NEXT:    [[STRIDED_VEC5:%.*]] = shufflevector <4 x i64> [[WIDE_VEC3]], <4 x i64> poison, <2 x i32> <i32 1, i32 3>
++; CHECK-NEXT:    [[TMP10:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT]], [[WIDE_LOAD]]
++; CHECK-NEXT:    [[TMP11:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT3]], [[WIDE_LOAD1]]
+ ; CHECK-NEXT:    [[TMP15:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT]], [[STRIDED_VEC2]]
+ ; CHECK-NEXT:    [[TMP16:%.*]] = mul <2 x i64> [[BROADCAST_SPLAT3]], [[STRIDED_VEC5]]
+-; CHECK-NEXT:    store <2 x i64> [[TMP15]], ptr [[TMP8]], align 8
+-; CHECK-NEXT:    store <2 x i64> [[TMP16]], ptr [[TMP9]], align 8
+-; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
++; CHECK-NEXT:    [[TMP17:%.*]] = shufflevector <2 x i64> [[TMP10]], <2 x i64> [[TMP15]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x i64> [[TMP17]], <4 x i64> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x i64> [[INTERLEAVED_VEC]], ptr [[TMP8]], align 8
++; CHECK-NEXT:    [[TMP18:%.*]] = shufflevector <2 x i64> [[TMP11]], <2 x i64> [[TMP16]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; CHECK-NEXT:    [[INTERLEAVED_VEC6:%.*]] = shufflevector <4 x i64> [[TMP18]], <4 x i64> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; CHECK-NEXT:    store <4 x i64> [[INTERLEAVED_VEC6]], ptr [[TMP9]], align 8
++; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
+ ; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], 100
+ ; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP3:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory.ll b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory.ll
+--- a/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory.ll
++++ b/llvm/test/Transforms/LoopVectorize/AArch64/transform-narrow-interleave-to-widen-memory.ll
+@@ -328,8 +328,10 @@
+ ; VF2-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+ ; VF2-NEXT:    [[TMP0:%.*]] = shl nsw i64 [[INDEX]], 1
+ ; VF2-NEXT:    [[TMP1:%.*]] = getelementptr inbounds i64, ptr [[DST]], i64 [[TMP0]]
+-; VF2-NEXT:    store <2 x i64> [[BROADCAST_SPLAT]], ptr [[TMP1]], align 8
+-; VF2-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 1
++; VF2-NEXT:    [[TMP2:%.*]] = shufflevector <2 x i64> [[BROADCAST_SPLAT]], <2 x i64> [[BROADCAST_SPLAT]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
++; VF2-NEXT:    [[INTERLEAVED_VEC:%.*]] = shufflevector <4 x i64> [[TMP2]], <4 x i64> poison, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
++; VF2-NEXT:    store <4 x i64> [[INTERLEAVED_VEC]], ptr [[TMP1]], align 8
++; VF2-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
+ ; VF2-NEXT:    [[TMP3:%.*]] = icmp eq i64 [[INDEX_NEXT]], 100
+ ; VF2-NEXT:    br i1 [[TMP3]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP7:![0-9]+]]
+ ; VF2:       [[MIDDLE_BLOCK]]:
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/WebAssembly/memory-interleave.ll b/llvm/test/Transforms/LoopVectorize/WebAssembly/memory-interleave.ll
+--- a/llvm/test/Transforms/LoopVectorize/WebAssembly/memory-interleave.ll
++++ b/llvm/test/Transforms/LoopVectorize/WebAssembly/memory-interleave.ll
+@@ -1779,7 +1779,7 @@
+ ; CHECK: LV: Scalar loop costs: 24
+ ; CHECK: LV: Vector loop of width 2 costs: 33
+ ; CHECK: LV: Vector loop of width 4 costs: 30
+-; CHECK: LV: Selecting VF: 4
++; CHECK: LV: Selecting VF: 1
+ define hidden void @four_floats_same_op(ptr noundef readonly captures(none) %a, ptr noundef readonly captures(none) %b, ptr noundef writeonly captures(none) %res, i32 noundef %N) {
+ entry:
+   %cmp45.not = icmp eq i32 %N, 0
+diff -ruN --strip-trailing-cr a/llvm/test/Transforms/LoopVectorize/X86/transform-narrow-interleave-to-widen-memory.ll b/llvm/test/Transforms/LoopVectorize/X86/transform-narrow-interleave-to-widen-memory.ll
+--- a/llvm/test/Transforms/LoopVectorize/X86/transform-narrow-interleave-to-widen-memory.ll
++++ b/llvm/test/Transforms/LoopVectorize/X86/transform-narrow-interleave-to-widen-memory.ll
+@@ -8,69 +8,15 @@
+ define void @test_4xi64(ptr noalias %data, ptr noalias %factor, i64 noundef %n) {
+ ; CHECK-LABEL: define void @test_4xi64(
+ ; CHECK-SAME: ptr noalias [[DATA:%.*]], ptr noalias [[FACTOR:%.*]], i64 noundef [[N:%.*]]) #[[ATTR0:[0-9]+]] {
+-; CHECK-NEXT:  [[ITER_CHECK:.*]]:
++; CHECK-NEXT:  [[ENTRY:.*]]:
+ ; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N]], 4
+-; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH:.*]], label %[[VECTOR_MAIN_LOOP_ITER_CHECK:.*]]
+-; CHECK:       [[VECTOR_MAIN_LOOP_ITER_CHECK]]:
+-; CHECK-NEXT:    [[MIN_ITERS_CHECK1:%.*]] = icmp ult i64 [[N]], 16
+-; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK1]], label %[[VEC_EPILOG_PH:.*]], label %[[VECTOR_PH:.*]]
++; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
+ ; CHECK:       [[VECTOR_PH]]:
+-; CHECK-NEXT:    [[N_MOD_VF1:%.*]] = urem i64 [[N]], 16
+-; CHECK-NEXT:    [[N_VEC1:%.*]] = sub i64 [[N]], [[N_MOD_VF1]]
+-; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
+-; CHECK:       [[VECTOR_BODY]]:
+-; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT1:%.*]], %[[VECTOR_BODY]] ]
+-; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 1
+-; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
+-; CHECK-NEXT:    [[TMP2:%.*]] = add i64 [[INDEX]], 3
+-; CHECK-NEXT:    [[TMP20:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[INDEX]]
+-; CHECK-NEXT:    [[TMP21:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[TMP0]]
+-; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[TMP1]]
+-; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[TMP2]]
+-; CHECK-NEXT:    [[TMP7:%.*]] = load i64, ptr [[TMP20]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT1:%.*]] = insertelement <4 x i64> poison, i64 [[TMP7]], i64 0
+-; CHECK-NEXT:    [[BROADCAST_SPLAT1:%.*]] = shufflevector <4 x i64> [[BROADCAST_SPLATINSERT1]], <4 x i64> poison, <4 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP8:%.*]] = load i64, ptr [[TMP21]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT5:%.*]] = insertelement <4 x i64> poison, i64 [[TMP8]], i64 0
+-; CHECK-NEXT:    [[BROADCAST_SPLAT6:%.*]] = shufflevector <4 x i64> [[BROADCAST_SPLATINSERT5]], <4 x i64> poison, <4 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP9:%.*]] = load i64, ptr [[TMP22]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT7:%.*]] = insertelement <4 x i64> poison, i64 [[TMP9]], i64 0
+-; CHECK-NEXT:    [[BROADCAST_SPLAT8:%.*]] = shufflevector <4 x i64> [[BROADCAST_SPLATINSERT7]], <4 x i64> poison, <4 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP10:%.*]] = load i64, ptr [[TMP6]], align 8
+-; CHECK-NEXT:    [[BROADCAST_SPLATINSERT9:%.*]] = insertelement <4 x i64> poison, i64 [[TMP10]], i64 0
+-; CHECK-NEXT:    [[BROADCAST_SPLAT10:%.*]] = shufflevector <4 x i64> [[BROADCAST_SPLATINSERT9]], <4 x i64> poison, <4 x i32> zeroinitializer
+-; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds { i64, i64, i64, i64 }, ptr [[DATA]], i64 [[INDEX]], i32 0
+-; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds { i64, i64, i64, i64 }, ptr [[DATA]], i64 [[TMP0]], i32 0
+-; CHECK-NEXT:    [[TMP13:%.*]] = getelementptr inbounds { i64, i64, i64, i64 }, ptr [[DATA]], i64 [[TMP1]], i32 0
+-; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds { i64, i64, i64, i64 }, ptr [[DATA]], i64 [[TMP2]], i32 0
+-; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x i64>, ptr [[TMP11]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD2:%.*]] = load <4 x i64>, ptr [[TMP12]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD3:%.*]] = load <4 x i64>, ptr [[TMP13]], align 8
+-; CHECK-NEXT:    [[WIDE_LOAD4:%.*]] = load <4 x i64>, ptr [[TMP23]], align 8
+-; CHECK-NEXT:    [[TMP15:%.*]] = mul <4 x i64> [[BROADCAST_SPLAT1]], [[WIDE_LOAD1]]
+-; CHECK-NEXT:    [[TMP16:%.*]] = mul <4 x i64> [[BROADCAST_SPLAT6]], [[WIDE_LOAD2]]
+-; CHECK-NEXT:    [[TMP17:%.*]] = mul <4 x i64> [[BROADCAST_SPLAT8]], [[WIDE_LOAD3]]
+-; CHECK-NEXT:    [[TMP18:%.*]] = mul <4 x i64> [[BROADCAST_SPLAT10]], [[WIDE_LOAD4]]
+-; CHECK-NEXT:    store <4 x i64> [[TMP15]], ptr [[TMP11]], align 8
+-; CHECK-NEXT:    store <4 x i64> [[TMP16]], ptr [[TMP12]], align 8
+-; CHECK-NEXT:    store <4 x i64> [[TMP17]], ptr [[TMP13]], align 8
+-; CHECK-NEXT:    store <4 x i64> [[TMP18]], ptr [[TMP23]], align 8
+-; CHECK-NEXT:    [[INDEX_NEXT1]] = add nuw i64 [[INDEX]], 4
+-; CHECK-NEXT:    [[TMP19:%.*]] = icmp eq i64 [[INDEX_NEXT1]], [[N_VEC1]]
+-; CHECK-NEXT:    br i1 [[TMP19]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
+-; CHECK:       [[MIDDLE_BLOCK]]:
+-; CHECK-NEXT:    [[CMP_N1:%.*]] = icmp eq i64 [[N]], [[N_VEC1]]
+-; CHECK-NEXT:    br i1 [[CMP_N1]], label %[[EXIT:.*]], label %[[VEC_EPILOG_ITER_CHECK:.*]]
+-; CHECK:       [[VEC_EPILOG_ITER_CHECK]]:
+-; CHECK-NEXT:    [[MIN_EPILOG_ITERS_CHECK:%.*]] = icmp ult i64 [[N_MOD_VF1]], 4
+-; CHECK-NEXT:    br i1 [[MIN_EPILOG_ITERS_CHECK]], label %[[VEC_EPILOG_SCALAR_PH]], label %[[VEC_EPILOG_PH]]
+-; CHECK:       [[VEC_EPILOG_PH]]:
+-; CHECK-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC1]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
+ ; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
+ ; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
+-; CHECK-NEXT:    br label %[[VEC_EPILOG_VECTOR_BODY:.*]]
+-; CHECK:       [[VEC_EPILOG_VECTOR_BODY]]:
+-; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[VEC_EPILOG_RESUME_VAL]], %[[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VEC_EPILOG_VECTOR_BODY]] ]
++; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
++; CHECK:       [[VECTOR_BODY]]:
++; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
+ ; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[IV]]
+ ; CHECK-NEXT:    [[TMP5:%.*]] = load i64, ptr [[ARRAYIDX]], align 8
+ ; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <4 x i64> poison, i64 [[TMP5]], i64 0
+@@ -81,15 +27,15 @@
+ ; CHECK-NEXT:    store <4 x i64> [[TMP4]], ptr [[TMP3]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 1
+ ; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP14]], label %[[VEC_EPILOG_MIDDLE_BLOCK:.*]], label %[[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP3:![0-9]+]]
+-; CHECK:       [[VEC_EPILOG_MIDDLE_BLOCK]]:
++; CHECK-NEXT:    br i1 [[TMP14]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
++; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT]], label %[[VEC_EPILOG_SCALAR_PH]]
+-; CHECK:       [[VEC_EPILOG_SCALAR_PH]]:
+-; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[VEC_EPILOG_MIDDLE_BLOCK]] ], [ [[N_VEC1]], %[[VEC_EPILOG_ITER_CHECK]] ], [ 0, %[[ITER_CHECK]] ]
++; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
++; CHECK:       [[SCALAR_PH]]:
++; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
+ ; CHECK-NEXT:    br label %[[LOOP:.*]]
+ ; CHECK:       [[LOOP]]:
+-; CHECK-NEXT:    [[IV1:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[VEC_EPILOG_SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
++; CHECK-NEXT:    [[IV1:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
+ ; CHECK-NEXT:    [[DATA_2:%.*]] = getelementptr inbounds i64, ptr [[FACTOR]], i64 [[IV1]]
+ ; CHECK-NEXT:    [[L_2:%.*]] = load i64, ptr [[DATA_2]], align 8
+ ; CHECK-NEXT:    [[DATA_0:%.*]] = getelementptr inbounds { i64, i64, i64, i64 }, ptr [[DATA]], i64 [[IV1]], i32 0
+@@ -110,7 +56,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_3]], ptr [[DATA_3]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP4:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP3:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -171,7 +117,7 @@
+ ; CHECK-NEXT:    store <8 x i64> [[INTERLEAVED_VEC]], ptr [[TMP4]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 4
+ ; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+@@ -194,7 +140,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_1]], ptr [[DATA_1]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP6:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP5:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -249,7 +195,7 @@
+ ; CHECK-NEXT:    store <8 x i64> [[INTERLEAVED_VEC]], ptr [[TMP4]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 4
+ ; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP7:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+@@ -272,7 +218,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_1]], ptr [[DATA_1]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP8:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP7:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -327,7 +273,7 @@
+ ; CHECK-NEXT:    store <8 x i64> [[INTERLEAVED_VEC]], ptr [[TMP4]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 4
+ ; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP9:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+@@ -350,7 +296,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_0]], ptr [[DATA_1]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP10:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP9:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -405,7 +351,7 @@
+ ; CHECK-NEXT:    store <8 x i64> [[INTERLEAVED_VEC]], ptr [[TMP4]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 4
+ ; CHECK-NEXT:    [[TMP9:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP9]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP11:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP9]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+@@ -428,7 +374,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_1]], ptr [[DATA_0]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP12:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP11:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -489,7 +435,7 @@
+ ; CHECK-NEXT:    store <8 x i64> [[INTERLEAVED_VEC]], ptr [[TMP16]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 4
+ ; CHECK-NEXT:    [[TMP15:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP15]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP13:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP15]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    br label %[[SCALAR_PH]]
+ ; CHECK:       [[SCALAR_PH]]:
+@@ -513,7 +459,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_1]], ptr [[DATA_1]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]], !llvm.loop [[LOOP14:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]], !llvm.loop [[LOOP13:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -573,7 +519,7 @@
+ ; CHECK-NEXT:    store <12 x i64> [[INTERLEAVED_VEC]], ptr [[TMP3]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 4
+ ; CHECK-NEXT:    [[TMP13:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP13]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP15:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP13]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP14:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
+ ; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
+@@ -598,7 +544,7 @@
+ ; CHECK-NEXT:    store i64 [[MUL_2]], ptr [[DATA_2]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP16:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP15:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -707,7 +653,7 @@
+ ; CHECK-NEXT:    store <24 x i32> [[INTERLEAVED_VEC]], ptr [[TMP5]], align 8
+ ; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[IV]], 8
+ ; CHECK-NEXT:    [[TMP15:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
+-; CHECK-NEXT:    br i1 [[TMP15]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP17:![0-9]+]]
++; CHECK-NEXT:    br i1 [[TMP15]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP16:![0-9]+]]
+ ; CHECK:       [[MIDDLE_BLOCK]]:
+ ; CHECK-NEXT:    br label %[[SCALAR_PH]]
+ ; CHECK:       [[SCALAR_PH]]:
+@@ -731,7 +677,7 @@
+ ; CHECK-NEXT:    store i32 [[MUL_2]], ptr [[DATA_2]], align 8
+ ; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV1]], 1
+ ; CHECK-NEXT:    [[EC:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
+-; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]], !llvm.loop [[LOOP18:![0-9]+]]
++; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]], !llvm.loop [[LOOP17:![0-9]+]]
+ ; CHECK:       [[EXIT]]:
+ ; CHECK-NEXT:    ret void
+ ;
+@@ -765,20 +711,19 @@
+ ; CHECK: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]], [[META2:![0-9]+]]}
+ ; CHECK: [[META1]] = !{!"llvm.loop.isvectorized", i32 1}
+ ; CHECK: [[META2]] = !{!"llvm.loop.unroll.runtime.disable"}
+-; CHECK: [[LOOP3]] = distinct !{[[LOOP3]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP4]] = distinct !{[[LOOP4]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP5]] = distinct !{[[LOOP5]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP6]] = distinct !{[[LOOP6]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP7]] = distinct !{[[LOOP7]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP8]] = distinct !{[[LOOP8]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP9]] = distinct !{[[LOOP9]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP10]] = distinct !{[[LOOP10]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP11]] = distinct !{[[LOOP11]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP12]] = distinct !{[[LOOP12]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP13]] = distinct !{[[LOOP13]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP14]] = distinct !{[[LOOP14]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP15]] = distinct !{[[LOOP15]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP16]] = distinct !{[[LOOP16]], [[META2]], [[META1]]}
+-; CHECK: [[LOOP17]] = distinct !{[[LOOP17]], [[META1]], [[META2]]}
+-; CHECK: [[LOOP18]] = distinct !{[[LOOP18]], [[META2]], [[META1]]}
++; CHECK: [[LOOP3]] = distinct !{[[LOOP3]], [[META2]], [[META1]]}
++; CHECK: [[LOOP4]] = distinct !{[[LOOP4]], [[META1]], [[META2]]}
++; CHECK: [[LOOP5]] = distinct !{[[LOOP5]], [[META2]], [[META1]]}
++; CHECK: [[LOOP6]] = distinct !{[[LOOP6]], [[META1]], [[META2]]}
++; CHECK: [[LOOP7]] = distinct !{[[LOOP7]], [[META2]], [[META1]]}
++; CHECK: [[LOOP8]] = distinct !{[[LOOP8]], [[META1]], [[META2]]}
++; CHECK: [[LOOP9]] = distinct !{[[LOOP9]], [[META2]], [[META1]]}
++; CHECK: [[LOOP10]] = distinct !{[[LOOP10]], [[META1]], [[META2]]}
++; CHECK: [[LOOP11]] = distinct !{[[LOOP11]], [[META2]], [[META1]]}
++; CHECK: [[LOOP12]] = distinct !{[[LOOP12]], [[META1]], [[META2]]}
++; CHECK: [[LOOP13]] = distinct !{[[LOOP13]], [[META2]], [[META1]]}
++; CHECK: [[LOOP14]] = distinct !{[[LOOP14]], [[META1]], [[META2]]}
++; CHECK: [[LOOP15]] = distinct !{[[LOOP15]], [[META2]], [[META1]]}
++; CHECK: [[LOOP16]] = distinct !{[[LOOP16]], [[META1]], [[META2]]}
++; CHECK: [[LOOP17]] = distinct !{[[LOOP17]], [[META2]], [[META1]]}
+ ;.
+diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/libc/test/src/math/libc_math_test_rules.bzl b/utils/bazel/llvm-project-overlay/libc/test/src/math/libc_math_test_rules.bzl
+--- a/utils/bazel/llvm-project-overlay/libc/test/src/math/libc_math_test_rules.bzl
++++ b/utils/bazel/llvm-project-overlay/libc/test/src/math/libc_math_test_rules.bzl
+@@ -35,6 +35,7 @@
+             "//libc:__support_fputil_manipulation_functions",
+             "//libc:__support_fputil_nearest_integer_operations",
+             "//libc:__support_fputil_normal_float",
++            "//libc:__support_macros_optimization",
+             "//libc:__support_macros_properties_architectures",
+             "//libc:__support_macros_properties_os",
+             "//libc:__support_macros_properties_types",
+diff -ruN --strip-trailing-cr a/utils/bazel/llvm-project-overlay/mlir/python/BUILD.bazel b/utils/bazel/llvm-project-overlay/mlir/python/BUILD.bazel
+--- a/utils/bazel/llvm-project-overlay/mlir/python/BUILD.bazel
++++ b/utils/bazel/llvm-project-overlay/mlir/python/BUILD.bazel
+@@ -1029,6 +1029,7 @@
+     td_file = "mlir/dialects/ShardOps.td",
+     deps = [
+         "//mlir:OpBaseTdFiles",
++        "//mlir:ShapeOpsTdFiles",
+         "//mlir:ShardTdFiles",
+     ],
+ )
diff --git a/third_party/llvm/toolchains.patch b/third_party/llvm/toolchains.patch
index 2370c1e..9aba685 100644
--- a/third_party/llvm/toolchains.patch
+++ b/third_party/llvm/toolchains.patch
@@ -55,4 +55,4 @@ index 2e3bff53ead9..8d01617effdc 100644
 +    "//llvm:macos_x86_64_default": native_arch_defines("X86", "x86_64-unknown-darwin"),
      "@bazel_tools//src/conditions:linux_aarch64": native_arch_defines("AArch64", "aarch64-unknown-linux-gnu"),
      "@bazel_tools//src/conditions:linux_ppc64le": native_arch_defines("PowerPC", "powerpc64le-unknown-linux-gnu"),
-     "@bazel_tools//src/conditions:linux_s390x": native_arch_defines("SystemZ", "systemz-unknown-linux_gnu"),
+     "@bazel_tools//src/conditions:linux_riscv64": native_arch_defines("RISCV", "riscv64-unknown-linux-gnu"),
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index b1675f1..13a0715 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "32de3b9ef9e7e8debc14416e968456ca13b48bea"
-    LLVM_SHA256 = "e048b05e1fb9366e224ea3c06f8473714114039bfad00e81db4ecb6409f23efa"
+    LLVM_COMMIT = "003101e5f891dfec614a101b26dff22d92234391"
+    LLVM_SHA256 = "1836426214890165c2a0890ce163459a2c901546c4085665bc1ea03a24b15c29"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index 55268d3..f2a8787 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -1,34 +1,542 @@
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
+--- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
++++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
+@@ -1,5 +1,6 @@
+ // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg --split-input-file --canonicalize | FileCheck %s
+ // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg="enable-primitive-ops=true" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-PRIMITIVE
++// RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg="capture-scalar-inputs=false" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-NO-CAPTURE
+ 
+ // CHECK: #map = affine_map<(d0, d1) -> (d0, d1)>
+ // CHECK-LABEL: func @float_add
+@@ -534,6 +535,19 @@
+   %0 = "stablehlo.sign"(%arg0) : (tensor<2x2xcomplex<f32>>)
+                           -> tensor<2x2xcomplex<f32>>
+   func.return %0 : tensor<2x2xcomplex<f32>>
++}
++
++// -----
++
++// CHECK-LABEL: func @float_tan
++// CHECK-PRIMITIVE-LABEL: func @float_tan
++func.func @float_tan(%arg0: tensor<2x2xf32>) -> tensor<2x2xf32> {
++  // CHECK: linalg.generic
++  // CHECK: tan
++  // CHECK-PRIMITIVE: linalg.map
++  // CHECK-PRIMITIVE: tan
++  %0 = "stablehlo.tan"(%arg0) : (tensor<2x2xf32>) -> tensor<2x2xf32>
++  func.return %0 : tensor<2x2xf32>
+ }
+ 
+ // -----
+@@ -926,6 +940,23 @@
+ // CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {
+ // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32
+ // CHECK-PRIMITIVE:        linalg.yield %[[RES]]
++
++// CHECK-NO-CAPTURE:      #[[SCALAR_MAP:.*]] = affine_map<(d0, d1) -> ()>
++// CHECK-NO-CAPTURE:      #[[ID_MAP:.*]] = affine_map<(d0, d1) -> (d0, d1)>
++// CHECK-NO-CAPTURE:      func @select_scalar_pred_dyn
++// CHECK-NO-CAPTURE-SAME:  (%[[PRED:.*]]: tensor<i1>, %[[LHS:.*]]: tensor<2x?xf32>, %[[RHS:.*]]: tensor<2x?xf32>)
++// CHECK-NO-CAPTURE-DAG:  %[[C1:.*]] = arith.constant 1
++// CHECK-NO-CAPTURE-DAG:  %[[DIM:.*]] =  tensor.dim %[[LHS]], %[[C1]]
++// CHECK-NO-CAPTURE-DAG:  %[[DST:.*]] = tensor.empty(%[[DIM]])
++// CHECK-NO-CAPTURE:      linalg.generic
++// CHECK-NO-CAPTURE-SAME:   indexing_maps = [#[[SCALAR_MAP]], #[[ID_MAP]], #[[ID_MAP]], #[[ID_MAP]]]
++// CHECK-NO-CAPTURE-SAME:   iterator_types = ["parallel", "parallel"]
++// CHECK-NO-CAPTURE-SAME:   ins(%[[PRED]], %[[LHS]], %[[RHS]] : tensor<i1>, tensor<2x?xf32>, tensor<2x?xf32>)
++// CHECK-NO-CAPTURE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)
++// CHECK-NO-CAPTURE-SAME:   {someattr}
++// CHECK-NO-CAPTURE:      ^bb0(%[[PRED_:.*]]: i1, %[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %{{.*}}: f32):
++// CHECK-NO-CAPTURE:        %[[RES:.*]] = arith.select %[[PRED_]], %[[LHS_]], %[[RHS_]] : f32
++// CHECK-NO-CAPTURE:        linalg.yield %[[RES]]
+ 
+ // -----
+ 
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
+--- stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
++++ stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
+@@ -140,12 +140,11 @@
+   // (any sign-op, or an integral abs-op).
+   // TODO(peiming, ajcbik): these all can potentially be optimized by applying
+   // value transform on sparse_tenosr.value memref
+-  if (isa<mlir::stablehlo::SignOp>(op) || isa<mlir::stablehlo::NegOp>(op) ||
++  if (isa<mlir::stablehlo::SignOp, mlir::stablehlo::NegOp,
++          mlir::stablehlo::TanOp>(op) ||
+       (isa<mlir::stablehlo::AbsOp>(op) && hasIntegralShapeType(op)) ||
+-      isa<chlo::AsinOp>(op) || isa<chlo::AsinhOp>(op) ||
+-      isa<chlo::AtanOp>(op) || isa<chlo::AtanhOp>(op) ||
+-      isa<chlo::BesselI1eOp>(op) || isa<chlo::SinhOp>(op) ||
+-      isa<chlo::TanOp>(op)) {
++      isa<chlo::AsinOp, chlo::AsinhOp, chlo::AtanOp, chlo::AtanhOp,
++          chlo::BesselI1eOp, chlo::SinhOp, chlo::TanOp>(op)) {
+     if (!sparse_tensor::getSparseTensorEncoding(op->getResult(0).getType()) &&
+         !sparse_tensor::getSparseTensorEncoding(op->getOperand(0).getType()))
+       return Value();
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h b/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
+--- stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
++++ stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
+@@ -153,14 +153,11 @@
+   using FOp = ::mlir::math::SinOp;
+   using COp = ::mlir::complex::SinOp;
+ };
+-// FIXME(Jakub)
+-/*
+ template <>
+ struct StablehloToScalarOp<stablehlo::TanOp> {
+   using FOp = ::mlir::math::TanOp;
+   using COp = ::mlir::complex::TanOp;
+ };
+-*/
+ template <>
+ struct StablehloToScalarOp<stablehlo::Atan2Op> {
+   using FOp = ::mlir::math::Atan2Op;
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td b/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
+--- stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
++++ stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
+@@ -39,7 +39,11 @@
+                  Option<"enableSparseOps", "enable-sparse-ops", "bool",
+                         /*default=*/"false",
+                         "Lower to Sparse Tensor ops (sparse_tensor.concatenate)"
+-                        "when possible, instead of linalg.generic">];
++                        "when possible, instead of linalg.generic">,
++                 Option<"captureScalarInputs", "capture-scalar-inputs", "bool",
++                        /*default=*/"true",
++                        "Capture scalar inputs in generic ops instead of"
++                        "passing as tensor-scalar argument.">];
+ }
+ 
+ #endif  // STABLEHLO_TO_LINALG_PASSES
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h b/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
+--- stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
++++ stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
+@@ -26,11 +26,12 @@
+ //===----------------------------------------------------------------------===//
+ 
+ /// Populates the patterns that convert from StableHLO to Linalg on tensors.
+-void populateStablehloToLinalgConversionPatterns(MLIRContext *context,
+-                                                 TypeConverter &typeConverter,
+-                                                 RewritePatternSet *patterns,
++void populateStablehloToLinalgConversionPatterns(MLIRContext* context,
++                                                 TypeConverter& typeConverter,
++                                                 RewritePatternSet* patterns,
+                                                  bool enablePrimitiveOps,
+-                                                 bool enableSparseOps);
++                                                 bool enableSparseOps,
++                                                 bool captureScalarInputs);
+ 
+ //===----------------------------------------------------------------------===//
+ // Fine-grained patterns used by the implementation.
+@@ -39,8 +40,9 @@
+ /// Populates the patterns that convert from elementwise StableHLO ops to Linalg
+ /// on tensors.
+ void populatePointwiseStablehloToLinalgConversionPatterns(
+-    MLIRContext *context, TypeConverter &typeConverter,
+-    RewritePatternSet *patterns, bool enablePrimitiveOps);
++    MLIRContext* context, TypeConverter& typeConverter,
++    RewritePatternSet* patterns, bool enablePrimitiveOps,
++    bool captureScalarInputs);
+ 
+ /// Populates the patterns that convert from convolution StableHLO ops to Linalg
+ /// on tensors.
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
++++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+@@ -2634,7 +2634,8 @@
+ 
+     RewritePatternSet patterns_(context);
+     populateStablehloToLinalgConversionPatterns(
+-        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps);
++        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps,
++        captureScalarInputs);
+     patterns = std::move(patterns_);
+ 
+     return success();
+@@ -2657,7 +2658,8 @@
+                                                  TypeConverter& typeConverter,
+                                                  RewritePatternSet* patterns,
+                                                  bool enablePrimitiveOps,
+-                                                 bool enableSparseOps) {
++                                                 bool enableSparseOps,
++                                                 bool captureScalarInputs) {
+   // clang-format off
+   patterns->add<ConcatenateConverter>(typeConverter, context,
+                                       enablePrimitiveOps);
+@@ -2680,7 +2682,8 @@
+       >(typeConverter, context);
+ 
+   detail::populatePointwiseStablehloToLinalgConversionPatterns(
+-      context, typeConverter, patterns, enablePrimitiveOps);
++      context, typeConverter, patterns, enablePrimitiveOps,
++      captureScalarInputs);
+ 
+   if (enableSparseOps) {
+     patterns->add<SparseConcatenateConverter>(typeConverter, context);
 diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
 --- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
 +++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
-@@ -33,6 +33,7 @@
- 
- template <typename OpTy>
- struct ScalarHloToFuncPatterns final : OpConversionPattern<OpTy> {
-+  // NOLINTNEXTLINE(clang-diagnostic-shadow-field)
-   ScalarHloToFuncPatterns(TypeConverter& typeConverter, MLIRContext* context,
-                           PatternBenefit benefit = 1)
-       : OpConversionPattern<OpTy>(typeConverter, context, benefit) {}
-@@ -51,6 +52,7 @@
- template <typename OpTy>
- struct ScalarHloToArithmeticPattern final : OpConversionPattern<OpTy> {
-   ScalarHloToArithmeticPattern(
-+      // NOLINTNEXTLINE(clang-diagnostic-shadow-field)
-       TypeConverter& typeConverter, MLIRContext* context,
-       llvm::function_ref<bool(Operation*)> filterFn = nullptr,
-       PatternBenefit benefit = 1)
-diff --ruN a/stablehlo/stablehlo/dialect/Base.td b/stablehlo/stablehlo/dialect/Base.td
---- stablehlo/stablehlo/dialect/Base.td
-+++ stablehlo/stablehlo/dialect/Base.td
-@@ -152,7 +152,7 @@
-     AnyTypeOf<[HLO_PerAxisQuantizedSignedInt, HLO_PerAxisQuantizedUnsignedInt], "per-axis integer quantized">;
- 
- // Token type.
--def HLO_Token : Type<CPred<"::llvm::isa<::mlir::stablehlo::TokenType>($_self)">, "token">;
-+def HLO_Token : Type<CPred<"::llvm::isa<TokenType>($_self)">, "token">;
- 
- // Any integer tensor types
- def HLO_IntTensor : RankedTensorOf<[HLO_Int]>;
+@@ -145,6 +145,7 @@
+       ScalarHloToArithmeticPattern<mlir::stablehlo::SineOp>,
+       ScalarHloToArithmeticPattern<mlir::stablehlo::SqrtOp>,
+       ScalarHloToArithmeticPattern<mlir::stablehlo::SubtractOp>,
++      ScalarHloToArithmeticPattern<mlir::stablehlo::TanOp>,
+       ScalarHloToArithmeticPattern<mlir::stablehlo::TanhOp>,
+       ScalarHloToArithmeticPattern<mlir::stablehlo::XorOp>>(typeConverter,
+                                                             context, filterFn);
+diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
+--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
++++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
+@@ -23,6 +23,7 @@
+ 
+ #include "llvm/ADT/STLExtras.h"
+ #include "llvm/ADT/SmallVector.h"
++#include "llvm/Support/Debug.h"
+ #include "mlir/Dialect/Linalg/IR/Linalg.h"
+ #include "mlir/Dialect/Tensor/IR/Tensor.h"
+ #include "mlir/IR/AffineMap.h"
+@@ -43,6 +44,8 @@
+ #include "stablehlo/conversions/linalg/transforms/Rewriters.h"
+ #include "stablehlo/dialect/StablehloOps.h"
+ 
++#define DEBUG_TYPE "stablehlo-conversions"
++
+ namespace mlir::stablehlo {
+ namespace {
+ int64_t getRank(Value v) { return cast<ShapedType>(v.getType()).getRank(); }
+@@ -142,6 +145,11 @@
+ struct PointwiseToLinalgMapConverter : OpConversionPattern<OpTy> {
+   using OpConversionPattern<OpTy>::OpConversionPattern;
+   using OpAdaptor = typename OpTy::Adaptor;
++
++  PointwiseToLinalgMapConverter(TypeConverter& typeConverter,
++                                MLIRContext* context, bool captureScalarInputs)
++      : OpConversionPattern<OpTy>(typeConverter, context),
++        captureScalarInputs(captureScalarInputs) {}
+ 
+   virtual FailureOr<Operation *> createLinalgOp(
+       OpTy &op, ConversionPatternRewriter &rewriter,
+@@ -190,8 +198,11 @@
+             rewriter, loc, cast<TypedValue<ShapedType>>(input),
+             cast<ShapedType>(emptyTensor.getType())));
+         scalarInputs.push_back(nullptr);
++      } else if (captureScalarInputs) {
++        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));
+       } else {
+-        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));
++        mappedInputs.push_back(input);
++        scalarInputs.push_back(nullptr);
+       }
+     }
+ 
+@@ -202,6 +213,8 @@
+     rewriter.replaceOp(op, (*mapOp)->getResults());
+     return success();
+   }
++
++  bool captureScalarInputs;
+ };
+ 
+ /// Converts a HLO operation to a linalg.generic op that contains the
+@@ -211,12 +224,12 @@
+   using PointwiseToLinalgMapConverter<OpTy>::PointwiseToLinalgMapConverter;
+   using OpAdaptor = typename OpTy::Adaptor;
+ 
+-  FailureOr<Operation *> createLinalgOp(OpTy &op,
+-                                        ConversionPatternRewriter &rewriter,
+-                                        ArrayRef<Value> mappedInputs,
+-                                        ArrayRef<Value> scalarVals,
+-                                        Value emptyTensor,
+-                                        int64_t maxRank) const override {
++  FailureOr<Operation*> createLinalgOp(OpTy& op,
++                                       ConversionPatternRewriter& rewriter,
++                                       ArrayRef<Value> mappedInputs,
++                                       ArrayRef<Value> scalarVals,
++                                       Value emptyTensor,
++                                       int64_t maxRank) const override {
+     // Create indexing maps.
+     AffineMap scalarMap = AffineMap::get(maxRank, 0, rewriter.getContext());
+     AffineMap idMap = rewriter.getMultiDimIdentityMap(maxRank);
+@@ -225,10 +238,10 @@
+       maps.push_back(isScalar(v) ? scalarMap : idMap);
+     maps.push_back(idMap);
+     bool failed = false;
+-    Operation *linalgOp = rewriter.create<linalg::GenericOp>(
++    Operation* linalgOp = rewriter.create<linalg::GenericOp>(
+         op.getLoc(), emptyTensor.getType(), mappedInputs, emptyTensor, maps,
+         getNParallelLoopsAttrs(maxRank),
+-        [&](OpBuilder &nestedBuilder, Location /*nested_loc*/,
++        [&](OpBuilder& nestedBuilder, Location /*nested_loc*/,
+             ValueRange args) {
+           Type innerResultTy = getElementTypeOrSelf(emptyTensor);
+           auto argvec =
+@@ -253,8 +266,9 @@
+ 
+ namespace detail {
+ void populatePointwiseStablehloToLinalgConversionPatterns(
+-    MLIRContext *context, TypeConverter &typeConverter,
+-    RewritePatternSet *patterns, bool enablePrimitiveOps) {
++    MLIRContext* context, TypeConverter& typeConverter,
++    RewritePatternSet* patterns, bool enablePrimitiveOps,
++    bool captureScalarInputs) {
+   if (enablePrimitiveOps) {
+     patterns->add<
+         PointwiseToLinalgMapConverter<mlir::stablehlo::AbsOp>,
+@@ -301,12 +315,12 @@
+         PointwiseToLinalgMapConverter<mlir::stablehlo::SineOp>,
+         PointwiseToLinalgMapConverter<mlir::stablehlo::SqrtOp>,
+         PointwiseToLinalgMapConverter<mlir::stablehlo::SubtractOp>,
++        PointwiseToLinalgMapConverter<mlir::stablehlo::TanOp>,
+         PointwiseToLinalgMapConverter<mlir::stablehlo::TanhOp>,
+-        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(typeConverter,
+-                                                               context);
++        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(
++        typeConverter, context, captureScalarInputs);
+     return;
+   }
+-
+   patterns
+       ->add<PointwiseToLinalgConverter<mlir::stablehlo::AbsOp>,
+             PointwiseToLinalgConverter<mlir::stablehlo::AddOp>,
+@@ -352,9 +366,10 @@
+             PointwiseToLinalgConverter<mlir::stablehlo::SineOp>,
+             PointwiseToLinalgConverter<mlir::stablehlo::SqrtOp>,
+             PointwiseToLinalgConverter<mlir::stablehlo::SubtractOp>,
++            PointwiseToLinalgConverter<mlir::stablehlo::TanOp>,
+             PointwiseToLinalgConverter<mlir::stablehlo::TanhOp>,
+-            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(typeConverter,
+-                                                                context);
++            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(
++          typeConverter, context, captureScalarInputs);
+ }
+ }  // namespace detail
+ }  // namespace mlir::stablehlo
+diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
+--- stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
++++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
+@@ -40,7 +40,7 @@
+ 
+ namespace {
+ 
+-Value buildRescaleMultiplier(bool scale32, OpBuilder& builder, Location loc,
++Value buildRescaleMultiplier(bool scale32, OpBuilder &builder, Location loc,
+                              ArrayRef<int32_t> multipliers) {
+   if (scale32) {
+     return tosa::getConstTensorInt<int32_t>(builder, loc, multipliers);
+@@ -51,7 +51,7 @@
+ }
+ 
+ // create a tosa rescale op and return its result value
+-Value buildRescale(PatternRewriter& rewriter, Location loc,
++Value buildRescale(PatternRewriter &rewriter, Location loc,
+                    ShapedType outputType, Value inputVal, int32_t multiplier,
+                    int32_t shift, int64_t inputZp, int64_t outputZp,
+                    bool doubleRound, bool scale32, bool perChannel) {
+@@ -85,7 +85,7 @@
+ }
+ 
+ // Creates TOSA rescale op with int32 output
+-Value buildRescaleToInt32(PatternRewriter& rewriter, Location loc,
++Value buildRescaleToInt32(PatternRewriter &rewriter, Location loc,
+                           Value inputVal, double inputScale, int64_t inputZp) {
+   auto inputType = cast<ShapedType>(inputVal.getType());
+   auto outputType = inputType.clone(rewriter.getI32Type());
+@@ -103,7 +103,7 @@
+ }
+ 
+ // Creates TOSA rescale op with int32 input
+-Value buildRescaleFromInt32(PatternRewriter& rewriter, Location loc,
++Value buildRescaleFromInt32(PatternRewriter &rewriter, Location loc,
+                             ShapedType outputType, Value inputVal,
+                             double outputScale, int64_t outputZp) {
+   // Input should be int32 type
+@@ -124,14 +124,14 @@
+ }
+ 
+ using UnaryRescaleScalesFn =
+-    void (*)(const quant::UniformQuantizedType& operandQType,
+-             const quant::UniformQuantizedType& resultQType,
+-             double& operandRescaleScale, double& resultRescaleScale);
+-
+-void GetUnaryRescaleScales(const quant::UniformQuantizedType& operandQType,
+-                           const quant::UniformQuantizedType& resultQType,
+-                           double& operandRescaleScale,
+-                           double& resultRescaleScale) {
++    void (*)(const quant::UniformQuantizedType &operandQType,
++             const quant::UniformQuantizedType &resultQType,
++             double &operandRescaleScale, double &resultRescaleScale);
++
++void GetUnaryRescaleScales(const quant::UniformQuantizedType &operandQType,
++                           const quant::UniformQuantizedType &resultQType,
++                           double &operandRescaleScale,
++                           double &resultRescaleScale) {
+   double operandScale = operandQType.getScale();
+   double resultScale = resultQType.getScale();
+ 
+@@ -145,7 +145,7 @@
+ 
+ template <typename StablehloOp>
+ LogicalResult matchAndRewriteUnaryOp(
+-    StablehloOp op, PatternRewriter& rewriter,
++    StablehloOp op, PatternRewriter &rewriter,
+     UnaryRescaleScalesFn rescaleScalesFn = GetUnaryRescaleScales) {
+   Value operand = op.getOperand();
+   Value result = op.getResult();
+@@ -190,21 +190,21 @@
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::AbsOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteUnaryOp(op, rewriter);
+ }
+ 
+ using BinaryRescaleScalesFn = void (*)(
+-    const quant::UniformQuantizedType& lhsQType,
+-    const quant::UniformQuantizedType& rhsQType,
+-    const quant::UniformQuantizedType& resultQType, double& lhsRescaleScale,
+-    double& rhsRescaleScale, double& resultRescaleScale);
+-
+-void GetAddSubRescaleScales(const quant::UniformQuantizedType& lhsQType,
+-                            const quant::UniformQuantizedType& rhsQType,
+-                            const quant::UniformQuantizedType& resultQType,
+-                            double& lhsRescaleScale, double& rhsRescaleScale,
+-                            double& resultRescaleScale) {
++    const quant::UniformQuantizedType &lhsQType,
++    const quant::UniformQuantizedType &rhsQType,
++    const quant::UniformQuantizedType &resultQType, double &lhsRescaleScale,
++    double &rhsRescaleScale, double &resultRescaleScale);
++
++void GetAddSubRescaleScales(const quant::UniformQuantizedType &lhsQType,
++                            const quant::UniformQuantizedType &rhsQType,
++                            const quant::UniformQuantizedType &resultQType,
++                            double &lhsRescaleScale, double &rhsRescaleScale,
++                            double &resultRescaleScale) {
+   // 1. Rescale inputs to scale = 2.0 x max(lhs.scale, rhs.scale)
+   // 2. Extra left shift to input to increase precision
+   // Where input_shift = 20 if input is 8-bit
+@@ -230,11 +230,11 @@
+       maxScale2x / (resultScale * static_cast<double>(1 << inputShift));
+ }
+ 
+-void GetMulDivRescaleScales(const quant::UniformQuantizedType& lhsQType,
+-                            const quant::UniformQuantizedType& rhsQType,
+-                            const quant::UniformQuantizedType& resultQType,
+-                            double& lhsRescaleScale, double& rhsRescaleScale,
+-                            double& resultRescaleScale) {
++void GetMulDivRescaleScales(const quant::UniformQuantizedType &lhsQType,
++                            const quant::UniformQuantizedType &rhsQType,
++                            const quant::UniformQuantizedType &resultQType,
++                            double &lhsRescaleScale, double &rhsRescaleScale,
++                            double &resultRescaleScale) {
+   double lhsScale = lhsQType.getScale();
+   double rhsScale = rhsQType.getScale();
+   double resultScale = resultQType.getScale();
+@@ -248,11 +248,11 @@
+   resultRescaleScale = lhsScale * rhsScale / resultScale;
+ }
+ 
+-void GetMinMaxRescaleScales(const quant::UniformQuantizedType& lhsQType,
+-                            const quant::UniformQuantizedType& rhsQType,
+-                            const quant::UniformQuantizedType& resultQType,
+-                            double& lhsRescaleScale, double& rhsRescaleScale,
+-                            double& resultRescaleScale) {
++void GetMinMaxRescaleScales(const quant::UniformQuantizedType &lhsQType,
++                            const quant::UniformQuantizedType &rhsQType,
++                            const quant::UniformQuantizedType &resultQType,
++                            double &lhsRescaleScale, double &rhsRescaleScale,
++                            double &resultRescaleScale) {
+   // 1. Rescale inputs to scale = max(lhs.scale, rhs.scale)
+   // 2. Extra left shift to input to increase precision
+   // Where input_shift = 20 if input is 8-bit
+@@ -280,7 +280,7 @@
+ }
+ 
+ template <typename StablehloOp>
+-LogicalResult matchAndRewriteBinaryOp(StablehloOp op, PatternRewriter& rewriter,
++LogicalResult matchAndRewriteBinaryOp(StablehloOp op, PatternRewriter &rewriter,
+                                       BinaryRescaleScalesFn rescaleScalesFn) {
+   Value lhs = op.getLhs();
+   Value rhs = op.getRhs();
+@@ -339,37 +339,37 @@
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::AddOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteBinaryOp(op, rewriter, GetAddSubRescaleScales);
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::SubtractOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteBinaryOp(op, rewriter, GetAddSubRescaleScales);
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::MulOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteBinaryOp(op, rewriter, GetMulDivRescaleScales);
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::DivOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteBinaryOp(op, rewriter, GetMulDivRescaleScales);
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::MinOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteBinaryOp(op, rewriter, GetMinMaxRescaleScales);
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::MaxOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteBinaryOp(op, rewriter, GetMinMaxRescaleScales);
+ }
+ 
+ LogicalResult matchAndRewriteCompareOp(stablehlo::CompareOp op,
+-                                       PatternRewriter& rewriter) {
++                                       PatternRewriter &rewriter) {
+   Value lhs = op.getLhs();
+   Value rhs = op.getRhs();
+   Value result = op.getResult();
+@@ -429,7 +429,7 @@
+ }
+ 
+ LogicalResult matchAndRewriteOp(stablehlo::CompareOp op,
+-                                PatternRewriter& rewriter) {
++                                PatternRewriter &rewriter) {
+   return matchAndRewriteCompareOp(op, rewriter);
+ }
+ 
+@@ -438,7 +438,7 @@
+     : public OpRewritePattern<StablehloOpType> {
+   using OpRewritePattern<StablehloOpType>::OpRewritePattern;
+   LogicalResult matchAndRewrite(StablehloOpType op,
+-                                PatternRewriter& rewriter) const override {
++                                PatternRewriter &rewriter) const override {
+     return matchAndRewriteOp(op, rewriter);
+   }
+ };
+@@ -446,7 +446,7 @@
+ struct StablehloQuantLegalizeToTosaRescalePass
+     : impl::StablehloQuantLegalizeToTosaRescalePassBase<
+           StablehloQuantLegalizeToTosaRescalePass> {
+-  LogicalResult initialize(MLIRContext* ctx) override {
++  LogicalResult initialize(MLIRContext *ctx) override {
+     RewritePatternSet patternList(ctx);
+     populateStablehloQuantLegalizeToTosaRescalePatterns(&patternList, ctx);
+     patterns = std::move(patternList);
+@@ -468,7 +468,7 @@
+ }  // namespace
+ 
+ void populateStablehloQuantLegalizeToTosaRescalePatterns(
+-    RewritePatternSet* patterns, MLIRContext* context) {
++    RewritePatternSet *patterns, MLIRContext *context) {
+   // unary ops
+   patterns->addWithLabel<QuantizedStablehloOpConversion<stablehlo::AbsOp>>(
+       {"StablehloQuantAbsOp"}, context);
 diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
 --- stablehlo/stablehlo/dialect/StablehloOps.cpp
 +++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -40,6 +548,97 @@ diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/
  #define GET_OP_CLASSES
  #include "stablehlo/dialect/StablehloOps.cpp.inc"
  
+diff --ruN a/stablehlo/stablehlo/integrations/c/VhloDialect.h b/stablehlo/stablehlo/integrations/c/VhloDialect.h
+--- stablehlo/stablehlo/integrations/c/VhloDialect.h
++++ stablehlo/stablehlo/integrations/c/VhloDialect.h
+@@ -13,7 +13,7 @@
+ #ifndef STABLEHLO_INTEGRATIONS_C_VHLO_DIALECT_H
+ #define STABLEHLO_INTEGRATIONS_C_VHLO_DIALECT_H
+ 
+-#include "mlir-c/RegisterEverything.h"
++#include "mlir-c/IR.h"
+ 
+ #ifdef __cplusplus
+ extern "C" {
+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
+--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
++++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
+@@ -110,6 +110,43 @@
+   }
+ }
+ 
++bool IsBoolean(ElementType elementType) {
++  MLIRContext ctx;
++  return getElementType(ctx, elementType).isInteger(1);
++}
++
++bool IsComplex(ElementType elementType) {
++  MLIRContext ctx;
++  auto type = dyn_cast<ComplexType>(getElementType(ctx, elementType));
++  return !!type;
++}
++
++bool IsFloat(ElementType elementType) {
++  MLIRContext ctx;
++  return getElementType(ctx, elementType).isFloat();
++}
++
++bool IsInteger(ElementType elementType, bool includeBool = false) {
++  MLIRContext ctx;
++  Type type = getElementType(ctx, elementType);
++  return type.isInteger() && (includeBool || !IsBoolean(elementType));
++}
++
++bool IsSignedInteger(ElementType elementType) {
++  MLIRContext ctx;
++  Type type = getElementType(ctx, elementType);
++
++  // Note that this is not the same as `type.isSignedInteger()`. Signed integers
++  // are not used in StableHLO.
++  return type.isSignlessInteger() && !IsBoolean(elementType);
++}
++
++bool IsUnsignedInteger(ElementType elementType) {
++  MLIRContext ctx;
++  return getElementType(ctx, elementType).isUnsignedInteger() &&
++         !IsBoolean(elementType);
++}
++
+ RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,
+                                 ElementType elementType) {
+   return makeTensorType(ctx, shape, getElementType(ctx, elementType));
+diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
+--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
++++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
+@@ -18,7 +18,6 @@
+ 
+ #include <complex>
+ #include <cstdint>
+-#include <source_location>
+ #include <type_traits>
+ #include <vector>
+ 
+@@ -68,6 +67,20 @@
+   // clang-format on
+ };
+ 
++bool IsBoolean(ElementType elementType);
++
++bool IsComplex(ElementType elementType);
++
++bool IsFloat(ElementType elementType);
++
++bool IsInteger(ElementType elementType, bool includeBool);
++
++// In StableHLO, we refer to signed integer as the MLIR's equivalent signless
++// integer. StableHLO does not have a notion of signless integers like MLIR.
++bool IsSignedInteger(ElementType elementType);
++
++bool IsUnsignedInteger(ElementType elementType);
++
+ Type getElementType(MLIRContext& ctx, ElementType elementType);
+ 
+ // Build a ranked tensor type with an element type of ElementType.
 diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
 --- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
 +++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
@@ -244,6 +843,79 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.ml
 +  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>
  }
  
+ // -----
+@@ -529,28 +532,15 @@
+ // IotaOp
+ 
+ // CHECK-LABEL: func @eval_iota
+-func.func @eval_iota() -> (tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {
+-  // CHECK-NOT: stablehlo.iota
+-  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<
+-  // CHECK-SAME: {{\[\[}}[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],
+-  // CHECK-SAME: {{\[}}[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],
+-  // CHECK-SAME: {{\[}}[2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2]]]> : tensor<3x4x5xi32>
+-
+-  // CHECK: [[RESULT1:%.*]] = stablehlo.constant dense<
+-  // CHECK-SAME: {{\[\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
+-  // CHECK-SAME: {{\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
+-  // CHECK-SAME: {{\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]> : tensor<3x4x5xi32>
+-
+-  // CHECK: [[RESULT2:%.*]] = stablehlo.constant dense<
+-  // CHECK-SAME: {{\[\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],
+-  // CHECK-SAME: {{\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],
+-  // CHECk-SAME: {{\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]]> : tensor<3x4x5xi32>
+-
++func.func @eval_iota() -> (tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {
++  // CHECK:      [[RESULT0:%.*]] = stablehlo.constant dense<0> : tensor<1xi32>
++  // CHECK-NEXT: [[RESULT1:%.*]] = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
++  // CHECK-NEXT: [[RESULT2:%.*]] = stablehlo.iota dim = 2 : tensor<3x4x5xi32>
+   // CHECK: return [[RESULT0]], [[RESULT1]], [[RESULT2]]
+-  %0 = stablehlo.iota dim = 0 : tensor<3x4x5xi32>
++  %0 = stablehlo.iota dim = 0 : tensor<1xi32>
+   %1 = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
+   %2 = stablehlo.iota dim = 2 : tensor<3x4x5xi32>
+-  func.return %0, %1, %2 : tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>
++  func.return %0, %1, %2 : tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>
+ }
+ 
+ // -----
+@@ -596,6 +586,37 @@
+   // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\[\[1, 2\], \[3, 4\]\]}}> : tensor<2x2xi32>
+   // CHECK-NEXT: return [[CST1]], [[CST2]]
+   return %0, %1 : tensor<1xi32>, tensor<2x2xi32>
++}
++
++// -----
++
++////////
++// SliceOp / DynamicSliceOp
++
++// CHECK-LABEL: @slice_fold
++func.func @slice_fold(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {
++  %c = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5]]> : tensor<6x1xi32>
++  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>
++  // CHECK: stablehlo.constant dense<2> : tensor<1x1xi32>
++  return %0 : tensor<1x1xi32>
++}
++
++// CHECK-LABEL: @slice_fold_splat
++func.func @slice_fold_splat(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {
++  %c = stablehlo.constant dense<1> : tensor<6x1xi32>
++  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>
++  // CHECK: stablehlo.constant dense<1> : tensor<1x1xi32>
++  return %0 : tensor<1x1xi32>
++}
++
++// CHECK-LABEL: @dynamic_slice_fold
++func.func @dynamic_slice_fold(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<1x1xi32> {
++  %0 = stablehlo.constant dense<256> : tensor<6x1xi32>
++  %1 = "stablehlo.dynamic_slice"(%0, %arg0, %arg1) <{slice_sizes = array<i64: 1, 1>}> : (tensor<6x1xi32>, tensor<i32>, tensor<i32>) -> tensor<1x1xi32>
++
++  // CHECK: %[[RESULT:.*]] = stablehlo.constant dense<256> : tensor<1x1xi32>
++  // CHECK: return %[[RESULT]]
++  return %1 : tensor<1x1xi32>
+ }
+ 
  // -----
 diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
 --- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -349,6 +1021,533 @@ diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b
    func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
      return %arg1 : tensor<?xf32>
    }
+diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
++++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+@@ -73,7 +73,7 @@
+ template <typename FromOpTy, typename ToOpTy>
+ struct HloNaryElementwiseAdaptor {
+   static ToOpTy createOp(FromOpTy fromOp, Type resultType,
+-                         ValueRange broadcastedOperands, OpBuilder& builder) {
++                         ValueRange broadcastedOperands, OpBuilder &builder) {
+     return builder.create<ToOpTy>(fromOp.getLoc(), resultType,
+                                   broadcastedOperands);
+   }
+@@ -118,7 +118,7 @@
+ struct HloCompareAdaptor {
+   static mlir::stablehlo::CompareOp createOp(
+       mlir::chlo::BroadcastCompareOp fromOp, Type resultType,
+-      ValueRange broadcastedOperands, OpBuilder& builder) {
++      ValueRange broadcastedOperands, OpBuilder &builder) {
+     auto chloDirection = fromOp.getComparisonDirection();
+     auto hloDirection = toStableHloComparisonDirection(chloDirection);
+     if (!hloDirection) return nullptr;
+@@ -140,9 +140,9 @@
+ // to take a ChloOpTy, NonBroadcastingOpTy, and an Adaptor as templated values.
+ template <template <typename, typename, typename> typename Pattern,
+           typename... ConstructorArgs>
+-static void populateForBroadcastingBinaryOp(MLIRContext* context,
+-                                            RewritePatternSet* patterns,
+-                                            ConstructorArgs&&... args) {
++static void populateForBroadcastingBinaryOp(MLIRContext *context,
++                                            RewritePatternSet *patterns,
++                                            ConstructorArgs &&...args) {
+ #define POPULATE_BCAST(ChloOp, HloOp)                                          \
+   patterns                                                                     \
+       ->add<Pattern<ChloOp, HloOp, HloNaryElementwiseAdaptor<ChloOp, HloOp>>>( \
+@@ -179,21 +179,21 @@
+       context, args...);
+ }
+ 
+-static Value getConstantLikeMaxFiniteValue(OpBuilder& b, Location loc,
++static Value getConstantLikeMaxFiniteValue(OpBuilder &b, Location loc,
+                                            Value val) {
+   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
+   return getConstantLike(
+       b, loc, llvm::APFloat::getLargest(ty.getFloatSemantics()), val);
+ }
+ 
+-static Value getConstantLikeInfValue(OpBuilder& b, Location loc, Value val,
++static Value getConstantLikeInfValue(OpBuilder &b, Location loc, Value val,
+                                      bool negative) {
+   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
+   return getConstantLike(
+       b, loc, llvm::APFloat::getInf(ty.getFloatSemantics(), negative), val);
+ }
+ 
+-static Value getConstantLikeSmallestNormalizedValue(OpBuilder& b, Location loc,
++static Value getConstantLikeSmallestNormalizedValue(OpBuilder &b, Location loc,
+                                                     Value val) {
+   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
+   return getConstantLike(
+@@ -239,7 +239,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       ChloOpTy op, typename ChloOpTy::Adaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     // Only rewrite for statically determinable non-broadcasting cases.
+     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());
+     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());
+@@ -329,7 +329,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       ChloOpTy op, typename ChloOpTy::Adaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     // Only support ranked operands.
+     Value lhs = adaptor.getLhs();
+     Value rhs = adaptor.getRhs();
+@@ -413,7 +413,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::ConstantLikeOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     auto resultTy = cast<ShapedType>(op.getType());
+ 
+     // Unranked uses are not supported.
+@@ -445,7 +445,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::BroadcastSelectOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     // Only support ranked operands.
+     Value pred = adaptor.getPred();
+     Value onTrue = adaptor.getOnTrue();
+@@ -533,7 +533,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::ConstantOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, op.getValue());
+     return success();
+   }
+@@ -541,7 +541,7 @@
+ 
+ template <typename FTy>
+ static Value materializeChebyshevPolynomialApproximation(
+-    OpBuilder& rewriter, Location loc, Value x, ArrayRef<FTy> coefficients) {
++    OpBuilder &rewriter, Location loc, Value x, ArrayRef<FTy> coefficients) {
+   Value b0 = getConstantLike(rewriter, loc, 0.0, x);
+   Value b1 = getConstantLike(rewriter, loc, 0.0, x);
+   Value b2 = getConstantLike(rewriter, loc, 0.0, x);
+@@ -561,7 +561,7 @@
+ }
+ 
+ template <typename FTy>
+-static Value materializeBesselI1eApproximation(OpBuilder& rewriter,
++static Value materializeBesselI1eApproximation(OpBuilder &rewriter,
+                                                Location loc, Value x,
+                                                ArrayRef<FTy> kI1eCoeffsA,
+                                                ArrayRef<FTy> kI1eCoeffsB) {
+@@ -594,7 +594,7 @@
+       loc, rewriter.create<mlir::stablehlo::SignOp>(loc, x), select);
+ }
+ 
+-Value materializeBesselI1eApproximationF32(OpBuilder& rewriter, Location loc,
++Value materializeBesselI1eApproximationF32(OpBuilder &rewriter, Location loc,
+                                            ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
+@@ -620,7 +620,7 @@
+                                                   kI1eCoeffsB);
+ }
+ 
+-static Value materializeBesselI1eApproximationF64(OpBuilder& rewriter,
++static Value materializeBesselI1eApproximationF64(OpBuilder &rewriter,
+                                                   Location loc,
+                                                   ValueRange args) {
+   Value x = args.front();
+@@ -663,10 +663,10 @@
+                                                    kI1eCoeffsA, kI1eCoeffsB);
+ }
+ 
+-static Value materializeWithUpcast(ConversionPatternRewriter& rewriter,
++static Value materializeWithUpcast(ConversionPatternRewriter &rewriter,
+                                    Location loc, ValueRange args,
+                                    FloatType minPrecisionTy,
+-                                   Value callback(OpBuilder&, Location,
++                                   Value callback(OpBuilder &, Location,
+                                                   ValueRange)) {
+   Type originalTy = getElementTypeOrSelf(args.front().getType());
+   auto floatOriginalTy = dyn_cast<FloatType>(originalTy);
+@@ -699,7 +699,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::BesselI1eOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Location loc = op.getLoc();
+     Value x = adaptor.getOperand();
+     Type ty = cast<ShapedType>(x.getType()).getElementType();
+@@ -725,7 +725,7 @@
+ };
+ 
+ template <typename FTy>
+-static Value materializePolynomialApproximation(OpBuilder& rewriter,
++static Value materializePolynomialApproximation(OpBuilder &rewriter,
+                                                 Location loc, Value x,
+                                                 ArrayRef<FTy> coefficients) {
+   if (coefficients.empty()) return getConstantLike(rewriter, loc, 0.0, x);
+@@ -746,7 +746,7 @@
+ // argument and derive the final approximation for all |x| >= 1.
+ // This implementation is based on Cephes.
+ static Value materializeErfcApproximationF64ForMagnituteGeOne(
+-    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {
++    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
+          "expect f64 element type");
+@@ -831,7 +831,7 @@
+ // Precondition is |x| <= 1. Use erfc approximation, otherwise.
+ // This implementation is based on Cephes.
+ static Value materializeErfApproximationF64ForMagnituteLeOne(
+-    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {
++    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
+          "expect f64 element type");
+@@ -856,7 +856,7 @@
+ }
+ 
+ // This implementation is based on Cephes.
+-static Value materializeErfApproximationF64(ConversionPatternRewriter& rewriter,
++static Value materializeErfApproximationF64(ConversionPatternRewriter &rewriter,
+                                             Location loc, ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
+@@ -884,7 +884,7 @@
+ }
+ 
+ static Value materializeErfcApproximationF64(
+-    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {
++    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
+          "expect f64 element type");
+@@ -916,7 +916,7 @@
+ // argument and derive the final approximation for all |x| >= 1.
+ // This implementation is based on Cephes.
+ static Value materializeErfcApproximationF32ForMagnitudeGeOne(
+-    OpBuilder& rewriter, Location loc, ValueRange args) {
++    OpBuilder &rewriter, Location loc, ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
+          "expect f32 element type");
+@@ -982,7 +982,7 @@
+ // Precondition is |x| <= 1. Use erfc approximation, otherwise.
+ // This implementation is based on Cephes.
+ static Value materializeErfApproximationF32ForMagnitudeLeOne(
+-    OpBuilder& rewriter, Location loc, ValueRange args) {
++    OpBuilder &rewriter, Location loc, ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
+          "expect f32 element type");
+@@ -1001,7 +1001,7 @@
+ }
+ 
+ // This is the same approximation as used in Eigen.
+-static Value materializeErfApproximationF32(OpBuilder& rewriter, Location loc,
++static Value materializeErfApproximationF32(OpBuilder &rewriter, Location loc,
+                                             ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
+@@ -1038,7 +1038,7 @@
+                                                    erf, ubErf);
+ }
+ 
+-static Value materializeErfcApproximationF32(OpBuilder& rewriter, Location loc,
++static Value materializeErfcApproximationF32(OpBuilder &rewriter, Location loc,
+                                              ValueRange args) {
+   Value x = args.front();
+   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
+@@ -1070,7 +1070,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::ErfOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Location loc = op.getLoc();
+     Value x = adaptor.getOperand();
+     Type ty = cast<ShapedType>(x.getType()).getElementType();
+@@ -1098,7 +1098,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::ErfcOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Location loc = op.getLoc();
+     Value x = adaptor.getOperand();
+     Type ty = cast<ShapedType>(x.getType()).getElementType();
+@@ -1121,7 +1121,7 @@
+   }
+ };
+ 
+-static Value erfInv32(OpBuilder& b, Location loc, ValueRange args) {
++static Value erfInv32(OpBuilder &b, Location loc, ValueRange args) {
+   constexpr int kDegree = 9;
+   constexpr std::array<float, 9> wLessThan5Constants = {
+       2.81022636e-08f,  3.43273939e-07f, -3.5233877e-06f,
+@@ -1178,7 +1178,7 @@
+       result);
+ }
+ 
+-static Value erfInv64(ConversionPatternRewriter& b, Location loc,
++static Value erfInv64(ConversionPatternRewriter &b, Location loc,
+                       ValueRange args) {
+   constexpr std::array<double, 23> wLessThan625Constants = {
+       -3.6444120640178196996e-21, -1.685059138182016589e-19,
+@@ -1298,7 +1298,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::ErfInvOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Location loc = op.getLoc();
+     if (op.getType().getElementType().isF64()) {
+       rewriter.replaceOp(op, erfInv64(rewriter, loc, adaptor.getOperands()));
+@@ -1338,7 +1338,7 @@
+ //   with   t(z) = z + kLanczosGamma + 1/2
+ //          a(z) = kBaseLanczosCoeff
+ //                   + sum(k = 1, n, kLanczosCoefficients[i] / (z + k))
+-Value materializeLgamma(OpBuilder& rewriter, Location loc, ValueRange args) {
++Value materializeLgamma(OpBuilder &rewriter, Location loc, ValueRange args) {
+   // If the input is less than 0.5 use Euler's reflection formula.
+   //   gamma(x) = pi / (sin(pi * x) * gamma(1 - x))
+   // Let z be
+@@ -1485,7 +1485,7 @@
+ // +/-89.4159851, due to rounding error when computing x +/- log(1/2).  The
+ // correct answer of 3.40281961e+38 (0x7f7fffec) is very close to max-float, so
+ // we deem this acceptable.
+-static Value materializeCoshApproximation(OpBuilder& rewriter, Location loc,
++static Value materializeCoshApproximation(OpBuilder &rewriter, Location loc,
+                                           ValueRange operands) {
+   mlir::chlo::CoshOp::Adaptor transformed(operands);
+   Value x = transformed.getOperand();
+@@ -1504,7 +1504,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::CoshOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     rewriter.replaceOp(
+         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),
+                                   rewriter.getF32Type(),
+@@ -1523,7 +1523,7 @@
+ //          a(z) = kBaseLanczosCoeff
+ //                   + sum(k = 1, n, kLanczosCoefficients[i] / (z + k))
+ //          a'(z) = - sum(k = 1, n, kLanczosCoefficients[i] / (z + k) / (z + k))
+-Value materializeDigamma(OpBuilder& rewriter, Location loc, ValueRange args) {
++Value materializeDigamma(OpBuilder &rewriter, Location loc, ValueRange args) {
+   // If the input is less than 0.5 use Euler's reflection formula.
+   //   digamma(x) = digamma(1 - x) - pi * cot(pi * x)
+   // Let z be
+@@ -1630,14 +1630,14 @@
+ 
+ namespace {
+ 
+-static Value getConstantLikeSmallestFiniteValue(OpBuilder& b, Location loc,
++static Value getConstantLikeSmallestFiniteValue(OpBuilder &b, Location loc,
+                                                 Value val) {
+   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
+   return getConstantLike(
+       b, loc, llvm::APFloat::getSmallest(ty.getFloatSemantics()), val);
+ }
+ 
+-static Value materializeZeta(OpBuilder& rewriter, Location loc,
++static Value materializeZeta(OpBuilder &rewriter, Location loc,
+                              ValueRange args) {
+   // Implementation ported from:
+   // https://github.com/openxla/xla/blob/7a067a7b88d2ffb15b1dc5e3c06f701a15f0391d/xla/client/lib/math.cc#L1912-L1917
+@@ -1790,7 +1790,7 @@
+ 
+ }  // namespace
+ 
+-Value materializePolygamma(OpBuilder& rewriter, Location loc, ValueRange args) {
++Value materializePolygamma(OpBuilder &rewriter, Location loc, ValueRange args) {
+   mlir::chlo::PolygammaOp::Adaptor transformed(args);
+   Value n = transformed.getN();
+   Value x = transformed.getX();
+@@ -1840,7 +1840,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::LgammaOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     FloatType minPrecisionTy = rewriter.getF32Type();
+     rewriter.replaceOp(
+         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),
+@@ -1854,7 +1854,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::DigammaOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     FloatType minPrecisionTy = rewriter.getF32Type();
+     rewriter.replaceOp(
+         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),
+@@ -1863,7 +1863,7 @@
+   }
+ };
+ 
+-static Value materializeNextAfter(ConversionPatternRewriter& rewriter,
++static Value materializeNextAfter(ConversionPatternRewriter &rewriter,
+                                   Location loc, ValueRange operands) {
+   mlir::chlo::NextAfterOp::Adaptor transformed(operands);
+   Value x = transformed.getX();
+@@ -1957,7 +1957,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::NextAfterOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     rewriter.replaceOp(
+         op, materializeNextAfter(rewriter, op.getLoc(), adaptor.getOperands()));
+     return success();
+@@ -1969,7 +1969,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::PolygammaOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Location loc = op.getLoc();
+     FloatType minPrecisionTy = rewriter.getF32Type();
+     rewriter.replaceOp(
+@@ -1989,7 +1989,7 @@
+ // +/-89.4159851, due to rounding error when computing x +/- log(1/2).  The
+ // correct answer of 3.40281961e+38 (0x7f7fffec) is very close to max-float, so
+ // we deem this acceptable.
+-static Value materializeSinhApproximationForLargeX(OpBuilder& rewriter,
++static Value materializeSinhApproximationForLargeX(OpBuilder &rewriter,
+                                                    Location loc,
+                                                    ValueRange operands) {
+   mlir::chlo::SinhOp::Adaptor transformed(operands);
+@@ -2007,7 +2007,7 @@
+ // Express `sinh` as
+ //   sinh(x) = (e^x - e^-x) / 2                     if |x| < 1
+ //           = e^(x + log(1/2)) - e^(-x + log(1/2)) otherwise.
+-static Value materializeSinhApproximation(OpBuilder& rewriter, Location loc,
++static Value materializeSinhApproximation(OpBuilder &rewriter, Location loc,
+                                           ValueRange operands) {
+   Value largeSinhResult =
+       materializeSinhApproximationForLargeX(rewriter, loc, operands);
+@@ -2043,7 +2043,7 @@
+ namespace {
+ 
+ ArrayAttr convertPrecisionConfig(mlir::ArrayAttr precisionConfig,
+-                                 ConversionPatternRewriter& rewriter) {
++                                 ConversionPatternRewriter &rewriter) {
+   std::vector<Attribute> precisions;
+   for (Attribute precision : precisionConfig.getValue()) {
+     switch (dyn_cast<mlir::chlo::PrecisionAttr>(precision).getValue()) {
+@@ -2077,7 +2077,7 @@
+ // In this implementation, the IR size increases by a factor of g. If this
+ // becomes a problem, we can try adding stablehlo.while to reduce the IR size.
+ LogicalResult handleRaggedDotMode1(mlir::chlo::RaggedDotOp op,
+-                                   ConversionPatternRewriter& rewriter) {
++                                   ConversionPatternRewriter &rewriter) {
+   Value lhs = op.getLhs();
+   Value rhs = op.getRhs();
+   chlo::RaggedDotDimensionNumbersAttr raggedDotDimensionNumbers =
+@@ -2231,7 +2231,7 @@
+ //   group_sizes : [g]
+ //   result : [g, b, m, n]
+ LogicalResult handleRaggedDotMode2(mlir::chlo::RaggedDotOp op,
+-                                   ConversionPatternRewriter& rewriter) {
++                                   ConversionPatternRewriter &rewriter) {
+   return failure();
+ }
+ 
+@@ -2241,7 +2241,7 @@
+ //   group_sizes : [g]
+ //   result : [b, m, n]
+ LogicalResult handleRaggedDotMode3(mlir::chlo::RaggedDotOp op,
+-                                   ConversionPatternRewriter& rewriter) {
++                                   ConversionPatternRewriter &rewriter) {
+   return failure();
+ }
+ 
+@@ -2254,7 +2254,7 @@
+   // dimension.
+   LogicalResult matchAndRewrite(
+       mlir::chlo::RaggedDotOp op, OpAdaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     if (op.getLhs().getType().getRank() < op.getRhs().getType().getRank()) {
+       return handleRaggedDotMode1(op, rewriter);
+     } else if (op.getLhs().getType().getRank() <
+@@ -2271,7 +2271,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::SinhOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Value x = adaptor.getOperand();
+     if (isa<ComplexType>(cast<ShapedType>(x.getType()).getElementType())) {
+       rewriter.replaceOp(op, materializeSinhApproximationForLargeX(
+@@ -2321,7 +2321,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::TopKOp op, OpAdaptor /*adaptor*/,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     auto operandType = dyn_cast<RankedTensorType>(op.getOperand().getType());
+     if (!operandType) return failure();
+     int64_t operandRank = operandType.getRank();
+@@ -2436,7 +2436,7 @@
+ 
+   LogicalResult matchAndRewrite(
+       mlir::chlo::ZetaOp op, OpAdaptor adaptor,
+-      ConversionPatternRewriter& rewriter) const override {
++      ConversionPatternRewriter &rewriter) const override {
+     Location loc = op.getLoc();
+     FloatType minPrecisionTy = rewriter.getF32Type();
+     rewriter.replaceOp(
+@@ -2452,7 +2452,7 @@
+ 
+ struct ChloLegalizeToStablehloPass final
+     : impl::ChloLegalizeToStablehloPassBase<ChloLegalizeToStablehloPass> {
+-  LogicalResult initialize(MLIRContext* context) override {
++  LogicalResult initialize(MLIRContext *context) override {
+     target = std::make_shared<ConversionTarget>(*context);
+     target->addIllegalDialect<chlo::ChloDialect>();
+     target->addLegalDialect<mlir::stablehlo::StablehloDialect,
+@@ -2482,8 +2482,8 @@
+ }  // namespace
+ 
+ namespace {
+-static void populateChloBroadcastingPatterns(MLIRContext* context,
+-                                             RewritePatternSet* patterns) {
++static void populateChloBroadcastingPatterns(MLIRContext *context,
++                                             RewritePatternSet *patterns) {
+   // Instantiate conversion templates for conforming binary elementwise ops
+   // that do not have different dtypes between operands and results and do
+   // not have special attributes that need to be preserved.
+@@ -2496,8 +2496,8 @@
+   patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);
+ }
+ 
+-static void populateChloDecompositionPatterns(MLIRContext* context,
+-                                              RewritePatternSet* patterns) {
++static void populateChloDecompositionPatterns(MLIRContext *context,
++                                              RewritePatternSet *patterns) {
+   populateWithGenerated(*patterns);
+   patterns
+       ->add<ConvertConstantOp, ConvertBesselI1eOp, ConvertCoshOp,
+@@ -2508,8 +2508,8 @@
+ }
+ }  // namespace
+ 
+-void populateChloToStablehloPatterns(MLIRContext* context,
+-                                     RewritePatternSet* patterns) {
++void populateChloToStablehloPatterns(MLIRContext *context,
++                                     RewritePatternSet *patterns) {
+   populateChloBroadcastingPatterns(context, patterns);
+   populateChloDecompositionPatterns(context, patterns);
+ }
 diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
 --- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
 +++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -361,10 +1560,71 @@ diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehl
                                << "\n  curr=" << key.toString()
                                << "\n  prev=" << prevKey.toString();
    }
+diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.td b/stablehlo/stablehlo/transforms/optimization/Passes.td
+--- stablehlo/stablehlo/transforms/optimization/Passes.td
++++ stablehlo/stablehlo/transforms/optimization/Passes.td
+@@ -23,14 +23,14 @@
+          "explicit MLIR `MemoryEffects`. Notably, this means `func.call` ops "
+          "will be assumed pure.">,
+   Option<"foldOpElementLimit", "fold-op-element-limit", "int64_t",
+-         /*default=*/"1",
++         /*default=*/"65536",
+          "Folding an op into a constant can sometimes come at the cost of "
+          "memory overhead. (This occurs if the op's inputs are reused, meaning "
+          "that they can't be deleted after the op is folded to a constant, or "
+-         "when folding operations like `iota` whose outputs take up more "
++         "when folding operations like `concat` whose outputs take up more "
+          "memory than their inputs.) In such cases, this config option sets an "
+          "upper limit on how many elements an op's result may have before the "
+-         "op is no longer folded.">,
++         "op is no longer folded. Splat folds are exempt from this limit.">,
+   Option<"optimizeFloat", "optimize-float", "bool", /*default=*/"true",
+          "Allow float optimizations that, though mathematically equivalent, "
+          "may result in slightly different quantization of floating-point "
 diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
 --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
 +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
-@@ -530,10 +530,15 @@
+@@ -74,12 +74,39 @@
+ 
+ static constexpr StablehloAggressiveFolderPassOptions kDefaultOptions;
+ 
++APSInt getAPSInt(Type type, uint64_t value) {
++  unsigned numBits;
++  bool isUnsigned;
++  if (auto integerType = dyn_cast<IntegerType>(type)) {
++    numBits = integerType.getWidth();
++    // Signless types are treated as signed, per StableHLO convention.
++    isUnsigned = integerType.isUnsignedInteger();
++  } else {
++    llvm::report_fatal_error("expected integer type");
++  }
++  return APSInt(
++      {/*numBits=*/numBits, value, /*isSigned=*/false, /*implicitTrunc=*/true},
++      /*isUnsigned=*/isUnsigned);
++}
++
+ template <typename T>
+ APSInt getAPSInt(unsigned bitWidth, T value, bool isSigned) {
+   return APSInt({/*numBits=*/bitWidth, static_cast<uint64_t>(value),
+                  /*isSigned=*/isSigned,
+                  /*implicitTrunc=*/true},
+                 /*isUnsigned=*/!isSigned);
++}
++
++APFloat getAPFloat(
++    Type type, double value,
++    llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {
++  auto floatType = dyn_cast<FloatType>(type);
++  if (!floatType) llvm::report_fatal_error("expected float type");
++
++  APFloat result(value);
++  bool unusedLosesInfo = false;
++  result.convert(floatType.getFloatSemantics(), roundingMode, &unusedLosesInfo);
++  return result;
+ }
+ 
+ LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,
+@@ -530,10 +557,15 @@
    using FoldOpRewritePattern<OpType>::matchAndRewrite;
    using FoldOpRewritePattern<OpType>::options;
  
@@ -382,7 +1642,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
        return success();
      return rewriter.notifyMatchFailure(op, "skipping fold of shape op dtype");
    }
-@@ -605,7 +610,8 @@
+@@ -605,7 +637,8 @@
                                  PatternRewriter& rewriter) const override {
      auto resultType = op.getType();
      if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
@@ -392,7 +1652,7 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
        return failure();
  
      SplatElementsAttr cstAttr;
-@@ -1104,7 +1110,7 @@
+@@ -1104,7 +1137,7 @@
          failed(validateShapeFoldDtype(rewriter, op, resultType)))
        return failure();
  
@@ -401,13 +1661,98 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFold
      if (!matchPattern(op.getOperand(), m_Constant(&attr)))
        return rewriter.notifyMatchFailure(op, "expected constant operand");
      rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));
+@@ -1256,21 +1289,48 @@
+       return rewriter.notifyMatchFailure(
+           op, "expected operand with static ranked tensor type");
+ 
+-    ElementsAttr els;
++    DenseElementsAttr els;
+     if (!matchPattern(operand, m_Constant(&els)))
+       return rewriter.notifyMatchFailure(
+           op, "expected constant integer or float operand");
+ 
++    // Short circuit on splat resizes
++    if (els.isSplat()) {
++      rewriter.replaceOpWithNewOp<ConstantOp>(op, els.resizeSplat(resultType));
++      return success();
++    }
++
+     DenseElementsAttr resAttr;
+-    if (auto data = els.tryGetValues<APInt>())
++    if (auto data = els.tryGetValues<APInt>(); succeeded(data))
+       resAttr = sliceType(op, *data);
+-    else if (auto data = els.tryGetValues<APFloat>())
++    else if (auto data = els.tryGetValues<APFloat>(); succeeded(data))
+       resAttr = sliceType(op, *data);
+     else
+       return rewriter.notifyMatchFailure(op.getLoc(),
+                                          "unsupported element type");
+ 
+     rewriter.replaceOpWithNewOp<ConstantOp>(op, resAttr);
++    return success();
++  }
++};
++
++// Pattern: dynamic_slice(splat_cst, start, end) -> resized_splat_cst
++struct FoldDynamicSliceOpPattern : public FoldOpRewritePattern<DynamicSliceOp> {
++  using FoldOpRewritePattern::FoldOpRewritePattern;
++
++  LogicalResult matchAndRewrite(DynamicSliceOp op,
++                                PatternRewriter& rewriter) const override {
++    auto resultType = op.getType();
++    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
++      return failure();
++
++    SplatElementsAttr inputSplatAttr;
++    if (!matchPattern(op.getOperand(), m_Constant(&inputSplatAttr)) ||
++        !inputSplatAttr)
++      return rewriter.notifyMatchFailure(op, "Input must be a splat constant.");
++
++    rewriter.replaceOpWithNewOp<ConstantOp>(
++        op, inputSplatAttr.resizeSplat(resultType));
+     return success();
+   }
+ };
+@@ -1482,6 +1542,14 @@
+       rewriter.replaceOpWithNewOp<ConstantOp>(
+           op, DenseIntElementsAttr::get(resultType, values));
+       return success();
++    }
++
++    // TODO: Support more iota folding, but doing so currently causes OOMs,
++    // so this pattern needs to be enabled more carefully.
++    if (outputSize != 1) {
++      return rewriter.notifyMatchFailure(
++          op, "expected output size to be 1, but got: " +
++                  std::to_string(outputSize));
+     }
+ 
+     int64_t sequences = 1;
+@@ -1881,6 +1949,7 @@
+   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);
+   patterns->add<FoldConvertOpPattern>(context, options, benefit);
+   patterns->add<FoldDivOpPattern>(context, options, benefit);
++  patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);
+   patterns->add<FoldGetDimensionSizeOpPattern>(context, options, benefit);
+   patterns->add<FoldMaxOpPattern>(context, options, benefit);
+   patterns->add<FoldMinOpPattern>(context, options, benefit);
 diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
 --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
 +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
-@@ -1309,6 +1309,17 @@
+@@ -331,7 +331,7 @@
+ DenseI64ArrayAttr getInvertedBroadcastDimensions(OpBuilder& b,
+                                                  ArrayRef<int64_t> dims) {
+   SmallVector<int64_t> permutation(dims.size());
+-  for (size_t i = 0; i < dims.size(); ++i) {
++  for (auto i = 0; i < dims.size(); ++i) {
+     permutation[dims[i]] = i;
+   }
+   return b.getDenseI64ArrayAttr(permutation);
+@@ -1308,6 +1308,17 @@
+ //////////////////////////////////
  // TransposeOp
  /////////////////////////////////
- 
++
 +DenseI64ArrayAttr getMergedTransposePermutation(OpBuilder& b,
 +                                                ArrayRef<int64_t> childPerm,
 +                                                ArrayRef<int64_t> parentPerm) {
@@ -418,10 +1763,9 @@ diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimp
 +  }
 +  return b.getDenseI64ArrayAttr(mergedPerm);
 +}
-+
+ 
  // Pattern: transpose(X, [no_mem_layout_change...]) -> reshape(X)
  struct TransposeIsReshape final : SimplifyOpRewritePattern<TransposeOp> {
-   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
 --- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
 +++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
diff --git a/third_party/stablehlo/workspace.bzl b/third_party/stablehlo/workspace.bzl
index db43355..982a75c 100644
--- a/third_party/stablehlo/workspace.bzl
+++ b/third_party/stablehlo/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive", "tf_mirror_urls")
 
 def repo():
     #
-    STABLEHLO_COMMIT = "baaf7475f8925cb0c5f9580408b3c0385f888487"
-    STABLEHLO_SHA256 = "c4b96f94d9d4aaa8b2dc88104579aab662aa33d59b79e77a9b75c8e0af3d9461"
+    STABLEHLO_COMMIT = "0a4440a5c8de45c4f9649bf3eb4913bf3f97da0d"
+    STABLEHLO_SHA256 = "f1620aafc2b6d730e2ee9c33b35a59a2656a11eed10b1ef8049f175eb4fbdd9c"
     #
 
     tf_http_archive(
