diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
@@ -1,5 +1,6 @@
 // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg --split-input-file --canonicalize | FileCheck %s
 // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg="enable-primitive-ops=true" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-PRIMITIVE
+// RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg="capture-scalar-inputs=false" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-NO-CAPTURE
 
 // CHECK: #map = affine_map<(d0, d1) -> (d0, d1)>
 // CHECK-LABEL: func @float_add
@@ -534,6 +535,19 @@
   %0 = "stablehlo.sign"(%arg0) : (tensor<2x2xcomplex<f32>>)
                           -> tensor<2x2xcomplex<f32>>
   func.return %0 : tensor<2x2xcomplex<f32>>
+}
+
+// -----
+
+// CHECK-LABEL: func @float_tan
+// CHECK-PRIMITIVE-LABEL: func @float_tan
+func.func @float_tan(%arg0: tensor<2x2xf32>) -> tensor<2x2xf32> {
+  // CHECK: linalg.generic
+  // CHECK: tan
+  // CHECK-PRIMITIVE: linalg.map
+  // CHECK-PRIMITIVE: tan
+  %0 = "stablehlo.tan"(%arg0) : (tensor<2x2xf32>) -> tensor<2x2xf32>
+  func.return %0 : tensor<2x2xf32>
 }
 
 // -----
@@ -926,6 +940,23 @@
 // CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {
 // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32
 // CHECK-PRIMITIVE:        linalg.yield %[[RES]]
+
+// CHECK-NO-CAPTURE:      #[[SCALAR_MAP:.*]] = affine_map<(d0, d1) -> ()>
+// CHECK-NO-CAPTURE:      #[[ID_MAP:.*]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-NO-CAPTURE:      func @select_scalar_pred_dyn
+// CHECK-NO-CAPTURE-SAME:  (%[[PRED:.*]]: tensor<i1>, %[[LHS:.*]]: tensor<2x?xf32>, %[[RHS:.*]]: tensor<2x?xf32>)
+// CHECK-NO-CAPTURE-DAG:  %[[C1:.*]] = arith.constant 1
+// CHECK-NO-CAPTURE-DAG:  %[[DIM:.*]] =  tensor.dim %[[LHS]], %[[C1]]
+// CHECK-NO-CAPTURE-DAG:  %[[DST:.*]] = tensor.empty(%[[DIM]])
+// CHECK-NO-CAPTURE:      linalg.generic
+// CHECK-NO-CAPTURE-SAME:   indexing_maps = [#[[SCALAR_MAP]], #[[ID_MAP]], #[[ID_MAP]], #[[ID_MAP]]]
+// CHECK-NO-CAPTURE-SAME:   iterator_types = ["parallel", "parallel"]
+// CHECK-NO-CAPTURE-SAME:   ins(%[[PRED]], %[[LHS]], %[[RHS]] : tensor<i1>, tensor<2x?xf32>, tensor<2x?xf32>)
+// CHECK-NO-CAPTURE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)
+// CHECK-NO-CAPTURE-SAME:   {someattr}
+// CHECK-NO-CAPTURE:      ^bb0(%[[PRED_:.*]]: i1, %[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %{{.*}}: f32):
+// CHECK-NO-CAPTURE:        %[[RES:.*]] = arith.select %[[PRED_]], %[[LHS_]], %[[RHS_]] : f32
+// CHECK-NO-CAPTURE:        linalg.yield %[[RES]]
 
 // -----
 
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
@@ -140,12 +140,11 @@
   // (any sign-op, or an integral abs-op).
   // TODO(peiming, ajcbik): these all can potentially be optimized by applying
   // value transform on sparse_tenosr.value memref
-  if (isa<mlir::stablehlo::SignOp>(op) || isa<mlir::stablehlo::NegOp>(op) ||
+  if (isa<mlir::stablehlo::SignOp, mlir::stablehlo::NegOp,
+          mlir::stablehlo::TanOp>(op) ||
       (isa<mlir::stablehlo::AbsOp>(op) && hasIntegralShapeType(op)) ||
-      isa<chlo::AsinOp>(op) || isa<chlo::AsinhOp>(op) ||
-      isa<chlo::AtanOp>(op) || isa<chlo::AtanhOp>(op) ||
-      isa<chlo::BesselI1eOp>(op) || isa<chlo::SinhOp>(op) ||
-      isa<chlo::TanOp>(op)) {
+      isa<chlo::AsinOp, chlo::AsinhOp, chlo::AtanOp, chlo::AtanhOp,
+          chlo::BesselI1eOp, chlo::SinhOp, chlo::TanOp>(op)) {
     if (!sparse_tensor::getSparseTensorEncoding(op->getResult(0).getType()) &&
         !sparse_tensor::getSparseTensorEncoding(op->getOperand(0).getType()))
       return Value();
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h b/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
--- stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
+++ stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
@@ -153,14 +153,11 @@
   using FOp = ::mlir::math::SinOp;
   using COp = ::mlir::complex::SinOp;
 };
-// FIXME(Jakub)
-/*
 template <>
 struct StablehloToScalarOp<stablehlo::TanOp> {
   using FOp = ::mlir::math::TanOp;
   using COp = ::mlir::complex::TanOp;
 };
-*/
 template <>
 struct StablehloToScalarOp<stablehlo::Atan2Op> {
   using FOp = ::mlir::math::Atan2Op;
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td b/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
--- stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
+++ stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
@@ -39,7 +39,11 @@
                  Option<"enableSparseOps", "enable-sparse-ops", "bool",
                         /*default=*/"false",
                         "Lower to Sparse Tensor ops (sparse_tensor.concatenate)"
-                        "when possible, instead of linalg.generic">];
+                        "when possible, instead of linalg.generic">,
+                 Option<"captureScalarInputs", "capture-scalar-inputs", "bool",
+                        /*default=*/"true",
+                        "Capture scalar inputs in generic ops instead of"
+                        "passing as tensor-scalar argument.">];
 }
 
 #endif  // STABLEHLO_TO_LINALG_PASSES
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h b/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
--- stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
+++ stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
@@ -26,11 +26,12 @@
 //===----------------------------------------------------------------------===//
 
 /// Populates the patterns that convert from StableHLO to Linalg on tensors.
-void populateStablehloToLinalgConversionPatterns(MLIRContext *context,
-                                                 TypeConverter &typeConverter,
-                                                 RewritePatternSet *patterns,
+void populateStablehloToLinalgConversionPatterns(MLIRContext* context,
+                                                 TypeConverter& typeConverter,
+                                                 RewritePatternSet* patterns,
                                                  bool enablePrimitiveOps,
-                                                 bool enableSparseOps);
+                                                 bool enableSparseOps,
+                                                 bool captureScalarInputs);
 
 //===----------------------------------------------------------------------===//
 // Fine-grained patterns used by the implementation.
@@ -39,8 +40,9 @@
 /// Populates the patterns that convert from elementwise StableHLO ops to Linalg
 /// on tensors.
 void populatePointwiseStablehloToLinalgConversionPatterns(
-    MLIRContext *context, TypeConverter &typeConverter,
-    RewritePatternSet *patterns, bool enablePrimitiveOps);
+    MLIRContext* context, TypeConverter& typeConverter,
+    RewritePatternSet* patterns, bool enablePrimitiveOps,
+    bool captureScalarInputs);
 
 /// Populates the patterns that convert from convolution StableHLO ops to Linalg
 /// on tensors.
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
@@ -2634,7 +2634,8 @@
 
     RewritePatternSet patterns_(context);
     populateStablehloToLinalgConversionPatterns(
-        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps);
+        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps,
+        captureScalarInputs);
     patterns = std::move(patterns_);
 
     return success();
@@ -2657,7 +2658,8 @@
                                                  TypeConverter& typeConverter,
                                                  RewritePatternSet* patterns,
                                                  bool enablePrimitiveOps,
-                                                 bool enableSparseOps) {
+                                                 bool enableSparseOps,
+                                                 bool captureScalarInputs) {
   // clang-format off
   patterns->add<ConcatenateConverter>(typeConverter, context,
                                       enablePrimitiveOps);
@@ -2680,7 +2682,8 @@
       >(typeConverter, context);
 
   detail::populatePointwiseStablehloToLinalgConversionPatterns(
-      context, typeConverter, patterns, enablePrimitiveOps);
+      context, typeConverter, patterns, enablePrimitiveOps,
+      captureScalarInputs);
 
   if (enableSparseOps) {
     patterns->add<SparseConcatenateConverter>(typeConverter, context);
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
@@ -145,6 +145,7 @@
       ScalarHloToArithmeticPattern<mlir::stablehlo::SineOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::SqrtOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::SubtractOp>,
+      ScalarHloToArithmeticPattern<mlir::stablehlo::TanOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::TanhOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::XorOp>>(typeConverter,
                                                             context, filterFn);
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
@@ -23,6 +23,7 @@
 
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/Debug.h"
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/AffineMap.h"
@@ -43,6 +44,8 @@
 #include "stablehlo/conversions/linalg/transforms/Rewriters.h"
 #include "stablehlo/dialect/StablehloOps.h"
 
+#define DEBUG_TYPE "stablehlo-conversions"
+
 namespace mlir::stablehlo {
 namespace {
 int64_t getRank(Value v) { return cast<ShapedType>(v.getType()).getRank(); }
@@ -142,6 +145,11 @@
 struct PointwiseToLinalgMapConverter : OpConversionPattern<OpTy> {
   using OpConversionPattern<OpTy>::OpConversionPattern;
   using OpAdaptor = typename OpTy::Adaptor;
+
+  PointwiseToLinalgMapConverter(TypeConverter& typeConverter,
+                                MLIRContext* context, bool captureScalarInputs)
+      : OpConversionPattern<OpTy>(typeConverter, context),
+        captureScalarInputs(captureScalarInputs) {}
 
   virtual FailureOr<Operation *> createLinalgOp(
       OpTy &op, ConversionPatternRewriter &rewriter,
@@ -190,8 +198,11 @@
             rewriter, loc, cast<TypedValue<ShapedType>>(input),
             cast<ShapedType>(emptyTensor.getType())));
         scalarInputs.push_back(nullptr);
+      } else if (captureScalarInputs) {
+        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));
       } else {
-        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));
+        mappedInputs.push_back(input);
+        scalarInputs.push_back(nullptr);
       }
     }
 
@@ -202,6 +213,8 @@
     rewriter.replaceOp(op, (*mapOp)->getResults());
     return success();
   }
+
+  bool captureScalarInputs;
 };
 
 /// Converts a HLO operation to a linalg.generic op that contains the
@@ -211,12 +224,12 @@
   using PointwiseToLinalgMapConverter<OpTy>::PointwiseToLinalgMapConverter;
   using OpAdaptor = typename OpTy::Adaptor;
 
-  FailureOr<Operation *> createLinalgOp(OpTy &op,
-                                        ConversionPatternRewriter &rewriter,
-                                        ArrayRef<Value> mappedInputs,
-                                        ArrayRef<Value> scalarVals,
-                                        Value emptyTensor,
-                                        int64_t maxRank) const override {
+  FailureOr<Operation*> createLinalgOp(OpTy& op,
+                                       ConversionPatternRewriter& rewriter,
+                                       ArrayRef<Value> mappedInputs,
+                                       ArrayRef<Value> scalarVals,
+                                       Value emptyTensor,
+                                       int64_t maxRank) const override {
     // Create indexing maps.
     AffineMap scalarMap = AffineMap::get(maxRank, 0, rewriter.getContext());
     AffineMap idMap = rewriter.getMultiDimIdentityMap(maxRank);
@@ -225,10 +238,10 @@
       maps.push_back(isScalar(v) ? scalarMap : idMap);
     maps.push_back(idMap);
     bool failed = false;
-    Operation *linalgOp = rewriter.create<linalg::GenericOp>(
+    Operation* linalgOp = rewriter.create<linalg::GenericOp>(
         op.getLoc(), emptyTensor.getType(), mappedInputs, emptyTensor, maps,
         getNParallelLoopsAttrs(maxRank),
-        [&](OpBuilder &nestedBuilder, Location /*nested_loc*/,
+        [&](OpBuilder& nestedBuilder, Location /*nested_loc*/,
             ValueRange args) {
           Type innerResultTy = getElementTypeOrSelf(emptyTensor);
           auto argvec =
@@ -253,8 +266,9 @@
 
 namespace detail {
 void populatePointwiseStablehloToLinalgConversionPatterns(
-    MLIRContext *context, TypeConverter &typeConverter,
-    RewritePatternSet *patterns, bool enablePrimitiveOps) {
+    MLIRContext* context, TypeConverter& typeConverter,
+    RewritePatternSet* patterns, bool enablePrimitiveOps,
+    bool captureScalarInputs) {
   if (enablePrimitiveOps) {
     patterns->add<
         PointwiseToLinalgMapConverter<mlir::stablehlo::AbsOp>,
@@ -301,12 +315,12 @@
         PointwiseToLinalgMapConverter<mlir::stablehlo::SineOp>,
         PointwiseToLinalgMapConverter<mlir::stablehlo::SqrtOp>,
         PointwiseToLinalgMapConverter<mlir::stablehlo::SubtractOp>,
+        PointwiseToLinalgMapConverter<mlir::stablehlo::TanOp>,
         PointwiseToLinalgMapConverter<mlir::stablehlo::TanhOp>,
-        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(typeConverter,
-                                                               context);
+        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(
+        typeConverter, context, captureScalarInputs);
     return;
   }
-
   patterns
       ->add<PointwiseToLinalgConverter<mlir::stablehlo::AbsOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::AddOp>,
@@ -352,9 +366,10 @@
             PointwiseToLinalgConverter<mlir::stablehlo::SineOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::SqrtOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::SubtractOp>,
+            PointwiseToLinalgConverter<mlir::stablehlo::TanOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::TanhOp>,
-            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(typeConverter,
-                                                                context);
+            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(
+          typeConverter, context, captureScalarInputs);
 }
 }  // namespace detail
 }  // namespace mlir::stablehlo
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
@@ -40,7 +40,7 @@
 
 namespace {
 
-Value buildRescaleMultiplier(bool scale32, OpBuilder& builder, Location loc,
+Value buildRescaleMultiplier(bool scale32, OpBuilder &builder, Location loc,
                              ArrayRef<int32_t> multipliers) {
   if (scale32) {
     return tosa::getConstTensorInt<int32_t>(builder, loc, multipliers);
@@ -51,7 +51,7 @@
 }
 
 // create a tosa rescale op and return its result value
-Value buildRescale(PatternRewriter& rewriter, Location loc,
+Value buildRescale(PatternRewriter &rewriter, Location loc,
                    ShapedType outputType, Value inputVal, int32_t multiplier,
                    int32_t shift, int64_t inputZp, int64_t outputZp,
                    bool doubleRound, bool scale32, bool perChannel) {
@@ -85,7 +85,7 @@
 }
 
 // Creates TOSA rescale op with int32 output
-Value buildRescaleToInt32(PatternRewriter& rewriter, Location loc,
+Value buildRescaleToInt32(PatternRewriter &rewriter, Location loc,
                           Value inputVal, double inputScale, int64_t inputZp) {
   auto inputType = cast<ShapedType>(inputVal.getType());
   auto outputType = inputType.clone(rewriter.getI32Type());
@@ -103,7 +103,7 @@
 }
 
 // Creates TOSA rescale op with int32 input
-Value buildRescaleFromInt32(PatternRewriter& rewriter, Location loc,
+Value buildRescaleFromInt32(PatternRewriter &rewriter, Location loc,
                             ShapedType outputType, Value inputVal,
                             double outputScale, int64_t outputZp) {
   // Input should be int32 type
@@ -124,14 +124,14 @@
 }
 
 using UnaryRescaleScalesFn =
-    void (*)(const quant::UniformQuantizedType& operandQType,
-             const quant::UniformQuantizedType& resultQType,
-             double& operandRescaleScale, double& resultRescaleScale);
-
-void GetUnaryRescaleScales(const quant::UniformQuantizedType& operandQType,
-                           const quant::UniformQuantizedType& resultQType,
-                           double& operandRescaleScale,
-                           double& resultRescaleScale) {
+    void (*)(const quant::UniformQuantizedType &operandQType,
+             const quant::UniformQuantizedType &resultQType,
+             double &operandRescaleScale, double &resultRescaleScale);
+
+void GetUnaryRescaleScales(const quant::UniformQuantizedType &operandQType,
+                           const quant::UniformQuantizedType &resultQType,
+                           double &operandRescaleScale,
+                           double &resultRescaleScale) {
   double operandScale = operandQType.getScale();
   double resultScale = resultQType.getScale();
 
@@ -145,7 +145,7 @@
 
 template <typename StablehloOp>
 LogicalResult matchAndRewriteUnaryOp(
-    StablehloOp op, PatternRewriter& rewriter,
+    StablehloOp op, PatternRewriter &rewriter,
     UnaryRescaleScalesFn rescaleScalesFn = GetUnaryRescaleScales) {
   Value operand = op.getOperand();
   Value result = op.getResult();
@@ -190,21 +190,21 @@
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::AbsOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteUnaryOp(op, rewriter);
 }
 
 using BinaryRescaleScalesFn = void (*)(
-    const quant::UniformQuantizedType& lhsQType,
-    const quant::UniformQuantizedType& rhsQType,
-    const quant::UniformQuantizedType& resultQType, double& lhsRescaleScale,
-    double& rhsRescaleScale, double& resultRescaleScale);
-
-void GetAddSubRescaleScales(const quant::UniformQuantizedType& lhsQType,
-                            const quant::UniformQuantizedType& rhsQType,
-                            const quant::UniformQuantizedType& resultQType,
-                            double& lhsRescaleScale, double& rhsRescaleScale,
-                            double& resultRescaleScale) {
+    const quant::UniformQuantizedType &lhsQType,
+    const quant::UniformQuantizedType &rhsQType,
+    const quant::UniformQuantizedType &resultQType, double &lhsRescaleScale,
+    double &rhsRescaleScale, double &resultRescaleScale);
+
+void GetAddSubRescaleScales(const quant::UniformQuantizedType &lhsQType,
+                            const quant::UniformQuantizedType &rhsQType,
+                            const quant::UniformQuantizedType &resultQType,
+                            double &lhsRescaleScale, double &rhsRescaleScale,
+                            double &resultRescaleScale) {
   // 1. Rescale inputs to scale = 2.0 x max(lhs.scale, rhs.scale)
   // 2. Extra left shift to input to increase precision
   // Where input_shift = 20 if input is 8-bit
@@ -230,11 +230,11 @@
       maxScale2x / (resultScale * static_cast<double>(1 << inputShift));
 }
 
-void GetMulDivRescaleScales(const quant::UniformQuantizedType& lhsQType,
-                            const quant::UniformQuantizedType& rhsQType,
-                            const quant::UniformQuantizedType& resultQType,
-                            double& lhsRescaleScale, double& rhsRescaleScale,
-                            double& resultRescaleScale) {
+void GetMulDivRescaleScales(const quant::UniformQuantizedType &lhsQType,
+                            const quant::UniformQuantizedType &rhsQType,
+                            const quant::UniformQuantizedType &resultQType,
+                            double &lhsRescaleScale, double &rhsRescaleScale,
+                            double &resultRescaleScale) {
   double lhsScale = lhsQType.getScale();
   double rhsScale = rhsQType.getScale();
   double resultScale = resultQType.getScale();
@@ -248,11 +248,11 @@
   resultRescaleScale = lhsScale * rhsScale / resultScale;
 }
 
-void GetMinMaxRescaleScales(const quant::UniformQuantizedType& lhsQType,
-                            const quant::UniformQuantizedType& rhsQType,
-                            const quant::UniformQuantizedType& resultQType,
-                            double& lhsRescaleScale, double& rhsRescaleScale,
-                            double& resultRescaleScale) {
+void GetMinMaxRescaleScales(const quant::UniformQuantizedType &lhsQType,
+                            const quant::UniformQuantizedType &rhsQType,
+                            const quant::UniformQuantizedType &resultQType,
+                            double &lhsRescaleScale, double &rhsRescaleScale,
+                            double &resultRescaleScale) {
   // 1. Rescale inputs to scale = max(lhs.scale, rhs.scale)
   // 2. Extra left shift to input to increase precision
   // Where input_shift = 20 if input is 8-bit
@@ -280,7 +280,7 @@
 }
 
 template <typename StablehloOp>
-LogicalResult matchAndRewriteBinaryOp(StablehloOp op, PatternRewriter& rewriter,
+LogicalResult matchAndRewriteBinaryOp(StablehloOp op, PatternRewriter &rewriter,
                                       BinaryRescaleScalesFn rescaleScalesFn) {
   Value lhs = op.getLhs();
   Value rhs = op.getRhs();
@@ -339,37 +339,37 @@
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::AddOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteBinaryOp(op, rewriter, GetAddSubRescaleScales);
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::SubtractOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteBinaryOp(op, rewriter, GetAddSubRescaleScales);
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::MulOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteBinaryOp(op, rewriter, GetMulDivRescaleScales);
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::DivOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteBinaryOp(op, rewriter, GetMulDivRescaleScales);
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::MinOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteBinaryOp(op, rewriter, GetMinMaxRescaleScales);
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::MaxOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteBinaryOp(op, rewriter, GetMinMaxRescaleScales);
 }
 
 LogicalResult matchAndRewriteCompareOp(stablehlo::CompareOp op,
-                                       PatternRewriter& rewriter) {
+                                       PatternRewriter &rewriter) {
   Value lhs = op.getLhs();
   Value rhs = op.getRhs();
   Value result = op.getResult();
@@ -429,7 +429,7 @@
 }
 
 LogicalResult matchAndRewriteOp(stablehlo::CompareOp op,
-                                PatternRewriter& rewriter) {
+                                PatternRewriter &rewriter) {
   return matchAndRewriteCompareOp(op, rewriter);
 }
 
@@ -438,7 +438,7 @@
     : public OpRewritePattern<StablehloOpType> {
   using OpRewritePattern<StablehloOpType>::OpRewritePattern;
   LogicalResult matchAndRewrite(StablehloOpType op,
-                                PatternRewriter& rewriter) const override {
+                                PatternRewriter &rewriter) const override {
     return matchAndRewriteOp(op, rewriter);
   }
 };
@@ -446,7 +446,7 @@
 struct StablehloQuantLegalizeToTosaRescalePass
     : impl::StablehloQuantLegalizeToTosaRescalePassBase<
           StablehloQuantLegalizeToTosaRescalePass> {
-  LogicalResult initialize(MLIRContext* ctx) override {
+  LogicalResult initialize(MLIRContext *ctx) override {
     RewritePatternSet patternList(ctx);
     populateStablehloQuantLegalizeToTosaRescalePatterns(&patternList, ctx);
     patterns = std::move(patternList);
@@ -468,7 +468,7 @@
 }  // namespace
 
 void populateStablehloQuantLegalizeToTosaRescalePatterns(
-    RewritePatternSet* patterns, MLIRContext* context) {
+    RewritePatternSet *patterns, MLIRContext *context) {
   // unary ops
   patterns->addWithLabel<QuantizedStablehloOpConversion<stablehlo::AbsOp>>(
       {"StablehloQuantAbsOp"}, context);
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -3164,6 +3164,7 @@
 using mlir::hlo::printVariadicOperandWithAttribute;
 using mlir::hlo::printVariadicSameOperandsAndResultType;
 
+using mlir::stablehlo::TokenType;
 #define GET_OP_CLASSES
 #include "stablehlo/dialect/StablehloOps.cpp.inc"
 
diff --ruN a/stablehlo/stablehlo/integrations/c/VhloDialect.h b/stablehlo/stablehlo/integrations/c/VhloDialect.h
--- stablehlo/stablehlo/integrations/c/VhloDialect.h
+++ stablehlo/stablehlo/integrations/c/VhloDialect.h
@@ -13,7 +13,7 @@
 #ifndef STABLEHLO_INTEGRATIONS_C_VHLO_DIALECT_H
 #define STABLEHLO_INTEGRATIONS_C_VHLO_DIALECT_H
 
-#include "mlir-c/RegisterEverything.h"
+#include "mlir-c/IR.h"
 
 #ifdef __cplusplus
 extern "C" {
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
@@ -110,6 +110,43 @@
   }
 }
 
+bool IsBoolean(ElementType elementType) {
+  MLIRContext ctx;
+  return getElementType(ctx, elementType).isInteger(1);
+}
+
+bool IsComplex(ElementType elementType) {
+  MLIRContext ctx;
+  auto type = dyn_cast<ComplexType>(getElementType(ctx, elementType));
+  return !!type;
+}
+
+bool IsFloat(ElementType elementType) {
+  MLIRContext ctx;
+  return getElementType(ctx, elementType).isFloat();
+}
+
+bool IsInteger(ElementType elementType, bool includeBool = false) {
+  MLIRContext ctx;
+  Type type = getElementType(ctx, elementType);
+  return type.isInteger() && (includeBool || !IsBoolean(elementType));
+}
+
+bool IsSignedInteger(ElementType elementType) {
+  MLIRContext ctx;
+  Type type = getElementType(ctx, elementType);
+
+  // Note that this is not the same as `type.isSignedInteger()`. Signed integers
+  // are not used in StableHLO.
+  return type.isSignlessInteger() && !IsBoolean(elementType);
+}
+
+bool IsUnsignedInteger(ElementType elementType) {
+  MLIRContext ctx;
+  return getElementType(ctx, elementType).isUnsignedInteger() &&
+         !IsBoolean(elementType);
+}
+
 RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,
                                 ElementType elementType) {
   return makeTensorType(ctx, shape, getElementType(ctx, elementType));
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
@@ -18,7 +18,6 @@
 
 #include <complex>
 #include <cstdint>
-#include <source_location>
 #include <type_traits>
 #include <vector>
 
@@ -68,6 +67,20 @@
   // clang-format on
 };
 
+bool IsBoolean(ElementType elementType);
+
+bool IsComplex(ElementType elementType);
+
+bool IsFloat(ElementType elementType);
+
+bool IsInteger(ElementType elementType, bool includeBool);
+
+// In StableHLO, we refer to signed integer as the MLIR's equivalent signless
+// integer. StableHLO does not have a notion of signless integers like MLIR.
+bool IsSignedInteger(ElementType elementType);
+
+bool IsUnsignedInteger(ElementType elementType);
+
 Type getElementType(MLIRContext& ctx, ElementType elementType);
 
 // Build a ranked tensor type with an element type of ElementType.
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
@@ -20,7 +20,7 @@
 #include <utility>
 #include <vector>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "llvm/ADT/DenseMap.h"
 #include "mlir/IR/BuiltinTypeInterfaces.h"
 #include "mlir/IR/BuiltinTypes.h"
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
@@ -15,7 +15,7 @@
 
 #include <string>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "llvm/Support/raw_ostream.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
 #include "mlir/IR/BuiltinOps.h"
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
@@ -29,9 +29,35 @@
 #include "stablehlo/dialect/TypeInference.h"
 #include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
 #include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
+#include "third_party/llvm/llvm-project/mlir/include/mlir/IR/Attributes.h"
 
 namespace mlir {
 namespace stablehlo {
+
+///////////////
+// Dialect Helpers
+///////////////
+
+MlirOp AttachFrontendAttribute(MlirBuilder& builder, MlirOp op, StringRef name,
+                               Attribute value) {
+  constexpr char kFrontendAttrName[] = "mhlo.frontend_attributes";
+  Operation* mlirOp = unwrap(op).getDefiningOp();
+  SmallVector<NamedAttribute> attrs;
+  DictionaryAttr frontendAttr =
+      mlirOp->getAttrOfType<DictionaryAttr>(kFrontendAttrName);
+  if (frontendAttr) {
+    for (NamedAttribute attr : frontendAttr.getValue()) {
+      // Populate all non-conflicting names.
+      if (attr.getName() != name) {
+        attrs.push_back(attr);
+      }
+    }
+  }
+  attrs.emplace_back(name, value);
+  mlirOp->setAttr(kFrontendAttrName,
+                  DictionaryAttr::get(&builder.getContext(), attrs));
+  return op;
+}
 
 /////////////////
 // MANUAL APIs
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
@@ -27,9 +27,22 @@
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
 #include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
+#include "third_party/llvm/llvm-project/mlir/include/mlir/IR/Attributes.h"
 
 namespace mlir {
 namespace stablehlo {
+
+///////////////
+// Dialect Helpers
+///////////////
+
+// Appends or overwrites an entry in the `mhlo.frontend_attributes` attribute
+//
+// of the given op.
+// Ex:
+//   stablehlo.abs %0 { mhlo.frontend_attributes = { "foo" = 123 } }
+MlirOp AttachFrontendAttribute(MlirBuilder& builder, MlirOp op, StringRef name,
+                               Attribute value);
 
 /////////////////
 // MANUAL APIs
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
@@ -17,7 +17,7 @@
 #include <cstdint>
 #include <string>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/DialectRegistry.h"
@@ -1592,5 +1592,57 @@
   EXPECT_EQ(expected, debugString(*module));
 }
 
+TEST(MlirBuilderTest, FrontendAttributesAppend) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+    %0 = stablehlo.exponential %arg0 {mhlo.frontend_attributes = {bar = "hello", foo = 123 : i32}} : tensor<2xf32>
+    return %0 : tensor<2xf32>
+  }
+})mlir";
+
+  StablehloModuleBuilder mb;
+  {
+    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
+    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
+    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
+    auto arg0 = func::Argument(fb, type);
+    auto exp = Exp(arg0);
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "foo", fb.getOpBuilder().getI32IntegerAttr(123));
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "bar", fb.getOpBuilder().getStringAttr("hello"));
+    func::Return(fb, {exp});
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_EQ(expected, debugString(*module));
+}
+
+TEST(MlirBuilderTest, FrontendAttributesOverwrite) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+    %0 = stablehlo.exponential %arg0 {mhlo.frontend_attributes = {foo = 456 : i32}} : tensor<2xf32>
+    return %0 : tensor<2xf32>
+  }
+})mlir";
+
+  StablehloModuleBuilder mb;
+  {
+    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
+    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
+    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
+    auto arg0 = func::Argument(fb, type);
+    auto exp = Exp(arg0);
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "foo", fb.getOpBuilder().getI32IntegerAttr(123));
+    stablehlo::AttachFrontendAttribute(
+        fb, exp, "foo", fb.getOpBuilder().getI32IntegerAttr(456));
+    func::Return(fb, {exp});
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_EQ(expected, debugString(*module));
+}
+
 }  // namespace stablehlo
 }  // namespace mlir
diff --ruN a/stablehlo/stablehlo/reference/InterpreterOps.cpp b/stablehlo/stablehlo/reference/InterpreterOps.cpp
--- stablehlo/stablehlo/reference/InterpreterOps.cpp
+++ stablehlo/stablehlo/reference/InterpreterOps.cpp
@@ -46,6 +46,7 @@
 #include "stablehlo/reference/ProcessGrid.h"
 #include "stablehlo/reference/Value.h"
 
+using mlir::stablehlo::TokenType;
 #define GET_OP_CLASSES
 #include "stablehlo/reference/InterpreterOps.cpp.inc"
 
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -22,21 +22,24 @@
 // CHECK-LABEL: func.func @broadcast_in_dim_fold_splat
 // CHECK-SAME:   ([[ARG0:%.+]]: tensor<3x3xi32>)
 func.func @broadcast_in_dim_fold_splat(%arg0: tensor<3x3xi32>)
-  -> (tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>) {
+  -> (tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>) {
   %c0 = stablehlo.constant dense<5> : tensor<i32>
   %c1 = stablehlo.constant dense<3.0> : tensor<f32>
-  %c2 = stablehlo.constant dense<1> : tensor<1x3xi32>
+  %c2 = stablehlo.constant dense<(1.0,2.0)> : tensor<complex<f32>>
+  %c3 = stablehlo.constant dense<1> : tensor<1x3xi32>
 
   %0 = stablehlo.broadcast_in_dim %c0, dims = [] : (tensor<i32>) -> tensor<6xi32>
   %1 = stablehlo.broadcast_in_dim %c1, dims = [] : (tensor<f32>) -> tensor<3xf32>
-  %2 = stablehlo.broadcast_in_dim %c2, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>
+  %2 = stablehlo.broadcast_in_dim %c2, dims = [] : (tensor<complex<f32>>) -> tensor<5xcomplex<f32>>
+  %3 = stablehlo.broadcast_in_dim %c3, dims = [1, 0] : (tensor<1x3xi32>) -> tensor<3x3xi32>
 
   // CHECK-DAG:  [[R0:%.+]] = stablehlo.constant dense<5> : tensor<6xi32>
   // CHECK-DAG:  [[R1:%.+]] = stablehlo.constant dense<3.000000e+00> : tensor<3xf32>
-  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>
-
-  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]]
-  return %0, %1, %2 : tensor<6xi32>, tensor<3xf32>, tensor<3x3xi32>
+  // CHECK-DAG:  [[R2:%.+]] = stablehlo.constant dense<(1.0{{.*}},2.0{{.*}})> : tensor<5xcomplex<f32>>
+  // CHECK-DAG:  [[R3:%.+]] = stablehlo.constant dense<1> : tensor<3x3xi32>
+
+  // CHECK-NEXT: return [[R0]], [[R1]], [[R2]], [[R3]]
+  return %0, %1, %2, %3 : tensor<6xi32>, tensor<3xf32>, tensor<5xcomplex<f32>>, tensor<3x3xi32>
 }
 
 // -----
@@ -529,28 +532,15 @@
 // IotaOp
 
 // CHECK-LABEL: func @eval_iota
-func.func @eval_iota() -> (tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {
-  // CHECK-NOT: stablehlo.iota
-  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<
-  // CHECK-SAME: {{\[\[}}[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],
-  // CHECK-SAME: {{\[}}[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],
-  // CHECK-SAME: {{\[}}[2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2]]]> : tensor<3x4x5xi32>
-
-  // CHECK: [[RESULT1:%.*]] = stablehlo.constant dense<
-  // CHECK-SAME: {{\[\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
-  // CHECK-SAME: {{\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
-  // CHECK-SAME: {{\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]> : tensor<3x4x5xi32>
-
-  // CHECK: [[RESULT2:%.*]] = stablehlo.constant dense<
-  // CHECK-SAME: {{\[\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],
-  // CHECK-SAME: {{\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],
-  // CHECk-SAME: {{\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]]> : tensor<3x4x5xi32>
-
+func.func @eval_iota() -> (tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {
+  // CHECK:      [[RESULT0:%.*]] = stablehlo.constant dense<0> : tensor<1xi32>
+  // CHECK-NEXT: [[RESULT1:%.*]] = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
+  // CHECK-NEXT: [[RESULT2:%.*]] = stablehlo.iota dim = 2 : tensor<3x4x5xi32>
   // CHECK: return [[RESULT0]], [[RESULT1]], [[RESULT2]]
-  %0 = stablehlo.iota dim = 0 : tensor<3x4x5xi32>
+  %0 = stablehlo.iota dim = 0 : tensor<1xi32>
   %1 = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
   %2 = stablehlo.iota dim = 2 : tensor<3x4x5xi32>
-  func.return %0, %1, %2 : tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>
+  func.return %0, %1, %2 : tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>
 }
 
 // -----
@@ -596,6 +586,37 @@
   // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\[\[1, 2\], \[3, 4\]\]}}> : tensor<2x2xi32>
   // CHECK-NEXT: return [[CST1]], [[CST2]]
   return %0, %1 : tensor<1xi32>, tensor<2x2xi32>
+}
+
+// -----
+
+////////
+// SliceOp / DynamicSliceOp
+
+// CHECK-LABEL: @slice_fold
+func.func @slice_fold(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {
+  %c = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5]]> : tensor<6x1xi32>
+  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>
+  // CHECK: stablehlo.constant dense<2> : tensor<1x1xi32>
+  return %0 : tensor<1x1xi32>
+}
+
+// CHECK-LABEL: @slice_fold_splat
+func.func @slice_fold_splat(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {
+  %c = stablehlo.constant dense<1> : tensor<6x1xi32>
+  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>
+  // CHECK: stablehlo.constant dense<1> : tensor<1x1xi32>
+  return %0 : tensor<1x1xi32>
+}
+
+// CHECK-LABEL: @dynamic_slice_fold
+func.func @dynamic_slice_fold(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<1x1xi32> {
+  %0 = stablehlo.constant dense<256> : tensor<6x1xi32>
+  %1 = "stablehlo.dynamic_slice"(%0, %arg0, %arg1) <{slice_sizes = array<i64: 1, 1>}> : (tensor<6x1xi32>, tensor<i32>, tensor<i32>) -> tensor<1x1xi32>
+
+  // CHECK: %[[RESULT:.*]] = stablehlo.constant dense<256> : tensor<1x1xi32>
+  // CHECK: return %[[RESULT]]
+  return %1 : tensor<1x1xi32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -132,6 +132,35 @@
 
   // CHECK-NEXT: return [[R0]], [[R5]]
   return %0, %5 : tensor<1x3x6xi32>, tensor<3x6x1xi32>
+}
+
+// CHECK-LABEL: func.func @broadcast_in_dim_prefer_nested_reshape
+// CHECK-SAME:   ([[ARG0:%[^ ]+]]: tensor<3x4xi32>)
+func.func @broadcast_in_dim_prefer_nested_reshape(%arg0: tensor<3x4xi32>) -> (tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>) {
+  // When `broadcast_in_dim(broadcast_in_dim(x))` could be optimized into either
+  // `broadcast_in_dim(reshape(x))` or `broadcast_in_dim(x)`, we want to select
+  // the former pattern.
+  //
+  // (We accomplish this by blocking the merge-composition pattern if the inner
+  // op can be replaced with a `reshape`. Simply adding benefit to the
+  // replace-with-reshape pattern isn't sufficient here because the outermost
+  // op, which only matches the merge-composition pattern, is traversed first.)
+
+  // CHECK-DAG: [[INNER_RESHAPE:%[^ ]+]] = stablehlo.reshape [[ARG0]] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>
+  // CHECK-DAG: [[BROADCAST_OF_RESHAPE:%[^ ]+]] = stablehlo.broadcast_in_dim [[INNER_RESHAPE]], dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>
+  %0 = stablehlo.broadcast_in_dim %arg0, dims = [0, 2] : (tensor<3x4xi32>) -> tensor<3x1x4xi32>
+  %1 = stablehlo.broadcast_in_dim %0, dims = [1, 0, 2] : (tensor<3x1x4xi32>) -> tensor<2x3x4x3xi32>
+
+  // When the inner op doesn't qualify for replacement with a `reshape` op,
+  // however (particularly when it meets some conditions but not others), ensure
+  // that we allow the merge-composition pattern to match.
+
+  // CHECK-DAG: [[MERGED_BROADCAST:%[^ ]+]] = stablehlo.broadcast_in_dim [[ARG0]], dims = [3, 2] : (tensor<3x4xi32>) -> tensor<2x3x4x3xi32>
+  %2 = stablehlo.broadcast_in_dim %arg0, dims = [2, 1] : (tensor<3x4xi32>) -> tensor<1x4x3xi32>
+  %3 = stablehlo.broadcast_in_dim %2, dims = [0, 2, 3] : (tensor<1x4x3xi32>) -> tensor<2x3x4x3xi32>
+
+  // CHECK-DAG: return [[BROADCAST_OF_RESHAPE]], [[MERGED_BROADCAST]]
+  return %1, %3 : tensor<2x3x4x3xi32>, tensor<2x3x4x3xi32>
 }
 
 // CHECK-LABEL: func.func @broadcast_in_dim_not_identity_broadcasts
@@ -1021,6 +1050,18 @@
   // CHECK-NOT: stablehlo.pad
   %1 = stablehlo.pad %arg0, %0, low = [0, 0], high = [0, 0], interior = [0, 0] : (tensor<256x1024xbf16>, tensor<bf16>) -> tensor<256x1024xbf16>
   return %1 : tensor<256x1024xbf16>
+}
+
+// We don't want to delete `pad` ops that move a tensor's values around without
+// affecting its dimensions.
+//
+// CHECK-LABEL: @pad_rotate_tensor_no_dim_change
+func.func @pad_rotate_tensor_no_dim_change(%arg0: tensor<50x50xf32>) -> tensor<50x50xf32> {
+  // CHECK: %[[RES:.+]] = stablehlo.pad
+  // CHECK: return %[[RES]]
+  %cst = stablehlo.constant dense<0.0> : tensor<f32>
+  %0 = stablehlo.pad %arg0, %cst, low = [0, -1], high = [0, 1], interior = [0, 0] : (tensor<50x50xf32>, tensor<f32>) -> tensor<50x50xf32>
+  return %0 : tensor<50x50xf32>
 }
 
 // -----
@@ -1810,6 +1851,15 @@
   return %0 : tensor<2x4x1x5xf32>
 }
 
+// CHECK-LABEL: @transpose_of_transpose
+func.func @transpose_of_transpose(%arg0 : tensor<1x2x3x4xf32>) -> tensor<1x2x3x4xf32> {
+  %0 = stablehlo.transpose %arg0, dims = [3,2,1,0] : (tensor<1x2x3x4xf32>) -> tensor<4x3x2x1xf32>
+  %1 = stablehlo.transpose %0, dims = [3,2,1,0] : (tensor<4x3x2x1xf32>) -> tensor<1x2x3x4xf32>
+  // CHECK-NOT: stablehlo.transpose
+  // CHECK: return %arg0
+  return %1 : tensor<1x2x3x4xf32>
+}
+
 // -----
 
 ////////
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
@@ -752,7 +752,7 @@
     %2 = call @refine_call_callee(%arg0_different_i32, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %2 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
@@ -770,7 +770,7 @@
     %2 = call @refine_call_callee(%arg0_different, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %2 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
@@ -789,7 +789,7 @@
     %4 = call @refine_call_callee(%arg0_new, %3) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %4 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
@@ -73,7 +73,7 @@
 template <typename FromOpTy, typename ToOpTy>
 struct HloNaryElementwiseAdaptor {
   static ToOpTy createOp(FromOpTy fromOp, Type resultType,
-                         ValueRange broadcastedOperands, OpBuilder& builder) {
+                         ValueRange broadcastedOperands, OpBuilder &builder) {
     return builder.create<ToOpTy>(fromOp.getLoc(), resultType,
                                   broadcastedOperands);
   }
@@ -118,7 +118,7 @@
 struct HloCompareAdaptor {
   static mlir::stablehlo::CompareOp createOp(
       mlir::chlo::BroadcastCompareOp fromOp, Type resultType,
-      ValueRange broadcastedOperands, OpBuilder& builder) {
+      ValueRange broadcastedOperands, OpBuilder &builder) {
     auto chloDirection = fromOp.getComparisonDirection();
     auto hloDirection = toStableHloComparisonDirection(chloDirection);
     if (!hloDirection) return nullptr;
@@ -140,9 +140,9 @@
 // to take a ChloOpTy, NonBroadcastingOpTy, and an Adaptor as templated values.
 template <template <typename, typename, typename> typename Pattern,
           typename... ConstructorArgs>
-static void populateForBroadcastingBinaryOp(MLIRContext* context,
-                                            RewritePatternSet* patterns,
-                                            ConstructorArgs&&... args) {
+static void populateForBroadcastingBinaryOp(MLIRContext *context,
+                                            RewritePatternSet *patterns,
+                                            ConstructorArgs &&...args) {
 #define POPULATE_BCAST(ChloOp, HloOp)                                          \
   patterns                                                                     \
       ->add<Pattern<ChloOp, HloOp, HloNaryElementwiseAdaptor<ChloOp, HloOp>>>( \
@@ -179,21 +179,21 @@
       context, args...);
 }
 
-static Value getConstantLikeMaxFiniteValue(OpBuilder& b, Location loc,
+static Value getConstantLikeMaxFiniteValue(OpBuilder &b, Location loc,
                                            Value val) {
   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
   return getConstantLike(
       b, loc, llvm::APFloat::getLargest(ty.getFloatSemantics()), val);
 }
 
-static Value getConstantLikeInfValue(OpBuilder& b, Location loc, Value val,
+static Value getConstantLikeInfValue(OpBuilder &b, Location loc, Value val,
                                      bool negative) {
   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
   return getConstantLike(
       b, loc, llvm::APFloat::getInf(ty.getFloatSemantics(), negative), val);
 }
 
-static Value getConstantLikeSmallestNormalizedValue(OpBuilder& b, Location loc,
+static Value getConstantLikeSmallestNormalizedValue(OpBuilder &b, Location loc,
                                                     Value val) {
   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
   return getConstantLike(
@@ -239,7 +239,7 @@
 
   LogicalResult matchAndRewrite(
       ChloOpTy op, typename ChloOpTy::Adaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     // Only rewrite for statically determinable non-broadcasting cases.
     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());
     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());
@@ -329,7 +329,7 @@
 
   LogicalResult matchAndRewrite(
       ChloOpTy op, typename ChloOpTy::Adaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     // Only support ranked operands.
     Value lhs = adaptor.getLhs();
     Value rhs = adaptor.getRhs();
@@ -413,7 +413,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::ConstantLikeOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     auto resultTy = cast<ShapedType>(op.getType());
 
     // Unranked uses are not supported.
@@ -445,7 +445,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::BroadcastSelectOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     // Only support ranked operands.
     Value pred = adaptor.getPred();
     Value onTrue = adaptor.getOnTrue();
@@ -533,7 +533,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::ConstantOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, op.getValue());
     return success();
   }
@@ -541,7 +541,7 @@
 
 template <typename FTy>
 static Value materializeChebyshevPolynomialApproximation(
-    OpBuilder& rewriter, Location loc, Value x, ArrayRef<FTy> coefficients) {
+    OpBuilder &rewriter, Location loc, Value x, ArrayRef<FTy> coefficients) {
   Value b0 = getConstantLike(rewriter, loc, 0.0, x);
   Value b1 = getConstantLike(rewriter, loc, 0.0, x);
   Value b2 = getConstantLike(rewriter, loc, 0.0, x);
@@ -561,7 +561,7 @@
 }
 
 template <typename FTy>
-static Value materializeBesselI1eApproximation(OpBuilder& rewriter,
+static Value materializeBesselI1eApproximation(OpBuilder &rewriter,
                                                Location loc, Value x,
                                                ArrayRef<FTy> kI1eCoeffsA,
                                                ArrayRef<FTy> kI1eCoeffsB) {
@@ -594,7 +594,7 @@
       loc, rewriter.create<mlir::stablehlo::SignOp>(loc, x), select);
 }
 
-Value materializeBesselI1eApproximationF32(OpBuilder& rewriter, Location loc,
+Value materializeBesselI1eApproximationF32(OpBuilder &rewriter, Location loc,
                                            ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
@@ -620,7 +620,7 @@
                                                   kI1eCoeffsB);
 }
 
-static Value materializeBesselI1eApproximationF64(OpBuilder& rewriter,
+static Value materializeBesselI1eApproximationF64(OpBuilder &rewriter,
                                                   Location loc,
                                                   ValueRange args) {
   Value x = args.front();
@@ -663,10 +663,10 @@
                                                    kI1eCoeffsA, kI1eCoeffsB);
 }
 
-static Value materializeWithUpcast(ConversionPatternRewriter& rewriter,
+static Value materializeWithUpcast(ConversionPatternRewriter &rewriter,
                                    Location loc, ValueRange args,
                                    FloatType minPrecisionTy,
-                                   Value callback(OpBuilder&, Location,
+                                   Value callback(OpBuilder &, Location,
                                                   ValueRange)) {
   Type originalTy = getElementTypeOrSelf(args.front().getType());
   auto floatOriginalTy = dyn_cast<FloatType>(originalTy);
@@ -699,7 +699,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::BesselI1eOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Location loc = op.getLoc();
     Value x = adaptor.getOperand();
     Type ty = cast<ShapedType>(x.getType()).getElementType();
@@ -725,7 +725,7 @@
 };
 
 template <typename FTy>
-static Value materializePolynomialApproximation(OpBuilder& rewriter,
+static Value materializePolynomialApproximation(OpBuilder &rewriter,
                                                 Location loc, Value x,
                                                 ArrayRef<FTy> coefficients) {
   if (coefficients.empty()) return getConstantLike(rewriter, loc, 0.0, x);
@@ -746,7 +746,7 @@
 // argument and derive the final approximation for all |x| >= 1.
 // This implementation is based on Cephes.
 static Value materializeErfcApproximationF64ForMagnituteGeOne(
-    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {
+    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
          "expect f64 element type");
@@ -831,7 +831,7 @@
 // Precondition is |x| <= 1. Use erfc approximation, otherwise.
 // This implementation is based on Cephes.
 static Value materializeErfApproximationF64ForMagnituteLeOne(
-    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {
+    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
          "expect f64 element type");
@@ -856,7 +856,7 @@
 }
 
 // This implementation is based on Cephes.
-static Value materializeErfApproximationF64(ConversionPatternRewriter& rewriter,
+static Value materializeErfApproximationF64(ConversionPatternRewriter &rewriter,
                                             Location loc, ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
@@ -884,7 +884,7 @@
 }
 
 static Value materializeErfcApproximationF64(
-    ConversionPatternRewriter& rewriter, Location loc, ValueRange args) {
+    ConversionPatternRewriter &rewriter, Location loc, ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF64() &&
          "expect f64 element type");
@@ -916,7 +916,7 @@
 // argument and derive the final approximation for all |x| >= 1.
 // This implementation is based on Cephes.
 static Value materializeErfcApproximationF32ForMagnitudeGeOne(
-    OpBuilder& rewriter, Location loc, ValueRange args) {
+    OpBuilder &rewriter, Location loc, ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
          "expect f32 element type");
@@ -982,7 +982,7 @@
 // Precondition is |x| <= 1. Use erfc approximation, otherwise.
 // This implementation is based on Cephes.
 static Value materializeErfApproximationF32ForMagnitudeLeOne(
-    OpBuilder& rewriter, Location loc, ValueRange args) {
+    OpBuilder &rewriter, Location loc, ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
          "expect f32 element type");
@@ -1001,7 +1001,7 @@
 }
 
 // This is the same approximation as used in Eigen.
-static Value materializeErfApproximationF32(OpBuilder& rewriter, Location loc,
+static Value materializeErfApproximationF32(OpBuilder &rewriter, Location loc,
                                             ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
@@ -1038,7 +1038,7 @@
                                                    erf, ubErf);
 }
 
-static Value materializeErfcApproximationF32(OpBuilder& rewriter, Location loc,
+static Value materializeErfcApproximationF32(OpBuilder &rewriter, Location loc,
                                              ValueRange args) {
   Value x = args.front();
   assert(cast<ShapedType>(x.getType()).getElementType().isF32() &&
@@ -1070,7 +1070,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::ErfOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Location loc = op.getLoc();
     Value x = adaptor.getOperand();
     Type ty = cast<ShapedType>(x.getType()).getElementType();
@@ -1098,7 +1098,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::ErfcOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Location loc = op.getLoc();
     Value x = adaptor.getOperand();
     Type ty = cast<ShapedType>(x.getType()).getElementType();
@@ -1121,7 +1121,7 @@
   }
 };
 
-static Value erfInv32(OpBuilder& b, Location loc, ValueRange args) {
+static Value erfInv32(OpBuilder &b, Location loc, ValueRange args) {
   constexpr int kDegree = 9;
   constexpr std::array<float, 9> wLessThan5Constants = {
       2.81022636e-08f,  3.43273939e-07f, -3.5233877e-06f,
@@ -1178,7 +1178,7 @@
       result);
 }
 
-static Value erfInv64(ConversionPatternRewriter& b, Location loc,
+static Value erfInv64(ConversionPatternRewriter &b, Location loc,
                       ValueRange args) {
   constexpr std::array<double, 23> wLessThan625Constants = {
       -3.6444120640178196996e-21, -1.685059138182016589e-19,
@@ -1298,7 +1298,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::ErfInvOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Location loc = op.getLoc();
     if (op.getType().getElementType().isF64()) {
       rewriter.replaceOp(op, erfInv64(rewriter, loc, adaptor.getOperands()));
@@ -1338,7 +1338,7 @@
 //   with   t(z) = z + kLanczosGamma + 1/2
 //          a(z) = kBaseLanczosCoeff
 //                   + sum(k = 1, n, kLanczosCoefficients[i] / (z + k))
-Value materializeLgamma(OpBuilder& rewriter, Location loc, ValueRange args) {
+Value materializeLgamma(OpBuilder &rewriter, Location loc, ValueRange args) {
   // If the input is less than 0.5 use Euler's reflection formula.
   //   gamma(x) = pi / (sin(pi * x) * gamma(1 - x))
   // Let z be
@@ -1485,7 +1485,7 @@
 // +/-89.4159851, due to rounding error when computing x +/- log(1/2).  The
 // correct answer of 3.40281961e+38 (0x7f7fffec) is very close to max-float, so
 // we deem this acceptable.
-static Value materializeCoshApproximation(OpBuilder& rewriter, Location loc,
+static Value materializeCoshApproximation(OpBuilder &rewriter, Location loc,
                                           ValueRange operands) {
   mlir::chlo::CoshOp::Adaptor transformed(operands);
   Value x = transformed.getOperand();
@@ -1504,7 +1504,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::CoshOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     rewriter.replaceOp(
         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),
                                   rewriter.getF32Type(),
@@ -1523,7 +1523,7 @@
 //          a(z) = kBaseLanczosCoeff
 //                   + sum(k = 1, n, kLanczosCoefficients[i] / (z + k))
 //          a'(z) = - sum(k = 1, n, kLanczosCoefficients[i] / (z + k) / (z + k))
-Value materializeDigamma(OpBuilder& rewriter, Location loc, ValueRange args) {
+Value materializeDigamma(OpBuilder &rewriter, Location loc, ValueRange args) {
   // If the input is less than 0.5 use Euler's reflection formula.
   //   digamma(x) = digamma(1 - x) - pi * cot(pi * x)
   // Let z be
@@ -1630,14 +1630,14 @@
 
 namespace {
 
-static Value getConstantLikeSmallestFiniteValue(OpBuilder& b, Location loc,
+static Value getConstantLikeSmallestFiniteValue(OpBuilder &b, Location loc,
                                                 Value val) {
   auto ty = cast<FloatType>(getElementTypeOrSelf(val.getType()));
   return getConstantLike(
       b, loc, llvm::APFloat::getSmallest(ty.getFloatSemantics()), val);
 }
 
-static Value materializeZeta(OpBuilder& rewriter, Location loc,
+static Value materializeZeta(OpBuilder &rewriter, Location loc,
                              ValueRange args) {
   // Implementation ported from:
   // https://github.com/openxla/xla/blob/7a067a7b88d2ffb15b1dc5e3c06f701a15f0391d/xla/client/lib/math.cc#L1912-L1917
@@ -1790,7 +1790,7 @@
 
 }  // namespace
 
-Value materializePolygamma(OpBuilder& rewriter, Location loc, ValueRange args) {
+Value materializePolygamma(OpBuilder &rewriter, Location loc, ValueRange args) {
   mlir::chlo::PolygammaOp::Adaptor transformed(args);
   Value n = transformed.getN();
   Value x = transformed.getX();
@@ -1840,7 +1840,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::LgammaOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     FloatType minPrecisionTy = rewriter.getF32Type();
     rewriter.replaceOp(
         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),
@@ -1854,7 +1854,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::DigammaOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     FloatType minPrecisionTy = rewriter.getF32Type();
     rewriter.replaceOp(
         op, materializeWithUpcast(rewriter, op.getLoc(), adaptor.getOperands(),
@@ -1863,7 +1863,7 @@
   }
 };
 
-static Value materializeNextAfter(ConversionPatternRewriter& rewriter,
+static Value materializeNextAfter(ConversionPatternRewriter &rewriter,
                                   Location loc, ValueRange operands) {
   mlir::chlo::NextAfterOp::Adaptor transformed(operands);
   Value x = transformed.getX();
@@ -1957,7 +1957,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::NextAfterOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     rewriter.replaceOp(
         op, materializeNextAfter(rewriter, op.getLoc(), adaptor.getOperands()));
     return success();
@@ -1969,7 +1969,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::PolygammaOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Location loc = op.getLoc();
     FloatType minPrecisionTy = rewriter.getF32Type();
     rewriter.replaceOp(
@@ -1989,7 +1989,7 @@
 // +/-89.4159851, due to rounding error when computing x +/- log(1/2).  The
 // correct answer of 3.40281961e+38 (0x7f7fffec) is very close to max-float, so
 // we deem this acceptable.
-static Value materializeSinhApproximationForLargeX(OpBuilder& rewriter,
+static Value materializeSinhApproximationForLargeX(OpBuilder &rewriter,
                                                    Location loc,
                                                    ValueRange operands) {
   mlir::chlo::SinhOp::Adaptor transformed(operands);
@@ -2007,7 +2007,7 @@
 // Express `sinh` as
 //   sinh(x) = (e^x - e^-x) / 2                     if |x| < 1
 //           = e^(x + log(1/2)) - e^(-x + log(1/2)) otherwise.
-static Value materializeSinhApproximation(OpBuilder& rewriter, Location loc,
+static Value materializeSinhApproximation(OpBuilder &rewriter, Location loc,
                                           ValueRange operands) {
   Value largeSinhResult =
       materializeSinhApproximationForLargeX(rewriter, loc, operands);
@@ -2043,7 +2043,7 @@
 namespace {
 
 ArrayAttr convertPrecisionConfig(mlir::ArrayAttr precisionConfig,
-                                 ConversionPatternRewriter& rewriter) {
+                                 ConversionPatternRewriter &rewriter) {
   std::vector<Attribute> precisions;
   for (Attribute precision : precisionConfig.getValue()) {
     switch (dyn_cast<mlir::chlo::PrecisionAttr>(precision).getValue()) {
@@ -2077,7 +2077,7 @@
 // In this implementation, the IR size increases by a factor of g. If this
 // becomes a problem, we can try adding stablehlo.while to reduce the IR size.
 LogicalResult handleRaggedDotMode1(mlir::chlo::RaggedDotOp op,
-                                   ConversionPatternRewriter& rewriter) {
+                                   ConversionPatternRewriter &rewriter) {
   Value lhs = op.getLhs();
   Value rhs = op.getRhs();
   chlo::RaggedDotDimensionNumbersAttr raggedDotDimensionNumbers =
@@ -2231,7 +2231,7 @@
 //   group_sizes : [g]
 //   result : [g, b, m, n]
 LogicalResult handleRaggedDotMode2(mlir::chlo::RaggedDotOp op,
-                                   ConversionPatternRewriter& rewriter) {
+                                   ConversionPatternRewriter &rewriter) {
   return failure();
 }
 
@@ -2241,7 +2241,7 @@
 //   group_sizes : [g]
 //   result : [b, m, n]
 LogicalResult handleRaggedDotMode3(mlir::chlo::RaggedDotOp op,
-                                   ConversionPatternRewriter& rewriter) {
+                                   ConversionPatternRewriter &rewriter) {
   return failure();
 }
 
@@ -2254,7 +2254,7 @@
   // dimension.
   LogicalResult matchAndRewrite(
       mlir::chlo::RaggedDotOp op, OpAdaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     if (op.getLhs().getType().getRank() < op.getRhs().getType().getRank()) {
       return handleRaggedDotMode1(op, rewriter);
     } else if (op.getLhs().getType().getRank() <
@@ -2271,7 +2271,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::SinhOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Value x = adaptor.getOperand();
     if (isa<ComplexType>(cast<ShapedType>(x.getType()).getElementType())) {
       rewriter.replaceOp(op, materializeSinhApproximationForLargeX(
@@ -2321,7 +2321,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::TopKOp op, OpAdaptor /*adaptor*/,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     auto operandType = dyn_cast<RankedTensorType>(op.getOperand().getType());
     if (!operandType) return failure();
     int64_t operandRank = operandType.getRank();
@@ -2436,7 +2436,7 @@
 
   LogicalResult matchAndRewrite(
       mlir::chlo::ZetaOp op, OpAdaptor adaptor,
-      ConversionPatternRewriter& rewriter) const override {
+      ConversionPatternRewriter &rewriter) const override {
     Location loc = op.getLoc();
     FloatType minPrecisionTy = rewriter.getF32Type();
     rewriter.replaceOp(
@@ -2452,7 +2452,7 @@
 
 struct ChloLegalizeToStablehloPass final
     : impl::ChloLegalizeToStablehloPassBase<ChloLegalizeToStablehloPass> {
-  LogicalResult initialize(MLIRContext* context) override {
+  LogicalResult initialize(MLIRContext *context) override {
     target = std::make_shared<ConversionTarget>(*context);
     target->addIllegalDialect<chlo::ChloDialect>();
     target->addLegalDialect<mlir::stablehlo::StablehloDialect,
@@ -2482,8 +2482,8 @@
 }  // namespace
 
 namespace {
-static void populateChloBroadcastingPatterns(MLIRContext* context,
-                                             RewritePatternSet* patterns) {
+static void populateChloBroadcastingPatterns(MLIRContext *context,
+                                             RewritePatternSet *patterns) {
   // Instantiate conversion templates for conforming binary elementwise ops
   // that do not have different dtypes between operands and results and do
   // not have special attributes that need to be preserved.
@@ -2496,8 +2496,8 @@
   patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);
 }
 
-static void populateChloDecompositionPatterns(MLIRContext* context,
-                                              RewritePatternSet* patterns) {
+static void populateChloDecompositionPatterns(MLIRContext *context,
+                                              RewritePatternSet *patterns) {
   populateWithGenerated(*patterns);
   patterns
       ->add<ConvertConstantOp, ConvertBesselI1eOp, ConvertCoshOp,
@@ -2508,8 +2508,8 @@
 }
 }  // namespace
 
-void populateChloToStablehloPatterns(MLIRContext* context,
-                                     RewritePatternSet* patterns) {
+void populateChloToStablehloPatterns(MLIRContext *context,
+                                     RewritePatternSet *patterns) {
   populateChloBroadcastingPatterns(context, patterns);
   populateChloDecompositionPatterns(context, patterns);
 }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -461,7 +461,7 @@
   LogicalResult emitDifferentRefinementContextError(func::FuncOp func,
                                                     RefinementKey key,
                                                     RefinementKey prevKey) {
-    return func.emitOpError() << "refined with invompatible refinement keys:"
+    return func.emitOpError() << "refined with incompatible refinement keys:"
                               << "\n  curr=" << key.toString()
                               << "\n  prev=" << prevKey.toString();
   }
diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.td b/stablehlo/stablehlo/transforms/optimization/Passes.td
--- stablehlo/stablehlo/transforms/optimization/Passes.td
+++ stablehlo/stablehlo/transforms/optimization/Passes.td
@@ -23,14 +23,14 @@
          "explicit MLIR `MemoryEffects`. Notably, this means `func.call` ops "
          "will be assumed pure.">,
   Option<"foldOpElementLimit", "fold-op-element-limit", "int64_t",
-         /*default=*/"1",
+         /*default=*/"65536",
          "Folding an op into a constant can sometimes come at the cost of "
          "memory overhead. (This occurs if the op's inputs are reused, meaning "
          "that they can't be deleted after the op is folded to a constant, or "
-         "when folding operations like `iota` whose outputs take up more "
+         "when folding operations like `concat` whose outputs take up more "
          "memory than their inputs.) In such cases, this config option sets an "
          "upper limit on how many elements an op's result may have before the "
-         "op is no longer folded.">,
+         "op is no longer folded. Splat folds are exempt from this limit.">,
   Option<"optimizeFloat", "optimize-float", "bool", /*default=*/"true",
          "Allow float optimizations that, though mathematically equivalent, "
          "may result in slightly different quantization of floating-point "
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -74,12 +74,39 @@
 
 static constexpr StablehloAggressiveFolderPassOptions kDefaultOptions;
 
+APSInt getAPSInt(Type type, uint64_t value) {
+  unsigned numBits;
+  bool isUnsigned;
+  if (auto integerType = dyn_cast<IntegerType>(type)) {
+    numBits = integerType.getWidth();
+    // Signless types are treated as signed, per StableHLO convention.
+    isUnsigned = integerType.isUnsignedInteger();
+  } else {
+    llvm::report_fatal_error("expected integer type");
+  }
+  return APSInt(
+      {/*numBits=*/numBits, value, /*isSigned=*/false, /*implicitTrunc=*/true},
+      /*isUnsigned=*/isUnsigned);
+}
+
 template <typename T>
 APSInt getAPSInt(unsigned bitWidth, T value, bool isSigned) {
   return APSInt({/*numBits=*/bitWidth, static_cast<uint64_t>(value),
                  /*isSigned=*/isSigned,
                  /*implicitTrunc=*/true},
                 /*isUnsigned=*/!isSigned);
+}
+
+APFloat getAPFloat(
+    Type type, double value,
+    llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {
+  auto floatType = dyn_cast<FloatType>(type);
+  if (!floatType) llvm::report_fatal_error("expected float type");
+
+  APFloat result(value);
+  bool unusedLosesInfo = false;
+  result.convert(floatType.getFloatSemantics(), roundingMode, &unusedLosesInfo);
+  return result;
 }
 
 LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,
@@ -530,10 +557,15 @@
   using FoldOpRewritePattern<OpType>::matchAndRewrite;
   using FoldOpRewritePattern<OpType>::options;
 
+  // TODO: Generalize all relevant folder patterns to support complex data
+  // types, then hard-code `allowComplex` to `true`.
   LogicalResult validateShapeFoldDtype(PatternRewriter& rewriter, OpType op,
-                                       ShapedType resultType) const {
+                                       ShapedType resultType,
+                                       bool allowComplex = false) const {
     if (resultType.getElementType().isInteger()) return success();
-    if (options.optimizeFloat && isa<FloatType>(resultType.getElementType()))
+    if (options.optimizeFloat &&
+        (allowComplex ? isa<FloatType, ComplexType>(resultType.getElementType())
+                      : isa<FloatType>(resultType.getElementType())))
       return success();
     return rewriter.notifyMatchFailure(op, "skipping fold of shape op dtype");
   }
@@ -605,7 +637,8 @@
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
     if (failed(validateStaticShapeResult(rewriter, op, resultType)) ||
-        failed(validateShapeFoldDtype(rewriter, op, resultType)))
+        failed(validateShapeFoldDtype(rewriter, op, resultType,
+                                      /*allowComplex=*/true)))
       return failure();
 
     SplatElementsAttr cstAttr;
@@ -1104,7 +1137,7 @@
         failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
-    DenseIntOrFPElementsAttr attr;
+    DenseElementsAttr attr;
     if (!matchPattern(op.getOperand(), m_Constant(&attr)))
       return rewriter.notifyMatchFailure(op, "expected constant operand");
     rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(resultType));
@@ -1256,21 +1289,48 @@
       return rewriter.notifyMatchFailure(
           op, "expected operand with static ranked tensor type");
 
-    ElementsAttr els;
+    DenseElementsAttr els;
     if (!matchPattern(operand, m_Constant(&els)))
       return rewriter.notifyMatchFailure(
           op, "expected constant integer or float operand");
 
+    // Short circuit on splat resizes
+    if (els.isSplat()) {
+      rewriter.replaceOpWithNewOp<ConstantOp>(op, els.resizeSplat(resultType));
+      return success();
+    }
+
     DenseElementsAttr resAttr;
-    if (auto data = els.tryGetValues<APInt>())
+    if (auto data = els.tryGetValues<APInt>(); succeeded(data))
       resAttr = sliceType(op, *data);
-    else if (auto data = els.tryGetValues<APFloat>())
+    else if (auto data = els.tryGetValues<APFloat>(); succeeded(data))
       resAttr = sliceType(op, *data);
     else
       return rewriter.notifyMatchFailure(op.getLoc(),
                                          "unsupported element type");
 
     rewriter.replaceOpWithNewOp<ConstantOp>(op, resAttr);
+    return success();
+  }
+};
+
+// Pattern: dynamic_slice(splat_cst, start, end) -> resized_splat_cst
+struct FoldDynamicSliceOpPattern : public FoldOpRewritePattern<DynamicSliceOp> {
+  using FoldOpRewritePattern::FoldOpRewritePattern;
+
+  LogicalResult matchAndRewrite(DynamicSliceOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
+      return failure();
+
+    SplatElementsAttr inputSplatAttr;
+    if (!matchPattern(op.getOperand(), m_Constant(&inputSplatAttr)) ||
+        !inputSplatAttr)
+      return rewriter.notifyMatchFailure(op, "Input must be a splat constant.");
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(
+        op, inputSplatAttr.resizeSplat(resultType));
     return success();
   }
 };
@@ -1482,6 +1542,14 @@
       rewriter.replaceOpWithNewOp<ConstantOp>(
           op, DenseIntElementsAttr::get(resultType, values));
       return success();
+    }
+
+    // TODO: Support more iota folding, but doing so currently causes OOMs,
+    // so this pattern needs to be enabled more carefully.
+    if (outputSize != 1) {
+      return rewriter.notifyMatchFailure(
+          op, "expected output size to be 1, but got: " +
+                  std::to_string(outputSize));
     }
 
     int64_t sequences = 1;
@@ -1881,6 +1949,7 @@
   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);
   patterns->add<FoldConvertOpPattern>(context, options, benefit);
   patterns->add<FoldDivOpPattern>(context, options, benefit);
+  patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);
   patterns->add<FoldGetDimensionSizeOpPattern>(context, options, benefit);
   patterns->add<FoldMaxOpPattern>(context, options, benefit);
   patterns->add<FoldMinOpPattern>(context, options, benefit);
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
@@ -331,7 +331,7 @@
 DenseI64ArrayAttr getInvertedBroadcastDimensions(OpBuilder& b,
                                                  ArrayRef<int64_t> dims) {
   SmallVector<int64_t> permutation(dims.size());
-  for (size_t i = 0; i < dims.size(); ++i) {
+  for (auto i = 0; i < dims.size(); ++i) {
     permutation[dims[i]] = i;
   }
   return b.getDenseI64ArrayAttr(permutation);
@@ -1308,6 +1308,17 @@
 //////////////////////////////////
 // TransposeOp
 /////////////////////////////////
+
+DenseI64ArrayAttr getMergedTransposePermutation(OpBuilder& b,
+                                                ArrayRef<int64_t> childPerm,
+                                                ArrayRef<int64_t> parentPerm) {
+  SmallVector<int64_t> mergedPerm;
+  mergedPerm.reserve(parentPerm.size());
+  for (int64_t parentIdx : parentPerm) {
+    mergedPerm.push_back(childPerm[parentIdx]);
+  }
+  return b.getDenseI64ArrayAttr(mergedPerm);
+}
 
 // Pattern: transpose(X, [no_mem_layout_change...]) -> reshape(X)
 struct TransposeIsReshape final : SimplifyOpRewritePattern<TransposeOp> {
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -43,6 +43,14 @@
     CPred<"llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()">,
     "same number of elements">;
 
+def BroadcastNotReducibleToReshape : Constraint<
+    CPred<"llvm::isa<stablehlo::BroadcastInDimOp>($0.getDefiningOp()) && "
+          "!("
+            "llvm::is_sorted($0.getDefiningOp<stablehlo::BroadcastInDimOp>().getBroadcastDimensions()) && "
+            "llvm::cast<ShapedType>($0.getType()).getNumElements() == llvm::cast<ShapedType>($1.getType()).getNumElements()"
+          ")">,
+    "is a broadcast_in_dim op that cannot be simplified to a reshape op">;
+
 def OperandsEqual : Constraint<CPred<"$0 == $1">, "operands are equal">;
 
 def RankEqual : Constraint<
@@ -61,6 +69,10 @@
 def AnyZero : AttrConstraint<
     CPred<"::mlir::matchPattern($_self, m_AnyAttrOf(m_Zero(), m_AnyZeroFloat()))">,
     "is int or float zero">;
+
+def ZeroArrayI64 : AttrConstraint<
+    CPred<"::llvm::all_of(::llvm::cast<DenseI64ArrayAttr>($_self).asArrayRef(), [](int64_t val) { return val == 0; })">,
+    "is an array of zeros">;
 
 def DenseIntElementsAttr : AttrConstraint<
     CPred<"llvm::isa<DenseIntElementsAttr>($_self)">,
@@ -120,6 +132,8 @@
 
 def MergeBroadcastDims : NativeCodeCall<"getMergedBroadcastDimensions($_builder, $0, $1)">;
 
+def MergePermutations : NativeCodeCall<"getMergedTransposePermutation($_builder, $0, $1)">;
+
 def StableHLO_ConvertOpWithShape : NativeCodeCall<
     "$_builder.create<stablehlo::ConvertOp>($_loc, $0.getType(), $1)">;
 
@@ -178,18 +192,23 @@
 
 // Pattern: broadcast_in_dim(broadcast_in_dim(X, [dimsA...]), [dimsB...])
 //       -> broadcast_in_dim(X, merge(dimsA, dimsB))
+//          [if the nested broadcast can't be simplified to a reshape]
 def BroadcastInDimOp_MergeComposition
-  : Pat<(StableHLO_BroadcastInDimOp
-            (StableHLO_BroadcastInDimOp $operand, $dims_parent), $dims),
+  : Pat<(StableHLO_BroadcastInDimOp:$outer_op
+            (StableHLO_BroadcastInDimOp:$inner_op $operand, $inner_dims),
+            $outer_dims),
         (StableHLO_BroadcastInDimOp
-            $operand, (MergeBroadcastDims $dims, $dims_parent))>;
+            $operand, (MergeBroadcastDims $outer_dims, $inner_dims)),
+        [(BroadcastNotReducibleToReshape $inner_op, $operand)]>;
 
 // Pattern: broadcast_in_dim(X, [sorted...]) -> reshape(X, [sorted...])
 //          [if same numel]
 def BroadcastInDimOp_ReplaceWithReshape
   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, SortedDims:$dims),
         (StableHLO_ReshapeOpWithShape $op, $operand),
-        [(NumberOfElementsEqual $op, $operand)]>;
+        [(NumberOfElementsEqual $op, $operand)],
+        [],
+        (addBenefit 1)>;
 
 // Pattern: broadcast_in_dim(X, [dims...]) -> transpose(X, [dims...])
 //          [if same numel & rank]
@@ -424,9 +443,9 @@
   : Pat<(StableHLO_PadOp:$pad
             $operand,
             $padding_value,
-            $edge_padding_low,
-            $edge_padding_high,
-            $interior_padding),
+            ZeroArrayI64:$edge_padding_low,
+            ZeroArrayI64:$edge_padding_high,
+            ZeroArrayI64:$interior_padding),
         (replaceWithValue $operand),
         [(TypesEqual $pad, $operand)]>;
 
@@ -539,6 +558,12 @@
   : Pat<(StableHLO_TransposeOp $lhs, IotaDims:$dims),
         (replaceWithValue $lhs)>;
 
+// Pattern: transpose(transpose(X)) -> transpose(X)
+def TransposeOp_TransposeOfTranspose
+  : Pat<(StableHLO_TransposeOp
+          (StableHLO_TransposeOp $child, $child_dims), $dims),
+        (StableHLO_TransposeOp $child, (MergePermutations $child_dims, $dims))>;
+
 ////////
 // GetTupleElementOp
 

