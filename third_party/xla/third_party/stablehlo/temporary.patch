diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/gather.mlir b/stablehlo/stablehlo/conversions/linalg/tests/gather.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/gather.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/gather.mlir
@@ -30,12 +30,11 @@
 // CHECK-SAME:           outs(%[[INIT]] : tensor<1x8x8xi32>)
 // CHECK-SAME:           {someattr}
 // CHECK:           ^bb0
-// CHECK-DAG:         %[[IDX0:.+]] = linalg.index 0
-// CHECK-DAG:         %[[IDX1:.+]] = linalg.index 1
-// CHECK-DAG:         %[[IDX2:.+]] = linalg.index 2
-// CHECK-DAG:         %[[S0_INT:.+]] = tensor.extract %[[START_INDICES]][%[[IDX0]], %[[IDX1]], %[[C0]]] : tensor<1x8x2xi32>
-// CHECK-DAG:         %[[S0:.+]] = arith.index_cast %[[S0_INT]] : i32 to index
-// CHECK-DAG:         %[[S1_INT:.+]] = tensor.extract %[[START_INDICES]][%[[IDX0]], %[[IDX1]], %[[C1]]] : tensor<1x8x2xi32>
+// CHECK-DAG:         %[[IDX1:.+]] = linalg.index 1
+// CHECK-DAG:         %[[IDX2:.+]] = linalg.index 2
+// CHECK-DAG:         %[[S0_INT:.+]] = tensor.extract %[[START_INDICES]][%[[C0]], %[[IDX1]], %[[C0]]] : tensor<1x8x2xi32>
+// CHECK-DAG:         %[[S0:.+]] = arith.index_cast %[[S0_INT]] : i32 to index
+// CHECK-DAG:         %[[S1_INT:.+]] = tensor.extract %[[START_INDICES]][%[[C0]], %[[IDX1]], %[[C1]]] : tensor<1x8x2xi32>
 // CHECK-DAG:         %[[S1:.+]] = arith.index_cast %[[S1_INT]] : i32 to index
 // CHECK-DAG:         %[[CLAMP0:.+]] = arith.maxsi %[[S0]], %[[C0]]  : index
 // CHECK-DAG:         %[[IN0:.+]] = arith.minsi %[[CLAMP0]], %[[C0]]
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir b/stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/miscellaneous.mlir
@@ -1233,6 +1233,7 @@
 //      CHECK: func @torch_index_select
 // CHECK-SAME:   %[[INPUT:[a-zA-Z0-9_]*]]
 // CHECK-SAME:   %[[INDEX:[a-zA-Z0-9_]*]]
+//      CHECK: %[[C0:.+]] = arith.constant 0 : index
 //      CHECK: %[[INIT1:.+]] = tensor.empty() :
 //      CHECK: %[[INIT2:.+]] = tensor.empty() :
 //      CHECK: linalg.generic {
@@ -1244,9 +1245,8 @@
 // CHECK-SAME: {someattr}
 //      CHECK: ^{{.+}}(%[[VAL:.+]]: i32, %{{.+}}: i32, %{{.+}}: i32):
 //      CHECK:   %[[CAST:.+]] = arith.index_cast %[[VAL]] : i32 to index
-//      CHECK:   %[[J:.+]] = linalg.index 1
 //      CHECK:   %[[K:.+]] = linalg.index 2
-//      CHECK:   %[[VAL2:.+]] = tensor.extract %[[INPUT]][%[[CAST]], %[[J]], %[[K]]] : tensor<5x1x5xi32>
+//      CHECK:   %[[VAL2:.+]] = tensor.extract %[[INPUT]][%[[CAST]], %[[C0]], %[[K]]] : tensor<5x1x5xi32>
 //      CHECK:   linalg.yield %[[VAL2]] : i32
 
 // -----
@@ -1265,6 +1265,7 @@
   } : (tensor<5x1x5xui32>, tensor<2xi32>) -> tensor<2x1x5xui32>
   func.return %0 : tensor<2x1x5xui32>
 }
+//      CHECK:   %[[C0:.+]] = arith.constant 0 : index
 //      CHECK:   %[[INPUT_SIGNLESS:.*]] = builtin.unrealized_conversion_cast %[[INPUT]] : tensor<5x1x5xui32> to tensor<5x1x5xi32>
 //      CHECK:   %[[INIT:.*]] = tensor.empty() : tensor<1x5xi32>
 //      CHECK:   %[[RES:.+]] = linalg.generic {
@@ -1274,9 +1275,8 @@
 // CHECK-SAME:   ins(%[[INDEX]], %[[INIT]] : tensor<2xi32>, tensor<1x5xi32>)
 //      CHECK:   ^{{.+}}(%[[VAL:.+]]: i32, %{{.+}}: i32, %{{.+}}: i32):
 //      CHECK:     %[[CAST:.+]] = arith.index_cast %[[VAL]] : i32 to index
-//      CHECK:     %[[J:.+]] = linalg.index 1
 //      CHECK:     %[[K:.+]] = linalg.index 2
-//      CHECK:     %[[VAL2:.+]] = tensor.extract %[[INPUT_SIGNLESS]][%[[CAST]], %[[J]], %[[K]]] : tensor<5x1x5xi32>
+//      CHECK:     %[[VAL2:.+]] = tensor.extract %[[INPUT_SIGNLESS]][%[[CAST]], %[[C0]], %[[K]]] : tensor<5x1x5xi32>
 //      CHECK:     linalg.yield %[[VAL2]] : i32
 //      CHECK:   %[[RES_UNSIGNED:.+]] = builtin.unrealized_conversion_cast %[[RES]] : tensor<2x1x5xi32> to tensor<2x1x5xui32>
 //      CHECK:   return %[[RES_UNSIGNED]]
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -2635,6 +2635,38 @@
                           inferredReturnTypes);
 }
 
+class FoldConstantCaseOp : public OpRewritePattern<CaseOp> {
+ public:
+  explicit FoldConstantCaseOp(MLIRContext* context)
+      : OpRewritePattern<CaseOp>(context) {}
+  LogicalResult matchAndRewrite(CaseOp op,
+                                PatternRewriter& rewriter) const override {
+    DenseIntElementsAttr branch;
+    if (!matchPattern(op.getIndex(), m_Constant(&branch))) return failure();
+
+    int index = *branch.getValues<int>().begin();
+    if (index >= op.getBranches().size() || index < 0) {
+      return failure();
+    }
+
+    Block &block = op.getBranches()[index].back();
+    IRMapping mapping;
+    for (auto &block_op : block.without_terminator()) {
+      rewriter.clone(block_op, mapping);
+    }
+    rewriter.replaceOp(
+        op, llvm::to_vector(llvm::map_range(
+            block.getTerminator()->getOperands(),
+            [&](Value v) { return mapping.lookupOrDefault(v); })));
+      return success();
+  }
+};
+
+void CaseOp::getCanonicalizationPatterns(RewritePatternSet& results,
+                                         MLIRContext* context) {
+  results.add<FoldConstantCaseOp>(context);
+}
+
 //===----------------------------------------------------------------------===//
 // SliceOp
 //===----------------------------------------------------------------------===//
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -1392,7 +1392,7 @@
   );
 
   let regions = (region VariadicRegion<SizedRegion<1>>:$branches /*case_i2*/);
-
+  let hasCanonicalizer = 1;
   let results = (outs Variadic<HLO_TensorOrPerAxisQuantizedTensorOrToken>);
 }
 
diff --ruN a/stablehlo/stablehlo/tests/TestUtils.cpp b/stablehlo/stablehlo/tests/TestUtils.cpp
--- stablehlo/stablehlo/tests/TestUtils.cpp
+++ stablehlo/stablehlo/tests/TestUtils.cpp
@@ -172,9 +172,9 @@
 
   void runOnOperation() override {
     GreedyRewriteConfig config;
-    config.maxIterations = 1;
-    config.useTopDownTraversal = true;
-    config.enableRegionSimplification = GreedySimplifyRegionLevel::Disabled;
+    config.setMaxIterations(1)
+        .setUseTopDownTraversal(true)
+        .setRegionSimplificationLevel(GreedySimplifyRegionLevel::Disabled);
     (void)applyPatternsGreedily(getOperation(), std::move(patterns));
   }
 
diff --ruN a/stablehlo/stablehlo/tests/canonicalize.mlir b/stablehlo/stablehlo/tests/canonicalize.mlir
--- stablehlo/stablehlo/tests/canonicalize.mlir
+++ stablehlo/stablehlo/tests/canonicalize.mlir
@@ -0,0 +1,13 @@
+// RUN: stablehlo-opt %s -pass-pipeline='builtin.module(func.func(canonicalize{test-convergence}))' -split-input-file -allow-unregistered-dialect | FileCheck %s
+
+func.func @fold_constant_case(%arg0: tensor<i32>, %arg1: tensor<i32>) -> (tensor<i32>) {
+ %0 = "stablehlo.constant"() <{value = dense<1> : tensor<i32>}> : () -> tensor<i32>
+ %1 = "stablehlo.case"(%0) ({
+   "stablehlo.return"(%arg0) : (tensor<i32>) -> ()
+ }, {
+  "stablehlo.return"(%arg1) : (tensor<i32>) -> ()
+ }) : (tensor<i32>) -> tensor<i32>
+ return %1 : tensor<i32>
+
+// CHECK: return %arg1
+}
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
@@ -750,7 +750,7 @@
     %2 = call @refine_call_callee(%arg0_different_i32, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %2 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
@@ -768,7 +768,7 @@
     %2 = call @refine_call_callee(%arg0_different, %1) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %2 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
@@ -787,7 +787,7 @@
     %4 = call @refine_call_callee(%arg0_new, %3) : (tensor<i32>, tensor<?xf32>) -> tensor<?xf32>
     return %4 : tensor<?xf32>
   }
-  // expected-error@+1{{'func.func' op refined with invompatible refinement keys}}
+  // expected-error@+1{{'func.func' op refined with incompatible refinement keys}}
   func.func @refine_call_callee(%arg0: tensor<i32>, %arg1: tensor<?xf32>) -> tensor<?xf32> {
     return %arg1 : tensor<?xf32>
   }
diff --ruN a/stablehlo/stablehlo/transforms/Passes.h b/stablehlo/stablehlo/transforms/Passes.h
--- stablehlo/stablehlo/transforms/Passes.h
+++ stablehlo/stablehlo/transforms/Passes.h
@@ -41,10 +41,6 @@
 namespace stablehlo {
 
 #define GEN_PASS_DECL
-
-std::unique_ptr<::mlir::Pass> createStablehloAggressiveSimplificationPass(
-    GreedyRewriteConfig config);
-
 #define GEN_PASS_REGISTRATION
 #include "stablehlo/transforms/Passes.h.inc"
 
@@ -61,10 +57,23 @@
                                      RewritePatternSet *patterns,
                                      TypeConverter *converter);
 
+inline void populateStablehloToVhloPatterns(RewritePatternSet *patterns,
+                                            MLIRContext *context,
+                                            TypeConverter *converter) {
+  populateStablehloToVhloPatterns(context, patterns, converter);
+}
+
 // Populates VHLO ops to StableHLO ops rewriting patterns.
+
 void populateVhloToStablehloPatterns(MLIRContext *context,
-                                     RewritePatternSet *patterns,
-                                     TypeConverter *converter);
+  RewritePatternSet *patterns,
+  TypeConverter *converter);
+
+inline void populateVhloToStablehloPatterns(RewritePatternSet *patterns,
+                                            TypeConverter *converter,
+                                            MLIRContext *context) {
+  populateVhloToStablehloPatterns(context, patterns, converter);
+}
 
 // Populates VHLO downgrade rewriting patterns.
 void populateVhloToVersionPatterns(MLIRContext *context,
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/transforms/StablehloCanonicalizeDynamism.cpp
@@ -308,11 +308,11 @@
       StablehloCanonicalizeDynamismPassBase;
 
   LogicalResult initialize(MLIRContext* context) override {
-    config.useTopDownTraversal = true;
-    config.enableRegionSimplification = GreedySimplifyRegionLevel::Aggressive;
-    config.maxIterations = 2;
-    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
-    config.strictMode = GreedyRewriteStrictness::AnyOp;
+    config.setUseTopDownTraversal(true)
+        .setRegionSimplificationLevel(GreedySimplifyRegionLevel::Aggressive)
+        .setMaxIterations(2)
+        .setMaxNumRewrites(GreedyRewriteConfig::kNoLimit)
+        .setStrictness(GreedyRewriteStrictness::AnyOp);
 
     RewritePatternSet patterns_(context);
     populateStablehloCanonicalizeDynamismPatterns(context, &patterns_);
@@ -325,7 +325,7 @@
     auto func = getOperation();
     if (failed(applyPatternsGreedily(func, patterns, config))) {
       func.emitError("Failed to converge StablehloCanonicalizeDynamism in ")
-          << config.maxIterations << " iterations";
+          << config.getMaxIterations() << " iterations";
     }
   }
 
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp b/stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp
--- stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp
+++ stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp
@@ -329,7 +329,7 @@
   LogicalResult initialize(MLIRContext *context) override {
     auto targetVersion = validateTargetVersion(targetVersionOption);
 
-    config.useTopDownTraversal = true;
+    config.setUseTopDownTraversal(true);
 
     RewritePatternSet patterns_(context);
     populateStablehloCompatibilityExpanderPatterns(context, &patterns_,
@@ -347,7 +347,7 @@
         failed(applyPatternsGreedily(module, patterns, config))) {
       module.emitError(
           "Failed to converge StableHLOCompatibilityExpanderPass in ")
-          << config.maxIterations << " iterations";
+          << config.getMaxIterations() << " iterations";
       signalPassFailure();
     }
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloComplexMathExpander.cpp b/stablehlo/stablehlo/transforms/StablehloComplexMathExpander.cpp
--- stablehlo/stablehlo/transforms/StablehloComplexMathExpander.cpp
+++ stablehlo/stablehlo/transforms/StablehloComplexMathExpander.cpp
@@ -51,7 +51,7 @@
 
  public:
   LogicalResult initialize(MLIRContext *context) override {
-    config.useTopDownTraversal = true;
+    config.setUseTopDownTraversal(true);
     RewritePatternSet patterns_(context);
     populateStablehloComplexMathExpanderPatterns(context, &patterns_);
     patterns = std::move(patterns_);
@@ -62,7 +62,7 @@
     auto func = getOperation();
     if (failed(applyPatternsGreedily(func, patterns, config))) {
       func.emitError("Failed to converge StableHLOComplexMathExpanderPass in ")
-          << config.maxIterations << " iterations";
+          << config.getMaxIterations() << " iterations";
       signalPassFailure();
     }
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeQDQToQuantizedOp.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeQDQToQuantizedOp.cpp
--- stablehlo/stablehlo/transforms/StablehloLegalizeQDQToQuantizedOp.cpp
+++ stablehlo/stablehlo/transforms/StablehloLegalizeQDQToQuantizedOp.cpp
@@ -115,7 +115,7 @@
     if (failed(applyPatternsGreedily(func, patterns, config))) {
       func.emitError(
           "Failed to converge StablehloLegalizeQDQToQuantizedOpPass in ")
-          << config.maxIterations << " iterations";
+          << config.getMaxIterations() << " iterations";
       signalPassFailure();
     }
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloLegalizeQuantizedOpToQDQ.cpp b/stablehlo/stablehlo/transforms/StablehloLegalizeQuantizedOpToQDQ.cpp
--- stablehlo/stablehlo/transforms/StablehloLegalizeQuantizedOpToQDQ.cpp
+++ stablehlo/stablehlo/transforms/StablehloLegalizeQuantizedOpToQDQ.cpp
@@ -125,7 +125,7 @@
     auto func = getOperation();
     if (failed(applyPatternsGreedily(func, patterns, config))) {
       func.emitError("Failed to converge StablehloLegalizeQuantizedOpToQDQ in ")
-          << config.maxIterations << " iterations";
+          << config.getMaxIterations() << " iterations";
       signalPassFailure();
     }
   }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/transforms/StablehloRefineShapes.cpp
@@ -460,7 +460,7 @@
   LogicalResult emitDifferentRefinementContextError(func::FuncOp func,
                                                     RefinementKey key,
                                                     RefinementKey prevKey) {
-    return func.emitOpError() << "refined with invompatible refinement keys:"
+    return func.emitOpError() << "refined with incompatible refinement keys:"
                               << "\n  curr=" << key.toString()
                               << "\n  prev=" << prevKey.toString();
   }
@@ -1023,14 +1023,14 @@
   // The algorithm behind this pass consists of a single traversal of the
   // function. This is sufficient because we only support one function per
   // program at the moment.
-  // TODO(#1048): Find out why .maxIterations = 1 no longer works.
+  // TODO(#1048): Find out why .setMaxIterations(1) no longer works.
   // There have been recent refactors to applyPatternsGreedily
   // upstream, and that might be the reason.
-  config.useTopDownTraversal = true;
-  config.enableRegionSimplification = GreedySimplifyRegionLevel::Aggressive;
-  config.maxIterations = 2;
-  config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
-  config.strictMode = GreedyRewriteStrictness::AnyOp;
+  config.setUseTopDownTraversal(true)
+      .setRegionSimplificationLevel(GreedySimplifyRegionLevel::Aggressive)
+      .setMaxIterations(2)
+      .setMaxNumRewrites(GreedyRewriteConfig::kNoLimit)
+      .setStrictness(GreedyRewriteStrictness::AnyOp);
 
   populateStablehloRefineShapesPatterns(context, &patterns);
   patterns.add<RefineCallOpPattern>(context, state);
@@ -1049,7 +1049,7 @@
 
   if (failed(applyPatternsGreedily(func, std::move(patterns), config)))
     func.emitError("Failed to converge StablehloRefineShapes in ")
-        << config.maxIterations << " iterations";
+        << config.getMaxIterations() << " iterations";
 
   return success();
 }
diff --ruN a/stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp b/stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp
--- stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp
+++ stablehlo/stablehlo/transforms/StablehloWrapInComposite.cpp
@@ -183,7 +183,7 @@
 
   void runOnOperation() override {
     GreedyRewriteConfig config;
-    config.strictMode = GreedyRewriteStrictness::ExistingOps;
+    config.setStrictness(GreedyRewriteStrictness::ExistingOps);
     if (failed(applyPatternsGreedily(getOperation(), std::move(patterns),
                                      config))) {
       signalPassFailure();
diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.h b/stablehlo/stablehlo/transforms/optimization/Passes.h
--- stablehlo/stablehlo/transforms/optimization/Passes.h
+++ stablehlo/stablehlo/transforms/optimization/Passes.h
@@ -16,6 +16,7 @@
 #ifndef STABLEHLO_TRANSFORMS_OPTIMIZATION_PASSES_H
 #define STABLEHLO_TRANSFORMS_OPTIMIZATION_PASSES_H
 
+#include <memory>
 #include <utility>
 
 #include "mlir/Dialect/Func/IR/FuncOps.h"
@@ -33,6 +34,15 @@
 #define GEN_PASS_DECL
 #define GEN_PASS_REGISTRATION
 #include "stablehlo/transforms/optimization/Passes.h.inc"
+
+std::unique_ptr<::mlir::Pass> createStablehloAggressiveSimplificationPass(
+    StablehloAggressiveSimplificationPassOptions options,
+    GreedyRewriteConfig rewriteConfig);
+
+inline std::unique_ptr<::mlir::Pass>
+createStablehloAggressiveSimplificationPass(GreedyRewriteConfig config) {
+  return createStablehloAggressiveSimplificationPass({}, config);
+}
 
 std::pair<StablehloAggressiveFolderPassOptions,
           StablehloAggressiveSimplificationPassOptions>
@@ -70,6 +80,13 @@
                                           RewritePatternSet *patterns,
                                           PatternBenefit benefit = 1);
 
+// TODO(gunhyun): To be deleted in the next integrate.
+inline void populateStablehloShapeFolderPatterns(RewritePatternSet *patterns,
+                                          MLIRContext *context,
+                                          PatternBenefit benefit = 1) {
+  populateStablehloShapeFolderPatterns(context, patterns, benefit);
+}
+
 /// Some workloads in XLA import StableHLO from HLO. Since there are a few
 /// differences in HLO (no implicit captures, lots of tuples, etc.), this
 /// set of patterns brings the imported HLO back to a more canonical form
@@ -90,6 +107,10 @@
     MLIRContext *context, RewritePatternSet *patterns,
     StablehloAggressiveFolderPassOptions &&options,
     PatternBenefit benefit = 1) = delete;
+void populateStablehloShapeFolderPatterns(
+    RewritePatternSet *patterns, MLIRContext *context,
+    StablehloAggressiveFolderPassOptions &&options,
+    PatternBenefit benefit = 1) = delete;
 void populateStablehloAggressiveFolderPatterns(
     MLIRContext *context, RewritePatternSet *patterns,
     StablehloAggressiveFolderPassOptions &&options,
@@ -98,6 +119,7 @@
     MLIRContext *context, RewritePatternSet *patterns,
     StablehloAggressiveSimplificationPassOptions &&options) = delete;
 
+
 }  // namespace stablehlo
 }  // namespace mlir
 

