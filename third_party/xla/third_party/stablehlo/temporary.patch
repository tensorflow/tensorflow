diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
--- stablehlo/BUILD.bazel
+++ stablehlo/BUILD.bazel
@@ -2000,6 +2000,7 @@
     deps = [
         ":attr_type_builder_util",
         ":mlir_builder",
+        ":stablehlo_broadcast_lowering",
         ":stablehlo_builder_inc",
         ":stablehlo_ops",
         ":stablehlo_type_inference",
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
@@ -18,6 +18,7 @@
 #include <cstdint>
 #include <optional>
 
+#include "llvm/ADT/SmallVectorExtras.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "mlir/IR/Attributes.h"
 #include "mlir/IR/BuiltinAttributes.h"
@@ -30,6 +31,7 @@
 #include "stablehlo/dialect/TypeInference.h"
 #include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
 #include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
+#include "stablehlo/transforms/StablehloBroadcastLowering.h"
 
 namespace mlir {
 namespace stablehlo {
@@ -94,6 +96,49 @@
       value));
 }
 
+
+MlirOp IotaLike(MlirOp input, int64_t dim, Type elementType) {
+  auto inputType = mlir::cast<RankedTensorType>(input.getType());
+  if (inputType.getRank() == 0) {
+    // Need to construct 1-D iota and reshape to 0-D.
+    auto iota = stablehlo::Iota(input.getBuilder(),
+                                inputType.clone({1}, elementType), dim);
+    return stablehlo::Reshape(iota, {});
+  }
+  if (inputType.hasStaticShape()) {
+    return stablehlo::Iota(input.getBuilder(), inputType.clone(elementType),
+                           dim);
+  }
+
+  // Use input's static shape and slice to the dynamic shape.
+  auto dims = mlir::stablehlo::getDimensions(input.getValue());
+  if (mlir::failed(dims)) llvm::report_fatal_error(
+      "failed to create dynamically shaped iota op, with MLIR error: ");
+
+  mlir::SmallVector<int64_t> iotaShape = llvm::map_to_vector(
+      *dims,
+      [&](mlir::stablehlo::DimensionInfo dim_size) { return dim_size.size; });
+  auto iotaType =
+      mlir::makeTensorType(input.getContext(), iotaShape, elementType);
+  mlir::MlirOp iota = mlir::stablehlo::Iota(input.getBuilder(), iotaType, dim);
+
+  // Slice bounded dimensions to the dynamic shape.
+  for (const mlir::stablehlo::DimensionInfo& dim : *dims) {
+    if (!dim.boundOp.has_value()) continue;
+
+    auto runtime_dim_size =
+        mlir::stablehlo::GetDimensionSize(input, dim.boundOpDim);
+    iota = mlir::stablehlo::SetDimensionSize(iota, runtime_dim_size,
+                                             dim.boundOpDim);
+  }
+  return iota;
+}
+
+MlirOp IotaLike(MlirOp input, int64_t dim, ElementType elementType) {
+  auto resultElementType = getElementType(input.getContext(), elementType);
+  return IotaLike(input, dim, resultElementType);
+}
+
 namespace {
 
 // Use preferred element type, if not use LHS element type.
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
@@ -58,6 +58,12 @@
 MlirOp Constant(MlirBuilder& builder, int64_t value);
 MlirOp Constant(MlirBuilder& builder, std::vector<int64_t> value);
 
+// IotaLike is a sugar API for iota that accounts for bounded dynamism in the
+// input tensor. Eventually this should be a chlo.iota_like op with a StableHLO
+// decomposition, but for now it will be housed as a builder API.
+MlirOp IotaLike(MlirOp input, int64_t dim, ElementType elementType);
+MlirOp IotaLike(MlirOp input, int64_t dim, Type elementType);
+
 // Better Dot / DotGeneral builders.
 // These ops don't support full type inference because the result element type
 // cannot be inferred from operands, however the result shape can be.
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
@@ -199,6 +199,79 @@
         &ctx, {Precision::HIGHEST, Precision::HIGHEST});
     auto dot = stablehlo::DotGeneral(arg0, arg1, dotDimsAttr, precision);
     func::Return(fb, dot);
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_TRUE(succeeded(mlir::verify(*module)));
+  EXPECT_EQ(expected, debugString(*module));
+}
+
+TEST(MlirBuilderTest, IotaLikeStatic) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<2x3xi64>) -> tensor<2x3xi64> {
+    %0 = stablehlo.iota dim = 1 : tensor<2x3xi64>
+    return %0 : tensor<2x3xi64>
+  }
+})mlir";
+  StablehloModuleBuilder mb;
+  {  // Build Main Func
+    func::FunctionBuilder fb(mb.get(), "main");
+    auto& ctx = fb.getContext();
+    auto type2x3xi64 = makeTensorType(ctx, {2, 3}, ElementType::I64);
+    auto arg0 = func::Argument(fb, type2x3xi64);
+    auto iota = stablehlo::IotaLike(arg0, 1, type2x3xi64.getElementType());
+    func::Return(fb, iota);
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_TRUE(succeeded(mlir::verify(*module)));
+  EXPECT_EQ(expected, debugString(*module));
+}
+
+TEST(MlirBuilderTest, IotaLikeScalar) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<i64>) -> tensor<i64> {
+    %0 = stablehlo.iota dim = 0 : tensor<1xi64>
+    %1 = stablehlo.reshape %0 : (tensor<1xi64>) -> tensor<i64>
+    return %1 : tensor<i64>
+  }
+})mlir";
+  StablehloModuleBuilder mb;
+  {  // Build Main Func
+    func::FunctionBuilder fb(mb.get(), "main");
+    auto& ctx = fb.getContext();
+    auto typei64 = makeTensorType(ctx, {}, ElementType::I64);
+    auto arg0 = func::Argument(fb, typei64);
+    auto iota = stablehlo::IotaLike(arg0, 0, typei64.getElementType());
+    func::Return(fb, iota);
+  }
+
+  OwningOpRef<ModuleOp> module = mb->build();
+  EXPECT_TRUE(succeeded(mlir::verify(*module)));
+  EXPECT_EQ(expected, debugString(*module));
+}
+
+TEST(MlirBuilderTest, IotaLikeDynamic) {
+  std::string expected = R"mlir(module {
+  func.func @main(%arg0: tensor<2x3xi64>, %arg1: tensor<i32>) -> tensor<?x3xi64, #stablehlo.bounds<2, ?>> {
+    %0 = stablehlo.set_dimension_size %arg0, %arg1, dim = 0 : (tensor<2x3xi64>, tensor<i32>) -> tensor<?x3xi64, #stablehlo.bounds<2, ?>>
+    %1 = stablehlo.iota dim = 1 : tensor<2x3xi64>
+    %2 = stablehlo.get_dimension_size %0, dim = 0 : (tensor<?x3xi64, #stablehlo.bounds<2, ?>>) -> tensor<i32>
+    %3 = stablehlo.set_dimension_size %1, %2, dim = 0 : (tensor<2x3xi64>, tensor<i32>) -> tensor<?x3xi64, #stablehlo.bounds<2, ?>>
+    return %3 : tensor<?x3xi64, #stablehlo.bounds<2, ?>>
+  }
+})mlir";
+  StablehloModuleBuilder mb;
+  {  // Build Main Func
+    func::FunctionBuilder fb(mb.get(), "main");
+    auto& ctx = fb.getContext();
+    auto type2x3xi64 = makeTensorType(ctx, {2, 3}, ElementType::I64);
+    auto typei32 = makeTensorType(ctx, {}, ElementType::I32);
+    auto arg0 = func::Argument(fb, type2x3xi64);
+    auto arg1 = func::Argument(fb, typei32);
+    auto sds = stablehlo::SetDimensionSize(arg0, arg1, 0);
+    auto iota = stablehlo::IotaLike(sds, 1, type2x3xi64.getElementType());
+    func::Return(fb, iota);
   }
 
   OwningOpRef<ModuleOp> module = mb->build();
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -584,6 +584,56 @@
   %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>
   %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>
   return %0, %1, %2 : tensor<i32>, tensor<ui32>, tensor<f32>
+}
+
+// CHECK-LABEL: @div_fold_cst_zero_nan
+func.func @div_fold_cst_zero_nan() -> (tensor<f32>) {
+  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
+  %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
+  %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>
+  // CHECK: stablehlo.constant dense<0x7F800000> : tensor<f32>
+  // CHECK-NOT: stablehlo.divide
+  return %0 : tensor<f32>
+}
+
+// CHECK-LABEL: @div_fold_cst_nan
+func.func @div_fold_cst_nan() -> (tensor<f32>) {
+  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
+  %cst_0 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>
+  // CHECK: stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  // CHECK-NOT: stablehlo.divide
+  return %0 : tensor<f32>
+}
+
+// -----
+
+////////
+// MaximumOp
+
+// CHECK-LABEL: @max_fold_cst_nan
+func.func @max_fold_cst_nan() -> (tensor<f32>) {
+  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
+  %cst_0 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  %0 = stablehlo.maximum %cst, %cst_0 : tensor<f32>
+  // CHECK: stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  // CHECK-NOT: stablehlo.maximum
+  return %0 : tensor<f32>
+}
+
+// -----
+
+////////
+// MinimumOp
+
+// CHECK-LABEL: @min_fold_cst_nan
+func.func @min_fold_cst_nan() -> (tensor<f32>) {
+  %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>
+  %cst_0 = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  %0 = stablehlo.minimum %cst, %cst_0 : tensor<f32>
+  // CHECK: stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  // CHECK-NOT: stablehlo.minimum
+  return %0 : tensor<f32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -1058,14 +1058,14 @@
   std::function<APInt(APInt, APInt)> foldIntFn;
 
   APFloat operator()(APFloat lhs, APFloat rhs) {
-    return lhs >= rhs ? lhs : rhs;
+    return llvm::maximum(lhs, rhs);
   }
   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
   static APInt foldUint(APInt lhs, APInt rhs) {
-    return lhs.uge(rhs) ? lhs : rhs;
+    return llvm::APIntOps::umax(lhs, rhs);
   }
   static APInt foldSint(APInt lhs, APInt rhs) {
-    return lhs.sge(rhs) ? lhs : rhs;
+    return llvm::APIntOps::smax(lhs, rhs);
   }
 };
 
@@ -1075,14 +1075,14 @@
   std::function<APInt(APInt, APInt)> foldIntFn;
 
   APFloat operator()(APFloat lhs, APFloat rhs) {
-    return lhs <= rhs ? lhs : rhs;
+    return llvm::minimum(lhs, rhs);
   }
   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
   static APInt foldUint(APInt lhs, APInt rhs) {
-    return lhs.ule(rhs) ? lhs : rhs;
+    return llvm::APIntOps::umin(lhs, rhs);
   }
   static APInt foldSint(APInt lhs, APInt rhs) {
-    return lhs.sle(rhs) ? lhs : rhs;
+    return llvm::APIntOps::smin(lhs, rhs);
   }
 };
 
@@ -1533,7 +1533,9 @@
   using FoldUnaryOpPattern::FoldUnaryOpPattern;
 
   static std::optional<APInt> EvaluateOp(APInt operand) { return -operand; }
-  static std::optional<APFloat> EvaluateOp(APFloat operand) { return -operand; }
+  static std::optional<APFloat> EvaluateOp(APFloat operand) {
+    return llvm::neg(operand);
+  }
 };
 
 struct FoldNotOpPattern : public FoldUnaryOpPattern<FoldNotOpPattern, NotOp> {

