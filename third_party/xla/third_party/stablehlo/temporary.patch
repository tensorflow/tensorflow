diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir b/stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir
@@ -11,10 +11,10 @@
   // CHECK-DAG: %[[MULTIPLIER_2:.+]] = "tosa.const"() <{values = dense<1431655765> : tensor<1xi32>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.add %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-1>>
   %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-1>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-1>>
@@ -32,10 +32,10 @@
   // CHECK-DAG: %[[MULTIPLIER_2:.+]] = "tosa.const"() <{values = dense<1431655765> : tensor<1xi32>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.subtract %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-1>>
   %0 = "stablehlo.subtract"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-1>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-1>>
@@ -52,10 +52,10 @@
   // CHECK-DAG: %[[MULTIPLIER_2:.+]] = "tosa.const"() <{values = dense<1717986918> : tensor<1xi32>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.multiply %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-1>>
   %0 = "stablehlo.multiply"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-1>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-1>>
@@ -74,10 +74,10 @@
   // CHECK-DAG: %[[ZP_MINUS_2:.+]] = "tosa.const"() <{values = dense<-2> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.divide %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-3>>
   %0 = "stablehlo.divide"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-2>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-3>>
@@ -97,10 +97,10 @@
   // CHECK-DAG: %[[SHIFT12:.+]] = "tosa.const"() <{values = dense<12> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.maximum %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-3>>
   %0 = "stablehlo.maximum"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-2>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-3>>
@@ -120,10 +120,10 @@
   // CHECK-DAG: %[[SHIFT12:.+]] = "tosa.const"() <{values = dense<12> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.minimum %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-3>>
   %0 = "stablehlo.minimum"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-2>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-3>>
@@ -140,9 +140,9 @@
   // CHECK-DAG: %[[SHIFT30:.+]] = "tosa.const"() <{values = dense<30> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V1:.+]] = stablehlo.abs %[[V0]] : tensor<20x20xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V1]], %[[MULTIPLIER_1]], %[[SHIFT33]], %[[ZP_0]], %[[ZP_MINUS_128]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V1]], %[[MULTIPLIER_1]], %[[SHIFT33]], %[[ZP_0]], %[[ZP_MINUS_128]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<20x20x!quant.uniform<i8:f32, 1.500000e-01:-128>>
   %0 = "stablehlo.abs"(%arg0) : (tensor<20x20x!quant.uniform<i8:f32, 0.025:-1>>) -> tensor<20x20x!quant.uniform<i8:f32, 1.5e-01:-128>>
   return %0 : tensor<20x20x!quant.uniform<i8:f32, 1.5e-01:-128>>
@@ -159,8 +159,8 @@
   // CHECK-DAG: %[[SHIFT12:.+]] = "tosa.const"() <{values = dense<12> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.compare GE, %[[V0]], %[[V1]], TOTALORDER :
   // CHECK: return %[[V2]]
   %0 = stablehlo.compare GE, %arg0, %arg1, TOTALORDER : (tensor<20x20x!quant.uniform<i8:f32, 0.025:-1>>, tensor<20x20x!quant.uniform<i8:f32, 0.075:-2>>) -> tensor<20x20xi1>
@@ -177,8 +177,8 @@
   // CHECK-DAG: %[[SHIFT15:.+]] = "tosa.const"() <{values = dense<15> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP16_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi16>}>
   // CHECK-DAG: %[[ZP32_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT17]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT15]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT17]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT15]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.compare LT, %[[V0]], %[[V1]], TOTALORDER :
   // CHECK: return %[[V2]]
   %0 = stablehlo.compare LT, %arg0, %arg1, TOTALORDER : (tensor<20x20x!quant.uniform<i16:f32, 0.025:0>>, tensor<20x20x!quant.uniform<i16:f32, 0.075:0>>) -> tensor<20x20xi1>
diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir b/stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir
@@ -7,7 +7,7 @@
   %shift = "tosa.const"() {values = dense<13> : tensor<1xi8>} : () -> tensor<1xi8>
   %input_zp = "tosa.const"() {values = dense<-1> : tensor<1xi8>} : () -> tensor<1xi8>
   %output_zp = "tosa.const"() {values = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
-  %0 = tosa.rescale %arg0, %multiplier, %shift, %input_zp, %output_zp {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true} :
+  %0 = tosa.rescale %arg0, %multiplier, %shift, %input_zp, %output_zp {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true} :
             (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<1xi32>, tensor<1xi8>, tensor<1xi8>, tensor<1xi32>) -> tensor<2x2xi32>
 
   // convert input quantized type to storage type
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
@@ -70,12 +70,14 @@
       outputZpVal.has_value() &&
       "buildRescale: Failed to create output zero-point tensor for RescaleOp.");
 
-  std::string roundingMode = doubleRound ? "DOUBLE_ROUND" : "SINGLE_ROUND";
+  auto roundingMode =
+      doubleRound ? RoundingMode::DOUBLE_ROUND : RoundingMode::SINGLE_ROUND;
 
   auto rescale_op = rewriter.create<RescaleOp>(
       loc, outputType, inputVal, multiplierVal, shiftVal, inputZpVal.value(),
       outputZpVal.value(), rewriter.getBoolAttr(scale32),
-      rewriter.getStringAttr(roundingMode), rewriter.getBoolAttr(perChannel),
+      RoundingModeAttr::get(rewriter.getContext(), roundingMode),
+      rewriter.getBoolAttr(perChannel),
       /*input_unsigned=*/rewriter.getBoolAttr(false),
       /*output_unsigned=*/rewriter.getBoolAttr(false));
 
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp
@@ -68,7 +68,7 @@
   auto roundingMode = op.getRoundingMode();
   bool perChannel = op.getPerChannel();
 
-  if (perChannel || roundingMode != "SINGLE_ROUND" || !scale32) {
+  if (perChannel || roundingMode != RoundingMode::SINGLE_ROUND || !scale32) {
     return rewriter.notifyMatchFailure(
         op,
         "per_channel, double_round, or scale32=false are not yet supported");
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -542,7 +542,7 @@
 // to allow erroring in StableHLO for these custom calls.
 LogicalResult CustomCallOp::verifyKnownCustomCalls() {
   // We have already verified that the output_operand_aliases have consistent
-  // types and valid indices. Here we verify buffer releated special custom_call
+  // types and valid indices. Here we verify buffer-related special custom_call
   // targets, and also verify that buffer operands used non-special custom_call
   // ops meet this requirements:
   //   A result with a buffer type should be mentioned in one pair of
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.cpp
@@ -1,140 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-
-#include <cstdint>
-
-#include "llvm/Support/ErrorHandling.h"
-#include "mlir/IR/Builders.h"
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/Support/LLVM.h"
-
-namespace mlir {
-
-//////////////////////
-// Builders - Location
-//////////////////////
-
-Location unknownLoc(MLIRContext& ctx) { return UnknownLoc::get(&ctx); }
-
-Location fileLineColLoc(MLIRContext& ctx, StringRef file, int64_t line,
-                        int64_t col) {
-  return FileLineColLoc::get(&ctx, file, line, col);
-}
-
-//////////////////////
-// Builders - Tensor Types
-//////////////////////
-
-Type getElementType(MLIRContext& ctx, ElementType elementType) {
-  Builder builder(&ctx);
-
-  switch (elementType) {
-    case ElementType::PRED:
-      return builder.getI1Type();
-    case ElementType::I2:
-      return builder.getI2Type();
-    case ElementType::I4:
-      return builder.getI4Type();
-    case ElementType::I8:
-      return builder.getI8Type();
-    case ElementType::I16:
-      return builder.getI16Type();
-    case ElementType::I32:
-      return builder.getI32Type();
-    case ElementType::I64:
-      return builder.getI64Type();
-    case ElementType::UI2:
-      return IntegerType::get(&ctx, 2, IntegerType::Unsigned);
-    case ElementType::UI4:
-      return IntegerType::get(&ctx, 4, IntegerType::Unsigned);
-    case ElementType::UI8:
-      return IntegerType::get(&ctx, 8, IntegerType::Unsigned);
-    case ElementType::UI16:
-      return IntegerType::get(&ctx, 16, IntegerType::Unsigned);
-    case ElementType::UI32:
-      return IntegerType::get(&ctx, 32, IntegerType::Unsigned);
-    case ElementType::UI64:
-      return IntegerType::get(&ctx, 64, IntegerType::Unsigned);
-    case ElementType::BF16:
-      return builder.getBF16Type();
-    case ElementType::F16:
-      return builder.getF16Type();
-    case ElementType::F32:
-      return builder.getF32Type();
-    case ElementType::F64:
-      return builder.getF64Type();
-    case ElementType::F4E2M1FN:
-      return Float4E2M1FNType::get(&ctx);
-    case ElementType::F6E2M3FN:
-      return Float6E2M3FNType::get(&ctx);
-    case ElementType::F6E3M2FN:
-      return Float6E3M2FNType::get(&ctx);
-    case ElementType::F8E3M4:
-      return Float8E3M4Type::get(&ctx);
-    case ElementType::F8E4M3:
-      return Float8E4M3Type::get(&ctx);
-    case ElementType::F8E4M3FN:
-      return Float8E4M3FNType::get(&ctx);
-    case ElementType::F8E4M3FNUZ:
-      return Float8E4M3FNUZType::get(&ctx);
-    case ElementType::F8E4M3B11FNUZ:
-      return Float8E4M3B11FNUZType::get(&ctx);
-    case ElementType::F8E5M2:
-      return Float8E5M2Type::get(&ctx);
-    case ElementType::F8E5M2FNUZ:
-      return Float8E5M2FNUZType::get(&ctx);
-    case ElementType::F8E8M0FNU:
-      return Float8E8M0FNUType::get(&ctx);
-    case ElementType::COMPLEXF32:
-      return ComplexType::get(builder.getF32Type());
-    case ElementType::COMPLEXF64:
-      return ComplexType::get(builder.getF64Type());
-    default:
-      llvm::report_fatal_error("Unsupported element type");
-  }
-}
-
-RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,
-                                ElementType elementType) {
-  return makeTensorType(ctx, shape, getElementType(ctx, elementType));
-}
-
-RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,
-                                Type elementType) {
-  return RankedTensorType::get(shape, elementType);
-}
-
-//////////////////////
-// Builders - Constant Literals
-//////////////////////
-
-namespace detail {
-
-APFloat toAPFloat(double val, FloatType floatType) {
-  llvm::APFloat apf(val);
-  const auto& fltSemantics = floatType.getFloatSemantics();
-  auto roundingMode = APFloat::rmNearestTiesToEven;
-  bool losesInfo;
-  apf.convert(fltSemantics, roundingMode, &losesInfo);
-  return apf;
-}
-
-}  // namespace detail
-
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h
@@ -1,282 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#ifndef STABLEHLO_BUILDER_ATTRTYPEBUILDERUTIL_H_
-#define STABLEHLO_BUILDER_ATTRTYPEBUILDERUTIL_H_
-
-#include <complex>
-#include <cstdint>
-#include <source_location>
-#include <type_traits>
-#include <vector>
-
-#include "llvm/ADT/APFloat.h"
-#include "llvm/ADT/APSInt.h"
-#include "llvm/ADT/STLExtras.h"
-#include "llvm/ADT/SmallVector.h"
-#include "llvm/ADT/TypeSwitch.h"
-#include "llvm/Support/ErrorHandling.h"
-#include "mlir/IR/Attributes.h"
-#include "mlir/IR/Builders.h"
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinTypeInterfaces.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/IR/Types.h"
-#include "mlir/Support/LLVM.h"
-
-namespace mlir {
-
-//////////////////////
-// Builders - Location
-//////////////////////
-
-Location unknownLoc(MLIRContext& ctx);
-Location fileLineColLoc(MLIRContext& ctx, StringRef file, int64_t line,
-                        int64_t col);
-
-//////////////////////
-// Builders - Tensor Types
-//////////////////////
-
-// POD type to Tensor element type map
-
-// List of supported Tensor Element Types.
-// This list is fairly XLA specific, used to provide sugar for the common
-// RankedTensorType's we'll need to build.
-enum class ElementType {
-  // clang-format off
-  PRED,
-  I2, I4, I8, I16, I32, I64,
-  UI2, UI4, UI8, UI16, UI32, UI64,
-  BF16, F16, F32, F64,
-  F4E2M1FN, F6E2M3FN, F6E3M2FN, F8E3M4, F8E4M3,
-  F8E4M3FN, F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F8E5M2FNUZ, F8E8M0FNU,
-  COMPLEXF32, COMPLEXF64
-  // clang-format on
-};
-
-Type getElementType(MLIRContext& ctx, ElementType elementType);
-
-// Build a ranked tensor type with an element type of ElementType.
-RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,
-                                ElementType elementType);
-
-// Build a ranked tensor type with an MLIR element type.
-RankedTensorType makeTensorType(MLIRContext& ctx, ArrayRef<int64_t> shape,
-                                Type elementType);
-
-//////////////////////
-// Builders - Constant Literals
-//////////////////////
-
-namespace detail {
-
-APFloat toAPFloat(double val, FloatType floatType);
-
-//////////////////////
-// Literal Conversion - Int
-//////////////////////
-inline IntegerAttr getIntegerAttr(llvm::APSInt value, IntegerType type) {
-  value.setIsSigned(type.isSigned());
-  APSInt ext = value.extOrTrunc(type.getIntOrFloatBitWidth());
-  return IntegerAttr::get(type, ext);
-}
-template <typename T>
-typename std::enable_if<std::is_integral<T>::value, IntegerAttr>::type
-getIntegerAttr(T value, IntegerType type) {
-  return getIntegerAttr(APSInt::get(value), type);
-}
-inline IntegerAttr getIntegerAttr(double value, IntegerType type) {
-  return getIntegerAttr(static_cast<int64_t>(value), type);
-}
-inline IntegerAttr getIntegerAttr(llvm::APFloat value, IntegerType type) {
-  return getIntegerAttr(value.convertToDouble(), type);
-}
-template <typename T>
-inline IntegerAttr getIntegerAttr(std::complex<T> value, IntegerType type) {
-  return getIntegerAttr(value.real(), type);
-}
-
-template <typename T>
-SmallVector<Attribute> getIntegerAttrs(ArrayRef<T> values, IntegerType type) {
-  return llvm::to_vector(llvm::map_range(values, [&](T value) -> Attribute {
-    return getIntegerAttr(value, type);
-  }));
-}
-
-//////////////////////
-// Literal Conversion - Float
-//////////////////////
-inline FloatAttr getFloatAttr(llvm::APFloat value, FloatType type) {
-  return FloatAttr::get(type, value);
-}
-inline FloatAttr getFloatAttr(double value, FloatType type) {
-  return getFloatAttr(toAPFloat(value, type), type);
-}
-template <typename T>
-typename std::enable_if<std::is_integral<T>::value, FloatAttr>::type
-getFloatAttr(T value, FloatType type) {
-  return getFloatAttr(static_cast<double>(value), type);
-}
-inline FloatAttr getFloatAttr(llvm::APSInt value, FloatType type) {
-  return getFloatAttr(value.roundToDouble(), type);
-}
-template <typename T>
-FloatAttr getFloatAttr(std::complex<T> value, FloatType type) {
-  return getFloatAttr(value.real(), type);
-}
-
-template <typename T>
-SmallVector<Attribute> getFloatAttrs(ArrayRef<T> values, FloatType type) {
-  return llvm::to_vector(llvm::map_range(
-      values, [&](T value) -> Attribute { return getFloatAttr(value, type); }));
-}
-
-//////////////////////
-// Literal Conversion - Complex
-//////////////////////
-template <typename T>
-typename std::enable_if<std::is_floating_point_v<T>,
-                        std::complex<APFloat>>::type
-getComplexValue(std::complex<T> value, FloatType floatType) {
-  FloatAttr realAttr = getFloatAttr(value.real(), floatType);
-  FloatAttr imagAttr = getFloatAttr(value.imag(), floatType);
-  return std::complex<APFloat>(realAttr.getValue(), imagAttr.getValue());
-}
-template <typename T>
-std::complex<APFloat> getComplexValue(T value, FloatType floatType) {
-  FloatAttr realAttr = getFloatAttr(value, floatType);
-  return std::complex<APFloat>(realAttr.getValue(), toAPFloat(0.0, floatType));
-}
-template <typename T>
-SmallVector<std::complex<APFloat>> getComplexValues(ArrayRef<T> values,
-                                                    FloatType floatType) {
-  return llvm::to_vector(
-      llvm::map_range(values, [&](T value) -> std::complex<APFloat> {
-        return getComplexValue(value, floatType);
-      }));
-}
-
-}  // namespace detail
-
-// Creates a DenseElementsAttr from a single value (splat) and a target
-// RankedTensorType.
-//
-// This function attempts to create a DenseElementsAttr by broadcasting the
-// provided `value` to the shape specified in `tensorType`. It supports
-// IntegerType, FloatType, and ComplexType.
-//
-// Supported input types (`T`):
-//   - For IntegerType: any arithmetic type. Arithmetic types will
-//   be cast to `int64_t`.
-//   - For FloatType: any arithmetic type. Arithmetic types will
-//   be cast to `double`.
-//   - For ComplexType: `std::complex<T>` and any arithmetic type. The
-//     imaginary part will be set to 0 if the input is an arithmetic type.
-//
-// Args:
-//   value: The value to broadcast.
-//   tensorType: The target RankedTensorType.
-//
-// Returns:
-//   A splat DenseElementsAttr.
-//
-// Raises fatal exception if the element type is unsupported.
-template <typename T>
-DenseElementsAttr makeConstant(T value, RankedTensorType tensorType) {
-  return TypeSwitch<Type, DenseElementsAttr>(tensorType.getElementType())
-      .template Case<IntegerType>([&](IntegerType type) -> DenseElementsAttr {
-        IntegerAttr intAttr = detail::getIntegerAttr(value, type);
-        return DenseElementsAttr::get(tensorType, intAttr);
-      })
-      .template Case<FloatType>([&](FloatType type) -> DenseElementsAttr {
-        FloatAttr floatAttr = detail::getFloatAttr(value, type);
-        return DenseElementsAttr::get(tensorType, floatAttr);
-      })
-      .template Case<ComplexType>([&](ComplexType type) -> DenseElementsAttr {
-        auto floatType = dyn_cast<FloatType>(type.getElementType());
-        if (!floatType)
-          llvm::report_fatal_error(
-              "makeConstant with non-float complex type is unsupported.");
-
-        std::complex<APFloat> complexValue =
-            detail::getComplexValue(value, floatType);
-        return DenseElementsAttr::get(tensorType, complexValue);
-      })
-      .Default([](Type) -> DenseElementsAttr {
-        llvm::report_fatal_error(
-            "makeConstant called with unsupported MLIR type, must be "
-            "IntegerType, FloatType, or ComplexType.");
-        return nullptr;
-      });
-}
-
-// Creates a DenseElementsAttr from a list of values and a target
-// RankedTensorType.
-//
-// This function may perform some literal coercions if the tensor element type
-// does not match the provided value type.
-//
-// See `makeConstant(T value, RankedTensorType tensorType)` for full details.
-template <typename T>
-DenseElementsAttr makeConstant(ArrayRef<T> values,
-                               RankedTensorType tensorType) {
-  return TypeSwitch<Type, DenseElementsAttr>(tensorType.getElementType())
-      .template Case<IntegerType>([&](IntegerType type) -> DenseElementsAttr {
-        SmallVector<Attribute> intAttrs = detail::getIntegerAttrs(values, type);
-        return DenseElementsAttr::get(tensorType, intAttrs);
-      })
-      .template Case<FloatType>([&](FloatType type) -> DenseElementsAttr {
-        SmallVector<Attribute> floatAttrs = detail::getFloatAttrs(values, type);
-        return DenseElementsAttr::get(tensorType, floatAttrs);
-      })
-      .template Case<ComplexType>([&](ComplexType type) -> DenseElementsAttr {
-        auto floatType = dyn_cast<FloatType>(type.getElementType());
-        if (!floatType)
-          llvm::report_fatal_error(
-              "makeConstant with non-float complex type is unsupported.");
-
-        SmallVector<std::complex<APFloat>> complexValues =
-            detail::getComplexValues(values, floatType);
-        return DenseElementsAttr::get(tensorType, complexValues);
-      })
-      .Default([&](Type) -> DenseElementsAttr {
-        llvm::report_fatal_error(
-            "makeConstant called with unsupported MLIR type, must be "
-            "IntegerType, FloatType, or ComplexType.");
-        return nullptr;
-      });
-}
-
-template <typename T>
-DenseElementsAttr makeConstant(const SmallVector<T>& values,
-                               RankedTensorType tensorType) {
-  // Force the compiler to call the overload that takes an SmallVector<T> to
-  // prevent infinite loops
-  return makeConstant(ArrayRef<T>(values), tensorType);
-}
-
-template <typename T>
-DenseElementsAttr makeConstant(const std::vector<T>& values,
-                               RankedTensorType tensorType) {
-  // Force the compiler to call the overload that takes an ArrayRef<T> to
-  // prevent infinite loops.
-  return makeConstant(ArrayRef<T>(values), tensorType);
-}
-
-}  // namespace mlir
-
-#endif  // STABLEHLO_BUILDER_ATTRTYPEBUILDERUTIL_H_
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
@@ -1,282 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include <complex>
-#include <cstdint>
-#include <limits>
-#include <string>
-#include <utility>
-#include <vector>
-
-#include "gtest/gtest.h"
-#include "llvm/ADT/DenseMap.h"
-#include "mlir/IR/BuiltinTypeInterfaces.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/IR/Types.h"
-#include "mlir/Support/DebugStringHelper.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-
-namespace mlir {
-
-TEST(AttrTypeBuilderUtilTest, TestMakeTensorType) {
-  MLIRContext context;
-  llvm::DenseMap<std::pair<SmallVector<int64_t>, ElementType>, std::string>
-      testCaseMap = {
-          {{{}, ElementType::PRED}, "tensor<i1>"},
-          {{{}, ElementType::I8}, "tensor<i8>"},
-          {{{}, ElementType::I16}, "tensor<i16>"},
-          {{{}, ElementType::I32}, "tensor<i32>"},
-          {{{}, ElementType::I64}, "tensor<i64>"},
-          {{{}, ElementType::UI8}, "tensor<ui8>"},
-          {{{}, ElementType::UI16}, "tensor<ui16>"},
-          {{{}, ElementType::UI32}, "tensor<ui32>"},
-          {{{}, ElementType::UI64}, "tensor<ui64>"},
-          {{{}, ElementType::BF16}, "tensor<bf16>"},
-          {{{}, ElementType::F16}, "tensor<f16>"},
-          {{{}, ElementType::F32}, "tensor<f32>"},
-          {{{}, ElementType::F64}, "tensor<f64>"},
-          {{{}, ElementType::F4E2M1FN}, "tensor<f4E2M1FN>"},
-          {{{}, ElementType::F6E2M3FN}, "tensor<f6E2M3FN>"},
-          {{{}, ElementType::F6E3M2FN}, "tensor<f6E3M2FN>"},
-          {{{}, ElementType::F8E3M4}, "tensor<f8E3M4>"},
-          {{{}, ElementType::F8E4M3}, "tensor<f8E4M3>"},
-          {{{}, ElementType::F8E4M3FN}, "tensor<f8E4M3FN>"},
-          {{{}, ElementType::F8E4M3FNUZ}, "tensor<f8E4M3FNUZ>"},
-          {{{}, ElementType::F8E4M3B11FNUZ}, "tensor<f8E4M3B11FNUZ>"},
-          {{{}, ElementType::F8E5M2}, "tensor<f8E5M2>"},
-          {{{}, ElementType::F8E5M2FNUZ}, "tensor<f8E5M2FNUZ>"},
-          {{{}, ElementType::F8E8M0FNU}, "tensor<f8E8M0FNU>"},
-          {{{1}, ElementType::F64}, "tensor<1xf64>"},
-          {{{1, 2, 3}, ElementType::F64}, "tensor<1x2x3xf64>"},
-      };
-  for (auto& [inputs, value] : testCaseMap) {
-    RankedTensorType type =
-        makeTensorType(context, inputs.first, inputs.second);
-    Type mlir_element_type = getElementType(context, inputs.second);
-    RankedTensorType type2 =
-        makeTensorType(context, inputs.first, mlir_element_type);
-    EXPECT_EQ(type, type2);
-    EXPECT_EQ(value, debugString(type));
-  }
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantSplat_Integer) {
-  MLIRContext context;
-
-  // Init with Int
-  auto i32_type = makeTensorType(context, {}, ElementType::I32);
-  EXPECT_EQ(mlir::debugString(makeConstant(1, i32_type)),
-            "dense<1> : tensor<i32>");
-
-  // Init with APSInt
-  EXPECT_EQ(mlir::debugString(makeConstant(APSInt::get(1), i32_type)),
-            "dense<1> : tensor<i32>");
-
-  // Init with Float
-  EXPECT_EQ(mlir::debugString(makeConstant(1.0, i32_type)),
-            "dense<1> : tensor<i32>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantSplat_Float) {
-  MLIRContext context;
-
-  // Float
-  auto f32_type = makeTensorType(context, {}, ElementType::F32);
-  EXPECT_EQ(mlir::debugString(makeConstant(1.0, f32_type)),
-            "dense<1.000000e+00> : tensor<f32>");
-
-  // Init with APFloat
-  FloatType f32_type2 = cast<FloatType>(f32_type.getElementType());
-  auto inf = APFloat::getInf(f32_type2.getFloatSemantics());
-  EXPECT_EQ(mlir::debugString(makeConstant(inf, f32_type)),
-            "dense<0x7F800000> : tensor<f32>");
-
-  // Init with Int
-  EXPECT_EQ(mlir::debugString(makeConstant(1, f32_type)),
-            "dense<1.000000e+00> : tensor<f32>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantSplat_Complex) {
-  MLIRContext context;
-
-  // Complex
-  auto c32_type = makeTensorType(context, {}, ElementType::COMPLEXF32);
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(std::complex<float>(1.0, 2.0), c32_type)),
-      "dense<(1.000000e+00,2.000000e+00)> : tensor<complex<f32>>");
-
-  auto c64_type = makeTensorType(context, {}, ElementType::COMPLEXF64);
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(std::complex<double>(1.0, 2.0), c64_type)),
-      "dense<(1.000000e+00,2.000000e+00)> : tensor<complex<f64>>");
-
-  // Init with int & float
-  EXPECT_EQ(mlir::debugString(makeConstant(1, c32_type)),
-            "dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f32>>");
-  EXPECT_EQ(mlir::debugString(makeConstant(1.0, c32_type)),
-            "dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f32>>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantSplatIntLimits) {
-  MLIRContext context;
-
-  // Test 64 bitwidth values and i8 values, everything in between should be
-  // uninteresting.
-
-  // Int8
-  auto i8_type = makeTensorType(context, {}, ElementType::I8);
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<int8_t>::max(), i8_type)),
-            "dense<127> : tensor<i8>");
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<int8_t>::min(), i8_type)),
-            "dense<-128> : tensor<i8>");
-
-  // uint8
-  auto u8_type = makeTensorType(context, {}, ElementType::UI8);
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<uint8_t>::max(), u8_type)),
-            "dense<255> : tensor<ui8>");
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<uint8_t>::min(), u8_type)),
-            "dense<0> : tensor<ui8>");
-
-  // int64
-  auto i64_type = makeTensorType(context, {}, ElementType::I64);
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<int64_t>::max(), i64_type)),
-            "dense<9223372036854775807> : tensor<i64>");
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<int64_t>::min(), i64_type)),
-            "dense<-9223372036854775808> : tensor<i64>");
-
-  // uint64
-  auto u64_type = makeTensorType(context, {}, ElementType::UI64);
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<uint64_t>::max(), u64_type)),
-            "dense<18446744073709551615> : tensor<ui64>");
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<uint64_t>::min(), u64_type)),
-            "dense<0> : tensor<ui64>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantSplatFloatLimits) {
-  MLIRContext context;
-
-  // Test 64 bitwidth values and f32 values, everything in between should be
-  // uninteresting, i.e. should behave like f32.
-
-  // Float16
-  auto f32_type = makeTensorType(context, {}, ElementType::F32);
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<float>::max(), f32_type)),
-            "dense<3.40282347E+38> : tensor<f32>");
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<float>::min(), f32_type)),
-            "dense<1.17549435E-38> : tensor<f32>");
-
-  // Float64
-  auto f64_type = makeTensorType(context, {}, ElementType::F64);
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<double>::max(), f64_type)),
-            "dense<1.7976931348623157E+308> : tensor<f64>");
-  EXPECT_EQ(mlir::debugString(
-                makeConstant(std::numeric_limits<double>::min(), f64_type)),
-            "dense<2.2250738585072014E-308> : tensor<f64>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantArray_Int) {
-  MLIRContext context;
-
-  // Int -- Vector
-  auto i32_type = makeTensorType(context, {2}, ElementType::I32);
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(std::vector<int32_t>{1, 2}, i32_type)),
-      "dense<[1, 2]> : tensor<2xi32>");
-
-  // Init with float
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(ArrayRef<float>{1.0, 2.0}, i32_type)),
-      "dense<[1, 2]> : tensor<2xi32>");
-
-  // Init with APSInt
-  EXPECT_EQ(mlir::debugString(makeConstant(
-                ArrayRef<APSInt>{APSInt::get(1), APSInt::get(2)}, i32_type)),
-            "dense<[1, 2]> : tensor<2xi32>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantArray_Float) {
-  MLIRContext context;
-
-  // Float - ArrayRef
-  auto f32_type = makeTensorType(context, {2}, ElementType::F32);
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(ArrayRef<float>{1.0, 2.0}, f32_type)),
-      "dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf32>");
-
-  // Init with APFloat
-  FloatType f32_type2 = cast<FloatType>(f32_type.getElementType());
-  auto inf = APFloat::getInf(f32_type2.getFloatSemantics());
-  auto nan = APFloat::getNaN(f32_type2.getFloatSemantics());
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(ArrayRef<APFloat>{inf, nan}, f32_type)),
-      "dense<[0x7F800000, 0x7FC00000]> : tensor<2xf32>");
-
-  // Init with Int
-  EXPECT_EQ(mlir::debugString(makeConstant(ArrayRef<int32_t>{1, 2}, f32_type)),
-            "dense<[1.000000e+00, 2.000000e+00]> : tensor<2xf32>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeConstantArray_Complex) {
-  MLIRContext context;
-
-  // Complex
-  auto c32_type = makeTensorType(context, {2}, ElementType::COMPLEXF32);
-  std::vector<std::complex<float>> complexValues = {{1.0, 2.0}, {3.0, 4.0}};
-  EXPECT_EQ(mlir::debugString(makeConstant(complexValues, c32_type)),
-            "dense<[(1.000000e+00,2.000000e+00), (3.000000e+00,4.000000e+00)]> "
-            ": tensor<2xcomplex<f32>>");
-
-  auto c64_type = makeTensorType(context, {2}, ElementType::COMPLEXF64);
-  EXPECT_EQ(mlir::debugString(makeConstant(complexValues, c64_type)),
-            "dense<[(1.000000e+00,2.000000e+00), (3.000000e+00,4.000000e+00)]> "
-            ": tensor<2xcomplex<f64>>");
-
-  // Init with int & float
-  EXPECT_EQ(mlir::debugString(makeConstant(ArrayRef<int32_t>{1, 2}, c32_type)),
-            "dense<[(1.000000e+00,0.000000e+00), (2.000000e+00,0.000000e+00)]> "
-            ": tensor<2xcomplex<f32>>");
-  EXPECT_EQ(
-      mlir::debugString(makeConstant(ArrayRef<double>{1.0, 2.0}, c32_type)),
-      "dense<[(1.000000e+00,0.000000e+00), (2.000000e+00,0.000000e+00)]> : "
-      "tensor<2xcomplex<f32>>");
-}
-
-TEST(AttrTypeBuilderUtilTest, TestMakeEmptyLiteral) {
-  MLIRContext context;
-
-  // Float - ArrayRef
-  auto f32_type = makeTensorType(context, {0}, ElementType::F32);
-  EXPECT_EQ(mlir::debugString(makeConstant(ArrayRef<double>{}, f32_type)),
-            "dense<> : tensor<0xf32>");
-
-  // Int -- Vector
-  auto i32_type = makeTensorType(context, {1, 0}, ElementType::I32);
-  EXPECT_EQ(mlir::debugString(makeConstant(std::vector<int32_t>{}, i32_type)),
-            "dense<> : tensor<1x0xi32>");
-}
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt b/stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt
--- stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt
+++ stablehlo/stablehlo/integrations/cpp/builder/CMakeLists.txt
@@ -1,146 +0,0 @@
-# Copyright 2025 The StableHLO Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-
-#####
-# TableGen
-#####
-
-set(LLVM_LINK_COMPONENTS
-  Support
-  TableGen
-)
-
-add_tablegen(mlir_builder_tblgen MLIR_BUILDER
-  EXPORT MLIR_BUILDER
-  PARTIAL_SOURCES_INTENDED
-  MlirBuilderTblgen.cpp
-)
-
-target_link_libraries(mlir_builder_tblgen
-  PRIVATE
-  MLIRTblgenLib
-  MLIRSupport)
-
-mlir_check_all_link_libraries(mlir_builder_tblgen)
-
-include(MlirBuilderTblgen.cmake)
-
-####
-## Attr / Type Builder Helpers
-####
-
-add_mlir_library(AttrTypeBuilderUtil
-  PARTIAL_SOURCES_INTENDED
-  AttrTypeBuilderUtil.cpp
-
-  LINK_LIBS PUBLIC
-  MLIRIR
-  MLIRSupport  # LLVMSupport is automatically added in AddLLVM.cmake.
-)
-
-####
-## MlirBuilder base class
-####
-
-add_mlir_library(MlirBuilder
-  PARTIAL_SOURCES_INTENDED
-  MlirBuilder.cpp
-
-  LINK_LIBS PUBLIC
-  AttrTypeBuilderUtil
-  MLIRIR
-  MLIRSupport  # LLVMSupport is automatically added in AddLLVM.cmake.
-)
-
-#####
-## Dialect-specific builders
-####
-
-set(LLVM_TARGET_DEFINITIONS "../../../dialect/ChloOps.td")
-mlir_builder_tblgen(ChloBuilder.h.inc -gen-builder-decls)
-mlir_builder_tblgen(ChloBuilder.cpp.inc -gen-builder-defs)
-add_public_tablegen_target(ChloBuilderIncGen)
-add_dependencies(mlir-headers ChloBuilderIncGen)
-
-add_mlir_library(ChloBuilder
-  PARTIAL_SOURCES_INTENDED
-  ChloBuilder.cpp
-
-  DEPENDS
-  ChloBuilderIncGen
-
-  LINK_LIBS PUBLIC
-  ChloOps
-  MlirBuilder
-  MLIRIR
-  MLIRSupport  # LLVMSupport is automatically added in AddLLVM.cmake.
-)
-
-list(GET MLIR_INCLUDE_DIRS 0 mlir_includes)
-set(LLVM_TARGET_DEFINITIONS "${mlir_includes}/mlir/Dialect/Func/IR/FuncOps.td")
-mlir_builder_tblgen(FuncBuilder.h.inc -gen-builder-decls)
-mlir_builder_tblgen(FuncBuilder.cpp.inc -gen-builder-defs)
-add_public_tablegen_target(FuncBuilderIncGen)
-add_dependencies(mlir-headers FuncBuilderIncGen)
-
-add_mlir_library(FuncBuilder
-  PARTIAL_SOURCES_INTENDED
-  FuncBuilder.cpp
-
-  DEPENDS
-  FuncBuilderIncGen
-
-  LINK_LIBS PUBLIC
-  MlirBuilder
-  MLIRFuncDialect
-  MLIRIR
-  MLIRSupport  # LLVMSupport is automatically added in AddLLVM.cmake.
-)
-
-set(LLVM_TARGET_DEFINITIONS "../../../dialect/StablehloOps.td")
-mlir_builder_tblgen(StablehloBuilder.h.inc -gen-builder-decls)
-mlir_builder_tblgen(StablehloBuilder.cpp.inc -gen-builder-defs)
-add_public_tablegen_target(StablehloBuilderIncGen)
-add_dependencies(mlir-headers StablehloBuilderIncGen)
-
-add_mlir_library(StablehloBuilder
-  PARTIAL_SOURCES_INTENDED
-  StablehloBuilder.cpp
-
-  DEPENDS
-  StablehloBuilderIncGen
-
-  LINK_LIBS PUBLIC
-  AttrTypeBuilderUtil
-  MlirBuilder
-  StablehloOps
-  StablehloTypeInference
-  MLIRFuncDialect
-  MLIRIR
-  MLIRInferTypeOpInterface
-  MLIRSupport  # LLVMSupport is automatically added in AddLLVM.cmake.
-)
-
-if (TARGET llvm_gtest)
-    set_target_properties(check-stablehlo-ci PROPERTIES FOLDER "Tests")
-    add_unittest(check-stablehlo-ci "unittests"
-      MlirBuilderTest.cpp
-      StablehloBuilderTest.cpp
-      AttrTypeBuilderUtilTest.cpp
-    )
-  else()
-    message(WARNING "gtest not found, unittests will not be available")
-  endif()
-
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.cpp
@@ -1,35 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include "ChloBuilder.h"
-
-#include <cstdint>
-
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/dialect/ChloOps.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-
-namespace mlir {
-namespace chlo {
-
-/////////////////
-// GENERATED APIs
-/////////////////
-
-#include "stablehlo/integrations/cpp/builder/ChloBuilder.cpp.inc"
-
-}  // namespace chlo
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/ChloBuilder.h
@@ -1,37 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#ifndef STABLEHLO_BUILDER_CHLOBUILDER_H_
-#define STABLEHLO_BUILDER_CHLOBUILDER_H_
-
-#include <cstdint>
-
-#include "llvm/ADT/SmallVector.h"
-#include "stablehlo/dialect/ChloOps.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-
-namespace mlir {
-namespace chlo {
-
-/////////////////
-// GENERATED APIs
-/////////////////
-
-#include "stablehlo/integrations/cpp/builder/ChloBuilder.h.inc"
-
-}  // namespace chlo
-}  // namespace mlir
-
-#endif  // STABLEHLO_BUILDER_CHLOBUILDER_H_
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.cpp
@@ -1,94 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include "stablehlo/integrations/cpp/builder/FuncBuilder.h"
-
-#include <cstdint>
-
-#include "mlir/Dialect/Func/IR/FuncOps.h"
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/Location.h"
-#include "mlir/IR/Types.h"
-#include "mlir/IR/Value.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-
-namespace mlir {
-namespace func {
-
-void updateFuncSignature(FuncOp func) {
-  TypeRange argTypes = func.getBody().getArgumentTypes();
-  TypeRange retTypes{};
-
-  auto ret = func.getOps<ReturnOp>();
-  if (!ret.empty()) {
-    ReturnOp retOp = *ret.begin();
-    retTypes = retOp.getOperandTypes();
-  }
-
-  func.setFunctionType(
-      FunctionType::get(func.getContext(), argTypes, retTypes));
-}
-
-// Must be called after arguments or return types are changed.
-void FunctionBuilder::notifySignatureChanged() {
-  FuncOp func = getOp();
-  updateFuncSignature(func);
-}
-
-MlirOp Argument(FunctionBuilder& fb, Type type) {
-  RegionBuilder rb = fb.getRegionBuilder();
-  MlirOp arg = ::mlir::Argument(rb, type);
-  fb.notifySignatureChanged();
-
-  // RegionBuilder will go out of scope, so swap the arg builder to the
-  // FunctionBuilder.
-  return swap(fb, arg);
-}
-
-void Return(FunctionBuilder& fb, MlirOp& value) {
-  return Return(fb, ArrayRef<MlirOp>{value});
-}
-
-void Return(FunctionBuilder& fb, ArrayRef<MlirOp> values) {
-  RegionBuilder rb = fb.getRegionBuilder();
-  Return(rb, values);
-  fb.notifySignatureChanged();
-}
-
-SmallVector<MlirOp> Call(MlirBuilder& builder, func::FuncOp func,
-                         ArrayRef<MlirOp> operands) {
-  return builder.createVariadic<CallOp>(func, unwrap(operands));
-}
-
-/////////////////
-// GENERATED APIs
-/////////////////
-
-FuncOp Func(MlirBuilder& mb, StringRef name,
-            const RegionBuilderCallback& body) {
-  FuncOp func = mb.createUnwrapped<func::FuncOp>(
-      name, FunctionType::get(&mb.getContext(), {}, {}));
-  RegionBuilder rb(mb, func->getRegion(0));
-  body(rb);
-  updateFuncSignature(func);
-  return func;
-}
-
-#include "stablehlo/integrations/cpp/builder/FuncBuilder.cpp.inc"
-
-}  // namespace func
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/FuncBuilder.h
@@ -1,85 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#ifndef STABLEHLO_BUILDER_FUNCBUILDER_H_
-#define STABLEHLO_BUILDER_FUNCBUILDER_H_
-
-#include <cstdint>
-#include <functional>
-#include <optional>
-#include <string>
-#include <utility>
-#include <vector>
-
-#include "mlir/Dialect/Func/IR/FuncOps.h"
-#include "mlir/IR/Builders.h"
-#include "mlir/IR/Location.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/IR/Value.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-
-namespace mlir {
-namespace func {
-
-class FunctionBuilder;
-
-// Functional wrappers for use in builder methods.
-MlirOp Argument(FunctionBuilder& fb, Type type);
-void Return(FunctionBuilder& fb, MlirOp& value);
-void Return(FunctionBuilder& fb, ArrayRef<MlirOp> values);
-
-// TODO: Do we need RegionOpBuilder? Region ops may be too custom to benefit
-// from a generic base class.
-class FunctionBuilder : public RegionOpBuilder<func::FuncOp> {
- public:
-  FunctionBuilder(ModuleBuilder& mb, std::string name,
-                  std::optional<Location> loc = std::nullopt)
-      : RegionOpBuilder(mb,
-                        func::FuncOp::create(
-                            mb.getOpBuilder(), loc.value_or(mb.getLoc()), name,
-                            FunctionType::get(&mb.getContext(), {}, {}))) {}
-
-  StringRef getName() { return getOp().getName(); }
-
-  void notifySignatureChanged() override;
-};
-
-// FuncOp builder, return the raw FuncOp. For a main function this value can
-// be ignored, but return the FuncOp since the handle can be stored for later
-// use with `Call` to infer output types based on function signature.
-func::FuncOp Func(MlirBuilder& mb, StringRef name,
-                  const RegionBuilderCallback& body);
-
-SmallVector<MlirOp> Call(MlirBuilder& builder, func::FuncOp func,
-                         ArrayRef<MlirOp> operands);
-
-/////////////////
-// GENERATED APIs
-/////////////////
-
-// AVOID THE GENERATED FUNC API.
-// It has worse UX and needs some fixes for func::Return to work with it.
-// Currently func::Return doesn't update the function signature by default.
-
-#include "stablehlo/integrations/cpp/builder/FuncBuilder.h.inc"
-
-// GENERATED CODE
-
-}  // namespace func
-
-}  // namespace mlir
-
-#endif  // STABLEHLO_BUILDER_FUNCBUILDER_H_
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.cpp
@@ -1,57 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include "MlirBuilder.h"
-
-#include "mlir/IR/Location.h"
-#include "mlir/IR/Value.h"
-#include "mlir/IR/ValueRange.h"
-#include "mlir/Support/LLVM.h"
-
-namespace mlir {
-
-// Region Builder APIs
-
-MlirOp Argument(RegionBuilder& rb, Type type) {
-  return MlirOp(rb, rb.getRegion().addArgument(type, rb.getLoc()));
-}
-
-Value unwrap(MlirOp const& value) { return value.getValue(); }
-
-SmallVector<Value> unwrap(ArrayRef<MlirOp> values) {
-  SmallVector<Value> ret;
-  ret.reserve(values.size());
-  for (MlirOp const& value : values) {
-    ret.push_back(unwrap(value));
-  }
-  return ret;
-}
-
-SmallVector<MlirOp> wrap(MlirBuilder& builder, ValueRange values) {
-  SmallVector<MlirOp> ret;
-  ret.reserve(values.size());
-  for (Value const& value : values) {
-    ret.emplace_back(builder, value);
-  }
-  return ret;
-}
-
-MlirOp swap(MlirBuilder& builder, MlirOp& value) {
-  return MlirOp(builder, value.getValue());
-}
-
-}  // namespace mlir
-
-// The following can be autogenerated
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilder.h
@@ -1,271 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#ifndef STABLEHLO_BUILDER_MLIRBUILDER_H_
-#define STABLEHLO_BUILDER_MLIRBUILDER_H_
-
-#include <functional>
-#include <source_location>
-#include <string>
-#include <utility>
-
-#include "llvm/Support/raw_ostream.h"
-#include "mlir/IR/Builders.h"
-#include "mlir/IR/BuiltinOps.h"
-#include "mlir/IR/Location.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/IR/OwningOpRef.h"
-#include "mlir/IR/Value.h"
-#include "mlir/IR/ValueRange.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-
-namespace mlir {
-
-// Forward declare
-class MlirBuilder;
-
-// We need a value wrapper in order to hold onto the Builder that knows where
-// all future ops that use this op should be inserted.
-class MlirOp {
- public:
-  MlirOp() : builder_(), value_() {}
-  MlirOp(MlirBuilder& builder, Value value)
-      : builder_(&builder), value_(value) {}
-  MlirBuilder& getBuilder() { return *builder_; }
-  Value getValue() const { return value_; }
-  Type getType() const { return value_.getType(); }
-  MLIRContext& getContext() { return *value_.getContext(); }
-
-  // Op is only invalid if default constructor is used.
-  // This is needed for conditionals, create a placeholder and set in if/else.
-  bool isValid() const { return builder_ != nullptr; }
-
-  std::string ToString() const {
-    std::string valueAsString;
-    llvm::raw_string_ostream stream(valueAsString);
-    value_.print(stream);
-    return valueAsString;
-  }
-
- private:
-  // Pointer not reference since this class must be copyable.
-  MlirBuilder* builder_;
-  Value value_;
-};
-
-// Base builder class that provides a reference to the MLIRContext and provides
-// Utilities that all builder subclasses can use.
-class MlirBuilder {
- public:
-  MlirBuilder(MLIRContext& context, Location loc)
-      : builder_(&context), loc_(loc) {}
-  MlirBuilder(OpBuilder& builder, Location loc)
-      : builder_(builder), loc_(loc) {}
-
-  MlirBuilder(const MlirBuilder&) = delete;
-  MLIRContext& getContext() { return *builder_.getContext(); }
-  OpBuilder& getOpBuilder() { return builder_; }
-  Location getLoc() { return loc_; }
-  void setLoc(Location loc) { loc_ = loc; }
-
-  // Forward to generated op builder using existing location / context.
-  template <typename OpTy, typename... Args>
-  MlirOp create(Args&&... args) {
-    return MlirOp(*this,
-                  OpTy::create(builder_, loc_, std::forward<Args>(args)...));
-  }
-
-  // Forward to generated op builder with no results using existing location /
-  // context.
-  template <typename OpTy, typename... Args>
-  void create0(Args&&... args) {
-    OpTy::create(builder_, loc_, std::forward<Args>(args)...);
-  }
-
-  template <typename OpTy, typename... Args>
-  SmallVector<MlirOp> createVariadic(Args&&... args) {
-    ValueRange values =
-        OpTy::create(builder_, loc_, std::forward<Args>(args)...).getResults();
-    SmallVector<MlirOp> ret;
-    for (Value value : values) {
-      ret.emplace_back(*this, value);
-    }
-    return ret;
-  }
-
-  // Forward to generated op builder using existing location / context.
-  // Used for ops with multiple results, but with a known number of results.
-  template <typename OpTy, int N, typename... Args>
-  SmallVector<MlirOp> createN(Args&&... args) {
-    SmallVector<MlirOp> ret = createVariadic<OpTy>(std::forward<Args>(args)...);
-    if (ret.size() != N)
-      llvm::report_fatal_error("Expected " + Twine(N) + " results from " +
-                               OpTy::getOperationName() + ", got " +
-                               Twine(ret.size()));
-    return ret;
-  }
-
-  // Forward to generated op builder with no results using existing location /
-  // context.
-  template <typename OpTy, typename... Args>
-  OpTy createUnwrapped(Args&&... args) {
-    return OpTy::create(builder_, loc_, std::forward<Args>(args)...);
-  }
-
-  Type getTensorOfShape();
-
- protected:
-  OpBuilder builder_;
-  Location loc_;
-};
-
-class ModuleBuilder : public MlirBuilder {
- public:
-  ModuleBuilder(MLIRContext& context, Location loc, StringRef name = "")
-      : MlirBuilder(context, loc), module_(ModuleOp::create(loc)) {
-    builder_.setInsertionPointToStart(module_->getBody());
-    if (!name.empty()) module_->setName(name);
-  }
-
-  // Optional Location argument, populate with unknown location if not provided.
-  explicit ModuleBuilder(MLIRContext& context, StringRef name = "")
-      : ModuleBuilder(context, UnknownLoc::get(&context), name) {}
-
-  // Note this method can only be called once.
-  OwningOpRef<ModuleOp> build() { return std::move(module_); }
-
- private:
-  OwningOpRef<ModuleOp> module_;
-};
-
-// Default Region Builder.
-// This is a class that can be used in auto-generated bindings for ops that
-// have a body region.
-//
-// A region builder is passed in a callback function, created with a reference
-// to a region of the op being constructed. Ops with multiple regions will
-// pass multiple RegionBuilders.
-//
-// stablehlo::Reduce({arg0, arg1}, [](RegionBuilder& rb){
-//   auto type = makeTensorType(rb.getContext(), {}, ElementType::I64);
-//   auto regArg0 = Argument(rb, type);
-//   auto regArg1 = Argument(rb, type);
-//   auto add = Add(regArg0, regArg1);
-//   func::Return(rb, add);
-// });
-//
-// For highly used ops with regions, it may make more sense for dialects to
-// create more declarative builders for better UX (FuncBuilder, for example).
-class RegionBuilder : public MlirBuilder {
- public:
-  RegionBuilder(MlirBuilder& builder, Region& region)
-      : MlirBuilder(builder.getOpBuilder(), builder.getLoc()),
-        region_(&region) {
-    // TODO: Only handles single-block regions.
-    // Consider passing in an int region ID to handle multiple blocks.
-    if (region_->getBlocks().empty()) builder_.createBlock(region_);
-    builder_.setInsertionPointToStart(&region_->getBlocks().front());
-  }
-
-  // Create a region builder for a given block, do not emplace a new block.
-  RegionBuilder(MlirBuilder& builder, Block& block)
-      : MlirBuilder(builder.getOpBuilder(), builder.getLoc()),
-        region_(block.getParent()) {
-    // Build at end of block.
-    builder_.setInsertionPointToEnd(&block);
-  }
-
-  Region& getRegion() { return *region_; }
-
-  template <typename OpTy>
-  OpTy getOp() {
-    return region_->getParentOfType<OpTy>();
-  }
-
- private:
-  Region* region_;
-};
-
-// Add an argument to the region body.
-MlirOp Argument(RegionBuilder& rb, Type type);
-
-using RegionBuilderCallback = std::function<void(RegionBuilder&)>;
-
-// A helper class for building ops that have a body region.
-template <typename OpTy>
-class RegionOpBuilder : public MlirBuilder {
- public:
-  RegionOpBuilder(MlirBuilder& builder, OpTy op)
-      : MlirBuilder(builder.getOpBuilder(), op.getLoc()), op_(op), region_() {
-    region_ = &op_->getRegion(0);
-    builder_.setInsertionPointToStart(&region_->emplaceBlock());
-  }
-
-  virtual ~RegionOpBuilder() = default;
-
-  OpTy& build() { return op_; }
-
-  RegionBuilder getRegionBuilder() {
-    return RegionBuilder(*this, region_->getBlocks().front());
-  }
-
- protected:
-  // A callback that can be overridden by subclasses to be notified when the
-  // signature of the region changes.
-  //
-  // This includes on calls to Return and Argument.
-  virtual void notifySignatureChanged() {}
-
-  // Protected op reference, users should only retrieve ops via `build`, but
-  // builders can access the op in an incomplete state.
-  OpTy getOp() { return op_; }
-
- private:
-  OpTy op_;
-  Region* region_;
-};
-
-///////////////
-// Builtin Dialect Helpers
-///////////////
-
-Value unwrap(MlirOp const& value);
-SmallVector<Value> unwrap(ArrayRef<MlirOp> values);
-SmallVector<MlirOp> wrap(MlirBuilder& builder, ValueRange values);
-
-// Change the builder associated with the MlirOp value.
-MlirOp swap(MlirBuilder& builder, MlirOp& value);
-
-// RAII class for temporarily changing the builder location.
-class ScopedBuilderLocation {
- public:
-  // Sets the builder location to `loc`.
-  ScopedBuilderLocation(MlirBuilder& builder, Location loc)
-      : builder_(builder), prev_(builder.getLoc()) {
-    builder.setLoc(loc);
-  }
-
-  // Restores the builder location to the previous value.
-  ~ScopedBuilderLocation() { builder_.setLoc(prev_); }
-
- protected:
-  MlirBuilder& builder_;
-  const Location prev_;
-};
-
-}  // namespace mlir
-
-#endif  // STABLEHLO_BUILDER_MLIRBUILDER_H_
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cmake b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cmake
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cmake
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cmake
@@ -1,45 +0,0 @@
-# Copyright 2025 The StableHLO Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-include(TableGen)
-
-set(MLIR_BUILDER_TABLEGEN_EXE mlir_builder_tblgen)
-
-function(mlir_builder_tblgen ofn)
-  tablegen(MLIR_BUILDER ${ARGV})
-  set(TABLEGEN_OUTPUT ${TABLEGEN_OUTPUT} ${CMAKE_CURRENT_BINARY_DIR}/${ofn}
-      PARENT_SCOPE)
-
-  # Get the current set of include paths for this td file.
-  cmake_parse_arguments(ARG "" "" "DEPENDS;EXTRA_INCLUDES" ${ARGN})
-  get_directory_property(tblgen_includes INCLUDE_DIRECTORIES)
-  list(APPEND tblgen_includes ${ARG_EXTRA_INCLUDES})
-  # Filter out any empty include items.
-  list(REMOVE_ITEM tblgen_includes "")
-
-  # Build the absolute path for the current input file.
-  if (IS_ABSOLUTE ${LLVM_TARGET_DEFINITIONS})
-    set(LLVM_TARGET_DEFINITIONS_ABSOLUTE ${LLVM_TARGET_DEFINITIONS})
-  else()
-    set(LLVM_TARGET_DEFINITIONS_ABSOLUTE ${CMAKE_CURRENT_SOURCE_DIR}/${LLVM_TARGET_DEFINITIONS})
-  endif()
-
-  # Append the includes used for this file to the tablegen_compile_commands
-  # file.
-  file(APPEND ${CMAKE_BINARY_DIR}/tablegen_compile_commands.yml
-      "--- !FileInfo:\n"
-      "  filepath: \"${LLVM_TARGET_DEFINITIONS_ABSOLUTE}\"\n"
-      "  includes: \"${CMAKE_CURRENT_SOURCE_DIR};${tblgen_includes}\"\n"
-  )
-endfunction()
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp
@@ -1,764 +0,0 @@
-/* Copyright 2025 The StableHLO Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include <cassert>
-#include <functional>
-#include <optional>
-#include <string>
-#include <utility>
-#include <vector>
-
-#include "llvm/ADT/STLExtras.h"
-#include "llvm/ADT/StringExtras.h"
-#include "llvm/ADT/Twine.h"
-#include "llvm/Support/CommandLine.h"
-#include "llvm/Support/ErrorHandling.h"
-#include "llvm/Support/FormatVariadic.h"
-#include "llvm/Support/InitLLVM.h"
-#include "llvm/Support/Signals.h"
-#include "llvm/Support/raw_ostream.h"
-#include "llvm/TableGen/Main.h"
-#include "llvm/TableGen/Record.h"
-#include "llvm/TableGen/TableGenBackend.h"
-#include "mlir/Support/IndentedOstream.h"
-#include "mlir/Support/LLVM.h"
-#include "mlir/TableGen/Argument.h"
-#include "mlir/TableGen/Attribute.h"
-#include "mlir/TableGen/Class.h"
-#include "mlir/TableGen/Format.h"
-#include "mlir/TableGen/Operator.h"
-#include "mlir/TableGen/Region.h"
-
-using llvm::raw_ostream;
-using llvm::RecordKeeper;
-using llvm::StringRef;
-using mlir::tblgen::Attribute;
-using mlir::tblgen::FmtContext;
-using mlir::tblgen::Method;
-using mlir::tblgen::MethodBody;
-using mlir::tblgen::MethodParameter;
-using mlir::tblgen::NamedAttribute;
-using mlir::tblgen::NamedRegion;
-using mlir::tblgen::Operator;
-
-namespace mlir {
-namespace {
-
-enum class ActionType {
-  GenBuilderHeader,
-  GenBuilderImpl,
-  GenBuilderDocs,
-};
-
-static llvm::cl::opt<ActionType> action(
-    llvm::cl::desc("action to perform"),
-    llvm::cl::values(clEnumValN(ActionType::GenBuilderHeader,
-                                "gen-builder-decls", "")),
-    llvm::cl::values(clEnumValN(ActionType::GenBuilderImpl, "gen-builder-defs",
-                                "")),
-    llvm::cl::values(clEnumValN(ActionType::GenBuilderDocs, "gen-builder-docs",
-                                "")));
-
-LogicalResult skipOperation(const Operator& op, StringRef reason) {
-  llvm::errs() << "Skipping " << op.getCppClassName() << ": " << reason << "\n";
-  return failure();
-}
-
-// Helpers
-
-/// Returns true if the SameArgumentAndResultTypes trait can be used to infer
-/// result types of the given operation.
-bool hasSameArgumentAndResultTypes(const Operator& op) {
-  return op.getTrait("::mlir::OpTrait::SameOperandsAndResultType") &&
-         op.getNumVariableLengthResults() == 0;
-}
-
-/// Returns true if the FirstAttrDerivedResultType trait can be used to infer
-/// result types of the given operation.
-/// TODO: Once the use of this is understood, it should be added to tablegen to
-/// simplify builders for ops that use this trait.
-// bool hasFirstAttrDerivedResultTypes(const Operator &op) {
-//   return op.getTrait("::mlir::OpTrait::FirstAttrDerivedResultType") &&
-//          op.getNumVariableLengthResults() == 0;
-// }
-
-/// Returns true if the InferTypeOpInterface can be used to infer result types
-/// of the given operation.
-bool hasInferTypeInterface(const Operator& op) {
-  return op.getTrait("::mlir::InferTypeOpInterface::Trait");
-}
-
-/// Returns true if there is a trait or interface that can be used to infer
-/// result types of the given operation.
-bool canInferType(const Operator& op) {
-  // TODO: Support hasFirstAttrDerivedResultTypes(op)
-  bool hasOutputType = op.getNumResults() > 0;
-  return !hasOutputType || hasSameArgumentAndResultTypes(op) ||
-         hasInferTypeInterface(op);
-}
-
-bool hasVariadicResult(const Operator& op) {
-  return llvm::any_of(op.getResults(),
-                      [](const auto& result) { return result.isVariadic(); });
-}
-
-bool hasSingleVariadicResult(const Operator& op) {
-  return op.getNumResults() == 1 && hasVariadicResult(op);
-}
-
-bool hasRegions(const Operator& op,
-                std::optional<int> numRegions = std::nullopt) {
-  if (numRegions.has_value()) {
-    return op.getNumRegions() == static_cast<unsigned int>(numRegions.value());
-  }
-  return op.getNumRegions() > 0;
-}
-
-bool isTerminator(const Operator& op) {
-  return op.getTrait("::mlir::OpTrait::IsTerminator");
-}
-
-// Returns true if we can use unwrapped value for the given `attr` in builders.
-bool canUseUnwrappedRawValue(const tblgen::Attribute& attr) {
-  return attr.getReturnType() != attr.getStorageType() &&
-         // We need to wrap the raw value into an attribute in the builder impl
-         // so we need to make sure that the attribute specifies how to do that.
-         !attr.getConstBuilderTemplate().empty();
-}
-
-///////
-// Build Signature
-///////
-
-class OpBuilderEmitter {
- public:
-  explicit OpBuilderEmitter(const Operator& op) : op_(op) {}
-
-  const Operator& getOp() { return op_; }
-
-  // Return the name of the builder method, the op name without "Op" suffix.
-  // I.e. AddOp --> Add
-  StringRef getMethodName() {
-    // Verify trait
-    auto opName = op_.getCppClassName();
-    if (opName.ends_with("Op")) return opName.drop_back(2);
-    return opName;
-  }
-
-  // Return the return type string.
-  std::string getReturnType();
-
-  // Get Operand parameters.
-  SmallVector<MethodParameter> getOperandParameters();
-
-  SmallVector<MethodParameter> getAttributeParameters();
-
-  // For each region, add a `RegionBuilderCallback` arg
-  SmallVector<MethodParameter> getRegionParameters();
-
-  struct BuilderParams {
-    // Only set if one of the operands can be used for builder ref
-    std::optional<MethodParameter> builderRefOperand;
-    // Only set if op does not support type inference, requires explicit type
-    std::optional<MethodParameter> outputShape;
-    SmallVector<MethodParameter> operands;
-    SmallVector<MethodParameter> attributes;
-    SmallVector<MethodParameter> regionBuilders;
-  };
-
-  // Returns a builder reference from an MlirOp operand, if one exists.
-  // If no required operand (not optional or variadic) exists, returns
-  // std::nullopt.
-  std::optional<MethodParameter> getBuilderFromOperands() {
-    auto builderIt = llvm::find_if(op_.getOperands(), [](const auto& operand) {
-      return !operand.isOptional() && !operand.isVariadic();
-    });
-    if (builderIt != op_.getOperands().end()) {
-      return MethodParameter("MlirBuilder &",
-                             (builderIt->name + ".getBuilder()").str());
-    }
-    return std::nullopt;
-  }
-
-  // Return a default builder reference, if no required operands exist, this
-  // parameter must be injected as the first argument.
-  MethodParameter getDefaultBuilder() {
-    if (isTerminator(getOp())) {
-      return MethodParameter("RegionBuilder &", "builder");
-    }
-    return MethodParameter("MlirBuilder &", "builder");
-  }
-
-  // If the op does not support type inference, return a default output shape
-  // parameter that must be injected.
-  MethodParameter getDefaultOutputShape() {
-    return MethodParameter("Type", "resultType");
-  }
-
-  // Returns a builder reference from an MlirOp operand, if one exists.
-  // If no required operand (not optional or variadic) exists, returns
-  // the default builder `builder`.
-  MethodParameter getBuilderRef() {
-    // Terminators use RegionBuilders.
-    if (isTerminator(getOp())) return getDefaultBuilder();
-
-    std::optional<MethodParameter> builderRef = getBuilderFromOperands();
-    if (builderRef.has_value()) return std::move(builderRef.value());
-    return getDefaultBuilder();
-  }
-
-  // Returns a reference to the mlir::OpBuilder, either using MlirOp operand or
-  // builder parameter. This is used to create attributes / types.
-  MethodParameter getOpBuilderRef() {
-    MethodParameter builderRef = getBuilderRef();
-    StringRef builderRefName = builderRef.getName();
-    return MethodParameter("::mlir::OpBuilder &",
-                           (builderRefName + ".getOpBuilder()").str());
-  }
-
-  // Returns all parameters needed for builder decl and defs.
-  // The ordering and use of these parameters depends on whether this is a decl
-  // or def.
-  BuilderParams getOpBuilderParameters() {
-    BuilderParams params;
-    params.builderRefOperand = getBuilderFromOperands();
-    if (!canInferType(getOp())) {
-      params.outputShape = getDefaultOutputShape();
-    }
-    params.operands = getOperandParameters();
-    params.attributes = getAttributeParameters();
-    params.regionBuilders = getRegionParameters();
-    return params;
-  }
-
-  // Using parameters from getOpBuilderParameters, return a method signature
-  // to be used for the builder method decl.
-  Method getMethodDecl() {
-    // Make a copy to move into method signature.
-    BuilderParams params = getOpBuilderParameters();
-    SmallVector<MethodParameter> parameters;
-    if (!params.builderRefOperand.has_value() || isTerminator(getOp())) {
-      parameters.push_back(getDefaultBuilder());
-    }
-    if (params.outputShape.has_value()) {
-      parameters.push_back(params.outputShape.value());
-    }
-    parameters.append(params.operands.begin(), params.operands.end());
-
-    // Insert RegionBuilders before attributes, since attrs can be optional.
-    parameters.append(params.regionBuilders.begin(),
-                      params.regionBuilders.end());
-
-    // Push optional / default attributes to the end.
-    llvm::sort(params.attributes,
-               [](const MethodParameter& a, const MethodParameter& b) {
-                 return a.hasDefaultValue() < b.hasDefaultValue();
-               });
-
-    parameters.append(params.attributes.begin(), params.attributes.end());
-    return Method(getReturnType(), getMethodName(), Method::None,
-                  std::move(parameters));
-  }
-
-  SmallVector<MethodParameter> getParametersForCall() {
-    // Make a copy to move into method signature.
-    BuilderParams params = getOpBuilderParameters();
-    SmallVector<MethodParameter> parameters;
-    if (params.outputShape.has_value()) {
-      parameters.push_back(getDefaultOutputShape());
-    }
-    for (auto& operand : params.operands) {
-      parameters.push_back(
-          MethodParameter("MlirOp &", "unwrap(" + operand.getName() + ")"));
-    }
-    // Skip regions, not used in builder calls.
-    parameters.append(params.attributes);
-    return parameters;
-  }
-
-  // Inset a call to the builder method into the given body.
-  // I.e. `lhs.getBuilder().create<AddOp>(unwrap(lhs), unwrap(rhs));`
-  void buildMethodBody(Method& method);
-
-  // Inset a call to the builder method into the given body.
-  // I.e. `lhs.getBuilder().create<AddOp>(unwrap(lhs), unwrap(rhs));`
-  void buildMethodCall(MethodBody& body);
-
-  // Insert a creation call and invoke region callbacks.
-  void buildMethodCallWithRegions(MethodBody& body);
-
-  // Write a description of the current builder method either to a code comment
-  // or a markdown doc string.
-  void buildMethodDescription(mlir::raw_indented_ostream& os,
-                              StringRef linePrefix);
-
-  // Write a doc string for the current builder method.
-  void buildMethodDoc(mlir::raw_indented_ostream& os, Method& method);
-
- private:
-  const Operator& op_;
-};
-
-std::string resultsStringSwitch(const Operator& op,
-                                std::function<std::string()> zero,
-                                std::function<std::string()> one,
-                                std::function<std::string(int)> many,
-                                std::function<std::string()> variadic) {
-  auto numResults = op.getNumResults();
-  if (numResults == 0) return zero();
-  if (hasSingleVariadicResult(op)) return variadic();
-  if (numResults == 1) return one();
-  if (numResults > 1) return many(numResults);
-  return "<<ResultStringSwitch error>>";
-}
-
-// Returns the return type of the builder method.
-//   Zero results     --> void
-//   One result       --> MlirOp
-//   N results        --> SmallVector<MlirOp, N>
-//   Single Variadic  --> SmallVector<MlirOp>
-std::string OpBuilderEmitter::getReturnType() {
-  return resultsStringSwitch(
-      getOp(),                    //
-      []() { return "void"; },    // zero
-      []() { return "MlirOp"; },  // one
-      [](int n) { return llvm::formatv("SmallVector<MlirOp, {0}>", n).str(); },
-      []() { return "SmallVector<MlirOp>"; });  // variadic
-}
-
-// Get operand params:
-//   Operand -> MlirOp &
-//   Optional Operand -> std::optional<MlirOp>
-//   Variadic Operand -> ArrayRef<MlirOp>
-SmallVector<MethodParameter> OpBuilderEmitter::getOperandParameters() {
-  auto op = getOp();
-  SmallVector<MethodParameter> parameters;
-  for (const auto& operand : op.getOperands()) {
-    if (operand.isOptional()) {
-      parameters.emplace_back("std::optional<MlirOp>", operand.name,
-                              /*optional=*/true);
-      continue;
-    }
-    if (operand.isVariadic()) {
-      parameters.emplace_back("ArrayRef<MlirOp>", operand.name);
-      continue;
-    }
-    // Regular operand.
-    parameters.emplace_back("MlirOp &", operand.name);
-  }
-  return parameters;
-}
-
-StringRef getAttributeType(Attribute attr) {
-  if (canUseUnwrappedRawValue(attr)) {
-    return attr.getReturnType();
-  }
-  return attr.getStorageType();
-}
-
-// Return a default value for an attribute.
-//   Optional & Default -> Default
-//   Optional & No Default -> {}
-//   Default -> Default
-std::optional<std::string> getAttributeDefaultValue(OpBuilderEmitter& emitter,
-                                                    Attribute attr) {
-  if (!attr.isOptional() && !attr.hasDefaultValue()) return std::nullopt;
-
-  FmtContext fctx;
-  fctx.withBuilder(emitter.getOpBuilderRef().getName());
-
-  if (canUseUnwrappedRawValue(attr) && attr.hasDefaultValue())
-    return tgfmt(attr.getDefaultValue(), &fctx);
-  return "{}";
-}
-
-// Get attribute params:
-// TODO: Support buildable attributes from default values with fmt gen.
-SmallVector<MethodParameter> OpBuilderEmitter::getAttributeParameters() {
-  auto op = getOp();
-  SmallVector<MethodParameter> attributeParameters;
-  for (auto& namedAttr : op.getAttributes()) {
-    Attribute attr = namedAttr.attr;
-    StringRef attrType = getAttributeType(attr);
-    std::optional<std::string> defaultValue =
-        getAttributeDefaultValue(*this, attr);
-
-    attributeParameters.emplace_back(
-        attrType, namedAttr.name, defaultValue.value_or(""), attr.isOptional());
-  }
-  return attributeParameters;
-}
-
-SmallVector<MethodParameter> OpBuilderEmitter::getRegionParameters() {
-  SmallVector<MethodParameter> regionParameters;
-  for (auto& region : getOp().getRegions()) {
-    regionParameters.emplace_back("const RegionBuilderCallback &", region.name);
-  }
-  return regionParameters;
-}
-
-///////
-// Build Impl
-///////
-
-class ScopedIndent {
- public:
-  explicit ScopedIndent(MethodBody& body) : body_(body) { body_.indent(); }
-  ~ScopedIndent() { body_.unindent(); }
-
- private:
-  MethodBody& body_;
-};
-
-// Zero:     builder.create0<Op>(...)
-// One:      builder.create<Op>(...)
-// Many:     builder.createN<Op, N>(...)
-// Variadic: builder.createVariadic<Op>(...)
-void OpBuilderEmitter::buildMethodCall(MethodBody& body) {
-  const Operator& op = getOp();
-  std::string builderRef = getBuilderRef().getName().str();
-
-  // Build comma separated list of parameters for the call.
-  std::string callParams;
-  llvm::raw_string_ostream os(callParams);
-  llvm::interleaveComma(getParametersForCall(), os,
-                        [&](MethodParameter arg) { os << arg.getName(); });
-
-  auto getCallTo = [&](StringRef methodName,
-                       std::optional<std::string> n = std::nullopt) {
-    // builder.createOp<Op[, N]>(...)
-    auto callFmt = "{0}.{1}<{2}{3}>({4});\n";
-    return llvm::formatv(callFmt, builderRef, methodName, op.getCppClassName(),
-                         n.value_or(""), callParams)
-        .str();
-  };
-
-  // builder.createUnwrapped<Op>(...)
-  if (hasRegions(op)) {
-    body << getCallTo("createUnwrapped");
-    return;
-  }
-
-  body << resultsStringSwitch(
-      op, [&]() { return getCallTo("create0"); },  // zero
-      [&]() { return getCallTo("create"); },       // one
-      [&](int n) { return getCallTo("createN", ", " + std::to_string(n)); },
-      [&]() { return getCallTo("createVariadic"); });  // variadic
-}
-
-void OpBuilderEmitter::buildMethodCallWithRegions(MethodBody& body) {
-  std::string builderRef = getBuilderRef().getName().str();
-
-  // OpTy op = builder.createUnwrapped(...);
-  Twine opVar = "_op";
-  body << getOp().getCppClassName() << " " << opVar << " = ";
-  buildMethodCall(body);
-
-  // RegionBuilder condBuilder(this, &_op->getRegion(1));
-  // cond(condBuilder);
-  //   {0} = region-name, {1} = builderRef {2} = op-name, {3} = region-idx
-  auto buildRegionFmt = R"(
-    RegionBuilder _{0}Builder({1}, {2}->getRegion({3}));
-    {0}(_{0}Builder);
-  )";
-
-  for (auto [idx, region] : llvm::enumerate(getOp().getRegions())) {
-    auto impl =
-        llvm::formatv(buildRegionFmt, region.name, builderRef, opVar, idx)
-            .str();
-    body.getStream().printReindented(impl);
-  }
-
-  body << "return"
-       << resultsStringSwitch(
-              getOp(), [&]() { return ";\n"; },  // zero
-              [&]() {                            // one
-                return llvm::formatv(" MlirOp({0}, {1});\n", builderRef, opVar);
-              },
-              [&](int n) {  // many
-                return llvm::formatv(" wrap({0}, {1}->getResults());\n",
-                                     builderRef, opVar)
-                    .str();
-              },
-              [&]() {  // variadic
-                return llvm::formatv(" wrap({0}, {1}->getResults());\n",
-                                     builderRef, opVar);
-              });
-}
-
-void OpBuilderEmitter::buildMethodBody(Method& method) {
-  MethodBody& body = method.body();
-  ScopedIndent indent(body);
-  if (hasRegions(getOp())) {
-    buildMethodCallWithRegions(body);
-    return;
-  }
-  body << "return ";
-  buildMethodCall(body);
-}
-
-void OpBuilderEmitter::buildMethodDescription(mlir::raw_indented_ostream& os,
-                                              StringRef linePrefix) {
-  std::string description;
-  llvm::raw_string_ostream ds(description);
-
-  if (isTerminator(op_)) {
-    ds << R"(
-This operation is a Region's Terminator. It can only be called in a RegionBuilder
-function callback when constructing the body of an op.)";
-  }
-  if (hasRegions(op_)) {
-    ds << R"(
-This operation has a body region built via a callback function.)";
-  }
-
-  if (!description.empty()) {
-    os << "\n";
-    ds << "\n";
-    os.printReindented(description, linePrefix);
-  }
-}
-
-// Returns a string that is either a link to the spec or the op name.
-// The spec link is only generated for ops in dialects that have a spec.
-//   known_dialect.op   --> [`known_dialect.op`](spec_link#op)
-//   unknown_dialect.op --> `unknown_dialect.op`
-std::string maybeSpecLinkedOpName(Operator const& op) {
-  std::string opName = op.getOperationName();
-  // The format will be filled with the lowercase op name with dialect
-  // stripped.
-  // TODO: These links dont always work, the latter arg should be the cpp
-  // namespaced op in all lowercase.
-  DenseMap<StringRef, StringRef> dialectToSpecFmt = {
-      // clang-format off
-    {"chlo", "https://openxla.org/stablehlo/generated/chlo#chlo{1}_chlo{1}op"},
-    {"func", "https://mlir.llvm.org/docs/Dialects/Func/#func{1}-func{1}op"},
-    {"sdy", "https://openxla.org/shardy/sdy_dialect#sdy{1}_sdy{1}op"},
-    {"stablehlo", "https://openxla.org/stablehlo/spec#{1}"},
-    {"tosa", "https://mlir.llvm.org/docs/Dialects/TOSA/#tosa{1}-mlirtosa{1}op"},
-      // clang-format on
-  };
-  auto dialect = op.getDialect().getName();
-  if (dialectToSpecFmt.contains(dialect)) {
-    StringRef baseUrlFmt = dialectToSpecFmt[dialect];
-    StringRef opHref = opName;
-    if (opHref.starts_with(dialect)) {
-      opHref = opHref.drop_front(dialect.size() + 1);
-    }
-    std::string urlFmt = "[`{0}`](" + baseUrlFmt.str() + ")";
-    return llvm::formatv(urlFmt.c_str(), opName, opHref).str() + "\n";
-  }
-
-  return llvm::formatv("`{0}`", opName).str() + " ";
-}
-
-void OpBuilderEmitter::buildMethodDoc(mlir::raw_indented_ostream& os,
-                                      Method& method) {
-  const Operator& op = getOp();
-
-  os << "### `" << op.getDialectName() << "::" << op.getCppClassName() << "`\n";
-  os << "\n";
-  os << "Creates a new " << maybeSpecLinkedOpName(op) << "operation.\n";
-  buildMethodDescription(os, "");
-  os << "\n";
-  os << "```c++\n";
-  method.writeDeclTo(os);
-  os << "```\n\n";
-}
-
-///////
-// Main entry point & validation
-///////
-
-LogicalResult verifyReturnType(const Operator& op) {
-  bool hasVariadicResult = llvm::any_of(
-      op.getResults(), [](const auto& result) { return result.isVariadic(); });
-  if (hasVariadicResult && op.getNumResults() > 1)
-    return skipOperation(op, "Only single variadic result supported");
-  return success();
-}
-
-// Must be operands followed by attributes.
-LogicalResult verifyArgumentOrder(const Operator& op) {
-  bool sawAttr = false;
-  for (const auto& arg : op.getArgs()) {
-    if (isa<NamedAttribute*>(arg)) {
-      sawAttr = true;
-      continue;
-    }
-    if (sawAttr) return skipOperation(op, "Attributes must be after operands");
-  }
-  if (llvm::any_of(op.getOperands(),
-                   [](auto operand) { return operand.isOptional(); }))
-    return skipOperation(op, "Optional operands not supported.");
-  return success();
-}
-
-LogicalResult verifyAttributes(const Operator& op) {
-  // TODO: Name conflicts cause issues, like StableHLO Transpose attr vs
-  // the free stablehlo::Transpose op builder method. The StableHLO enum kind
-  // should be renamed.
-  llvm::DenseSet<StringRef> knownBadTypes = {"StableHLO_TransposeAttr"};
-  bool hasBadType =
-      llvm::any_of(op.getAttributes(), [&](const NamedAttribute& attr) {
-        return knownBadTypes.contains(attr.attr.getDefName());
-      });
-  if (hasBadType) return skipOperation(op, "Attributes have known bad types");
-  return success();
-}
-
-LogicalResult verifyRegions(const Operator& op) {
-  if (llvm::any_of(op.getRegions(),
-                   [](const auto& region) { return region.isVariadic(); }))
-    return skipOperation(op, "Variadic regions not supported");
-  return success();
-}
-
-LogicalResult verifyOpTraits(const Operator& op) {
-  if (op.getTrait("::mlir::FunctionOpInterface::Trait"))
-    return skipOperation(op, "FunctionOpInterface not supported");
-  if (op.skipDefaultBuilders())
-    return skipOperation(op, "Op does not use MLIR's default builders");
-  return success();
-}
-
-// Returns an OpBuilderEmitter if possible.
-// This is mostly limited by features, and more ops can have op builder emitters
-// as feature support is added.
-// Some supported patterns:
-// - [X] Op has one or more Value operands.
-// - [X] Op has one or more results.
-// - [X] Op has no required attributes.
-// - [X] Op has no operands.
-// - [X] Op cannot infer type (take result type as argument).
-// - [X] Op has no results.
-// - [X] Op has no required MlirOp operands.
-// - [X] Op has single variadic operand / result.
-// - [X] Op has required attributes.
-// - [X] Op has optional attribute followed by non-optional attribute.
-// - [ ] Op has a region.
-// - [ ] Op has multiple operands results, with some variadic(s).
-// - [ ] Op Optional operands.
-// - [ ] Op declares attributes before operands (chlo.constant_like).
-// - [ ] Op uses `FirstAttrDerivedResultTypes` to infer result type (tosa.const)
-// - [ ] Op method is a name conflict (triangular_solve Transpose is enum & fn).
-// - [ ] Op does not use MLIR's default builders.
-FailureOr<OpBuilderEmitter> getAndVerifyOpBuilderEmitter(const Operator& op) {
-  // Verify return type
-  if (failed(verifyReturnType(op))) return failure();
-
-  // Verify arg order is operands -> attributes
-  if (failed(verifyArgumentOrder(op))) return failure();
-
-  // Verify attributes
-  if (failed(verifyAttributes(op))) return failure();
-
-  // Verify Regions -- no variadic regions yet.
-  if (failed(verifyRegions(op))) return failure();
-
-  // Verify op traits -- no FunctionOpInterface yet.
-  if (failed(verifyOpTraits(op))) return failure();
-
-  return OpBuilderEmitter(op);
-}
-
-void WriteOperatorBuilder(OpBuilderEmitter& emitter,
-                          mlir::raw_indented_ostream& os) {
-  Method method = emitter.getMethodDecl();
-  emitter.buildMethodBody(method);
-
-  // ASSUMPTION: Operation has at least one operand.
-  // Need to switch on operations that don't have operands to take a builder as
-  // an argument.
-  switch (action) {
-    case ActionType::GenBuilderHeader:
-      method.writeDeclTo(os);
-      return;
-    case ActionType::GenBuilderImpl:
-      method.writeDefTo(os, "");
-      return;
-    case ActionType::GenBuilderDocs:
-      emitter.buildMethodDoc(os, method);
-      return;
-  }
-  llvm::report_fatal_error("[WriteOperatorBuilder] Unknown enum value.");
-}
-
-void writeFileHeader(mlir::raw_indented_ostream& os, StringRef header) {
-  if (action == ActionType::GenBuilderDocs) {
-    os << "# " << header << "\n\n";
-    os << "[TOC]\n\n";
-    os << "## Builder Methods\n\n";
-    return;
-  }
-  emitSourceFileHeader(header, os);
-}
-
-void writeSkipped(mlir::raw_indented_ostream& os,
-                  std::vector<Operator> skipped) {
-  if (skipped.empty()) return;
-
-  std::string prefix = "// Skipped ";
-  if (action == ActionType::GenBuilderDocs) {
-    prefix = " - ";
-    os << "## Skipped Operations\n\n";
-    os << "Unable to generate builder for the following operations:\n\n";
-  }
-  for (const auto& op : skipped) {
-    os << prefix << maybeSpecLinkedOpName(op) << "\n";
-  }
-}
-
-// The function below has a non-constant reference as that is required by LLVM's
-// TableGenMain.
-// NOLINTNEXTLINE
-bool GenerateStablehloBuilderMain(raw_ostream& os,
-                                  const RecordKeeper& records) {
-  mlir::raw_indented_ostream indentedOs(os);
-  // Get the list of StableHLO operations that are allowed to be directly
-  // converted to HLO without intermediate MHLO step.
-
-  // Emit file header.
-  auto opList = records.getAllDerivedDefinitions("Op");
-  auto dialect = Operator(opList[0]).getDialect().getName();
-  auto header = ("`" + dialect + "` MLIR Dialect Builder API").str();
-  writeFileHeader(indentedOs, header);
-
-  // Emit all the MLIR Builders
-  std::vector<Operator> skipped;
-  for (const auto* def : records.getAllDerivedDefinitions("Op")) {
-    Operator op(def);
-    FailureOr<OpBuilderEmitter> emitter = getAndVerifyOpBuilderEmitter(op);
-    if (failed(emitter)) {
-      skipped.push_back(op);
-      continue;
-    }
-    WriteOperatorBuilder(emitter.value(), indentedOs);
-  }
-
-  writeSkipped(indentedOs, skipped);
-
-  return false;
-}
-
-}  // namespace
-}  // namespace mlir
-
-int main(int argc, char** argv) {
-  llvm::sys::PrintStackTraceOnErrorSignal(argv[0]);
-  llvm::InitLLVM y(argc, argv);
-  llvm::cl::ParseCommandLineOptions(argc, argv);
-  return TableGenMain(argv[0], &mlir::GenerateStablehloBuilderMain);
-}
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
@@ -1,308 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include <string>
-
-#include "gtest/gtest.h"
-#include "llvm/Support/raw_ostream.h"
-#include "mlir/Dialect/Func/IR/FuncOps.h"
-#include "mlir/IR/BuiltinOps.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/Location.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/IR/OperationSupport.h"
-#include "mlir/IR/OwningOpRef.h"
-#include "mlir/IR/Types.h"
-#include "mlir/IR/Verifier.h"
-#include "mlir/Support/DebugStringHelper.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/dialect/Register.h"
-#include "stablehlo/dialect/StablehloOps.h"
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-#include "stablehlo/integrations/cpp/builder/ChloBuilder.h"
-#include "stablehlo/integrations/cpp/builder/FuncBuilder.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.h"
-
-namespace mlir {
-namespace stablehlo {
-namespace {
-
-// Wrap a module builder and register the classes needed
-class StablehloModuleBuilder {
- public:
-  StablehloModuleBuilder()
-      : context_(), module_builder_(context_, UnknownLoc::get(&context_)) {
-    DialectRegistry registry;
-    stablehlo::registerAllDialects(registry);
-    context_.appendDialectRegistry(registry);
-    context_.loadAllAvailableDialects();
-  }
-
-  ModuleBuilder& get() { return module_builder_; }
-  ModuleBuilder* operator->() { return &module_builder_; }
-
- private:
-  MLIRContext context_;
-  ModuleBuilder module_builder_;
-};
-
-}  // namespace
-
-TEST(MlirBuilderTest, SimpleAdd) {
-  std::string expected = R"(module {
-  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {
-    %c = stablehlo.constant dense<1> : tensor<i64>
-    %0 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    %1 = stablehlo.add %arg0, %0 : tensor<2xi64>
-    return %1 : tensor<2xi64>
-  }
-})";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type2xi64 = RankedTensorType::get({2}, fb.getOpBuilder().getI64Type());
-    auto arg0 = func::Argument(fb, type2xi64);
-    auto cst = stablehlo::Constant(fb, 1);
-    auto broadcast = stablehlo::BroadcastInDim(type2xi64, cst, {});
-    auto add = stablehlo::Add(arg0, broadcast);
-    func::Return(fb, {add});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, MultipleReturn) {
-  std::string expected = R"(module {
-  func.func @main(%arg0: tensor<2xi64>) -> (tensor<2xi64>, tensor<2xi64>) {
-    %c = stablehlo.constant dense<1> : tensor<2xi64>
-    return %arg0, %c : tensor<2xi64>, tensor<2xi64>
-  }
-})";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type2xi64 = makeTensorType(fb.getContext(), {2}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type2xi64);
-    auto cst = stablehlo::Constant(fb, mlir::makeConstant(1L, type2xi64));
-    func::Return(fb, {arg0, cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, NoReturn) {
-  std::string expected = R"(module {
-  func.func @main() {
-    return
-  }
-})";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    func::Return(fb, {});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, MixedDialectProgram) {
-  std::string expected = R"(module {
-  func.func @main(%arg0: tensor<4xi64>) -> tensor<2xi64> {
-    %c = stablehlo.constant dense<1> : tensor<i64>
-    %0 = chlo.broadcast_add %arg0, %c : (tensor<4xi64>, tensor<i64>) -> tensor<4xi64>
-    %values, %indices = chlo.top_k(%0, k = 2) : tensor<4xi64> -> (tensor<2xi64>, tensor<2xi32>)
-    %1 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i64>) -> tensor<2xi64>
-    return %1 : tensor<2xi64>
-  }
-})";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto type4xi64 = makeTensorType(fb.getContext(), {4}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type4xi64);
-    auto cst = stablehlo::Constant(fb, 1);
-    auto add = chlo::BroadcastAdd(arg0, cst);
-    auto topkAndIndices = chlo::TopK(add, 2);
-    auto broadcast =
-        stablehlo::BroadcastInDim(topkAndIndices[0].getType(), cst, {});
-    func::Return(fb, broadcast);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, TestSourceLocation) {
-  std::string expected = R"(#loc1 = loc("main.mlir":1:1)
-module {
-  func.func @main(%arg0: tensor<i64> loc("main.mlir":1:1)) -> tensor<i64> {
-    %c = stablehlo.constant dense<1> : tensor<i64> loc(#loc2)
-    return %c : tensor<i64> loc(#loc1)
-  } loc(#loc1)
-} loc(#loc)
-#loc = loc(unknown)
-#loc2 = loc("constant.mlir":10:20)
-)";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    ScopedBuilderLocation loc(
-        mb.get(), fileLineColLoc(mb->getContext(), "main.mlir", 1, 1));
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto type2xi64 = makeTensorType(fb.getContext(), {}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type2xi64);
-    static_cast<void>(arg0);  // unused
-
-    // This would typically be a library call, emulate with a lambda.
-    auto buildCst = [type2xi64](MlirBuilder& b) {
-      ScopedBuilderLocation loc(
-          b, fileLineColLoc(b.getContext(), "constant.mlir", 10, 20));
-      return stablehlo::Constant(b, mlir::makeConstant(1L, type2xi64));
-    };
-
-    func::Return(fb, buildCst(fb));
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  std::string moduleString;
-  llvm::raw_string_ostream os(moduleString);
-  module->print(os, OpPrintingFlags().enableDebugInfo());
-  EXPECT_EQ(expected, moduleString);
-}
-
-////////
-// Region Tests
-////////
-
-TEST(MlirBuilderTest, TestOpWithMultipleRegions) {
-  std::string expected = R"(module {
-  func.func @main(%arg0: tensor<i64>) -> tensor<i64> {
-    %c = stablehlo.constant dense<1> : tensor<i64>
-    %0 = stablehlo.while(%iterArg = %arg0) : tensor<i64>
-    cond {
-      %1 = stablehlo.compare  LT, %iterArg, %c : (tensor<i64>, tensor<i64>) -> tensor<i1>
-      stablehlo.return %1 : tensor<i1>
-    } do {
-      %1 = stablehlo.subtract %iterArg, %c : tensor<i64>
-      stablehlo.return %1 : tensor<i64>
-    }
-    return %0 : tensor<i64>
-  }
-})";
-
-  StablehloModuleBuilder mb;
-  func::FunctionBuilder fb(mb.get(), "main");
-  auto arg0Type = makeTensorType(fb.getContext(), {}, ElementType::I64);
-  auto arg0 = func::Argument(fb, arg0Type);
-
-  auto cst = stablehlo::Constant(fb, mlir::makeConstant(1L, arg0Type));
-
-  auto loop = stablehlo::While(
-      fb, {arg0},
-      [&cst](RegionBuilder& cond) {
-        auto loopArg0 = Argument(cond, cst.getType());
-        auto cmp = stablehlo::Compare(loopArg0, cst,
-                                      stablehlo::ComparisonDirection::LT);
-        stablehlo::Return(cond, cmp);
-      },
-      [&cst](RegionBuilder& body) {
-        auto loopArg0 = Argument(body, cst.getType());
-        auto add = stablehlo::Subtract(loopArg0, cst);
-        stablehlo::Return(body, add);
-      });
-  func::Return(fb, loop);
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(module.get())));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-////////
-// Func Dialect Tests
-////////
-
-TEST(FuncBuilderTest, TestFuncCallbackApi) {
-  std::string expected = R"(module {
-  func.func @main(%arg0: tensor<i64>, %arg1: tensor<i64>) -> tensor<i64> {
-    %0 = stablehlo.add %arg0, %arg1 : tensor<i64>
-    return %0 : tensor<i64>
-  }
-})";
-
-  StablehloModuleBuilder mb;
-  func::Func(mb.get(), "main", [](RegionBuilder& rb) {
-    auto type = makeTensorType(rb.getContext(), {}, ElementType::I64);
-    auto regArg0 = Argument(rb, type);
-    auto regArg1 = Argument(rb, type);
-    auto add = Add(regArg0, regArg1);
-    func::Return(rb, add);
-  });
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(FuncBuilderTest, TestCallOp) {
-  std::string expected = R"(module {
-  func.func @callee(%arg0: tensor<i64>, %arg1: tensor<i64>) -> tensor<i64> {
-    %0 = stablehlo.add %arg0, %arg1 : tensor<i64>
-    return %0 : tensor<i64>
-  }
-  func.func @main(%arg0: tensor<i64>, %arg1: tensor<i64>) -> tensor<i64> {
-    %0 = call @callee(%arg0, %arg1) : (tensor<i64>, tensor<i64>) -> tensor<i64>
-    return %0 : tensor<i64>
-  }
-})";
-
-  StablehloModuleBuilder mb;
-
-  // Build subfunction
-  func::FuncOp callee;
-  auto type = makeTensorType(mb->getContext(), {}, ElementType::I64);
-  {
-    func::FunctionBuilder fb(mb.get(), "callee");
-    auto regArg0 = func::Argument(fb, type);
-    auto regArg1 = func::Argument(fb, type);
-    auto add = Add(regArg0, regArg1);
-    func::Return(fb, add);
-    callee = fb.build();
-  }
-
-  // Build main function
-  {
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto arg0 = func::Argument(fb, type);
-    auto arg1 = func::Argument(fb, type);
-    auto call = func::Call(fb, callee, {arg0, arg1});
-    func::Return(fb, call);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-}  // namespace stablehlo
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/README.md b/stablehlo/stablehlo/integrations/cpp/builder/README.md
--- stablehlo/stablehlo/integrations/cpp/builder/README.md
+++ stablehlo/stablehlo/integrations/cpp/builder/README.md
@@ -1,245 +0,0 @@
-# Declarative MLIR Builder APIs
-
-Goal: Provide a builder that abstracts away the notion of location and insertion
-point for use cases that construct full graphs from C++.
-
-See `MlirBuilderTest.cpp` for examples.
-
-## Usage
-
-The builders look fairly similar to XlaBuilder's declarative style, see
-`MlirBuilderTest.cpp` for a few example programs:
-
-```c++
-StablehloModuleBuilder mb;
-{  // Build Main Func
-  ScopedBuilderLocation loc(mb.get(), fileLineColLoc(mb.get(), "main.mlir"));
-  func::FunctionBuilder fb(mb.get(), mb->getLoc(), "main");
-  auto type4xi64 = RankedTensorType::get({4}, fb.getOpBuilder().getI64Type());
-  auto arg0 = func::Argument(fb, type4xi64);
-  auto cst = stablehlo::Constant(fb, 1);
-  auto add = chlo::BroadcastAdd(arg0, cst);
-  auto topkAndIndices = chlo::TopK(add, 2);
-  auto broadcast =
-      stablehlo::BroadcastInDim(topkAndIndices[0].getType(), cst, {});
-  auto equal = tosa::Equal(topkAndIndices[0], broadcast);
-  func::Return(fb, {equal});
-}
-
-mb->build()->dump();
-// module {
-//  func.func @main(%arg0: tensor<4xi64>) -> tensor<2xi1> {
-//    %c = stablehlo.constant dense<1> : tensor<i64>
-//    %0 = chlo.broadcast_add %arg0, %c : (tensor<4xi64>, tensor<i64>) -> tensor<4xi64>
-//    %values, %indices = chlo.top_k(%0, k = 2) : tensor<4xi64> -> (tensor<2xi64>, tensor<2xi32>)
-//    %1 = stablehlo.broadcast_in_dim %c, dims = [] : (tensor<i64>) -> tensor<2xi64>
-//    %2 = tosa.equal %values, %1 : (tensor<2xi64>, tensor<2xi64>) -> tensor<2xi1>
-//    return %2 : tensor<2xi1>
-//  }
-// }
-```
-
-## Technical Details - Add support for a new dialect
-
-### 1. Add a BUILD rule for builder generation
-
-Build rule requires opset tablegen file -
-[stablehlo_builder](stablehlo/integrations/cpp/builder/BUILD)
-example:
-
-```bazel
-gentbl_cc_library(
-    name = "stablehlo_builder_inc",
-    tbl_outs = {
-        "StablehloBuilder.h.inc": ["-gen-builder-decls"],
-        "StablehloBuilder.cpp.inc": ["-gen-builder-defs"],
-        "StablehloBuilder.md": ["-gen-builder-docs"],
-    },
-    tblgen = ":mlir_builder_tblgen",
-    td_file = "stablehlo/dialect/StablehloOps.td",
-    deps = [
-        "@llvm-project//mlir:InferTypeOpInterfaceTdFiles",
-        "@llvm-project//mlir:OpBaseTdFiles",
-        "@llvm-project//mlir:SideEffectInterfacesTdFiles",
-        ":stablehlo_ops_td_filegroup",
-    ],
-)
-```
-
-This will generate `StablehloBuilder.h.inc` and `StablehloBuilder.cpp.inc` files
-that can be used in a cc_library target:
-
-```cpp
-$ bazel build -- //stablehlo/integrations/cpp/builder:stablehlo_builder_inc_filegroup
-MlirOp Abs(MlirOp &operand);
-MlirOp Add(MlirOp &lhs, MlirOp &rhs);
-MlirOp AfterAll(MlirBuilder &builder, ArrayRef<MlirOp> inputs);
-...
-MlirOp BitcastConvert(Type resultType, MlirOp &operand);
-MlirOp BroadcastInDim(Type resultType, MlirOp &operand, ::llvm::ArrayRef<int64_t> broadcast_dimensions);
-...
-```
-
-### 2. Make a cc_library target for generated files
-
-Add a [Builder.h][header] declaration file and [Builder.cpp][impl] impl file:
-
-```cpp
-// MyBuilder.h
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.h.inc"
-
-// MyBuilder.cpp
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.cpp.inc"
-```
-
-[header]:TODO
-[impl]:TODO
-
-### 3. Add custom methods for high-priority UX methods
-
-In some cases the default generated method doesn't have adequate UX for a very
-important construct that doesn't capture its semantics well statically in ODS
-(like `func.func` or `stablehlo.constant`). For these methods, you can add any
-declarations to the cc_library to improve UX. These methods should be kept to
-a minimum, we should aim to generate as much as possible.
-
-```cpp
-// Builder for stablehlo.constant : tensor<i64> scalar
-MlirOp Constant(MlirBuilder& builder, int64_t value);
-
-// Builder impl
-MlirOp Constant(MlirBuilder& builder, int64_t value) {
-  return builder.create<stablehlo::ConstantOp>(DenseIntElementsAttr::get(
-      RankedTensorType::get({}, builder.getOpBuilder().getI64Type()), value));
-}
-```
-
-In a perfect world all ops would capture their semantic information in ODS and
-we can generate perfect builders - currently we're missing details like "WhileOp
-forwards its operands to each of its regions" or "func op must have its
-signature match its region operand / return". These are the cases that require
-custom builders, and we should design them in a future-codegenable way.
-
-## Current status
-
-### Outcomes
-
-Some positive outcomes to these APIs:
-
-+ All dialects own their own APIs and can interop pretty easily by abstracting
-  away source location and insertion point behind an abstract type.
-+ Provides *pretty good* out of the box builder methods (tried with TOSA and
-  generated reasonable methods for 73 ops).
-+ We can likely make the surface level of builder APIs MLIR-free for g3
-  building without visibility.
-+ This is extensible to arbitrary types so long as the opsets support type
-  inference or accept explicit types as references.
-+ Uses the simpler Attribute forms, i.e. int64_t instead of IntegerAttr(64)
-
-### Opset Coverage
-
-When we can't generate a viable interface yet, we skip the op.
-
-With generated build rules, today we are generating:
-
-+ 112/114 StableHLO Ops
-+ 48/48 CHLO ops
-+ 75/76 TOSA ops
-+ 15/16 Shardy ops
-+ 3/5 Func ops
-
-```txt
-Skipping CaseOp: Variadic regions not supported
-Skipping ConstantLikeOp: Attributes must be after operands
-Skipping CustomOp: Attributes must be after operands
-Skipping NamedComputationOp: Attributes must be after operands
-Skipping RngBitGeneratorOp: Attributes must be after operands
-Skipping TriangularSolveOp: Attributes have known bad types
-Skipping VariableWriteOp: Attributes must be after operands
-```
-
-## Next steps
-
-There are some limitations to the current codegen, and we should add support.
-These restrictions are captured in a code comment in `MlirBuilderTblgen.cpp`:
-
-```txt
-// Some supported patterns:
-// - [X] Op has one or more Value operands.
-// - [X] Op has one or more results.
-// - [X] Op has no required attributes.
-// - [X] Op has no operands.
-// - [X] Op cannot infer type (take result type as argument).
-// - [X] Op has no results.
-// - [X] Op has no required MlirOp operands.
-// - [X] Op has single variadic operand / result.
-// - [X] Op has required attributes.
-// - [X] Op has optional attribute followed by non-optional attribute.
-// - [ ] Op has multiple operands / results, some variadic.
-// - [ ] Op has a region.
-// - [ ] Op declares attributes before operands (chlo.constant_like).
-// - [ ] Op method is a name conflict (triangular_solve Transpose is enum & fn).
-// - [ ] Op uses `FirstAttrDerivedResultTypes` to infer result type (tosa.const)
-```
-
-Notably simple Attributes are the next thing to figure out how to support (ints,
-i64 arrays, etc), followed by dialect-specific attribute (channel_handle,
-result_accuracy).
-
-In general most of these work items are not massive, O(hours) not days.
-
-The potentially trickier design points are:
-
-+ How to build Types?
-  + A builder for common upstream types would probably suffice.
-  + We probably want our own `MakeShape` method for StableHLO types.
-+ How to build ops with regions.
-  + Can likely take some hints from FunctionBuilder.
-
-## The rough edges
-
-### Should RegionBuilders come before attributes?
-
-Regions are required arguments (at least we only support required regions
-currently), and attributes can be optional. To allow max-default-values, we
-push all optional attributes to the end of the function declaration.
-
-The question becomes, where should regions go? Before attributes or after the
-last required attribute?
-
-```cpp
-// (1) Before attributes
-// There's something odd about region builder coming before function name here:
-void Func(MlirBuilder &builder, const RegionBuilderCallback &body, ::llvm::StringRef sym_name,
-          ::mlir::FunctionType function_type, /*optional*/::mlir::StringAttr sym_visibility = {},
-          /*optional*/::mlir::ArrayAttr arg_attrs = {}, /*optional*/::mlir::ArrayAttr res_attrs = {},
-          /*optional*/bool no_inline = false);
-
-// (2) After the last required attr
-// There's something odd about region being between attributes if specifying optional attrs
-void Func(MlirBuilder &builder, ::llvm::StringRef sym_name, ::mlir::FunctionType function_type,
-         const RegionBuilderCallback &body, /*optional*/::mlir::StringAttr sym_visibility = {},
-        /*optional*/::mlir::ArrayAttr arg_attrs = {}, /*optional*/::mlir::ArrayAttr res_attrs = {},
-        /*optional*/bool no_inline = false);
-```
-
-Today we chose option (1), high priority functions like Func can provide a
-custom method which takes name before region if needed.
-
-### Type Inference Crashes on Failure today
-
-*This is certainly fixable, but unclear if we want to.*
-
-XlaBuilder crashes on failure as well, so maybe not an issue. But XlaBuilder
-also tries to implicitly broadcast for many of its implementations, which we
-probably should avoid.
-
-### Generated Func API is tricky, may need a way to filter this
-
-Currently we filter FunctionOpInterface ops, they tend to require more custom
-logic to update type signatures and register things outside of just creating a
-region.
-
-+ Requires overloaded `func::Return` to update the function signature when
-  value is returned.
-+ Currently requires specifying the FuncType up front which is bad UX.
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.cpp
@@ -1,159 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.h"
-
-#include <cstdint>
-#include <optional>
-
-#include "llvm/Support/ErrorHandling.h"
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinTypes.h"
-#include "mlir/IR/Types.h"
-#include "mlir/IR/Value.h"
-#include "mlir/Interfaces/InferTypeOpInterface.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/dialect/StablehloOps.h"
-#include "stablehlo/dialect/TypeInference.h"
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-
-namespace mlir {
-namespace stablehlo {
-
-/////////////////
-// MANUAL APIs
-/////////////////
-
-MlirOp ConvertElementType(MlirOp input, Type resultElementType) {
-  MlirOp operand = input;
-  auto inputType = mlir::cast<RankedTensorType>(input.getType());
-  auto resultType = inputType.clone(resultElementType);
-  if (isa<ComplexType>(inputType.getElementType()) &&
-      !isa<ComplexType>(resultElementType)) {
-    operand = stablehlo::Real(operand);
-  }
-  return stablehlo::Convert(resultType, operand);
-}
-
-MlirOp ConvertElementType(MlirOp input, ElementType resultElementTypeKind) {
-  auto resultElementType =
-      getElementType(input.getContext(), resultElementTypeKind);
-  return ConvertElementType(input, resultElementType);
-}
-
-// These are not finalized APIs, just an example of hiding the ugly for
-// important leaf ops.
-MlirOp Constant(MlirBuilder& builder, int64_t value) {
-  return builder.create<stablehlo::ConstantOp>(DenseIntElementsAttr::get(
-      RankedTensorType::get({}, builder.getOpBuilder().getI64Type()), value));
-}
-MlirOp Constant(MlirBuilder& builder, std::vector<int64_t> value) {
-  auto numel = static_cast<int64_t>(value.size());
-  return builder.create<stablehlo::ConstantOp>(DenseIntElementsAttr::get(
-      RankedTensorType::get({numel}, builder.getOpBuilder().getI64Type()),
-      value));
-}
-
-namespace {
-
-// Use preferred element type, if not use LHS element type.
-Type getDotResultType(RankedTensorType lhsType, ShapedTypeComponents type,
-                      std::optional<ElementType> preferredResultType) {
-  Type elementType =
-      preferredResultType.has_value()
-          ? getElementType(*lhsType.getContext(), preferredResultType.value())
-          : lhsType.getElementType();
-  return RankedTensorType::get(type.getDims(), elementType,
-                               type.getAttribute());
-}
-
-}  // namespace
-
-MlirOp Dot(MlirOp lhs, MlirOp rhs, ArrayAttr precisionConfig,
-           std::optional<ElementType> preferredElementType) {
-  SmallVector<ShapedTypeComponents> inferredShape;
-  auto lhsType = mlir::dyn_cast<RankedTensorType>(lhs.getType());
-  auto rhsType = mlir::dyn_cast<RankedTensorType>(rhs.getType());
-  if (!lhsType || !rhsType)
-    llvm::report_fatal_error(
-        "Failed to infer dot op type from lhs and rhs types.");
-
-  if (failed(hlo::inferDotOp(lhs.getValue().getLoc(), lhsType, rhsType,
-                             precisionConfig, inferredShape)) ||
-      inferredShape.size() != 1)
-    llvm::report_fatal_error(
-        "Failed to infer dot op type from lhs and rhs types.");
-
-  auto resultType =
-      getDotResultType(lhsType, inferredShape[0], preferredElementType);
-  return stablehlo::Dot(resultType, lhs, rhs, precisionConfig);
-}
-
-MlirOp DotGeneral(MlirOp lhs, MlirOp rhs, DotDimensionNumbersAttr dotDimsAttr,
-                  ArrayAttr precisionConfig,
-                  std::optional<ElementType> preferredElementType) {
-  SmallVector<ShapedTypeComponents> inferredShape;
-  auto lhsType = mlir::dyn_cast<RankedTensorType>(lhs.getType());
-  auto rhsType = mlir::dyn_cast<RankedTensorType>(rhs.getType());
-  if (!lhsType || !rhsType)
-    llvm::report_fatal_error(
-        "Failed to infer dot op type from lhs and rhs types.");
-
-  if (failed(hlo::inferDotGeneralOp(lhs.getValue().getLoc(), lhsType, rhsType,
-                                    dotDimsAttr.getLhsBatchingDimensions(),
-                                    dotDimsAttr.getRhsBatchingDimensions(),
-                                    dotDimsAttr.getLhsContractingDimensions(),
-                                    dotDimsAttr.getRhsContractingDimensions(),
-                                    precisionConfig, inferredShape)) ||
-      inferredShape.size() != 1)
-    llvm::report_fatal_error(
-        "Failed to infer dot op type from lhs and rhs types.");
-
-  auto resultType =
-      getDotResultType(lhsType, inferredShape[0], preferredElementType);
-  return stablehlo::DotGeneral(resultType, lhs, rhs, dotDimsAttr,
-                               precisionConfig);
-}
-
-MlirOp Reshape(MlirOp input, ArrayRef<int64_t> newShape) {
-  auto type = mlir::dyn_cast<RankedTensorType>(input.getType());
-  if (!type)
-    llvm::report_fatal_error("expected ranked tensor input to reshape");
-  auto newType = type.clone(newShape);
-  return stablehlo::Reshape(newType, input);
-}
-
-SmallVector<MlirOp> Arguments(RegionBuilder& rb, WhileOp op) {
-  // Already init arguments, just return.
-  if (!rb.getRegion().getArguments().empty()) {
-    return wrap(rb, rb.getRegion().getArguments());
-  }
-  SmallVector<MlirOp> operands;
-  operands.reserve(op.getOperands().size());
-  for (auto operand : op.getOperands()) {
-    operands.push_back(mlir::Argument(rb, operand.getType()));
-  }
-  return operands;
-}
-
-/////////////////
-// GENERATED APIs
-/////////////////
-
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.cpp.inc"
-
-}  // namespace stablehlo
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilder.h
@@ -1,82 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#ifndef STABLEHLO_BUILDER_STABLEHLOBUILDER_H_
-#define STABLEHLO_BUILDER_STABLEHLOBUILDER_H_
-
-#include <cstdint>
-#include <optional>
-#include <vector>
-
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinOps.h"
-#include "mlir/IR/Types.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/dialect/StablehloOps.h"
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-
-namespace mlir {
-namespace stablehlo {
-
-/////////////////
-// MANUAL APIs
-/////////////////
-// There should be some manual APIs for each dialect for the APIs that require
-// max usability like constant.
-// I.e. sugar for int64_t -> tensor<i64>
-// Or std::vector<int64_t> -> tensor<Nxi64>
-
-MlirOp ConvertElementType(MlirOp input, ElementType resultElementTypeKind);
-MlirOp ConvertElementType(MlirOp input, Type resultElementType);
-
-MlirOp Constant(MlirBuilder& builder, int64_t value);
-MlirOp Constant(MlirBuilder& builder, std::vector<int64_t> value);
-
-// Better Dot / DotGeneral builders.
-// These ops don't support full type inference because the result element type
-// cannot be inferred from operands, however the result shape can be.
-//
-// The generated APIs require specifying the full result type, which requires
-// callsite to compute the proper shape, these APIs take a preferred result
-// type and will infer the result shape from the operands.
-MlirOp Dot(MlirOp lhs, MlirOp rhs, ArrayAttr precisionConfig = ArrayAttr(),
-           std::optional<ElementType> preferredElementType = std::nullopt);
-MlirOp DotGeneral(
-    MlirOp lhs, MlirOp rhs, DotDimensionNumbersAttr dotDimsAttr,
-    ArrayAttr precisionConfig = ArrayAttr(),
-    std::optional<ElementType> preferredElementType = std::nullopt);
-
-MlirOp Reshape(MlirOp input, ArrayRef<int64_t> newShape);
-
-// Get all arguments for a given region of a while op
-// Initializes the region arguments given the WhileOp operands.
-SmallVector<MlirOp> Arguments(RegionBuilder& rb, WhileOp op);
-
-/////////////////
-// GENERATED APIs
-/////////////////
-
-// stablehlo::While - UX issue, all regions need to declare all arguments, even
-//   if unused
-// stablehlo::Case - Can't generate builder for op with variadic region yet, add
-//   manually.
-
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.h.inc"
-
-}  // namespace stablehlo
-}  // namespace mlir
-
-#endif  // STABLEHLO_BUILDER_STABLEHLOBUILDER_H_
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
@@ -1,1596 +0,0 @@
-/* Copyright 2025 The OpenXLA Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#include <complex>
-#include <cstdint>
-#include <string>
-
-#include "gtest/gtest.h"
-#include "mlir/IR/BuiltinAttributes.h"
-#include "mlir/IR/BuiltinOps.h"
-#include "mlir/IR/DialectRegistry.h"
-#include "mlir/IR/MLIRContext.h"
-#include "mlir/IR/OwningOpRef.h"
-#include "mlir/IR/Verifier.h"
-#include "mlir/Support/DebugStringHelper.h"
-#include "mlir/Support/LLVM.h"
-#include "stablehlo/dialect/Register.h"
-#include "stablehlo/dialect/StablehloOps.h"
-#include "stablehlo/integrations/cpp/builder/AttrTypeBuilderUtil.h"
-#include "stablehlo/integrations/cpp/builder/FuncBuilder.h"
-#include "stablehlo/integrations/cpp/builder/MlirBuilder.h"
-#include "stablehlo/integrations/cpp/builder/StablehloBuilder.h"
-
-namespace mlir {
-namespace stablehlo {
-
-namespace {
-
-// Wrap a module builder and register the classes needed
-class StablehloModuleBuilder {
- public:
-  StablehloModuleBuilder()
-      : context_(), module_builder_(context_, mlir::unknownLoc(context_)) {
-    DialectRegistry registry;
-    registerAllDialects(registry);
-    context_.appendDialectRegistry(registry);
-    context_.loadAllAvailableDialects();
-  }
-
-  ModuleBuilder& get() { return module_builder_; }
-  ModuleBuilder* operator->() { return &module_builder_; }
-
- private:
-  MLIRContext context_;
-  ModuleBuilder module_builder_;
-};
-
-// TODO: Make a FileCheck matcher
-
-}  // namespace
-
-TEST(MlirBuilderTest, SmokeTest) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {
-    %c = stablehlo.constant dense<1> : tensor<2xi64>
-    %0 = stablehlo.add %arg0, %c : tensor<2xi64>
-    return %0 : tensor<2xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type2xi64);
-    auto cst = Constant(fb, mlir::makeConstant(1L, type2xi64));
-    auto add = Add(arg0, cst);
-    func::Return(fb, {add});
-  }
-
-  // TODO: Make these a FileCheck based test.
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, BinaryOps) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xi64>) -> tensor<2xi64> {
-    %c = stablehlo.constant dense<1> : tensor<2xi64>
-    %0 = stablehlo.add %arg0, %c : tensor<2xi64>
-    %1 = stablehlo.subtract %arg0, %0 : tensor<2xi64>
-    %2 = stablehlo.multiply %arg0, %1 : tensor<2xi64>
-    %3 = stablehlo.divide %arg0, %2 : tensor<2xi64>
-    return %3 : tensor<2xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type2xi64);
-    auto cst = Constant(fb, mlir::makeConstant(1L, type2xi64));
-    auto add = Add(arg0, cst);
-    auto sub = Subtract(arg0, add);
-    auto mul = Mul(arg0, sub);
-    auto div = Div(arg0, mul);
-    func::Return(fb, div);
-  }
-
-  // TODO: Make these a FileCheck based test.
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, UnaryOps) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
-    %0 = stablehlo.abs %arg0 : tensor<2xf32>
-    %1 = stablehlo.sine %0 : tensor<2xf32>
-    %2 = stablehlo.cosine %1 : tensor<2xf32>
-    return %2 : tensor<2xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::F32);
-    auto arg0 = func::Argument(fb, type2xi64);
-    auto abs = Abs(arg0);
-    auto sine = Sine(abs);
-    auto cosine = Cosine(sine);
-    func::Return(fb, cosine);
-  }
-
-  // TODO: Make these a FileCheck based test.
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, DotOp) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2x3xf32>, %arg1: tensor<3x2xf32>) -> tensor<2x2xf32> {
-    %0 = stablehlo.dot %arg0, %arg1, precision = [HIGHEST, HIGHEST] : (tensor<2x3xf32>, tensor<3x2xf32>) -> tensor<2x2xf32>
-    return %0 : tensor<2x2xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto& ctx = fb.getContext();
-    auto type2x3xi64 = makeTensorType(ctx, {2, 3}, ElementType::F32);
-    auto type3x2xi64 = makeTensorType(ctx, {3, 2}, ElementType::F32);
-    auto arg0 = func::Argument(fb, type2x3xi64);
-    auto arg1 = func::Argument(fb, type3x2xi64);
-    auto precision = PrecisionConfigAttr::get(
-        &ctx, {Precision::HIGHEST, Precision::HIGHEST});
-    auto dot = stablehlo::Dot(arg0, arg1, precision);
-    func::Return(fb, dot);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, DotGeneralOp) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2x2x2xi64>, %arg1: tensor<2x2x2xi64>) -> tensor<2x2x2xi64> {
-    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [HIGHEST, HIGHEST] : (tensor<2x2x2xi64>, tensor<2x2x2xi64>) -> tensor<2x2x2xi64>
-    return %0 : tensor<2x2x2xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto& ctx = fb.getContext();
-    auto type2x2x2xi64 = makeTensorType(ctx, {2, 2, 2}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type2x2x2xi64);
-    auto arg1 = func::Argument(fb, type2x2x2xi64);
-
-    // TODO(UX): Can we make DotDimensionNumbersAttr have better builders?
-    auto dotDimsAttr = DotDimensionNumbersAttr::get(
-        &ctx, /*lhsBatchingDimensions=*/{0},
-        /*rhsBatchingDimensions=*/{0}, /*lhsContractingDimensions=*/{2},
-        /*rhsContractingDimensions=*/{1});
-
-    auto precision = PrecisionConfigAttr::get(
-        &ctx, {Precision::HIGHEST, Precision::HIGHEST});
-    auto dot = stablehlo::DotGeneral(arg0, arg1, dotDimsAttr, precision);
-    func::Return(fb, dot);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ReduceOp) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xi64>) -> tensor<i64> {
-    %c = stablehlo.constant dense<1> : tensor<i64>
-    %0 = stablehlo.reduce(%arg0 init: %c) applies stablehlo.add across dimensions = [0] : (tensor<2xi64>, tensor<i64>) -> tensor<i64>
-    return %0 : tensor<i64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto type2xi64 = makeTensorType(mb->getContext(), {2}, ElementType::I64);
-    auto typei64 = makeTensorType(mb->getContext(), {}, ElementType::I64);
-    auto arg0 = func::Argument(fb, type2xi64);
-    auto cst = stablehlo::Constant(fb, mlir::makeConstant(1L, typei64));
-    auto reduce = stablehlo::Reduce(
-        fb, {arg0}, {cst},
-        [&typei64](RegionBuilder& body) {
-          buildReduceBody<AddOp>(typei64.getElementType(), body.getRegion(),
-                                 body.getOpBuilder());
-        },
-        /*dimensions=*/{0});
-    func::Return(fb, reduce);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, GatherOp) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<3xi64>, %arg1: tensor<1x1xi64>) -> tensor<1xi64> {
-    %0 = "stablehlo.gather"(%arg0, %arg1) <{dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = array<i64: 1>}> : (tensor<3xi64>, tensor<1x1xi64>) -> tensor<1xi64>
-    return %0 : tensor<1xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto& ctx = fb.getContext();
-    auto arg0 = func::Argument(fb, makeTensorType(ctx, {3}, ElementType::I64));
-    auto arg1 =
-        func::Argument(fb, makeTensorType(ctx, {1, 1}, ElementType::I64));
-    // TODO(UX): A bit verbose. Could use a better attr builder function.
-    auto gatherDims = GatherDimensionNumbersAttr::get(
-        &ctx, /*offset_dims=*/{}, /*collapsed_slice_dims=*/{0},
-        /*operandBatchingDims=*/{},
-        /*startIndicesBatchingDims=*/{}, /*startIndexMap=*/{0},
-        /*index_vector_dim=*/1);
-    auto gather =
-        stablehlo::Gather(arg0, arg1, gatherDims, /*slice_sizes=*/{1});
-    func::Return(fb, gather);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, WhileOp) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<i64>) -> tensor<i64> {
-    %c = stablehlo.constant dense<1> : tensor<i64>
-    %0 = stablehlo.while(%iterArg = %arg0) : tensor<i64>
-    cond {
-      %1 = stablehlo.compare  LT, %iterArg, %c : (tensor<i64>, tensor<i64>) -> tensor<i1>
-      stablehlo.return %1 : tensor<i1>
-    } do {
-      %1 = stablehlo.subtract %iterArg, %c : tensor<i64>
-      stablehlo.return %1 : tensor<i64>
-    }
-    return %0 : tensor<i64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {  // Build Main Func
-    func::FunctionBuilder fb(mb.get(), "main");
-    auto typei64 = makeTensorType(mb->getContext(), {}, ElementType::I64);
-    auto arg0 = func::Argument(fb, typei64);
-    auto cst = Constant(fb, mlir::makeConstant(1L, typei64));
-    auto whl = While(
-        fb, arg0,
-        [&cst](RegionBuilder& cond) {
-          // Note: always use `Arguments(...)` to init block args for WhileOp.
-          auto args = Arguments(cond, cond.getOp<WhileOp>());
-          auto lt = Compare(args[0], cst, ComparisonDirection::LT);
-          return Return(cond, lt);
-        },
-        [&cst](RegionBuilder& body) {
-          auto args = Arguments(body, body.getOp<WhileOp>());
-          auto sub1 = Subtract(args[0], cst);
-          return Return(body, sub1);
-        });
-    func::Return(fb, whl);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_TRUE(succeeded(mlir::verify(*module)));
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantPRED) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i1> {
-    %c = stablehlo.constant dense<true> : tensor<i1>
-    return %c : tensor<i1>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::PRED);
-    auto cst = stablehlo::Constant(fb, makeConstant(true, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i32> {
-    %c = stablehlo.constant dense<1> : tensor<i32>
-    return %c : tensor<i32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::I32);
-    auto cst = stablehlo::Constant(fb, makeConstant(1, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<f32> {
-    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-    return %cst : tensor<f32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::F32);
-    auto cst = stablehlo::Constant(fb, makeConstant(1.0, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantComplexF32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<complex<f32>> {
-    %cst = stablehlo.constant dense<(1.000000e+00,2.000000e+00)> : tensor<complex<f32>>
-    return %cst : tensor<complex<f32>>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::COMPLEXF32);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(std::complex<double>(1.0, 2.0), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantPredFromInt) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i1> {
-    %c = stablehlo.constant dense<true> : tensor<i1>
-    return %c : tensor<i1>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::PRED);
-    auto cst = stablehlo::Constant(fb, makeConstant(1, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantPredFromFloat) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i1> {
-    %c = stablehlo.constant dense<true> : tensor<i1>
-    return %c : tensor<i1>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::PRED);
-    auto cst = stablehlo::Constant(fb, makeConstant(1.0, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantPredFromComplexF32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i1> {
-    %c = stablehlo.constant dense<true> : tensor<i1>
-    return %c : tensor<i1>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::PRED);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(std::complex<double>(1.0, 2.0), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI32FromPred) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i32> {
-    %c = stablehlo.constant dense<1> : tensor<i32>
-    return %c : tensor<i32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::I32);
-    auto cst = stablehlo::Constant(fb, makeConstant(true, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI32FromFloat) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i32> {
-    %c = stablehlo.constant dense<0> : tensor<i32>
-    return %c : tensor<i32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::I32);
-    auto cst = stablehlo::Constant(fb, makeConstant(0.0, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI32FromComplexF32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<i32> {
-    %c = stablehlo.constant dense<1> : tensor<i32>
-    return %c : tensor<i32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::I32);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(std::complex<double>(1.0, 2.0), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF32FromPred) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<f32> {
-    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-    return %cst : tensor<f32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::F32);
-    auto cst = stablehlo::Constant(fb, makeConstant(true, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF32FromInt) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<f32> {
-    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
-    return %cst : tensor<f32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::F32);
-    auto cst = stablehlo::Constant(fb, makeConstant(0, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF32FromComplexF32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<f32> {
-    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>
-    return %cst : tensor<f32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::F32);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(std::complex<double>(1.0, 2.0), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantComplexF32FromPred) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<complex<f32>> {
-    %cst = stablehlo.constant dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f32>>
-    return %cst : tensor<complex<f32>>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::COMPLEXF32);
-    auto cst = stablehlo::Constant(fb, makeConstant(true, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantComplexF32FromInt) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<complex<f32>> {
-    %cst = stablehlo.constant dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f32>>
-    return %cst : tensor<complex<f32>>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::COMPLEXF32);
-    auto cst = stablehlo::Constant(fb, makeConstant(1, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantComplexF32FromFloat) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<complex<f32>> {
-    %cst = stablehlo.constant dense<(1.000000e+00,0.000000e+00)> : tensor<complex<f32>>
-    return %cst : tensor<complex<f32>>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {}, ElementType::COMPLEXF32);
-    auto cst = stablehlo::Constant(fb, makeConstant(1.0, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantPREDArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<2xi1> {
-    %c = stablehlo.constant dense<[true, false]> : tensor<2xi1>
-    return %c : tensor<2xi1>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {2}, ElementType::PRED);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<bool>({true, false}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI2Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<4xi2> {
-    %c = stablehlo.constant dense<[-2, -1, 0, 1]> : tensor<4xi2>
-    return %c : tensor<4xi2>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {4}, ElementType::I2);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<int64_t>({-2, -1, 0, 1}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantUI2Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<4xui2> {
-    %c = stablehlo.constant dense<[0, 1, 2, 3]> : tensor<4xui2>
-    return %c : tensor<4xui2>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {4}, ElementType::UI2);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<uint64_t>({0, 1, 2, 3}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI4Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<5xi4> {
-    %c = stablehlo.constant dense<[-8, -1, 0, 1, 7]> : tensor<5xi4>
-    return %c : tensor<5xi4>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {5}, ElementType::I4);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<int64_t>({-8, -1, 0, 1, 7}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantUI4Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<3xui4> {
-    %c = stablehlo.constant dense<[0, 8, 15]> : tensor<3xui4>
-    return %c : tensor<3xui4>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {3}, ElementType::UI4);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<uint64_t>({0, 8, 15}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI8Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<5xi8> {
-    %c = stablehlo.constant dense<[-128, -9, 0, 8, 127]> : tensor<5xi8>
-    return %c : tensor<5xi8>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {5}, ElementType::I8);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<int8_t>({-128, -9, 0, 8, 127}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantUI8Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<3xui8> {
-    %c = stablehlo.constant dense<[0, 16, 255]> : tensor<3xui8>
-    return %c : tensor<3xui8>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {3}, ElementType::UI8);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<uint8_t>({0, 16, 255}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI16Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<5xi16> {
-    %c = stablehlo.constant dense<[-32768, -129, 0, 128, 32767]> : tensor<5xi16>
-    return %c : tensor<5xi16>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {5}, ElementType::I16);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(ArrayRef<int64_t>({-32768, -129, 0, 128, 32767}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantUI16Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<3xui16> {
-    %c = stablehlo.constant dense<[0, 256, 65535]> : tensor<3xui16>
-    return %c : tensor<3xui16>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {3}, ElementType::UI16);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<uint64_t>({0, 256, 65535}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI32Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<5xi32> {
-    %c = stablehlo.constant dense<[-2147483648, -65537, 0, 65536, 2147483647]> : tensor<5xi32>
-    return %c : tensor<5xi32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {5}, ElementType::I32);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(
-                ArrayRef<int64_t>({-2147483648, -65537, 0, 65536, 2147483647}),
-                type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantUI32Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<3xui32> {
-    %c = stablehlo.constant dense<[0, 65536, 4294967295]> : tensor<3xui32>
-    return %c : tensor<3xui32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {3}, ElementType::UI32);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<uint64_t>({0, 65536, 4294967295}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI64Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<5xi64> {
-    %c = stablehlo.constant dense<[-9223372036854775808, -2147483649, 0, 2147483648, 9223372036854775807]> : tensor<5xi64>
-    return %c : tensor<5xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {5}, ElementType::I64);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(ArrayRef<int64_t>({-9223372036854775807 - 1, -2147483649,
-                                        0, 2147483648, 9223372036854775807}),
-                     type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantUI64Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<3xui64> {
-    %c = stablehlo.constant dense<[0, 4294967296, 18446744073709551615]> : tensor<3xui64>
-    return %c : tensor<3xui64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {3}, ElementType::UI64);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(
-            ArrayRef<uint64_t>({0, 4294967296, 18446744073709551615UL}), type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF4E2M1FNArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf4E2M1FN> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 0.000000e+00, 0.000000e+00, 3.000000e+00, 6.000000e+00, 6.000000e+00, 1.000000e+00, 6.000000e+00]> : tensor<10xf4E2M1FN>
-    return %cst : tensor<10xf4E2M1FN>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F4E2M1FN);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x07, 0x0F, 0x01, 0x09}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF6E2M3FNArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf6E2M3FN> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.250000e-01, 3.250000e+00, 7.500000e+00, 7.500000e+00, 1.000000e+00, 7.500000e+00]> : tensor<10xf6E2M3FN>
-    return %cst : tensor<10xf6E2M3FN>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F6E2M3FN);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x1F, 0x3F, 0x01, 0x21}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF6E3M2FNArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf6E3M2FN> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.250000e-01, 3.000000e+00, 2.800000e+01, 2.800000e+01, 1.000000e+00, 2.800000e+01]> : tensor<10xf6E3M2FN>
-    return %cst : tensor<10xf6E3M2FN>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F6E3M2FN);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x1F, 0x3F, 0x01, 0x21}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E3M4Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E3M4> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 9.375000e-02, 3.125000e+00, 0x70, 0x70, 1.000000e+00, 0x70]> : tensor<10xf8E3M4>
-    return %cst : tensor<10xf8E3M4>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F8E3M4);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E4M3B11FNUZArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E4M3B11FNUZ> {
-    %cst = stablehlo.constant dense<[0.000000e+00, 0.000000e+00, 1.000000e+00, 1.250000e-01, 1.015630e-01, 3.250000e+00, 0x80, 0x80, 1.000000e+00, 0x80]> : tensor<10xf8E4M3B11FNUZ>
-    return %cst : tensor<10xf8E4M3B11FNUZ>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type =
-        makeTensorType(fb.getContext(), {10}, ElementType::F8E4M3B11FNUZ);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E4M3Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E4M3> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.015630e-01, 3.250000e+00, 1.280000e+02, 0x78, 1.000000e+00, 1.280000e+02]> : tensor<10xf8E4M3>
-    return %cst : tensor<10xf8E4M3>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F8E4M3);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E4M3FNArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E4M3FN> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.015630e-01, 3.250000e+00, 1.280000e+02, 2.560000e+02, 1.000000e+00, 1.280000e+02]> : tensor<10xf8E4M3FN>
-    return %cst : tensor<10xf8E4M3FN>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F8E4M3FN);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E4M3FNUZArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E4M3FNUZ> {
-    %cst = stablehlo.constant dense<[0.000000e+00, 0.000000e+00, 1.000000e+00, 1.250000e-01, 1.015630e-01, 3.250000e+00, 1.280000e+02, 0x80, 1.000000e+00, 1.280000e+02]> : tensor<10xf8E4M3FNUZ>
-    return %cst : tensor<10xf8E4M3FNUZ>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F8E4M3FNUZ);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E5M2Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E5M2> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 9.375000e-02, 3.000000e+00, 1.280000e+02, 2.560000e+02, 1.000000e+00, 1.280000e+02]> : tensor<10xf8E5M2>
-    return %cst : tensor<10xf8E5M2>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F8E5M2);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E5M2FNUZArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<10xf8E5M2FNUZ> {
-    %cst = stablehlo.constant dense<[0.000000e+00, 0.000000e+00, 1.000000e+00, 1.250000e-01, 9.375000e-02, 3.000000e+00, 1.280000e+02, 2.560000e+02, 1.000000e+00, 1.280000e+02]> : tensor<10xf8E5M2FNUZ>
-    return %cst : tensor<10xf8E5M2FNUZ>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {10}, ElementType::F8E5M2FNUZ);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.1415,
-                                           0x7F, 0xFF, 0x01, 0x81}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF8E8M0FNUArray) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<8xf8E8M0FNU> {
-    %cst = stablehlo.constant dense<[5.877470e-39, 1.000000e+00, 1.250000e-01, 1.250000e-01, 4.000000e+00, 5.877470e-39, 1.280000e+02, 2.560000e+02]> : tensor<8xf8E8M0FNU>
-    return %cst : tensor<8xf8E8M0FNU>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {8}, ElementType::F8E8M0FNU);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>(
-                             {0.0, 1.0, 0.125, 0.1, 3.1415, 0x00, 0x80, 0xFF}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantBF16Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<11xbf16> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.000980e-01, 3.140630e+00, 3.264000e+04, 6.553600e+04, 3.276800e+04, 1.000000e+00, 3.276800e+04]> : tensor<11xbf16>
-    return %cst : tensor<11xbf16>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {11}, ElementType::BF16);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.140630,
-                                       0x7F80, 0xFF80, 0x7FFF, 0x0001, 0x8001}),
-                     type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF16Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<11xf16> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 9.997550e-02, 3.140630e+00, 3.174400e+04, 6.451200e+04, 3.276800e+04, 1.000000e+00, 3.276800e+04]> : tensor<11xf16>
-    return %cst : tensor<11xf16>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {11}, ElementType::F16);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1, 3.140630,
-                                       0x7C00, 0xFC00, 0x7FFF, 0x0001, 0x8001}),
-                     type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF32Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<11xf32> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.000000e-01, 3.14159274, 2.13909504E+9, 4.28657869E+9, 2.14748365E+9, 1.000000e+00, 2.14748365E+9]> : tensor<11xf32>
-    return %cst : tensor<11xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {11}, ElementType::F32);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(ArrayRef<double>({0.0, -0.0, 1.0, 0.125, 0.1,
-                                           3.14159274, 0x7F800000, 0xFF800000,
-                                           0x7FFFFFFF, 0x00000001, 0x80000001}),
-                         type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantF64Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<11xf64> {
-    %cst = stablehlo.constant dense<[0.000000e+00, -0.000000e+00, 1.000000e+00, 1.250000e-01, 1.000000e-01, 3.1415926535897931, 9.2188684372274053E+18, 1.8442240474082181E+19, 9.2233720368547758E+18, 1.000000e+00, 9.2233720368547758E+18]> : tensor<11xf64>
-    return %cst : tensor<11xf64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {11}, ElementType::F64);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(
-                ArrayRef<long double>({0.0, -0.0, 1.0, 0.125, 0.1,
-                                       3.1415926535897931, 0x7FF0000000000000,
-                                       0xFFF0000000000000, 0x7FFFFFFFFFFFFFFF,
-                                       0x0000000000000001, 0x8000000000000001}),
-                type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantComplexF32Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<2xcomplex<f32>> {
-    %cst = stablehlo.constant dense<[(1.500000e+00,2.500000e+00), (3.500000e+00,4.500000e+00)]> : tensor<2xcomplex<f32>>
-    return %cst : tensor<2xcomplex<f32>>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {2}, ElementType::COMPLEXF32);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(ArrayRef<std::complex<double>>({{1.5, 2.5}, {3.5, 4.5}}),
-                     type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantComplexF64Array) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<2xcomplex<f64>> {
-    %cst = stablehlo.constant dense<[(1.500000e+00,2.500000e+00), (3.500000e+00,4.500000e+00)]> : tensor<2xcomplex<f64>>
-    return %cst : tensor<2xcomplex<f64>>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {2}, ElementType::COMPLEXF64);
-    auto cst = stablehlo::Constant(
-        fb,
-        makeConstant(ArrayRef<std::complex<double>>({{1.5, 2.5}, {3.5, 4.5}}),
-                     type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConvertElementTypeF32ToI32) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<2xi32> {
-    %cst = stablehlo.constant dense<1.000000e+00> : tensor<2xf32>
-    %0 = stablehlo.convert %cst : (tensor<2xf32>) -> tensor<2xi32>
-    return %0 : tensor<2xi32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto& ctx = fb.getContext();
-    auto type = makeTensorType(ctx, {2}, ElementType::F32);
-    auto cst = stablehlo::Constant(fb, makeConstant(1.0, type));
-    auto converted = stablehlo::ConvertElementType(cst, ElementType::I32);
-    func::Return(fb, converted);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConvertElementTypeComplexToReal) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<2xf32> {
-    %cst = stablehlo.constant dense<(1.000000e+00,0.000000e+00)> : tensor<2xcomplex<f32>>
-    %0 = stablehlo.real %cst : (tensor<2xcomplex<f32>>) -> tensor<2xf32>
-    %1 = stablehlo.convert %0 : tensor<2xf32>
-    return %1 : tensor<2xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto& ctx = fb.getContext();
-    auto type = makeTensorType(ctx, {2}, ElementType::COMPLEXF32);
-    auto cst = stablehlo::Constant(fb, makeConstant(1.0, type));
-    auto converted = stablehlo::ConvertElementType(cst, ElementType::F32);
-    func::Return(fb, converted);
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI64SmallVector) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<8xi64> {
-    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7]> : tensor<8xi64>
-    return %c : tensor<8xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {8}, ElementType::I64);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(SmallVector<int64_t>{0, 1, 2, 3, 4, 5, 6, 7}, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ConstantI64Vector) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<8xi64> {
-    %c = stablehlo.constant dense<[0, 1, 2, 3, 4, 5, 6, 7]> : tensor<8xi64>
-    return %c : tensor<8xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {8}, ElementType::I64);
-    auto cst = stablehlo::Constant(
-        fb, makeConstant(std::vector<int64_t>{0, 1, 2, 3, 4, 5, 6, 7}, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, EmptyConstantI64) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<0xi64> {
-    %c = stablehlo.constant dense<> : tensor<0xi64>
-    return %c : tensor<0xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {0}, ElementType::I64);
-    auto cst = stablehlo::Constant(fb, makeConstant(ArrayRef<int64_t>{}, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, EmptyConstantMismatchedTypeI64) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<0xi64> {
-    %c = stablehlo.constant dense<> : tensor<0xi64>
-    return %c : tensor<0xi64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {0}, ElementType::I64);
-    // Pass double data with i64 type.
-    auto cst = stablehlo::Constant(fb, makeConstant(ArrayRef<double>{}, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, EmptyConstantMismatchedTypeAPIntF64) {
-  std::string expected = R"mlir(module {
-  func.func @main() -> tensor<0xf64> {
-    %cst = stablehlo.constant dense<> : tensor<0xf64>
-    return %cst : tensor<0xf64>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {0}, ElementType::F64);
-    // Pass double data with i64 type.
-    auto cst = stablehlo::Constant(fb, makeConstant(ArrayRef<int64_t>{}, type));
-    func::Return(fb, {cst});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-////////
-// Custom Attribute Tests
-////////
-
-TEST(MlirBuilderTest, ResultAccuracyAttrDefault) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
-    %0 = stablehlo.exponential %arg0 : tensor<2xf32>
-    return %0 : tensor<2xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
-    auto arg0 = func::Argument(fb, type);
-    auto exp = Exp(arg0);
-    func::Return(fb, {exp});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ResultAccuracyAttrHighest) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
-    %0 = stablehlo.exponential %arg0 {result_accuracy = #stablehlo.result_accuracy<mode = #stablehlo.result_accuracy_mode<HIGHEST>>} : tensor<2xf32>
-    return %0 : tensor<2xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
-    auto arg0 = func::Argument(fb, type);
-    auto resultAccuracy =
-        ResultAccuracyAttr::get(&fb.getContext(), ResultAccuracyMode::HIGHEST);
-    auto exp = Exp(arg0, resultAccuracy);
-    func::Return(fb, {exp});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-TEST(MlirBuilderTest, ResultAccuracyAttrTolerance) {
-  std::string expected = R"mlir(module {
-  func.func @main(%arg0: tensor<2xf32>) -> tensor<2xf32> {
-    %0 = stablehlo.exponential %arg0 {result_accuracy = #stablehlo.result_accuracy<atol = 1.000000e-05, ulps = 5, mode = #stablehlo.result_accuracy_mode<TOLERANCE>>} : tensor<2xf32>
-    return %0 : tensor<2xf32>
-  }
-})mlir";
-
-  StablehloModuleBuilder mb;
-  {
-    Location funcLoc = fileLineColLoc(mb->getContext(), "main.mlir", 1, 1);
-    func::FunctionBuilder fb(mb.get(), "main", funcLoc);
-    auto type = makeTensorType(fb.getContext(), {2}, ElementType::F32);
-    auto arg0 = func::Argument(fb, type);
-    auto resultAccuracy =
-        ResultAccuracyAttr::get(&fb.getContext(), /*atol=*/APFloat(1e-5),
-                                /*rtol=*/APFloat(0.0), /*ulps=*/5);
-    auto exp = Exp(arg0, resultAccuracy);
-    func::Return(fb, {exp});
-  }
-
-  OwningOpRef<ModuleOp> module = mb->build();
-  EXPECT_EQ(expected, debugString(*module));
-}
-
-}  // namespace stablehlo
-}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir
--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir
+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir
@@ -11,6 +11,30 @@
 }
 
 // -----
+
+// CHECK-LABEL: @addStaticBroadcastExpanding
+func.func @addStaticBroadcastExpanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {
+  // CHECK:      %[[BROADCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<4xf32>
+  // CHECK-NEXT: stablehlo.add %arg0, %[[BROADCAST]]
+  // CHECK-NOT: shape
+  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<4xf32>, tensor<f32>) -> tensor<4xf32>
+  func.return %0 : tensor<4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: @addStaticBroadcastSameRank
+func.func @addStaticBroadcastSameRank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {
+  // CHECK:      %[[ARG0_B:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<1x4xf32>) -> tensor<4x4xf32>
+  // CHECK-NEXT: %[[ARG1_B:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<4x1xf32>) -> tensor<4x4xf32>
+  // CHECK-NEXT: stablehlo.add %[[ARG0_B]], %[[ARG1_B]] : tensor<4x4xf32>
+  // CHECK-NOT: shape
+  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<4x1xf32>) -> tensor<4x4xf32>
+  func.return %0 : tensor<4x4xf32>
+}
+
+// -----
+
 
 // CHECK-LABEL: @dynamicBroadcast
 // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -150,8 +150,8 @@
 ////////
 // CompareOp
 
-// CHECK-LABEL: func.func @compare_folds
-func.func @compare_folds()
+// CHECK-LABEL: func.func @compare_fold_int
+func.func @compare_fold_int()
   -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {
   %cn1 = stablehlo.constant dense<-1> : tensor<i32>
   %c0 = stablehlo.constant dense<0> : tensor<i32>
@@ -176,6 +176,270 @@
          tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>
 }
 
+// -----
+
+// CHECK-LABEL: func.func @compare_fold_float
+func.func @compare_fold_float()
+  -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {
+  %c0 = stablehlo.constant dense<0.0> : tensor<f32>
+  %c1 = stablehlo.constant dense<0.01> : tensor<f32>
+  %c2 = stablehlo.constant dense<-0.01> : tensor<f32>
+  %c3 = stablehlo.constant dense<42.1> : tensor<f32>
+  %c4 = stablehlo.constant dense<-50.0> : tensor<f32>
+
+  %0 = stablehlo.compare EQ, %c0, %c0, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %1 = stablehlo.compare EQ, %c1, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %2 = stablehlo.compare NE, %c0, %c0, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %3 = stablehlo.compare NE, %c1, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %4 = stablehlo.compare GT, %c3, %c3, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %5 = stablehlo.compare GT, %c3, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %6 = stablehlo.compare GE, %c3, %c3, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %7 = stablehlo.compare GE, %c3, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %8 = stablehlo.compare LT, %c2, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %9 = stablehlo.compare LT, %c2, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %10 = stablehlo.compare LE, %c2, %c2, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %11 = stablehlo.compare LE, %c2, %c4, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  // CHECK-DAG:  [[FALSE:%.+]] = stablehlo.constant dense<false> : tensor<i1>
+  // CHECK-DAG:  [[TRUE:%.+]] = stablehlo.constant dense<true> : tensor<i1>
+
+  // CHECK-NEXT: return [[TRUE]], [[FALSE]], [[FALSE]], [[TRUE]], [[FALSE]], [[TRUE]], [[TRUE]], [[TRUE]], [[FALSE]], [[FALSE]], [[TRUE]], [[FALSE]]
+  return %0, %1, %2, %3, %4, %5, %6, %7, %8, %9, %10, %11 :
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>
+}
+
+// -----
+
+// CHECK-LABEL: func.func @compare_fold_float_edge_cases
+func.func @compare_fold_float_edge_cases()
+  -> (tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+      tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>) {
+  %zero = stablehlo.constant dense<0.0> : tensor<f32>
+  %pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32>
+  %neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32>
+  %nan = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+
+  // CHECK-DAG:  [[FALSE:%.+]] = stablehlo.constant dense<false> : tensor<i1>
+  // CHECK-DAG:  [[TRUE:%.+]] = stablehlo.constant dense<true> : tensor<i1>
+
+  %0 = stablehlo.compare EQ, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %1 = stablehlo.compare EQ, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %2 = stablehlo.compare EQ, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %3 = stablehlo.compare EQ, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %4 = stablehlo.compare EQ, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %5 = stablehlo.compare EQ, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %6 = stablehlo.compare EQ, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %7 = stablehlo.compare EQ, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %8 = stablehlo.compare EQ, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %9 = stablehlo.compare EQ, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %10 = stablehlo.compare EQ, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %11 = stablehlo.compare EQ, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %12 = stablehlo.compare EQ, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %13 = stablehlo.compare EQ, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %14 = stablehlo.compare EQ, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %15 = stablehlo.compare EQ, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  %16 = stablehlo.compare NE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %17 = stablehlo.compare NE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %18 = stablehlo.compare NE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %19 = stablehlo.compare NE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %20 = stablehlo.compare NE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %21 = stablehlo.compare NE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %22 = stablehlo.compare NE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %23 = stablehlo.compare NE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %24 = stablehlo.compare NE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %25 = stablehlo.compare NE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %26 = stablehlo.compare NE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %27 = stablehlo.compare NE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %28 = stablehlo.compare NE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %29 = stablehlo.compare NE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %30 = stablehlo.compare NE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %31 = stablehlo.compare NE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  %32 = stablehlo.compare GT, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %33 = stablehlo.compare GT, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %34 = stablehlo.compare GT, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %35 = stablehlo.compare GT, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %36 = stablehlo.compare GT, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %37 = stablehlo.compare GT, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %38 = stablehlo.compare GT, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %39 = stablehlo.compare GT, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %40 = stablehlo.compare GT, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %41 = stablehlo.compare GT, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %42 = stablehlo.compare GT, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %43 = stablehlo.compare GT, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %44 = stablehlo.compare GT, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %45 = stablehlo.compare GT, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %46 = stablehlo.compare GT, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %47 = stablehlo.compare GT, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  %48 = stablehlo.compare GE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %49 = stablehlo.compare GE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %50 = stablehlo.compare GE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %51 = stablehlo.compare GE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %52 = stablehlo.compare GE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %53 = stablehlo.compare GE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %54 = stablehlo.compare GE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %55 = stablehlo.compare GE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %56 = stablehlo.compare GE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %57 = stablehlo.compare GE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %58 = stablehlo.compare GE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %59 = stablehlo.compare GE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %60 = stablehlo.compare GE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %61 = stablehlo.compare GE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %62 = stablehlo.compare GE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %63 = stablehlo.compare GE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  %64 = stablehlo.compare LT, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %65 = stablehlo.compare LT, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %66 = stablehlo.compare LT, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %67 = stablehlo.compare LT, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %68 = stablehlo.compare LT, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %69 = stablehlo.compare LT, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %70 = stablehlo.compare LT, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %71 = stablehlo.compare LT, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %72 = stablehlo.compare LT, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %73 = stablehlo.compare LT, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %74 = stablehlo.compare LT, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %75 = stablehlo.compare LT, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %76 = stablehlo.compare LT, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %77 = stablehlo.compare LT, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %78 = stablehlo.compare LT, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %79 = stablehlo.compare LT, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  %80 = stablehlo.compare LE, %zero, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %81 = stablehlo.compare LE, %zero, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %82 = stablehlo.compare LE, %zero, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %83 = stablehlo.compare LE, %zero, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %84 = stablehlo.compare LE, %pos_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %85 = stablehlo.compare LE, %pos_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %86 = stablehlo.compare LE, %pos_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %87 = stablehlo.compare LE, %pos_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %88 = stablehlo.compare LE, %neg_inf, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %89 = stablehlo.compare LE, %neg_inf, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %90 = stablehlo.compare LE, %neg_inf, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %91 = stablehlo.compare LE, %neg_inf, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %92 = stablehlo.compare LE, %nan, %zero, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %93 = stablehlo.compare LE, %nan, %pos_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %94 = stablehlo.compare LE, %nan, %neg_inf, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+  %95 = stablehlo.compare LE, %nan, %nan, FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>
+
+  // CHECK: return [[TRUE]],  [[FALSE]], [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],
+
+  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[TRUE]],  [[TRUE]],
+  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[TRUE]],
+  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[TRUE]],
+  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[TRUE]],
+
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],
+
+  // CHECK-SAME:   [[TRUE]],  [[FALSE]], [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],
+
+  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]],
+
+  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[TRUE]],  [[FALSE]], [[FALSE]],
+  // CHECK-SAME:   [[TRUE]],  [[TRUE]],  [[TRUE]],  [[FALSE]],
+  // CHECK-SAME:   [[FALSE]], [[FALSE]], [[FALSE]], [[FALSE]]
+
+  return  %0,  %1,  %2,  %3,
+          %4,  %5,  %6,  %7,
+          %8,  %9, %10, %11,
+         %12, %13, %14, %15,
+
+         %16, %17, %18, %19,
+         %20, %21, %22, %23,
+         %24, %25, %26, %27,
+         %28, %29, %30, %31,
+
+         %32, %33, %34, %35,
+         %36, %37, %38, %39,
+         %40, %41, %42, %43,
+         %44, %45, %46, %47,
+
+         %48, %49, %50, %51,
+         %52, %53, %54, %55,
+         %56, %57, %58, %59,
+         %60, %61, %62, %63,
+
+         %64, %65, %66, %67,
+         %68, %69, %70, %71,
+         %72, %73, %74, %75,
+         %76, %77, %78, %79,
+
+         %80, %81, %82, %83,
+         %84, %85, %86, %87,
+         %88, %89, %90, %91,
+         %92, %93, %94, %95 :
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>,
+         tensor<i1>, tensor<i1>, tensor<i1>, tensor<i1>
+}
 
 // -----
 
@@ -218,8 +482,7 @@
   %cst_2 = stablehlo.constant dense<2.0> : tensor<f32>
   // CHECK: stablehlo.constant dense<1> : tensor<i32>
   // CHECK: stablehlo.constant dense<1> : tensor<ui32>
-  // CHECK: stablehlo.divide{{.*}} : tensor<f32>
-  // DISABLED-CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  // CHECK: stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
   %0 = stablehlo.divide %cst, %cst : tensor<i32>
   %1 = stablehlo.divide %cst_1, %cst_1 : tensor<ui32>
   %2 = stablehlo.divide %cst_2, %cst_2 : tensor<f32>
@@ -485,6 +748,19 @@
   // CHECK-NEXT: return [[RESULT0]]
   %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<10>>
   return %0 : tensor<?xf32, #stablehlo.bounds<10>>
+}
+
+// -----
+
+// Don't fold when washing away a bounded dimension, not safe to replace with
+// operand when types mismatch.
+// CHECK-LABEL: func.func @no_fold_set_dimension_size_bounded_input
+func.func @no_fold_set_dimension_size_bounded_input(%arg0: tensor<?x4xf32, #stablehlo.bounds<8, ?>>) -> tensor<8x4xf32> {
+  %c = stablehlo.constant dense<8> : tensor<i32>
+  // CHECK: [[RESULT0:%.+]] = stablehlo.set_dimension_size
+  // CHECK-NEXT: return [[RESULT0]]
+  %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<?x4xf32, #stablehlo.bounds<8, ?>>, tensor<i32>) -> tensor<8x4xf32>
+  return %0 : tensor<8x4xf32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -1633,17 +1633,17 @@
   %s4 = stablehlo.select %4, %arg1, %arg2 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>
   %s5 = stablehlo.select %5, %arg1, %arg3 : (tensor<2xi1>, tensor<2xi32>, tensor<2xi32>) -> tensor<2xi32>
 
-  // CHECK-DAG:  [[C0:%.+]] = stablehlo.compare EQ, [[ARG0]], [[ARG1]], SIGNED
-  // CHECK-DAG:  [[C1:%.+]] = stablehlo.compare NE, [[ARG0]], [[ARG1]], SIGNED
-
-  // CHECK-DAG:  [[S0:%.+]] = stablehlo.select [[C0]], [[ARG0]], [[ARG1]]
-  // CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]
-  // CHECK-DAG:  [[S2:%.+]] = stablehlo.maximum [[ARG0]], [[ARG1]]
-  // CHECK-DAG:  [[S3:%.+]] = stablehlo.maximum [[ARG0]], [[ARG2]]
-  // CHECK-DAG:  [[S4:%.+]] = stablehlo.minimum [[ARG1]], [[ARG2]]
-  // CHECK-DAG:  [[S5:%.+]] = stablehlo.minimum [[ARG1]], [[ARG3]]
-
-  // CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]]
+  // DISABLED-CHECK-DAG:  [[C0:%.+]] = stablehlo.compare EQ, [[ARG0]], [[ARG1]], SIGNED
+  // DISABLED-CHECK-DAG:  [[C1:%.+]] = stablehlo.compare NE, [[ARG0]], [[ARG1]], SIGNED
+
+  // DISABLED-CHECK-DAG:  [[S0:%.+]] = stablehlo.select [[C0]], [[ARG0]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S2:%.+]] = stablehlo.maximum [[ARG0]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S3:%.+]] = stablehlo.maximum [[ARG0]], [[ARG2]]
+  // DISABLED-CHECK-DAG:  [[S4:%.+]] = stablehlo.minimum [[ARG1]], [[ARG2]]
+  // DISABLED-CHECK-DAG:  [[S5:%.+]] = stablehlo.minimum [[ARG1]], [[ARG3]]
+
+  // DISABLED-CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]]
   return %s0, %s1, %s2, %s3, %s4, %s5 :
          tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>
 }
@@ -1674,23 +1674,23 @@
   %s6 = stablehlo.select %6, %arg3, %arg2 : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
   %s7 = stablehlo.select %7, %arg2, %arg3 : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
 
-  // CHECK-DAG:  [[C1:%.+]] = stablehlo.compare GT, [[ARG1]], [[ARG2]], SIGNED
-  // CHECK-DAG:  [[C3:%.+]] = stablehlo.compare GE, [[ARG1]], [[ARG2]], SIGNED
-
-  // CHECK-DAG:  [[S0:%.+]] = stablehlo.minimum [[ARG0]], [[ARG1]]
-  // CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]
-  // CHECK-DAG:  [[S2:%.+]] = stablehlo.minimum [[ARG3]], [[ARG1]]
-  // CHECK-DAG:  [[S3:%.+]] = stablehlo.select [[C3]], [[ARG0]], [[ARG2]]
-
-  // CHECK-DAG:  [[C5:%.+]] = stablehlo.compare LT, [[ARG0]], [[ARG2]], SIGNED
-  // CHECK-DAG:  [[C7:%.+]] = stablehlo.compare LE, [[ARG0]], [[ARG2]], SIGNED
-
-  // CHECK-DAG:  [[S4:%.+]] = stablehlo.maximum [[ARG2]], [[ARG1]]
-  // CHECK-DAG:  [[S5:%.+]] = stablehlo.select [[C5]], [[ARG1]], [[ARG2]]
-  // CHECK-DAG:  [[S6:%.+]] = stablehlo.maximum [[ARG3]], [[ARG2]]
-  // CHECK-DAG:  [[S7:%.+]] = stablehlo.select [[C7]], [[ARG2]], [[ARG3]]
-
-  // CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]], [[S6]], [[S7]]
+  // DISABLED-CHECK-DAG:  [[C1:%.+]] = stablehlo.compare GT, [[ARG1]], [[ARG2]], SIGNED
+  // DISABLED-CHECK-DAG:  [[C3:%.+]] = stablehlo.compare GE, [[ARG1]], [[ARG2]], SIGNED
+
+  // DISABLED-CHECK-DAG:  [[S0:%.+]] = stablehlo.minimum [[ARG0]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S1:%.+]] = stablehlo.select [[C1]], [[ARG0]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S2:%.+]] = stablehlo.minimum [[ARG3]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S3:%.+]] = stablehlo.select [[C3]], [[ARG0]], [[ARG2]]
+
+  // DISABLED-CHECK-DAG:  [[C5:%.+]] = stablehlo.compare LT, [[ARG0]], [[ARG2]], SIGNED
+  // DISABLED-CHECK-DAG:  [[C7:%.+]] = stablehlo.compare LE, [[ARG0]], [[ARG2]], SIGNED
+
+  // DISABLED-CHECK-DAG:  [[S4:%.+]] = stablehlo.maximum [[ARG2]], [[ARG1]]
+  // DISABLED-CHECK-DAG:  [[S5:%.+]] = stablehlo.select [[C5]], [[ARG1]], [[ARG2]]
+  // DISABLED-CHECK-DAG:  [[S6:%.+]] = stablehlo.maximum [[ARG3]], [[ARG2]]
+  // DISABLED-CHECK-DAG:  [[S7:%.+]] = stablehlo.select [[C7]], [[ARG2]], [[ARG3]]
+
+  // DISABLED-CHECK-NEXT: return [[S0]], [[S1]], [[S2]], [[S3]], [[S4]], [[S5]], [[S6]], [[S7]]
   return %s0, %s1, %s2, %s3, %s4, %s5, %s6, %s7 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>,
                                                   tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>
 }
@@ -2040,12 +2040,12 @@
 
 // CHECK-LABEL: @push_shape_ops_to_end
 func.func @push_shape_ops_to_end(%arg0 : tensor<12xf32>) -> tensor<3x4x2x1xf32> {
-  // CHECK: %[[COS:.+]] = stablehlo.cosine %arg0 : tensor<12xf32>
-  // CHECK: %[[ABS:.+]] = stablehlo.abs %[[COS]] : tensor<12xf32>
-  // CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[ABS]] : (tensor<12xf32>) -> tensor<3x4xf32>
-  // CHECK: %[[BROADCAST:.+]] = stablehlo.broadcast %[[RESHAPE]], sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>
-  // CHECK: %[[TRANSPOSE:.+]] = stablehlo.transpose %[[BROADCAST]], dims = [2, 3, 1, 0] : (tensor<1x2x3x4xf32>) -> tensor<3x4x2x1xf32>
-  // CHECK: return %[[TRANSPOSE]]
+  // DISABLED-CHECK: %[[COS:.+]] = stablehlo.cosine %arg0 : tensor<12xf32>
+  // DISABLED-CHECK: %[[ABS:.+]] = stablehlo.abs %[[COS]] : tensor<12xf32>
+  // DISABLED-CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[ABS]] : (tensor<12xf32>) -> tensor<3x4xf32>
+  // DISABLED-CHECK: %[[BROADCAST:.+]] = stablehlo.broadcast %[[RESHAPE]], sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>
+  // DISABLED-CHECK: %[[TRANSPOSE:.+]] = stablehlo.transpose %[[BROADCAST]], dims = [2, 3, 1, 0] : (tensor<1x2x3x4xf32>) -> tensor<3x4x2x1xf32>
+  // DISABLED-CHECK: return %[[TRANSPOSE]]
   %0 = stablehlo.reshape %arg0 : (tensor<12xf32>) -> tensor<3x4xf32>
   %1 = stablehlo.broadcast %0, sizes = [1, 2] : (tensor<3x4xf32>) -> tensor<1x2x3x4xf32>
   %2 = stablehlo.cosine %1 : (tensor<1x2x3x4xf32>) -> tensor<1x2x3x4xf32>
@@ -2059,9 +2059,9 @@
 
 // CHECK-LABEL: @reorder_with_type_change
 func.func @reorder_with_type_change(%arg0 : tensor<3x4xi32>) -> tensor<12xi64> {
-  // CHECK: %[[CONVERT:.+]] = stablehlo.convert %arg0 : (tensor<3x4xi32>) -> tensor<3x4xi64>
-  // CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[CONVERT]] : (tensor<3x4xi64>) -> tensor<12xi64>
-  // CHECK: return %[[RESHAPE]]
+  // DISABLED-CHECK: %[[CONVERT:.+]] = stablehlo.convert %arg0 : (tensor<3x4xi32>) -> tensor<3x4xi64>
+  // DISABLED-CHECK: %[[RESHAPE:.+]] = stablehlo.reshape %[[CONVERT]] : (tensor<3x4xi64>) -> tensor<12xi64>
+  // DISABLED-CHECK: return %[[RESHAPE]]
   %0 = stablehlo.reshape %arg0 : (tensor<3x4xi32>) -> tensor<12xi32>
   %1 = stablehlo.convert %0 : (tensor<12xi32>) -> tensor<12xi64>
   return %1 : tensor<12xi64>
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_target_independent_optimization.mlir
@@ -59,3 +59,34 @@
   }
   return %0 : tensor<i64>
 }
+
+// -----
+
+// Check that we properly handle expressions involving NaN terms or variables
+// that could potentially be NaN.
+
+// CHECK-LABEL: @fold_constant_nan_to_nan
+func.func @fold_constant_nan_to_nan() -> tensor<f32> {
+  // CHECK: [[NAN:%.*]] = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  // CHECK: return [[NAN]] : tensor<f32>
+  %zero = stablehlo.constant dense<0.0> : tensor<f32>
+  %one = stablehlo.constant dense<1.0> : tensor<f32>
+  %nan = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  %nan_times_zero = stablehlo.multiply %nan, %zero : tensor<f32>
+  %result = stablehlo.add %one, %nan_times_zero : tensor<f32>
+  return %result : tensor<f32>
+}
+
+// TODO: Consider adding an `--assume-non-nan` pass option to override this.
+// CHECK-LABEL: @do_not_assume_non_nan
+func.func @do_not_assume_non_nan(%arg0: tensor<f32>) -> tensor<f32> {
+  // Note: These two checks are out of order on purpose: [[RESULT]] binds to the
+  // `return` op first and then looks backward for the corresponding assignment.
+  // CHECK-DAG: return [[RESULT:.*]] : tensor<f32>
+  // CHECK-DAG: [[RESULT]] = stablehlo.{{(add|multiply).*}} : tensor<f32>
+  %zero = stablehlo.constant dense<0.0> : tensor<f32>
+  %one = stablehlo.constant dense<1.0> : tensor<f32>
+  %arg_times_zero = stablehlo.multiply %arg0, %zero : tensor<f32>
+  %result = stablehlo.add %one, %arg_times_zero : tensor<f32>
+  return %result : tensor<f32>
+}
diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
@@ -24,6 +24,7 @@
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/Sequence.h"
 #include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/Debug.h"
 #include "mlir/Dialect/Arith/IR/Arith.h"
 #include "mlir/Dialect/Complex/IR/Complex.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
@@ -55,6 +56,8 @@
 // compilation, M_PI will not be defined.
 #define _USE_MATH_DEFINES
 
+#define DEBUG_TYPE "chlo-legalize-to-stablehlo"
+
 namespace mlir {
 namespace stablehlo {
 
@@ -198,6 +201,31 @@
       val);
 }
 
+// Broadcast using numpy-style broadcasting semantics.
+// This is only valid if the CHLO op has static shaped operands, and no
+// explicitly specified broadcast_dimensions.
+//
+// Asserts that input is ranked tensor type.
+Value numpyBroadcastIfNeeded(Value op, RankedTensorType opResultType,
+                             PatternRewriter& rewriter) {
+  RankedTensorType inputType = cast<RankedTensorType>(op.getType());
+  RankedTensorType broadcastedResultType =
+      opResultType.clone(inputType.getElementType());
+
+  // No broadcasting needed if input type matches broadcasted result type.
+  if (inputType == broadcastedResultType) return op;
+
+  // broadcast dims are the last dims for numpy style broadcasting.
+  int64_t inputRank = inputType.getRank();
+  int64_t resultRank = opResultType.getRank();
+  auto broadcastDimensions =
+      llvm::to_vector(llvm::seq<int64_t>(resultRank - inputRank, resultRank));
+  return stablehlo::BroadcastInDimOp::create(rewriter, op.getLoc(),
+                                             broadcastedResultType, op,
+                                             broadcastDimensions)
+      .getResult();
+}
+
 //===----------------------------------------------------------------------===//
 // Broadcasting Patterns.
 //===----------------------------------------------------------------------===//
@@ -215,24 +243,69 @@
     // Only rewrite for statically determinable non-broadcasting cases.
     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());
     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());
-    if (!lhsType || !rhsType) return failure();
-
-    // Requires rank broadcast.
-    if (lhsType.getRank() != rhsType.getRank()) return failure();
-
-    // Any dynamic dimension may require broadcasting and requires more
-    // analysis.
-    if (!lhsType.hasStaticShape() || !rhsType.hasStaticShape()) {
-      return failure();
-    }
-
-    if (!llvm::equal(lhsType.getShape(), rhsType.getShape())) {
-      return failure();
-    }
+    if (!lhsType || !rhsType || lhsType.getShape() != rhsType.getShape() ||
+        !lhsType.hasStaticShape() || !rhsType.hasStaticShape())
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected LHS and RHS to be ranked tensors with matching shapes that "
+          "are all static");
 
     rewriter.replaceOp(
         op, ValueRange{Adaptor::createOp(op, op.getType(),
                                          adaptor.getOperands(), rewriter)});
+    return success();
+  }
+};
+
+// Converts binary ops that statically determined to use default numpy
+// broadcasting to simple StableHLO broadcasting ops without shape dialect.
+template <typename ChloOpTy, typename HloOpTy, typename Adaptor>
+struct ConvertTrivialNumpyBroadcastBinaryOp final
+    : OpConversionPattern<ChloOpTy> {
+  using OpConversionPattern<ChloOpTy>::OpConversionPattern;
+
+  LogicalResult matchAndRewrite(
+      ChloOpTy op, typename ChloOpTy::Adaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    // Only rewrite for statically determinable non-broadcasting cases.
+    auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());
+    auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());
+    if (!lhsType || !rhsType || !lhsType.hasStaticShape() ||
+        !rhsType.hasStaticShape())
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected LHS and RHS to be ranked tensor types with static "
+          "shape");
+
+    // Rely on CHLO type inference to figure out the proper broadcasted shape.
+    auto resultType = dyn_cast<RankedTensorType>(op.getResult().getType());
+    if (!resultType || !resultType.hasStaticShape())
+      return rewriter.notifyMatchFailure(
+          op, "expected result to be a ranked tensor type with static shape");
+
+    auto lhs = adaptor.getLhs();
+    auto rhs = adaptor.getRhs();
+    auto broadcastDimensions = adaptor.getBroadcastDimensions();
+    if (broadcastDimensions &&
+        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions))
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected implicit broadcast_dimensions or numpy-style broadcasting");
+
+    LLVM_DEBUG(llvm::dbgs()
+               << "CHLO Decomposing " << op->getName() << " with broadcast "
+               << lhsType << " x " << rhsType << " -> " << resultType << "\n");
+
+    // If operands are static directly create stablehlo broadcasting ops.
+    // Use numpy-style broadcasting with using StableHLO broadcast ops,
+    // when user didn't specify broadcast_dimensions.
+    auto lhsBroadcast =
+        numpyBroadcastIfNeeded(adaptor.getLhs(), resultType, rewriter);
+    auto rhsBroadcast =
+        numpyBroadcastIfNeeded(adaptor.getRhs(), resultType, rewriter);
+    auto result = Adaptor::createOp(op, resultType,
+                                    {lhsBroadcast, rhsBroadcast}, rewriter);
+    rewriter.replaceOp(op, {result.getResult()});
     return success();
   }
 };
@@ -2416,6 +2489,8 @@
   // not have special attributes that need to be preserved.
   populateForBroadcastingBinaryOp<ConvertTrivialNonBroadcastBinaryOp>(
       context, patterns, 10);
+  populateForBroadcastingBinaryOp<ConvertTrivialNumpyBroadcastBinaryOp>(
+      context, patterns, 10);
   populateForBroadcastingBinaryOp<ConvertRankedDynamicBroadcastBinaryOp>(
       context, patterns, 5);
   patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -23,6 +23,7 @@
 #include <string>
 #include <utility>
 
+#include "llvm/ADT/APFloat.h"
 #include "llvm/ADT/APInt.h"
 #include "llvm/ADT/APSInt.h"
 #include "llvm/ADT/FloatingPointMode.h"
@@ -86,6 +87,18 @@
       /*isUnsigned=*/isUnsigned);
 }
 
+APFloat getAPFloat(
+    Type type, double value,
+    llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {
+  auto floatType = dyn_cast<FloatType>(type);
+  if (!floatType) llvm::report_fatal_error("expected float type");
+
+  APFloat result(value);
+  bool losesInfo = false;
+  result.convert(floatType.getFloatSemantics(), roundingMode, &losesInfo);
+  return result;
+}
+
 LogicalResult validateStaticShapeResult(PatternRewriter& rewriter,
                                         Operation* op, ShapedType resultType) {
   if (!resultType.hasStaticShape())
@@ -94,26 +107,30 @@
   return success();
 }
 
-template <typename Fn>
-static TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,
-                                       Fn&& folder) {
+/// Unary constant folder that uses a generic folder function to handle both
+/// ints and floats.
+template <typename Fn, typename IntResultType = IntegerAttr,
+          typename FloatResultType = FloatAttr>
+TypedAttr foldUnaryOpIntOrFloat(Type resultType, TypedAttr operand,
+                                Fn&& folder) {
   Type elemTy = getElementTypeOrSelf(operand);
 
   Attribute res;
   if (isa<IntegerType>(elemTy))
-    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void>(operand,
-                                                                      folder);
+    res = constFoldUnaryOp<IntegerAttr, IntegerAttr::ValueType, void,
+                           IntResultType>(operand, folder);
   if (isa<FloatType>(elemTy))
-    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void>(operand,
-                                                                  folder);
+    res = constFoldUnaryOp<FloatAttr, FloatAttr::ValueType, void,
+                           FloatResultType>(operand, folder);
   if (res) return cast<TypedAttr>(res);
 
   return nullptr;
 }
 
-/// Binary constant folder that used a generic folder function to handle both
+/// Unary constant folder that uses a generic folder function to handle both
 /// ints and floats.
-template <typename Fn>
+template <typename Fn, typename IntResultType = IntegerAttr,
+          typename FloatResultType = FloatAttr>
 FailureOr<TypedAttr> foldUnaryOpIntOrFloat(PatternRewriter& rewriter,
                                            Operation* op, Fn&& folder) {
   if (op->getNumOperands() != 1 || op->getNumResults() != 1)
@@ -124,35 +141,38 @@
 
   if (!attr) return rewriter.notifyMatchFailure(op, "operand not constants");
 
-  TypedAttr res = foldUnaryOpIntOrFloat(op->getResultTypes()[0], attr, folder);
+  TypedAttr res = foldUnaryOpIntOrFloat<Fn, IntResultType, FloatResultType>(
+      op->getResultTypes()[0], attr, std::forward<Fn>(folder));
   if (!res) return rewriter.notifyMatchFailure(op, "folding failed");
 
   return res;
 }
 
-/// Binary constant folder that used a generic folder function to handle both
+/// Binary constant folder that uses a generic folder function to handle both
 /// ints and floats.
-template <typename Fn>
-static TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs,
-                                        TypedAttr rhs, Fn&& folder) {
+template <typename Fn, typename IntResultType = IntegerAttr,
+          typename FloatResultType = FloatAttr>
+TypedAttr foldBinaryOpIntOrFloat(Type resultType, TypedAttr lhs, TypedAttr rhs,
+                                 Fn&& folder) {
   Attribute operands[2] = {lhs, rhs};
   Type elemTy = getElementTypeOrSelf(lhs);
 
   Attribute res;
   if (isa<IntegerType>(elemTy))
-    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void>(
-        operands, resultType, folder);
+    res = constFoldBinaryOp<IntegerAttr, IntegerAttr::ValueType, void,
+                            IntResultType>(operands, resultType, folder);
   if (isa<FloatType>(elemTy))
-    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void>(
-        operands, resultType, folder);
+    res = constFoldBinaryOp<FloatAttr, FloatAttr::ValueType, void,
+                            FloatResultType>(operands, resultType, folder);
   if (res) return cast<TypedAttr>(res);
 
   return nullptr;
 }
 
-/// Binary constant folder that used a generic folder function to handle both
+/// Binary constant folder that uses a generic folder function to handle both
 /// ints and floats.
-template <typename Fn>
+template <typename Fn, typename IntResultType = IntegerAttr,
+          typename FloatResultType = FloatAttr>
 FailureOr<TypedAttr> foldBinaryOpIntOrFloat(PatternRewriter& rewriter,
                                             Operation* op, Fn&& folder) {
   if (op->getNumOperands() != 2 || op->getNumResults() != 1)
@@ -165,8 +185,8 @@
   if (!lhsAttr || !rhsAttr)
     return rewriter.notifyMatchFailure(op, "lhs & rhs operands not constants");
 
-  TypedAttr res =
-      foldBinaryOpIntOrFloat(op->getResultTypes()[0], lhsAttr, rhsAttr, folder);
+  TypedAttr res = foldBinaryOpIntOrFloat<Fn, IntResultType, FloatResultType>(
+      op->getResultTypes()[0], lhsAttr, rhsAttr, std::forward<Fn>(folder));
   if (!res) return rewriter.notifyMatchFailure(op, "folding failed");
 
   return res;
@@ -371,23 +391,38 @@
 struct FoldAndOpPattern : public ShapeOpRewritePattern<AndOp> {
   using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
-  LogicalResult matchAndRewrite(mlir::stablehlo::AndOp op,
-                                PatternRewriter& rewriter) const override {
-    // TODO: Support more int types
+  LogicalResult matchAndRewrite(AndOp op,
+                                PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
-    if (!resultType.getElementType().isInteger(1))
-      return rewriter.notifyMatchFailure(op, "expected boolean element type");
-
-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldAnd{});
-    if (failed(res)) return failure();
-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
-    return success();
-  }
-
-  struct FoldAnd {
+    auto resultElementType = resultType.getElementType();
+    FailureOr<TypedAttr> result;
+
+    if (resultElementType.isInteger(/*width=*/1)) {
+      result = foldBinaryOpIntOrFloat(rewriter, op, FoldLogicalAnd{});
+    } else if (resultElementType.isInteger()) {
+      result = foldBinaryOpIntOrFloat(rewriter, op, FoldBitwiseAnd{});
+    } else {
+      return rewriter.notifyMatchFailure(op, "Expected integral element type.");
+    }
+
+    if (failed(result)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op,
+                                                             result.value());
+    return success();
+  }
+
+  struct FoldLogicalAnd {
     APInt operator()(APInt lhs, APInt rhs) const {
       return APInt(lhs.getBitWidth(), !lhs.isZero() && !rhs.isZero());
     }
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {
+      return std::nullopt;
+    }
+  };
+
+  struct FoldBitwiseAnd {
+    APInt operator()(APInt lhs, APInt rhs) const { return lhs & rhs; }
+
     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {
       return std::nullopt;
     }
@@ -426,7 +461,7 @@
     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
-    auto res = foldBinaryOpIntOrFloat(
+    auto res = foldBinaryOpIntOrFloat<FoldCompare, IntegerAttr, IntegerAttr>(
         rewriter, op,
         FoldCompare(op.getComparisonDirection(), op.getCompareType()));
     if (failed(res)) return failure();
@@ -441,9 +476,29 @@
     ComparisonDirection direction;
     std::optional<ComparisonType> kind;
 
-    // TODO: Enable float folding.
-    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
-      return std::nullopt;
+    APInt operator()(APFloat lhs, APFloat rhs) {
+      bool result = false;
+      switch (direction) {
+        case ComparisonDirection::EQ:
+          result = lhs == rhs;
+          break;
+        case ComparisonDirection::NE:
+          result = lhs != rhs;
+          break;
+        case ComparisonDirection::GE:
+          result = lhs >= rhs;
+          break;
+        case ComparisonDirection::GT:
+          result = lhs > rhs;
+          break;
+        case ComparisonDirection::LE:
+          result = lhs <= rhs;
+          break;
+        case ComparisonDirection::LT:
+          result = lhs < rhs;
+          break;
+      }
+      return APInt(/*bitwidth=*/1, result);
     }
     APInt operator()(APInt lhs, APInt rhs) {
       bool result = false;
@@ -509,6 +564,20 @@
     Operation* terminator = blockToInline->getTerminator();
     ValueRange results = terminator->getOperands();
 
+    // TODO: Add support for complex, quantized, and token return types.
+    // Currently, this pattern only supports int and float return types. We'll
+    // need a more general equivalent of `getZeroAttr` to support other types.
+    SmallVector<TypedAttr> placeholderAttrs;
+    for (auto result : op.getResults()) {
+      TypedAttr placeholderAttr = rewriter.getZeroAttr(result.getType());
+      if (!placeholderAttr)
+        return rewriter.notifyMatchFailure(
+            op,
+            "The case op's return type isn't currently supported by this "
+            "optimization pattern.");
+      placeholderAttrs.push_back(placeholderAttr);
+    }
+
     // Inline the active branch of the `case` op.
     rewriter.inlineBlockBefore(blockToInline, op, blockArgs);
     rewriter.replaceAllOpUsesWith(op, results);
@@ -521,9 +590,9 @@
     Block& noopBlock = region.emplaceBlock();
     SmallVector<Value> placeholderResults;
     rewriter.setInsertionPointToEnd(&noopBlock);
-    for (auto result : op.getResults()) {
-      placeholderResults.push_back(rewriter.create<ConstantOp>(
-          region.getLoc(), rewriter.getZeroAttr(result.getType())));
+    for (auto placeholderAttr : placeholderAttrs) {
+      placeholderResults.push_back(
+          rewriter.create<ConstantOp>(region.getLoc(), placeholderAttr));
     }
     rewriter.create<stablehlo::ReturnOp>(region.getLoc(), placeholderResults);
 
@@ -628,10 +697,7 @@
         : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
     std::function<APInt(APInt, APInt)> foldIntFn;
 
-    // TODO: Enable float folding.
-    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
-      return std::nullopt;  // return lhs / rhs;
-    }
+    APFloat operator()(APFloat lhs, APFloat rhs) { return lhs / rhs; }
     APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
     static APInt foldUint(APInt lhs, APInt rhs) { return lhs.udiv(rhs); }
     static APInt foldSint(APInt lhs, APInt rhs) { return lhs.sdiv(rhs); }
@@ -669,9 +735,8 @@
       : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
   std::function<APInt(APInt, APInt)> foldIntFn;
 
-  // TODO: Enable float folding.
-  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
-    return std::nullopt;  // return lhs >= rhs ? lhs : rhs;
+  APFloat operator()(APFloat lhs, APFloat rhs) {
+    return lhs >= rhs ? lhs : rhs;
   }
   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
   static APInt foldUint(APInt lhs, APInt rhs) {
@@ -687,9 +752,8 @@
       : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
   std::function<APInt(APInt, APInt)> foldIntFn;
 
-  // TODO: Enable float folding.
-  std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
-    return std::nullopt;  // return lhs <= rhs ? lhs : rhs;
+  APFloat operator()(APFloat lhs, APFloat rhs) {
+    return lhs <= rhs ? lhs : rhs;
   }
   APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
   static APInt foldUint(APInt lhs, APInt rhs) {
@@ -706,11 +770,14 @@
   LogicalResult matchAndRewrite(MaxOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
+    auto resultElementType = resultType.getElementType();
     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
-    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedInt));
+    bool isUnsignedIntOrBool = resultElementType.isUnsignedInteger() ||
+                               resultElementType.isInteger(/*width=*/1);
+    auto res =
+        foldBinaryOpIntOrFloat(rewriter, op, FoldMax(isUnsignedIntOrBool));
     if (failed(res)) return failure();
     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
     return success();
@@ -723,11 +790,14 @@
   LogicalResult matchAndRewrite(MinOp op,
                                 PatternRewriter& rewriter) const override {
     auto resultType = op.getType();
+    auto resultElementType = resultType.getElementType();
     if (failed(validateShapeFoldDtype(rewriter, op, resultType)))
       return failure();
 
-    bool isUnsignedInt = resultType.getElementType().isUnsignedInteger();
-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedInt));
+    bool isUnsignedIntOrBool = resultElementType.isUnsignedInteger() ||
+                               resultElementType.isInteger(/*width=*/1);
+    auto res =
+        foldBinaryOpIntOrFloat(rewriter, op, FoldMin(isUnsignedIntOrBool));
     if (failed(res)) return failure();
     rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
     return success();
@@ -786,21 +856,36 @@
 
   LogicalResult matchAndRewrite(OrOp op,
                                 PatternRewriter& rewriter) const override {
-    // TODO: Support more int types
     auto resultType = op.getType();
-    if (!resultType.getElementType().isInteger(1))
-      return rewriter.notifyMatchFailure(op, "expected boolean element type");
-
-    auto res = foldBinaryOpIntOrFloat(rewriter, op, FoldOr{});
-    if (failed(res)) return failure();
-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
-    return success();
-  }
-
-  struct FoldOr {
+    auto resultElementType = resultType.getElementType();
+    FailureOr<TypedAttr> result;
+
+    if (resultElementType.isInteger(/*width=*/1)) {
+      result = foldBinaryOpIntOrFloat(rewriter, op, FoldLogicalOr{});
+    } else if (resultElementType.isInteger()) {
+      result = foldBinaryOpIntOrFloat(rewriter, op, FoldBitwiseOr{});
+    } else {
+      return rewriter.notifyMatchFailure(op, "Expected integral element type.");
+    }
+
+    if (failed(result)) return failure();
+    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op,
+                                                             result.value());
+    return success();
+  }
+
+  struct FoldLogicalOr {
     APInt operator()(APInt lhs, APInt rhs) const {
       return APInt(lhs.getBitWidth(), !lhs.isZero() || !rhs.isZero());
     }
+    std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {
+      return std::nullopt;
+    }
+  };
+
+  struct FoldBitwiseOr {
+    APInt operator()(APInt lhs, APInt rhs) const { return lhs | rhs; }
+
     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) const {
       return std::nullopt;
     }
@@ -828,9 +913,12 @@
         : foldIntFn(isUnsignedInt ? foldUint : foldSint) {}
     std::function<APInt(APInt, APInt)> foldIntFn;
 
-    // TODO: Enable float folding.
     std::optional<APFloat> operator()(APFloat lhs, APFloat rhs) {
-      return std::nullopt;  // return lhs.remainder(rhs);
+      // `APFloat::mod` requires both operands to have identical semantics.
+      if (&lhs.getSemantics() != &rhs.getSemantics()) return std::nullopt;
+
+      lhs.mod(rhs);  // This modifies `lhs` in place.
+      return lhs;    // `lhs` now holds the result.
     }
     APInt operator()(APInt lhs, APInt rhs) { return foldIntFn(lhs, rhs); }
     static APInt foldUint(APInt lhs, APInt rhs) { return lhs.urem(rhs); }
@@ -925,6 +1013,11 @@
     auto resultType = op.getType();
     // No need to verify static shape or dtype here since we aren't evaluating
     // dtype, just folding set_dim_size ops with no semantic meaning.
+
+    // Don't fold if the input is dynamic and we're washing away the bound.
+    if (op.getOperand().getType() != op.getType())
+      return rewriter.notifyMatchFailure(
+          op, "operand and result type must be the same");
 
     SplatElementsAttr cstSplatAttr;
     matchPattern(op.getSize(), m_Constant(&cstSplatAttr));
@@ -963,8 +1056,16 @@
   struct FoldSign {
     FoldSign(Type elementType) : elementType(elementType) {}
     Type elementType;
-    // TODO: Enable float folding.
-    std::optional<APFloat> operator()(APFloat operand) { return std::nullopt; }
+    double result;
+    APFloat operator()(APFloat operand) {
+      if (operand.isNegative())
+        result = -1.0;
+      else if (operand.isZero())
+        result = 0.0;
+      else
+        result = 1.0;
+      return getAPFloat(elementType, result);
+    }
 
     APInt operator()(APInt operand) {
       // SignOp only supports signed integers.
@@ -1220,13 +1321,9 @@
 
     for (auto [inputValue, bodyArg] :
          llvm::zip_equal(op.getOperands(), body.getArguments())) {
-      auto inputConstantOp = inputValue.getDefiningOp<ConstantOp>();
-      if (!inputConstantOp)
-        return rewriter.notifyMatchFailure(op, "Input must be a constant.");
-
-      auto inputConstantAttr =
-          dyn_cast_or_null<DenseElementsAttr>(inputConstantOp.getValue());
-      if (!inputConstantAttr)
+      SplatElementsAttr constantSplatAttr;
+      if (!matchPattern(inputValue, m_Constant(&constantSplatAttr)) ||
+          !constantSplatAttr)
         return rewriter.notifyMatchFailure(op,
                                            "Input must be a splat constant.");
 
@@ -1236,7 +1333,7 @@
             op, "Could not get the shape of the body argument.");
 
       bodyArgConstantAttrs.push_back(DenseElementsAttr::get(
-          bodyArgShapedType, inputConstantAttr.getSplatValue<Attribute>()));
+          bodyArgShapedType, constantSplatAttr.getSplatValue<Attribute>()));
     }
 
     for (BlockArgument bodyArg : body.getArguments()) {
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
@@ -64,7 +64,7 @@
 static constexpr StablehloAggressiveSimplificationPassOptions kDefaultOptions;
 
 static bool isIotaRange(ArrayRef<int64_t> dims) {
-  return llvm::all_of(llvm::enumerate(dims), [](const auto &it) {
+  return llvm::all_of(llvm::enumerate(dims), [](const auto& it) {
     return static_cast<int64_t>(it.index()) == it.value();
   });
 }
@@ -72,20 +72,20 @@
 template <typename OpType>
 struct SimplifyOpRewritePattern : OpRewritePattern<OpType> {
   SimplifyOpRewritePattern(
-      MLIRContext *context,
-      const StablehloAggressiveSimplificationPassOptions &options,
+      MLIRContext* context,
+      const StablehloAggressiveSimplificationPassOptions& options,
       PatternBenefit benefit = 1, ArrayRef<StringRef> generatedNames = {})
       : OpRewritePattern<OpType>(context, benefit, generatedNames),
         options(options) {}
 
   // Prevent `options` from binding to a temporary.
   SimplifyOpRewritePattern(
-      MLIRContext *context,
-      StablehloAggressiveSimplificationPassOptions &&options,
+      MLIRContext* context,
+      StablehloAggressiveSimplificationPassOptions&& options,
       PatternBenefit benefit = 1,
       ArrayRef<StringRef> generatedNames = {}) = delete;
 
-  const StablehloAggressiveSimplificationPassOptions &options;
+  const StablehloAggressiveSimplificationPassOptions& options;
 };
 
 /// Matches when either of the submatchers match.
@@ -93,7 +93,7 @@
 struct m_AnyOf {
   m_AnyOf(MatcherA a, MatcherB b) : matcherA(a), matcherB(b) {}
 
-  bool match(Operation *op) { return matcherA.match(op) || matcherB.match(op); }
+  bool match(Operation* op) { return matcherA.match(op) || matcherB.match(op); }
 
   MatcherA matcherA;
   MatcherB matcherB;
@@ -146,7 +146,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(CompareOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     RankedTensorType type = op.getType();
 
     // Bail out on non-integer comparison.
@@ -211,7 +211,7 @@
  public:
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
   LogicalResult matchAndRewrite(ConcatenateOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     if (op.getInputs().size() != 1 ||
         op.getInputs().front().getType() != op.getType())
       return rewriter.notifyMatchFailure(op, "not single operand noop-concat");
@@ -227,7 +227,7 @@
  public:
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
   LogicalResult matchAndRewrite(ConcatenateOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     auto axis = op.getDimension();
     llvm::SmallVector<Value> newOperands = llvm::to_vector(
         llvm::make_filter_range(op.getOperands(), [&](Value operand) {
@@ -249,8 +249,8 @@
 class ConcatenateOpFlatten : public SimplifyOpRewritePattern<ConcatenateOp> {
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
   LogicalResult matchAndRewrite(ConcatenateOp op,
-                                PatternRewriter &rewriter) const override {
-    auto getFlattenedOperands = [&](const Value &val) -> ValueRange {
+                                PatternRewriter& rewriter) const override {
+    auto getFlattenedOperands = [&](const Value& val) -> ValueRange {
       auto definingOp = dyn_cast_or_null<ConcatenateOp>(val.getDefiningOp());
       // To avoid inflate the memory footprint, only flatten the
       // ConcatenateOp when it has only one use.
@@ -293,7 +293,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(CustomCallOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     constexpr StringRef kMhloBackendConfigAttrName = "mhlo.backend_config";
 
     if (op.getApiVersion() != CustomCallApiVersion::API_VERSION_ORIGINAL)
@@ -327,7 +327,7 @@
 /////////////////////////////////
 
 // Used in DRR file.
-DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder &b,
+DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder& b,
                                                ArrayRef<int64_t> dims,
                                                ArrayRef<int64_t> dimsParent) {
   auto mergedDims = llvm::map_to_vector(
@@ -350,8 +350,8 @@
 /// the op is used outside of the HLO dialect (e.g. in func.return). In these
 /// cases, we insert a stablehlo.convert to smooth things out.
 template <typename OpTy, typename... Args>
-static OpTy refineOpWithNewOp(PatternRewriter &rewriter, Operation *op,
-                              Args &&...args) {
+static OpTy refineOpWithNewOp(PatternRewriter& rewriter, Operation* op,
+                              Args&&... args) {
   auto newOp = rewriter.create<OpTy>(op->getLoc(), std::forward<Args>(args)...);
 
   llvm::SmallVector<Value> replacementResults;
@@ -360,7 +360,7 @@
   for (auto [opResult, newOpResult] :
        llvm::zip(op->getResults(), newOp->getResults())) {
     Value replacementResult = newOpResult;
-    if (llvm::any_of(opResult.getUsers(), [&](Operation *user) {
+    if (llvm::any_of(opResult.getUsers(), [&](Operation* user) {
           return user->getDialect() != op->getDialect();
         }))
       replacementResult = rewriter.create<ConvertOp>(
@@ -379,7 +379,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(DynamicBroadcastInDimOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     RankedTensorType operandType = op.getOperand().getType();
     if (!operandType.hasStaticShape())
       return rewriter.notifyMatchFailure(op, "requires operand static shape");
@@ -410,7 +410,7 @@
 // DynamicGatherOp
 /////////////////////////////////
 
-DenseI64ArrayAttr convertToI64Array(OpBuilder &b, Attribute attr) {
+DenseI64ArrayAttr convertToI64Array(OpBuilder& b, Attribute attr) {
   auto denseAttr = cast<ElementsAttr>(attr);
   SmallVector<int64_t> result;
   result.reserve(denseAttr.getNumElements());
@@ -427,7 +427,7 @@
   using SimplifyOpRewritePattern<DynamicIotaOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(DynamicIotaOp iota,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     // Result type has static shape, replace with iota.
     auto resultTy = cast<ShapedType>(iota.getType());
     if (!resultTy.hasStaticShape())
@@ -447,7 +447,7 @@
   using SimplifyOpRewritePattern<DynamicIotaOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(DynamicIotaOp iota,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     auto resultType = cast<ShapedType>(iota.getType());
     if (resultType.getRank() < 2)
       return rewriter.notifyMatchFailure(iota, "requires rank >= 2");
@@ -496,7 +496,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(DynamicReshapeOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     // This is a noop when the output type is already a static shape.
     RankedTensorType type = op.getType();
     if (!type.hasStaticShape())
@@ -516,8 +516,8 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(DynamicReshapeOp op,
-                                PatternRewriter &rewriter) const override {
-    Operation *defOp = op.getOperand().getDefiningOp();
+                                PatternRewriter& rewriter) const override {
+    Operation* defOp = op.getOperand().getDefiningOp();
     if (!defOp ||
         !defOp->hasTrait<mlir::OpTrait::SameOperandsAndResultShape>()) {
       return rewriter.notifyMatchFailure(
@@ -549,7 +549,7 @@
   using SimplifyOpRewritePattern<DynamicSliceOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(DynamicSliceOp dynamicSlice,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     Value input = dynamicSlice.getOperand();
     auto inputType = cast<ShapedType>(input.getType());
     if (!inputType.hasStaticShape())
@@ -558,7 +558,7 @@
 
     auto sliceSizes = dynamicSlice.getSliceSizes();
     SmallVector<int64_t, 4> tempStartIndices;
-    for (const auto &indexAndSliceStart :
+    for (const auto& indexAndSliceStart :
          llvm::enumerate(dynamicSlice.getStartIndices())) {
       APInt val;
       Value start = indexAndSliceStart.value();
@@ -579,7 +579,7 @@
     // pack them into a single tensor.
     auto sliceStartIndices = rewriter.getDenseI64ArrayAttr(tempStartIndices);
     SmallVector<int64_t, 4> tempSliceLimits;
-    for (const auto &[start, size] : llvm::zip(tempStartIndices, sliceSizes)) {
+    for (const auto& [start, size] : llvm::zip(tempStartIndices, sliceSizes)) {
       tempSliceLimits.push_back(start + size);
     }
     auto sliceLimits = rewriter.getDenseI64ArrayAttr(tempSliceLimits);
@@ -605,7 +605,7 @@
   using SimplifyOpRewritePattern<RealDynamicSliceOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(RealDynamicSliceOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     // This rewrite only works for unit strides because DynamicSliceOp
     // doesn't support strides (i.e. it implicitly has unit strides).
     DenseIntElementsAttr stridesAttr;
@@ -670,11 +670,11 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(ReduceOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     // If all returned values in the ReduceOp region exists outside the
     // region, replace the ReduceOp with those values.
     if (auto retOp = dyn_cast<ReturnOp>(op.getBody().front().getTerminator())) {
-      Region *retRegion = retOp->getParentRegion();
+      Region* retRegion = retOp->getParentRegion();
       if (llvm::any_of(retOp.getResults(), [retRegion](Value result) {
             return result.getParentRegion() == retRegion;
           }))
@@ -693,7 +693,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(ReduceOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     // We require all reduce shapes to be the same, up to the element types, so
     // we can just use the first operand and the first result as
     // representatives.
@@ -733,7 +733,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(ReduceOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     SmallVector<OpResult, 4> usedResults;
     llvm::copy_if(op.getResults(), std::back_inserter(usedResults),
                   [](OpResult result) { return !result.use_empty(); });
@@ -745,7 +745,7 @@
     const auto numOperands = op.getNumOperands();
     const auto numOperandPairs = numOperands / pairSize;
 
-    Block &reducerBlock = op.getBody().front();
+    Block& reducerBlock = op.getBody().front();
     auto retOp = cast<ReturnOp>(reducerBlock.getTerminator());
 
     assert(numOperandPairs == op.getNumResults() &&
@@ -757,10 +757,10 @@
       if (v.getParentRegion() == reducerBody) workList.push_back(v);
     };
 
-    SmallPtrSet<Operation *, 16> usedOps;
+    SmallPtrSet<Operation*, 16> usedOps;
     SmallBitVector usedArgs(numOperands);
     SmallBitVector usedReturnOperands(numOperandPairs);
-    for (const auto &usedResult : usedResults) {
+    for (const auto& usedResult : usedResults) {
       auto resultNo = usedResult.getResultNumber();
       usedReturnOperands.set(resultNo);
 
@@ -774,9 +774,9 @@
           const auto pairNo = blockArg.getArgNumber() % numOperandPairs;
           usedArgs.set(pairNo);
           usedArgs.set(pairNo + numOperandPairs);
-        } else if (auto *defOp = definition.getDefiningOp()) {
+        } else if (auto* defOp = definition.getDefiningOp()) {
           usedOps.insert(defOp);
-          for (const auto &operand : defOp->getOperands())
+          for (const auto& operand : defOp->getOperands())
             addToWorkList(operand);
         }
       }
@@ -785,7 +785,7 @@
     const auto newNumOperandPairs = usedResults.size();
     const auto newNumOperands = newNumOperandPairs * pairSize;
     if (newNumOperands != usedArgs.count()) {
-      return rewriter.notifyMatchFailure(op, [&](Diagnostic &diag) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
         diag << "non-conservative case: " << newNumOperandPairs
              << " return results should be matched with " << newNumOperands
              << " operands, but got " << usedArgs.count();
@@ -809,7 +809,7 @@
     auto newOp =
         rewriter.create<ReduceOp>(op.getLoc(), newInputs, newInitVals,
                                   op.getDimensionsAttr(), newElementTypes);
-    Block *newReducerBlock = rewriter.createBlock(&newOp.getBody());
+    Block* newReducerBlock = rewriter.createBlock(&newOp.getBody());
 
     IRMapping mapper;
     for (auto arg : reducerBlock.getArguments())
@@ -818,11 +818,11 @@
                    newReducerBlock->addArgument(arg.getType(), arg.getLoc()));
 
     rewriter.setInsertionPointToStart(newReducerBlock);
-    for (Operation &op : reducerBlock.getOperations())
+    for (Operation& op : reducerBlock.getOperations())
       if (usedOps.contains(&op)) rewriter.clone(op, mapper);
 
     SmallVector<Value> newReturnOperands;
-    for (const auto &en : llvm::enumerate(retOp.getOperands()))
+    for (const auto& en : llvm::enumerate(retOp.getOperands()))
       if (usedReturnOperands[en.index()])
         newReturnOperands.push_back(mapper.lookup(en.value()));
 
@@ -830,7 +830,7 @@
 
     // Build new results list (unused entries will be null).
     SmallVector<Value> newResults(op.getNumResults());
-    for (const auto &[i, result] : llvm::enumerate(usedResults)) {
+    for (const auto& [i, result] : llvm::enumerate(usedResults)) {
       newResults[result.getResultNumber()] = newOp.getResult(i);
     }
 
@@ -851,7 +851,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(GetDimensionSizeOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     // Fold get_dimension_size when the queried dim is statically known.
     RankedTensorType operandTy = op.getOperand().getType();
 
@@ -877,7 +877,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(GatherOp gather,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     DenseIntElementsAttr index;
     if (!matchPattern(gather.getStartIndices(), m_Constant(&index)))
       return failure();
@@ -952,7 +952,7 @@
   using SimplifyOpRewritePattern<IotaOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(IotaOp iota,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     auto resultTy = cast<ShapedType>(iota.getType());
     if (resultTy.getRank() < 2)
       return rewriter.notifyMatchFailure(iota, "itoa not broadcastable");
@@ -989,7 +989,7 @@
   using SimplifyOpRewritePattern<PadOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(PadOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     auto operand = op.getOperand();
     auto padVal = op.getPaddingValue();
 
@@ -1028,7 +1028,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(SelectOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     RankedTensorType type = op.getType();
 
     Value trueVal = op.getOnTrue();
@@ -1079,7 +1079,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(SelectOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     Value pred = op.getPred();
     Value trueVal = op.getOnTrue();
     Value falseVal = op.getOnFalse();
@@ -1133,7 +1133,7 @@
   using SimplifyOpRewritePattern<SliceOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(SliceOp slice,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     auto resultTy = cast<ShapedType>(slice.getType());
     if (!resultTy.hasStaticShape())
       return rewriter.notifyMatchFailure(slice, "result shape not static");
@@ -1228,12 +1228,12 @@
   using SimplifyOpRewritePattern<SortOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(SortOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     DenseSet<unsigned> erasedArgs;
     unsigned numOperands = op.getNumOperands();
     for (unsigned i = 0; i < numOperands; ++i) {
       if (!op.getResult(i).use_empty()) continue;
-      Block &block = op.getComparator().front();
+      Block& block = op.getComparator().front();
       if (!block.getArgument(i * 2).use_empty()) continue;
       if (!block.getArgument(i * 2 + 1).use_empty()) continue;
       erasedArgs.insert(i);
@@ -1242,7 +1242,7 @@
 
     SmallVector<Value> newOperands;
     BitVector erasedBlockArgs(op.getNumOperands() * 2);
-    for (const auto &en : llvm::enumerate(op.getInputs())) {
+    for (const auto& en : llvm::enumerate(op.getInputs())) {
       if (erasedArgs.contains(en.index())) {
         erasedBlockArgs.set(en.index() * 2);
         erasedBlockArgs.set(en.index() * 2 + 1);
@@ -1253,7 +1253,7 @@
 
     auto newOp = rewriter.create<SortOp>(op.getLoc(), newOperands,
                                          op.getDimension(), op.getIsStable());
-    Region &region = newOp.getComparator();
+    Region& region = newOp.getComparator();
     rewriter.inlineRegionBefore(op.getComparator(), region, region.end());
     region.front().eraseArguments(erasedBlockArgs);
 
@@ -1278,7 +1278,7 @@
   using SimplifyOpRewritePattern<SortOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(SortOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     if (op.getResults().empty() ||
         static_cast<int64_t>(op.getDimension()) != -1)
       return rewriter.notifyMatchFailure(op,
@@ -1304,7 +1304,7 @@
   using SimplifyOpRewritePattern::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(TransposeOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     auto input = op.getOperand();
     auto permutation = op.getPermutation();
 
@@ -1340,7 +1340,7 @@
   using SimplifyOpRewritePattern<TupleOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(TupleOp op,
-                                PatternRewriter &rewriter) const override {
+                                PatternRewriter& rewriter) const override {
     if (op.getVal().empty())
       return rewriter.notifyMatchFailure(op, "empty tuple");
 
@@ -1356,7 +1356,7 @@
           op, "tuple predecessor type does not match");
 
     // Check that this is a repacking of the parent tuple.
-    for (const auto &elementAndIdx : llvm::enumerate(op.getVal())) {
+    for (const auto& elementAndIdx : llvm::enumerate(op.getVal())) {
       auto elementOp = elementAndIdx.value().getDefiningOp<GetTupleElementOp>();
       if (!elementOp ||
           elementOp.getIndexAttr().getInt() !=
@@ -1385,9 +1385,9 @@
   using SimplifyOpRewritePattern<WhileOp>::SimplifyOpRewritePattern;
 
   LogicalResult matchAndRewrite(WhileOp whileOp,
-                                PatternRewriter &rewriter) const override {
-    Block *cond = whileOp.SingleBlock::getBody(0);
-    Block *body = whileOp.SingleBlock::getBody(1);
+                                PatternRewriter& rewriter) const override {
+    Block* cond = whileOp.SingleBlock::getBody(0);
+    Block* body = whileOp.SingleBlock::getBody(1);
     auto bodyReturnOp = cast<ReturnOp>(body->getTerminator());
     if (!llvm::any_of(llvm::zip(whileOp->getOperands(), body->getArguments(),
                                 bodyReturnOp->getOperands()),
@@ -1400,10 +1400,10 @@
     SmallVector<Value> newOperands, resultsToReplace;
     SmallVector<unsigned> invariantArgIdxs;
     BitVector invariantArgIdxBitVector(cond->getNumArguments());
-    for (const auto &enumeratedOperands : llvm::enumerate(llvm::zip(
+    for (const auto& enumeratedOperands : llvm::enumerate(llvm::zip(
              whileOp.getOperands(), cond->getArguments(), body->getArguments(),
              bodyReturnOp->getOperands(), whileOp->getResults()))) {
-      const auto &operands = enumeratedOperands.value();
+      const auto& operands = enumeratedOperands.value();
       Value whileOperand = std::get<0>(operands);
       BlockArgument condBlockArg = std::get<1>(operands);
       BlockArgument bodyBlockArg = std::get<2>(operands);
@@ -1455,14 +1455,14 @@
 // Pattern: op(X : zero_extent_tensor) -> constant([])
 struct ZeroExtentToEmptyConstant final : RewritePattern {
   explicit ZeroExtentToEmptyConstant(
-      MLIRContext *context,
+      MLIRContext* context,
       StablehloAggressiveSimplificationPassOptions options,
       PatternBenefit benefit = 1)
       : RewritePattern(MatchAnyOpTypeTag(), benefit, context),
         options(options) {}
 
-  LogicalResult matchAndRewrite(Operation *op,
-                                PatternRewriter &rewriter) const override {
+  LogicalResult matchAndRewrite(Operation* op,
+                                PatternRewriter& rewriter) const override {
     auto loc = op->getLoc();
 
     if (!isa_and_present<StablehloDialect>(op->getDialect()))
@@ -1492,10 +1492,10 @@
 
     // If one of the operands is a zero-extent tensor, replace the operand with
     // an empty tensor.
-    for (OpOperand &operand : op->getOpOperands()) {
+    for (OpOperand& operand : op->getOpOperands()) {
       auto operandType = getMaybeZeroExtentType(operand.get().getType());
       if (!operandType || operand.get().getDefiningOp<ConstantOp>()) continue;
-      Operation *owner = operand.getOwner();
+      Operation* owner = operand.getOwner();
       int operandNum = operand.getOperandNumber();
       auto emptyConstantOp = rewriter.create<ConstantOp>(
           loc, operandType.value(),
@@ -1514,13 +1514,13 @@
 struct ReorderElementwiseAndShapeOp final
     : OpTraitRewritePattern<OpTrait::Elementwise> {
   explicit ReorderElementwiseAndShapeOp(
-      MLIRContext *context,
+      MLIRContext* context,
       StablehloAggressiveSimplificationPassOptions options,
       PatternBenefit benefit = 1)
       : OpTraitRewritePattern(context, benefit), options(options) {}
 
-  LogicalResult matchAndRewrite(Operation *op,
-                                PatternRewriter &rewriter) const override {
+  LogicalResult matchAndRewrite(Operation* op,
+                                PatternRewriter& rewriter) const override {
     if (op->getOperands().size() != 1)
       return rewriter.notifyMatchFailure(op, "expected to be unary");
 
@@ -1575,7 +1575,7 @@
       : StablehloAggressiveSimplificationPassBase() {}
 
   void runOnOperation() override {
-    MLIRContext *context = &getContext();
+    MLIRContext* context = &getContext();
     RewritePatternSet patterns(context);
 
     StablehloAggressiveSimplificationPassOptions options{
@@ -1597,12 +1597,13 @@
 }  // namespace
 
 void populateStablehloCanonicalizationPatterns(
-    MLIRContext *context, RewritePatternSet *patterns,
-    const StablehloAggressiveSimplificationPassOptions &options,
+    MLIRContext* context, RewritePatternSet* patterns,
+    const StablehloAggressiveSimplificationPassOptions& options,
     PatternBenefit benefit) {
   populateWithGenerated(*patterns);
+  // TODO: Re-enable `CompareSelectIntoMinMax` after fixing legalization issue.
   patterns->add<
-      CompareOpCanon, CompareSelectIntoMinMax, ConcatenateOpFlatten,
+      CompareOpCanon, /*CompareSelectIntoMinMax,*/ ConcatenateOpFlatten,
       ConcatenateOpNoop, ConcatenateOpRemoveEmpty,
       CustomCallUnregisteredBackendConfigToFfi, DynamicIotaOpToBroadcast,
       DynamicReshapeOpSameOperandAndResultShape, DynamicSliceOpToSlice,
@@ -1614,8 +1615,11 @@
       context, options, benefit);
 
   // Generic patterns
-  patterns->add<ReorderElementwiseAndShapeOp, ZeroExtentToEmptyConstant>(
-      context, options, benefit);
+  // TODO: Re-enable `ReorderElementwiseAndShapeOp` after fixing BF16 precision
+  // emulation issue in XLA-CPU.
+  patterns->add<
+      /*ReorderElementwiseAndShapeOp,*/
+      ZeroExtentToEmptyConstant>(context, options, benefit);
 
   // TODO: Dynamism Refinements, consider merging with canonicalize dynamism
   patterns
@@ -1623,22 +1627,22 @@
             DynamicReshapeOpIsStatic, DynamicIotaIsStatic>(context, options);
 }
 
-void populateStablehloCanonicalizationPatterns(MLIRContext *context,
-                                               RewritePatternSet *patterns,
+void populateStablehloCanonicalizationPatterns(MLIRContext* context,
+                                               RewritePatternSet* patterns,
                                                PatternBenefit benefit) {
   populateStablehloCanonicalizationPatterns(context, patterns, kDefaultOptions,
                                             benefit);
 }
 
 void populateStablehloHloImportCanonicalizationPatterns(
-    MLIRContext *context, RewritePatternSet *patterns,
-    const StablehloAggressiveSimplificationPassOptions &options) {
+    MLIRContext* context, RewritePatternSet* patterns,
+    const StablehloAggressiveSimplificationPassOptions& options) {
   patterns->add<ReshapeOp_RemoveNoop, GetTupleElementOp_UnpackTuple>(context);
   patterns->add<TupleIsRepacking, WhileOpImplicitCapture>(context, options);
 }
 
 void populateStablehloHloImportCanonicalizationPatterns(
-    MLIRContext *context, RewritePatternSet *patterns) {
+    MLIRContext* context, RewritePatternSet* patterns) {
   populateStablehloHloImportCanonicalizationPatterns(context, patterns,
                                                      kDefaultOptions);
 }
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -234,7 +234,7 @@
 //
 // No-op, but wrap in ConvertOp to preserve dynamic output shape. This can be
 // important if the result is returned, in which case refining the type would
-// require also updating the funciton signature.
+// require also updating the function signature.
 def DynamicBroadcastInDimOp_ReplaceNoopWithConvert
   : Pat<(StableHLO_DynamicBroadcastInDimOp:$op
             $operand, $shape, IotaDims:$dims, $expanding, $nonexpanding),
@@ -387,9 +387,10 @@
 
 // Pattern: multiply(X, 0i) -> 0i
 //
-// Multiplication by 0. This fold is not trivial for floats in presence of NaNs.
+// Multiplication by 0. This fold is not trivial for floats in presence of NaNs,
+// so we currently only enable it for ints.
 def MulOp_FoldToZero
-  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero AnyZero:$value)),
+  : Pat<(StableHLO_MulOp $lhs, (StableHLO_ConstantOp:$zero IntZero:$value)),
         (replaceWithValue $zero)>;
 
 // Pattern: multiply(X, 1i) -> X

