diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
--- stablehlo/BUILD.bazel
+++ stablehlo/BUILD.bazel
@@ -1231,6 +1231,7 @@
     strip_include_prefix = ".",
     deps = [
         ":base",
+        ":chlo_ops",
         ":stablehlo_aggressive_simplification_inc_gen",
         ":stablehlo_ops",
         ":stablehlo_pass_inc_gen",
diff --ruN a/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir b/stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
--- stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
+++ stablehlo/stablehlo/conversions/linalg/tests/pointwise.mlir
@@ -1,5 +1,6 @@
 // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg --split-input-file --canonicalize | FileCheck %s
 // RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg="enable-primitive-ops=true" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-PRIMITIVE
+// RUN: stablehlo-opt %s --stablehlo-legalize-to-linalg="capture-scalar-inputs=false" --split-input-file --canonicalize | FileCheck %s --check-prefix=CHECK-NO-CAPTURE
 
 // CHECK: #map = affine_map<(d0, d1) -> (d0, d1)>
 // CHECK-LABEL: func @float_add
@@ -534,6 +535,19 @@
   %0 = "stablehlo.sign"(%arg0) : (tensor<2x2xcomplex<f32>>)
                           -> tensor<2x2xcomplex<f32>>
   func.return %0 : tensor<2x2xcomplex<f32>>
+}
+
+// -----
+
+// CHECK-LABEL: func @float_tan
+// CHECK-PRIMITIVE-LABEL: func @float_tan
+func.func @float_tan(%arg0: tensor<2x2xf32>) -> tensor<2x2xf32> {
+  // CHECK: linalg.generic
+  // CHECK: tan
+  // CHECK-PRIMITIVE: linalg.map
+  // CHECK-PRIMITIVE: tan
+  %0 = "stablehlo.tan"(%arg0) : (tensor<2x2xf32>) -> tensor<2x2xf32>
+  func.return %0 : tensor<2x2xf32>
 }
 
 // -----
@@ -926,6 +940,23 @@
 // CHECK-PRIMITIVE:      (%[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32) {
 // CHECK-PRIMITIVE:        %[[RES:.*]] = arith.select %[[PRED_ELEM]], %[[LHS_]], %[[RHS_]] : f32
 // CHECK-PRIMITIVE:        linalg.yield %[[RES]]
+
+// CHECK-NO-CAPTURE:      #[[SCALAR_MAP:.*]] = affine_map<(d0, d1) -> ()>
+// CHECK-NO-CAPTURE:      #[[ID_MAP:.*]] = affine_map<(d0, d1) -> (d0, d1)>
+// CHECK-NO-CAPTURE:      func @select_scalar_pred_dyn
+// CHECK-NO-CAPTURE-SAME:  (%[[PRED:.*]]: tensor<i1>, %[[LHS:.*]]: tensor<2x?xf32>, %[[RHS:.*]]: tensor<2x?xf32>)
+// CHECK-NO-CAPTURE-DAG:  %[[C1:.*]] = arith.constant 1
+// CHECK-NO-CAPTURE-DAG:  %[[DIM:.*]] =  tensor.dim %[[LHS]], %[[C1]]
+// CHECK-NO-CAPTURE-DAG:  %[[DST:.*]] = tensor.empty(%[[DIM]])
+// CHECK-NO-CAPTURE:      linalg.generic
+// CHECK-NO-CAPTURE-SAME:   indexing_maps = [#[[SCALAR_MAP]], #[[ID_MAP]], #[[ID_MAP]], #[[ID_MAP]]]
+// CHECK-NO-CAPTURE-SAME:   iterator_types = ["parallel", "parallel"]
+// CHECK-NO-CAPTURE-SAME:   ins(%[[PRED]], %[[LHS]], %[[RHS]] : tensor<i1>, tensor<2x?xf32>, tensor<2x?xf32>)
+// CHECK-NO-CAPTURE-SAME:   outs(%[[DST]] : tensor<2x?xf32>)
+// CHECK-NO-CAPTURE-SAME:   {someattr}
+// CHECK-NO-CAPTURE:      ^bb0(%[[PRED_:.*]]: i1, %[[LHS_:.*]]: f32, %[[RHS_:.*]]: f32, %{{.*}}: f32):
+// CHECK-NO-CAPTURE:        %[[RES:.*]] = arith.select %[[PRED_]], %[[LHS_]], %[[RHS_]] : f32
+// CHECK-NO-CAPTURE:        linalg.yield %[[RES]]
 
 // -----
 
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
@@ -140,12 +140,11 @@
   // (any sign-op, or an integral abs-op).
   // TODO(peiming, ajcbik): these all can potentially be optimized by applying
   // value transform on sparse_tenosr.value memref
-  if (isa<mlir::stablehlo::SignOp>(op) || isa<mlir::stablehlo::NegOp>(op) ||
+  if (isa<mlir::stablehlo::SignOp, mlir::stablehlo::NegOp,
+          mlir::stablehlo::TanOp>(op) ||
       (isa<mlir::stablehlo::AbsOp>(op) && hasIntegralShapeType(op)) ||
-      isa<chlo::AsinOp>(op) || isa<chlo::AsinhOp>(op) ||
-      isa<chlo::AtanOp>(op) || isa<chlo::AtanhOp>(op) ||
-      isa<chlo::BesselI1eOp>(op) || isa<chlo::SinhOp>(op) ||
-      isa<chlo::TanOp>(op)) {
+      isa<chlo::AsinOp, chlo::AsinhOp, chlo::AtanOp, chlo::AtanhOp,
+          chlo::BesselI1eOp, chlo::SinhOp, chlo::TanOp>(op)) {
     if (!sparse_tensor::getSparseTensorEncoding(op->getResult(0).getType()) &&
         !sparse_tensor::getSparseTensorEncoding(op->getOperand(0).getType()))
       return Value();
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h b/stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
--- stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
+++ stablehlo/stablehlo/conversions/linalg/transforms/MapStablehloToScalarOp.h
@@ -153,14 +153,11 @@
   using FOp = ::mlir::math::SinOp;
   using COp = ::mlir::complex::SinOp;
 };
-// FIXME(Jakub)
-/*
 template <>
 struct StablehloToScalarOp<stablehlo::TanOp> {
   using FOp = ::mlir::math::TanOp;
   using COp = ::mlir::complex::TanOp;
 };
-*/
 template <>
 struct StablehloToScalarOp<stablehlo::Atan2Op> {
   using FOp = ::mlir::math::Atan2Op;
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td b/stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
--- stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
+++ stablehlo/stablehlo/conversions/linalg/transforms/Passes.td
@@ -39,7 +39,11 @@
                  Option<"enableSparseOps", "enable-sparse-ops", "bool",
                         /*default=*/"false",
                         "Lower to Sparse Tensor ops (sparse_tensor.concatenate)"
-                        "when possible, instead of linalg.generic">];
+                        "when possible, instead of linalg.generic">,
+                 Option<"captureScalarInputs", "capture-scalar-inputs", "bool",
+                        /*default=*/"true",
+                        "Capture scalar inputs in generic ops instead of"
+                        "passing as tensor-scalar argument.">];
 }
 
 #endif  // STABLEHLO_TO_LINALG_PASSES
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h b/stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
--- stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
+++ stablehlo/stablehlo/conversions/linalg/transforms/Rewriters.h
@@ -26,11 +26,12 @@
 //===----------------------------------------------------------------------===//
 
 /// Populates the patterns that convert from StableHLO to Linalg on tensors.
-void populateStablehloToLinalgConversionPatterns(MLIRContext *context,
-                                                 TypeConverter &typeConverter,
-                                                 RewritePatternSet *patterns,
+void populateStablehloToLinalgConversionPatterns(MLIRContext* context,
+                                                 TypeConverter& typeConverter,
+                                                 RewritePatternSet* patterns,
                                                  bool enablePrimitiveOps,
-                                                 bool enableSparseOps);
+                                                 bool enableSparseOps,
+                                                 bool captureScalarInputs);
 
 //===----------------------------------------------------------------------===//
 // Fine-grained patterns used by the implementation.
@@ -39,8 +40,9 @@
 /// Populates the patterns that convert from elementwise StableHLO ops to Linalg
 /// on tensors.
 void populatePointwiseStablehloToLinalgConversionPatterns(
-    MLIRContext *context, TypeConverter &typeConverter,
-    RewritePatternSet *patterns, bool enablePrimitiveOps);
+    MLIRContext* context, TypeConverter& typeConverter,
+    RewritePatternSet* patterns, bool enablePrimitiveOps,
+    bool captureScalarInputs);
 
 /// Populates the patterns that convert from convolution StableHLO ops to Linalg
 /// on tensors.
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
@@ -2634,7 +2634,8 @@
 
     RewritePatternSet patterns_(context);
     populateStablehloToLinalgConversionPatterns(
-        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps);
+        context, converter, &patterns_, enablePrimitiveOps, enableSparseOps,
+        captureScalarInputs);
     patterns = std::move(patterns_);
 
     return success();
@@ -2657,7 +2658,8 @@
                                                  TypeConverter& typeConverter,
                                                  RewritePatternSet* patterns,
                                                  bool enablePrimitiveOps,
-                                                 bool enableSparseOps) {
+                                                 bool enableSparseOps,
+                                                 bool captureScalarInputs) {
   // clang-format off
   patterns->add<ConcatenateConverter>(typeConverter, context,
                                       enablePrimitiveOps);
@@ -2680,7 +2682,8 @@
       >(typeConverter, context);
 
   detail::populatePointwiseStablehloToLinalgConversionPatterns(
-      context, typeConverter, patterns, enablePrimitiveOps);
+      context, typeConverter, patterns, enablePrimitiveOps,
+      captureScalarInputs);
 
   if (enableSparseOps) {
     patterns->add<SparseConcatenateConverter>(typeConverter, context);
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToArith.cpp
@@ -145,6 +145,7 @@
       ScalarHloToArithmeticPattern<mlir::stablehlo::SineOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::SqrtOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::SubtractOp>,
+      ScalarHloToArithmeticPattern<mlir::stablehlo::TanOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::TanhOp>,
       ScalarHloToArithmeticPattern<mlir::stablehlo::XorOp>>(typeConverter,
                                                             context, filterFn);
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgPointwise.cpp
@@ -23,6 +23,7 @@
 
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/Debug.h"
 #include "mlir/Dialect/Linalg/IR/Linalg.h"
 #include "mlir/Dialect/Tensor/IR/Tensor.h"
 #include "mlir/IR/AffineMap.h"
@@ -43,6 +44,8 @@
 #include "stablehlo/conversions/linalg/transforms/Rewriters.h"
 #include "stablehlo/dialect/StablehloOps.h"
 
+#define DEBUG_TYPE "stablehlo-conversions"
+
 namespace mlir::stablehlo {
 namespace {
 int64_t getRank(Value v) { return cast<ShapedType>(v.getType()).getRank(); }
@@ -142,6 +145,11 @@
 struct PointwiseToLinalgMapConverter : OpConversionPattern<OpTy> {
   using OpConversionPattern<OpTy>::OpConversionPattern;
   using OpAdaptor = typename OpTy::Adaptor;
+
+  PointwiseToLinalgMapConverter(TypeConverter& typeConverter,
+                                MLIRContext* context, bool captureScalarInputs)
+      : OpConversionPattern<OpTy>(typeConverter, context),
+        captureScalarInputs(captureScalarInputs) {}
 
   virtual FailureOr<Operation *> createLinalgOp(
       OpTy &op, ConversionPatternRewriter &rewriter,
@@ -190,8 +198,11 @@
             rewriter, loc, cast<TypedValue<ShapedType>>(input),
             cast<ShapedType>(emptyTensor.getType())));
         scalarInputs.push_back(nullptr);
+      } else if (captureScalarInputs) {
+        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));
       } else {
-        scalarInputs.push_back(rewriter.create<tensor::ExtractOp>(loc, input));
+        mappedInputs.push_back(input);
+        scalarInputs.push_back(nullptr);
       }
     }
 
@@ -202,6 +213,8 @@
     rewriter.replaceOp(op, (*mapOp)->getResults());
     return success();
   }
+
+  bool captureScalarInputs;
 };
 
 /// Converts a HLO operation to a linalg.generic op that contains the
@@ -211,12 +224,12 @@
   using PointwiseToLinalgMapConverter<OpTy>::PointwiseToLinalgMapConverter;
   using OpAdaptor = typename OpTy::Adaptor;
 
-  FailureOr<Operation *> createLinalgOp(OpTy &op,
-                                        ConversionPatternRewriter &rewriter,
-                                        ArrayRef<Value> mappedInputs,
-                                        ArrayRef<Value> scalarVals,
-                                        Value emptyTensor,
-                                        int64_t maxRank) const override {
+  FailureOr<Operation*> createLinalgOp(OpTy& op,
+                                       ConversionPatternRewriter& rewriter,
+                                       ArrayRef<Value> mappedInputs,
+                                       ArrayRef<Value> scalarVals,
+                                       Value emptyTensor,
+                                       int64_t maxRank) const override {
     // Create indexing maps.
     AffineMap scalarMap = AffineMap::get(maxRank, 0, rewriter.getContext());
     AffineMap idMap = rewriter.getMultiDimIdentityMap(maxRank);
@@ -225,10 +238,10 @@
       maps.push_back(isScalar(v) ? scalarMap : idMap);
     maps.push_back(idMap);
     bool failed = false;
-    Operation *linalgOp = rewriter.create<linalg::GenericOp>(
+    Operation* linalgOp = rewriter.create<linalg::GenericOp>(
         op.getLoc(), emptyTensor.getType(), mappedInputs, emptyTensor, maps,
         getNParallelLoopsAttrs(maxRank),
-        [&](OpBuilder &nestedBuilder, Location /*nested_loc*/,
+        [&](OpBuilder& nestedBuilder, Location /*nested_loc*/,
             ValueRange args) {
           Type innerResultTy = getElementTypeOrSelf(emptyTensor);
           auto argvec =
@@ -253,8 +266,9 @@
 
 namespace detail {
 void populatePointwiseStablehloToLinalgConversionPatterns(
-    MLIRContext *context, TypeConverter &typeConverter,
-    RewritePatternSet *patterns, bool enablePrimitiveOps) {
+    MLIRContext* context, TypeConverter& typeConverter,
+    RewritePatternSet* patterns, bool enablePrimitiveOps,
+    bool captureScalarInputs) {
   if (enablePrimitiveOps) {
     patterns->add<
         PointwiseToLinalgMapConverter<mlir::stablehlo::AbsOp>,
@@ -301,12 +315,12 @@
         PointwiseToLinalgMapConverter<mlir::stablehlo::SineOp>,
         PointwiseToLinalgMapConverter<mlir::stablehlo::SqrtOp>,
         PointwiseToLinalgMapConverter<mlir::stablehlo::SubtractOp>,
+        PointwiseToLinalgMapConverter<mlir::stablehlo::TanOp>,
         PointwiseToLinalgMapConverter<mlir::stablehlo::TanhOp>,
-        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(typeConverter,
-                                                               context);
+        PointwiseToLinalgMapConverter<mlir::stablehlo::XorOp>>(
+        typeConverter, context, captureScalarInputs);
     return;
   }
-
   patterns
       ->add<PointwiseToLinalgConverter<mlir::stablehlo::AbsOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::AddOp>,
@@ -352,9 +366,10 @@
             PointwiseToLinalgConverter<mlir::stablehlo::SineOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::SqrtOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::SubtractOp>,
+            PointwiseToLinalgConverter<mlir::stablehlo::TanOp>,
             PointwiseToLinalgConverter<mlir::stablehlo::TanhOp>,
-            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(typeConverter,
-                                                                context);
+            PointwiseToLinalgConverter<mlir::stablehlo::XorOp>>(
+          typeConverter, context, captureScalarInputs);
 }
 }  // namespace detail
 }  // namespace mlir::stablehlo
diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir b/stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/legalize_quant_ops_to_tosa_rescale.mlir
@@ -11,10 +11,10 @@
   // CHECK-DAG: %[[MULTIPLIER_2:.+]] = "tosa.const"() <{values = dense<1431655765> : tensor<1xi32>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.add %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-1>>
   %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-1>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-1>>
@@ -32,10 +32,10 @@
   // CHECK-DAG: %[[MULTIPLIER_2:.+]] = "tosa.const"() <{values = dense<1431655765> : tensor<1xi32>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT13]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT11]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.subtract %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT50]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-1>>
   %0 = "stablehlo.subtract"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-1>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-1>>
@@ -52,10 +52,10 @@
   // CHECK-DAG: %[[MULTIPLIER_2:.+]] = "tosa.const"() <{values = dense<1717986918> : tensor<1xi32>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.multiply %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_1]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-1>>
   %0 = "stablehlo.multiply"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-1>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-1>>
@@ -74,10 +74,10 @@
   // CHECK-DAG: %[[ZP_MINUS_2:.+]] = "tosa.const"() <{values = dense<-2> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT30]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.divide %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_2]], %[[SHIFT37]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-3>>
   %0 = "stablehlo.divide"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-2>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-3>>
@@ -97,10 +97,10 @@
   // CHECK-DAG: %[[SHIFT12:.+]] = "tosa.const"() <{values = dense<12> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.maximum %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-3>>
   %0 = "stablehlo.maximum"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-2>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-3>>
@@ -120,10 +120,10 @@
   // CHECK-DAG: %[[SHIFT12:.+]] = "tosa.const"() <{values = dense<12> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK-DAG: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK-DAG: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.minimum %[[V0]], %[[V1]] : tensor<2x2xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V2]], %[[MULTIPLIER_1]], %[[SHIFT51]], %[[ZP_0]], %[[ZP_MINUS_3]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<2x2x!quant.uniform<i8:f32, 1.500000e-01:-3>>
   %0 = "stablehlo.minimum"(%arg0, %arg1) : (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<2x2x!quant.uniform<i8:f32, 0.075:-2>>)
             -> tensor<2x2x!quant.uniform<i8:f32, 1.5e-01:-3>>
@@ -140,9 +140,9 @@
   // CHECK-DAG: %[[SHIFT30:.+]] = "tosa.const"() <{values = dense<30> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT30]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V1:.+]] = stablehlo.abs %[[V0]] : tensor<20x20xi32>
-  // CHECK: %[[V3:.+]] = tosa.rescale %[[V1]], %[[MULTIPLIER_1]], %[[SHIFT33]], %[[ZP_0]], %[[ZP_MINUS_128]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V3:.+]] = tosa.rescale %[[V1]], %[[MULTIPLIER_1]], %[[SHIFT33]], %[[ZP_0]], %[[ZP_MINUS_128]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: return %[[V3]] : tensor<20x20x!quant.uniform<i8:f32, 1.500000e-01:-128>>
   %0 = "stablehlo.abs"(%arg0) : (tensor<20x20x!quant.uniform<i8:f32, 0.025:-1>>) -> tensor<20x20x!quant.uniform<i8:f32, 1.5e-01:-128>>
   return %0 : tensor<20x20x!quant.uniform<i8:f32, 1.5e-01:-128>>
@@ -159,8 +159,8 @@
   // CHECK-DAG: %[[SHIFT12:.+]] = "tosa.const"() <{values = dense<12> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_MINUS_1:.+]] = "tosa.const"() <{values = dense<-1> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT12]], %[[ZP_MINUS_1]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT10]], %[[ZP_MINUS_2]], %[[ZP_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.compare GE, %[[V0]], %[[V1]], TOTALORDER :
   // CHECK: return %[[V2]]
   %0 = stablehlo.compare GE, %arg0, %arg1, TOTALORDER : (tensor<20x20x!quant.uniform<i8:f32, 0.025:-1>>, tensor<20x20x!quant.uniform<i8:f32, 0.075:-2>>) -> tensor<20x20xi1>
@@ -177,8 +177,8 @@
   // CHECK-DAG: %[[SHIFT15:.+]] = "tosa.const"() <{values = dense<15> : tensor<1xi8>}>
   // CHECK-DAG: %[[ZP16_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi16>}>
   // CHECK-DAG: %[[ZP32_0:.+]] = "tosa.const"() <{values = dense<0> : tensor<1xi32>}>
-  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT17]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
-  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT15]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true}
+  // CHECK: %[[V0:.+]] = tosa.rescale %arg0, %[[MULTIPLIER_2]], %[[SHIFT17]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
+  // CHECK: %[[V1:.+]] = tosa.rescale %arg1, %[[MULTIPLIER_1]], %[[SHIFT15]], %[[ZP16_0]], %[[ZP32_0]] {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true}
   // CHECK: %[[V2:.+]] = stablehlo.compare LT, %[[V0]], %[[V1]], TOTALORDER :
   // CHECK: return %[[V2]]
   %0 = stablehlo.compare LT, %arg0, %arg1, TOTALORDER : (tensor<20x20x!quant.uniform<i16:f32, 0.025:0>>, tensor<20x20x!quant.uniform<i16:f32, 0.075:0>>) -> tensor<20x20xi1>
diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir b/stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/legalize_tosa_rescale_to_stablehlo.mlir
@@ -7,7 +7,7 @@
   %shift = "tosa.const"() {values = dense<13> : tensor<1xi8>} : () -> tensor<1xi8>
   %input_zp = "tosa.const"() {values = dense<-1> : tensor<1xi8>} : () -> tensor<1xi8>
   %output_zp = "tosa.const"() {values = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
-  %0 = tosa.rescale %arg0, %multiplier, %shift, %input_zp, %output_zp {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = "SINGLE_ROUND", scale32 = true} :
+  %0 = tosa.rescale %arg0, %multiplier, %shift, %input_zp, %output_zp {input_unsigned = false, output_unsigned = false, per_channel = false, rounding_mode = SINGLE_ROUND, scale32 = true} :
             (tensor<2x2x!quant.uniform<i8:f32, 0.025:-1>>, tensor<1xi32>, tensor<1xi8>, tensor<1xi8>, tensor<1xi32>) -> tensor<2x2xi32>
 
   // convert input quantized type to storage type
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloQuantLegalizeToTosaRescale.cpp
@@ -70,12 +70,14 @@
       outputZpVal.has_value() &&
       "buildRescale: Failed to create output zero-point tensor for RescaleOp.");
 
-  std::string roundingMode = doubleRound ? "DOUBLE_ROUND" : "SINGLE_ROUND";
+  auto roundingMode =
+      doubleRound ? RoundingMode::DOUBLE_ROUND : RoundingMode::SINGLE_ROUND;
 
   auto rescale_op = rewriter.create<RescaleOp>(
       loc, outputType, inputVal, multiplierVal, shiftVal, inputZpVal.value(),
       outputZpVal.value(), rewriter.getBoolAttr(scale32),
-      rewriter.getStringAttr(roundingMode), rewriter.getBoolAttr(perChannel),
+      RoundingModeAttr::get(rewriter.getContext(), roundingMode),
+      rewriter.getBoolAttr(perChannel),
       /*input_unsigned=*/rewriter.getBoolAttr(false),
       /*output_unsigned=*/rewriter.getBoolAttr(false));
 
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/TosaRescaleLegalizeToStablehlo.cpp
@@ -68,7 +68,7 @@
   auto roundingMode = op.getRoundingMode();
   bool perChannel = op.getPerChannel();
 
-  if (perChannel || roundingMode != "SINGLE_ROUND" || !scale32) {
+  if (perChannel || roundingMode != RoundingMode::SINGLE_ROUND || !scale32) {
     return rewriter.notifyMatchFailure(
         op,
         "per_channel, double_round, or scale32=false are not yet supported");
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/AttrTypeBuilderUtilTest.cpp
@@ -20,7 +20,7 @@
 #include <utility>
 #include <vector>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "llvm/ADT/DenseMap.h"
 #include "mlir/IR/BuiltinTypeInterfaces.h"
 #include "mlir/IR/BuiltinTypes.h"
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTblgen.cpp
@@ -50,7 +50,6 @@
 using mlir::tblgen::MethodBody;
 using mlir::tblgen::MethodParameter;
 using mlir::tblgen::NamedAttribute;
-using mlir::tblgen::NamedRegion;
 using mlir::tblgen::Operator;
 
 namespace mlir {
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/MlirBuilderTest.cpp
@@ -15,7 +15,7 @@
 
 #include <string>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "llvm/Support/raw_ostream.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
 #include "mlir/IR/BuiltinOps.h"
diff --ruN a/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp b/stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
--- stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
+++ stablehlo/stablehlo/integrations/cpp/builder/StablehloBuilderTest.cpp
@@ -17,7 +17,7 @@
 #include <cstdint>
 #include <string>
 
-#include "gtest/gtest.h"
+#include "testing/base/public/gunit.h"
 #include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/DialectRegistry.h"
diff --ruN a/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir b/stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir
--- stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir
+++ stablehlo/stablehlo/tests/chlo/chlo_legalize_to_stablehlo_broadcast.mlir
@@ -11,6 +11,30 @@
 }
 
 // -----
+
+// CHECK-LABEL: @addStaticBroadcastExpanding
+func.func @addStaticBroadcastExpanding(%arg0: tensor<4xf32>, %arg1: tensor<f32>) -> tensor<4xf32> {
+  // CHECK:      %[[BROADCAST:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<4xf32>
+  // CHECK-NEXT: stablehlo.add %arg0, %[[BROADCAST]]
+  // CHECK-NOT: shape
+  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<4xf32>, tensor<f32>) -> tensor<4xf32>
+  func.return %0 : tensor<4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: @addStaticBroadcastSameRank
+func.func @addStaticBroadcastSameRank(%arg0: tensor<1x4xf32>, %arg1: tensor<4x1xf32>) -> tensor<4x4xf32> {
+  // CHECK:      %[[ARG0_B:.+]] = stablehlo.broadcast_in_dim %arg0, dims = [0, 1] : (tensor<1x4xf32>) -> tensor<4x4xf32>
+  // CHECK-NEXT: %[[ARG1_B:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [0, 1] : (tensor<4x1xf32>) -> tensor<4x4xf32>
+  // CHECK-NEXT: stablehlo.add %[[ARG0_B]], %[[ARG1_B]] : tensor<4x4xf32>
+  // CHECK-NOT: shape
+  %0 = chlo.broadcast_add %arg0, %arg1 : (tensor<1x4xf32>, tensor<4x1xf32>) -> tensor<4x4xf32>
+  func.return %0 : tensor<4x4xf32>
+}
+
+// -----
+
 
 // CHECK-LABEL: @dynamicBroadcast
 // CHECK-SAME: %[[ARG0:.+]]: tensor<?xf32>
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -529,28 +529,15 @@
 // IotaOp
 
 // CHECK-LABEL: func @eval_iota
-func.func @eval_iota() -> (tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {
-  // CHECK-NOT: stablehlo.iota
-  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<
-  // CHECK-SAME: {{\[\[}}[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]],
-  // CHECK-SAME: {{\[}}[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]],
-  // CHECK-SAME: {{\[}}[2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2], [2, 2, 2, 2, 2]]]> : tensor<3x4x5xi32>
-
-  // CHECK: [[RESULT1:%.*]] = stablehlo.constant dense<
-  // CHECK-SAME: {{\[\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
-  // CHECK-SAME: {{\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]],
-  // CHECK-SAME: {{\[}}[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]> : tensor<3x4x5xi32>
-
-  // CHECK: [[RESULT2:%.*]] = stablehlo.constant dense<
-  // CHECK-SAME: {{\[\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],
-  // CHECK-SAME: {{\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]],
-  // CHECk-SAME: {{\[}}[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]]> : tensor<3x4x5xi32>
-
+func.func @eval_iota() -> (tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>) {
+  // CHECK:      [[RESULT0:%.*]] = stablehlo.constant dense<0> : tensor<1xi32>
+  // CHECK-NEXT: [[RESULT1:%.*]] = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
+  // CHECK-NEXT: [[RESULT2:%.*]] = stablehlo.iota dim = 2 : tensor<3x4x5xi32>
   // CHECK: return [[RESULT0]], [[RESULT1]], [[RESULT2]]
-  %0 = stablehlo.iota dim = 0 : tensor<3x4x5xi32>
+  %0 = stablehlo.iota dim = 0 : tensor<1xi32>
   %1 = stablehlo.iota dim = 1 : tensor<3x4x5xi32>
   %2 = stablehlo.iota dim = 2 : tensor<3x4x5xi32>
-  func.return %0, %1, %2 : tensor<3x4x5xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>
+  func.return %0, %1, %2 : tensor<1xi32>, tensor<3x4x5xi32>, tensor<3x4x5xi32>
 }
 
 // -----
@@ -596,6 +583,37 @@
   // CHECK-DAG:  [[CST2:%.+]] = stablehlo.constant dense<{{\[\[1, 2\], \[3, 4\]\]}}> : tensor<2x2xi32>
   // CHECK-NEXT: return [[CST1]], [[CST2]]
   return %0, %1 : tensor<1xi32>, tensor<2x2xi32>
+}
+
+// -----
+
+////////
+// SliceOp / DynamicSliceOp
+
+// CHECK-LABEL: @slice_fold
+func.func @slice_fold(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {
+  %c = stablehlo.constant dense<[[0], [1], [2], [3], [4], [5]]> : tensor<6x1xi32>
+  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>
+  // CHECK: stablehlo.constant dense<2> : tensor<1x1xi32>
+  return %0 : tensor<1x1xi32>
+}
+
+// CHECK-LABEL: @slice_fold_splat
+func.func @slice_fold_splat(%arg0: tensor<6x1xi32>) -> tensor<1x1xi32> {
+  %c = stablehlo.constant dense<1> : tensor<6x1xi32>
+  %0 = stablehlo.slice %c [2:3, 0:1] : (tensor<6x1xi32>) -> tensor<1x1xi32>
+  // CHECK: stablehlo.constant dense<1> : tensor<1x1xi32>
+  return %0 : tensor<1x1xi32>
+}
+
+// CHECK-LABEL: @dynamic_slice_fold
+func.func @dynamic_slice_fold(%arg0: tensor<i32>, %arg1: tensor<i32>) -> tensor<1x1xi32> {
+  %0 = stablehlo.constant dense<256> : tensor<6x1xi32>
+  %1 = "stablehlo.dynamic_slice"(%0, %arg0, %arg1) <{slice_sizes = array<i64: 1, 1>}> : (tensor<6x1xi32>, tensor<i32>, tensor<i32>) -> tensor<1x1xi32>
+
+  // CHECK: %[[RESULT:.*]] = stablehlo.constant dense<256> : tensor<1x1xi32>
+  // CHECK: return %[[RESULT]]
+  return %1 : tensor<1x1xi32>
 }
 
 // -----
@@ -712,18 +730,412 @@
 // -----
 
 ////////
+// AbsOp
+
+// CHECK-LABEL: func @fold_abs
+func.func @fold_abs() -> (tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[INT_ZERO:%.*]] = stablehlo.constant dense<0> : tensor<i32>
+  // CHECK-DAG: [[INT_TEN:%.*]] = stablehlo.constant dense<10> : tensor<i32>
+  // CHECK-DAG: [[FLOAT_ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[FLOAT_HALF:%.*]] = stablehlo.constant dense<5.0{{.*}}e-01> : tensor<f32>
+  // CHECK-DAG: [[FLOAT_INF:%.*]] = stablehlo.constant dense<0x7F800000> : tensor<f32>
+  // CHECK:     return [[INT_ZERO]], [[INT_TEN]], [[INT_TEN]], [[FLOAT_ZERO]], [[FLOAT_HALF]], [[FLOAT_HALF]], [[FLOAT_INF]], [[FLOAT_INF]]
+
+  %int_zero = stablehlo.constant dense<0> : tensor<i32>
+  %int_neg_ten = stablehlo.constant dense<-10> : tensor<i32>
+  %int_pos_ten = stablehlo.constant dense<10> : tensor<i32>
+
+  %float_zero = stablehlo.constant dense<0.0> : tensor<f32>
+  %float_neg_half = stablehlo.constant dense<-0.5> : tensor<f32>
+  %float_pos_half = stablehlo.constant dense<0.5> : tensor<f32>
+  %float_neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32> // -inf
+  %float_pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32> // +inf
+
+  %0 = stablehlo.abs %int_zero : tensor<i32>
+  %1 = stablehlo.abs %int_neg_ten : tensor<i32>
+  %2 = stablehlo.abs %int_pos_ten : tensor<i32>
+
+  %3 = stablehlo.abs %float_zero : tensor<f32>
+  %4 = stablehlo.abs %float_neg_half : tensor<f32>
+  %5 = stablehlo.abs %float_pos_half : tensor<f32>
+  %6 = stablehlo.abs %float_neg_inf : tensor<f32>
+  %7 = stablehlo.abs %float_pos_inf : tensor<f32>
+
+  func.return %0, %1, %2, %3, %4, %5, %6, %7 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// CosineOp
+
+// CHECK-LABEL: func @fold_cosine
+func.func @fold_cosine() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<{{1\.0000.*}}> : tensor<f32>
+  // CHECK-DAG: [[SQRT_THREE_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\.8660.*|8\.660.*[Ee]-01}}> : tensor<f32>
+  // CHECK-DAG: [[SQRT_TWO_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\.7071.*|7\.071.*[Ee]-01}}> : tensor<f32>
+  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<{{0\.5000.*|5\.000.*[Ee]-01|0.4999.*|4\.999.*[Ee]-01}}> : tensor<f32>
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<{{-?(0\.0000.*|[0-9]\.[0-9]*[Ee]-(0?[5-9]|[1-9][0-9]))}}> : tensor<f32>
+  // CHECK:     return [[ONE]], [[SQRT_THREE_OVER_TWO]], [[SQRT_TWO_OVER_TWO]], [[HALF]], [[ZERO]]
+
+  %0 = stablehlo.constant dense<0.0> : tensor<f32>
+  %1 = stablehlo.constant dense<0.5235987755982989> : tensor<f32> // pi/6
+  %2 = stablehlo.constant dense<0.7853981633974483> : tensor<f32> // pi/4
+  %3 = stablehlo.constant dense<1.0471975511965977> : tensor<f32> // pi/3
+  %4 = stablehlo.constant dense<1.5707963267948966> : tensor<f32> // pi/2
+
+  %5 = stablehlo.cosine %0 : tensor<f32>
+  %6 = stablehlo.cosine %1 : tensor<f32>
+  %7 = stablehlo.cosine %2 : tensor<f32>
+  %8 = stablehlo.cosine %3 : tensor<f32>
+  %9 = stablehlo.cosine %4 : tensor<f32>
+
+  func.return %5, %6, %7, %8, %9 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// ErfOp
+
+// CHECK-LABEL: func @fold_erf
+func.func @fold_erf() -> (tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[RESULT0:%.*]] = stablehlo.constant dense<-0.52049{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[RESULT1:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[RESULT2:%.*]] = stablehlo.constant dense<0.90000{{.*}}> : tensor<f32>
+  // CHECK:     return [[RESULT0]], [[RESULT1]], [[RESULT2]]
+
+  %0 = stablehlo.constant dense<-0.5> : tensor<f32>
+  %1 = stablehlo.constant dense<0.0> : tensor<f32>
+  %2 = stablehlo.constant dense<1.1631> : tensor<f32>
+
+  %3 = chlo.erf %0 : tensor<f32> -> tensor<f32>
+  %4 = chlo.erf %1 : tensor<f32> -> tensor<f32>
+  %5 = chlo.erf %2 : tensor<f32> -> tensor<f32>
+
+  func.return %3, %4, %5 : tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// ExpOp
+
+// CHECK-LABEL: func @fold_exponential
+func.func @fold_exponential() -> (tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[E:%.*]] = stablehlo.constant dense<2.718{{.*}}> : tensor<f32>
+  // CHECK:     return [[ONE]], [[E]]
+
+  %0 = stablehlo.constant dense<0.0> : tensor<f32>
+  %1 = stablehlo.constant dense<1.0> : tensor<f32>
+
+  %2 = stablehlo.exponential %0 : tensor<f32>
+  %3 = stablehlo.exponential %1 : tensor<f32>
+
+  func.return %2, %3 : tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// LogOp
+
+// CHECK-LABEL: func @fold_log
+func.func @fold_log() -> (tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<{{1\.0.*|0\.999.*}}> : tensor<f32>
+  // CHECK-DAG: [[DO_NOT_FOLD_LOG_ZERO:%.*]] = stablehlo.log [[ZERO]] : tensor<f32>
+  // CHECK:     return [[ZERO]], [[ONE]], [[DO_NOT_FOLD_LOG_ZERO]]
+
+  %0 = stablehlo.constant dense<1.0> : tensor<f32>
+  %1 = stablehlo.constant dense<2.718281828459045> : tensor<f32>
+  %2 = stablehlo.constant dense<0.0> : tensor<f32>
+
+  %3 = stablehlo.log %0 : tensor<f32>
+  %4 = stablehlo.log %1 : tensor<f32>
+  %5 = stablehlo.log %2 : tensor<f32>
+
+  func.return %3, %4, %5 : tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// LogisticOp
+
+// CHECK-LABEL: func @fold_logistic
+func.func @fold_logistic() -> (tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<5.0{{.*}}e-01> : tensor<f32>
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  // CHECK:     return [[ZERO]], [[HALF]], [[ONE]]
+
+  %neg_inf = stablehlo.constant dense<0xFF800000> : tensor<f32>
+  %zero = stablehlo.constant dense<0.0> : tensor<f32>
+  %pos_inf = stablehlo.constant dense<0x7F800000> : tensor<f32>
+
+  %0 = stablehlo.logistic %neg_inf : tensor<f32>
+  %1 = stablehlo.logistic %zero : tensor<f32>
+  %2 = stablehlo.logistic %pos_inf : tensor<f32>
+
+  func.return %0, %1, %2 : tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// NegOp
+
+// CHECK-LABEL: func @fold_negate
+func.func @fold_negate() -> (tensor<i32>, tensor<i32>, tensor<f32>) {
+  // CHECK-DAG: [[RESULT0:%.*]] = stablehlo.constant dense<-4> : tensor<i32>
+  // CHECK-DAG: [[RESULT1:%.*]] = stablehlo.constant dense<0> : tensor<i32>
+  // CHECK-DAG: [[RESULT2:%.*]] = stablehlo.constant dense<9.999{{.*}}e+02> : tensor<f32>
+  // CHECK:     return [[RESULT0]], [[RESULT1]], [[RESULT2]]
+
+  %0 = stablehlo.constant dense<4> : tensor<i32>
+  %1 = stablehlo.constant dense<0> : tensor<i32>
+  %2 = stablehlo.constant dense<-999.9> : tensor<f32>
+
+  %3 = stablehlo.negate %0 : tensor<i32>
+  %4 = stablehlo.negate %1 : tensor<i32>
+  %5 = stablehlo.negate %2 : tensor<f32>
+
+  func.return %3, %4, %5 : tensor<i32>, tensor<i32>, tensor<f32>
+}
+
+// -----
+
+////////
+// NotOp
+
+// CHECK-LABEL: func @fold_not
+func.func @fold_not() -> (tensor<i32>, tensor<i32>) {
+  // CHECK-DAG: [[RESULT0:%.*]] = stablehlo.constant dense<42> : tensor<i32>
+  // CHECK-DAG: [[RESULT1:%.*]] = stablehlo.constant dense<-1> : tensor<i32>
+  // CHECK:     return [[RESULT0]], [[RESULT1]]
+
+  %0 = stablehlo.constant dense<-43> : tensor<i32>
+  %1 = stablehlo.constant dense<0> : tensor<i32>
+
+  %2 = stablehlo.not %0 : tensor<i32>
+  %3 = stablehlo.not %1 : tensor<i32>
+
+  func.return %2, %3 : tensor<i32>, tensor<i32>
+}
+
+// -----
+
+////////
+// RoundOp
+
+// CHECK-LABEL: func @fold_round_nearest_afz
+func.func @fold_round_nearest_afz() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[NEG_THREE:%.*]] = stablehlo.constant dense<-3.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[NEG_TWO:%.*]] = stablehlo.constant dense<-2.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[TWO:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[THREE:%.*]] = stablehlo.constant dense<3.0{{.*}}> : tensor<f32>
+  // CHECK:     return [[NEG_THREE]], [[NEG_TWO]], [[ZERO]], [[ONE]], [[ONE]], [[TWO]], [[THREE]]
+
+  %0 = stablehlo.constant dense<-2.5> : tensor<f32>
+  %1 = stablehlo.constant dense<-1.5> : tensor<f32>
+  %2 = stablehlo.constant dense<0.4> : tensor<f32>
+  %3 = stablehlo.constant dense<0.5> : tensor<f32>
+  %4 = stablehlo.constant dense<0.6> : tensor<f32>
+  %5 = stablehlo.constant dense<1.5> : tensor<f32>
+  %6 = stablehlo.constant dense<2.5> : tensor<f32>
+
+  %7 = stablehlo.round_nearest_afz %0 : tensor<f32>
+  %8 = stablehlo.round_nearest_afz %1 : tensor<f32>
+  %9 = stablehlo.round_nearest_afz %2 : tensor<f32>
+  %10 = stablehlo.round_nearest_afz %3 : tensor<f32>
+  %11 = stablehlo.round_nearest_afz %4 : tensor<f32>
+  %12 = stablehlo.round_nearest_afz %5 : tensor<f32>
+  %13 = stablehlo.round_nearest_afz %6 : tensor<f32>
+
+  func.return %7, %8, %9, %10, %11, %12, %13 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// RoundNearestEvenOp
+
+// CHECK-LABEL: func @fold_round_nearest_even
+func.func @fold_round_nearest_even() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[NEG_TWO:%.*]] = stablehlo.constant dense<-2.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[TWO:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>
+  // CHECK:     return [[NEG_TWO]], [[NEG_TWO]], [[ZERO]], [[ZERO]], [[ONE]], [[TWO]], [[TWO]]
+
+  %0 = stablehlo.constant dense<-2.5> : tensor<f32>
+  %1 = stablehlo.constant dense<-1.5> : tensor<f32>
+  %2 = stablehlo.constant dense<0.4> : tensor<f32>
+  %3 = stablehlo.constant dense<0.5> : tensor<f32>
+  %4 = stablehlo.constant dense<0.6> : tensor<f32>
+  %5 = stablehlo.constant dense<1.5> : tensor<f32>
+  %6 = stablehlo.constant dense<2.5> : tensor<f32>
+
+  %7 = stablehlo.round_nearest_even %0 : tensor<f32>
+  %8 = stablehlo.round_nearest_even %1 : tensor<f32>
+  %9 = stablehlo.round_nearest_even %2 : tensor<f32>
+  %10 = stablehlo.round_nearest_even %3 : tensor<f32>
+  %11 = stablehlo.round_nearest_even %4 : tensor<f32>
+  %12 = stablehlo.round_nearest_even %5 : tensor<f32>
+  %13 = stablehlo.round_nearest_even %6 : tensor<f32>
+
+  func.return %7, %8, %9, %10, %11, %12, %13 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// RsqrtOp
+
+// CHECK-LABEL: func @fold_rsqrt
+func.func @fold_rsqrt() -> (tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<5.0{{.*}}e-01> : tensor<f32>
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[DO_NOT_FOLD_RSQRT_ZERO:%.*]] = stablehlo.rsqrt [[ZERO]] : tensor<f32>
+  // CHECK:     return [[HALF]], [[DO_NOT_FOLD_RSQRT_ZERO]]
+
+  %0 = stablehlo.constant dense<4.0> : tensor<f32>
+  %1 = stablehlo.constant dense<0.0> : tensor<f32>
+
+  %2 = stablehlo.rsqrt %0 : tensor<f32>
+  %3 = stablehlo.rsqrt %1 : tensor<f32>
+
+  func.return %2, %3 : tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// SignOp
+
+// CHECK-LABEL: func @fold_sign
+func.func @fold_sign() -> (tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[INT_NEG_ONE:%.*]] = stablehlo.constant dense<-1> : tensor<i32>
+  // CHECK-DAG: [[INT_ZERO:%.*]] = stablehlo.constant dense<0> : tensor<i32>
+  // CHECK-DAG: [[INT_POS_ONE:%.*]] = stablehlo.constant dense<1> : tensor<i32>
+  // CHECK-DAG: [[FLOAT_NEG_ONE:%.*]] = stablehlo.constant dense<-1.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[FLOAT_ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[FLOAT_POS_ONE:%.*]] = stablehlo.constant dense<1.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[FLOAT_NAN:%.*]] = stablehlo.constant dense<0x7FC00000> : tensor<f32>
+  // CHECK-DAG: [[DO_NOT_FOLD_SIGN_NAN:%.*]] = stablehlo.sign [[FLOAT_NAN]] : tensor<f32>
+  // CHECK:     return [[INT_NEG_ONE]], [[INT_ZERO]], [[INT_POS_ONE]], [[FLOAT_NEG_ONE]], [[FLOAT_ZERO]], [[FLOAT_POS_ONE]], [[DO_NOT_FOLD_SIGN_NAN]]
+
+  %0 = stablehlo.constant dense<-7> : tensor<i32>
+  %1 = stablehlo.constant dense<0> : tensor<i32>
+  %2 = stablehlo.constant dense<25> : tensor<i32>
+  %3 = stablehlo.constant dense<-2.5> : tensor<f32>
+  %4 = stablehlo.constant dense<0.0> : tensor<f32>
+  %5 = stablehlo.constant dense<0.1> : tensor<f32>
+  %6 = stablehlo.constant dense<0x7FC00000> : tensor<f32> // NaN
+
+  %7 = stablehlo.sign %0 : tensor<i32>
+  %8 = stablehlo.sign %1 : tensor<i32>
+  %9 = stablehlo.sign %2 : tensor<i32>
+  %10 = stablehlo.sign %3 : tensor<f32>
+  %11 = stablehlo.sign %4 : tensor<f32>
+  %12 = stablehlo.sign %5 : tensor<f32>
+  %13 = stablehlo.sign %6 : tensor<f32>
+
+  func.return %7, %8, %9, %10, %11, %12, %13 : tensor<i32>, tensor<i32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// SineOp
+
+// CHECK-LABEL: func @fold_sine
+func.func @fold_sine() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<{{0\.0000.*}}> : tensor<f32>
+  // CHECK-DAG: [[HALF:%.*]] = stablehlo.constant dense<{{0\.5000.*|5\.000.*[Ee]-01|0.4999.*|4\.999.*[Ee]-01}}> : tensor<f32>
+  // CHECK-DAG: [[SQRT_TWO_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\.7071.*|7\.071.*[Ee]-01}}> : tensor<f32>
+  // CHECK-DAG: [[SQRT_THREE_OVER_TWO:%.*]] = stablehlo.constant dense<{{0\.8660.*|8\.660.*[Ee]-01}}> : tensor<f32>
+  // CHECK-DAG: [[ONE:%.*]] = stablehlo.constant dense<{{1\.0000.*|0\.9999.*|9\.999.*[Ee]-01}}> : tensor<f32>
+  // CHECK:     return [[ZERO]], [[HALF]], [[SQRT_TWO_OVER_TWO]], [[SQRT_THREE_OVER_TWO]], [[ONE]]
+
+  %0 = stablehlo.constant dense<0.0> : tensor<f32>
+  %1 = stablehlo.constant dense<0.5235987755982989> : tensor<f32> // pi/6
+  %2 = stablehlo.constant dense<0.7853981633974483> : tensor<f32> // pi/4
+  %3 = stablehlo.constant dense<1.0471975511965977> : tensor<f32> // pi/3
+  %4 = stablehlo.constant dense<1.5707963267948966> : tensor<f32> // pi/2
+
+  %5 = stablehlo.sine %0 : tensor<f32>
+  %6 = stablehlo.sine %1 : tensor<f32>
+  %7 = stablehlo.sine %2 : tensor<f32>
+  %8 = stablehlo.sine %3 : tensor<f32>
+  %9 = stablehlo.sine %4 : tensor<f32>
+
+  func.return %5, %6, %7, %8, %9 : tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
 // SqrtOp
 
 // CHECK-LABEL: func @fold_sqrt
-func.func @fold_sqrt() -> (tensor<f32>) {
-  // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>
-  // CHECK: return [[RESULT0]]
+func.func @fold_sqrt() -> (tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[TWO:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[NEG_ONE:%.*]] = stablehlo.constant dense<-1.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[DO_NOT_FOLD_FLOAT_SQRT_NEG_ONE:%.*]] = stablehlo.sqrt [[NEG_ONE]] : tensor<f32>
+  // CHECK:     return [[TWO]], [[DO_NOT_FOLD_FLOAT_SQRT_NEG_ONE]]
+
   %0 = stablehlo.constant dense<4.0> : tensor<f32>
-  %1 = stablehlo.sqrt %0 : tensor<f32>
-  func.return %1 : tensor<f32>
-}
-
-//
+  %1 = stablehlo.constant dense<-1.0> : tensor<f32>
+
+  %2 = stablehlo.sqrt %0 : tensor<f32>
+  %3 = stablehlo.sqrt %1 : tensor<f32>
+
+  func.return %2, %3 : tensor<f32>, tensor<f32>
+}
+
+// -----
+
+////////
+// TanOp
+
+// CHECK-LABEL: func @fold_tan
+func.func @fold_tan() -> (tensor<f32>) {
+  // CHECK: [[ONE:%.*]] = stablehlo.constant dense<{{1\.0.*|0\.999.*}}> : tensor<f32>
+  // CHECK: return [[ONE]]
+  %pi_over_4 = stablehlo.constant dense<0.7853981633974483> : tensor<f32>
+  %result = stablehlo.tan %pi_over_4 : tensor<f32>
+  func.return %result : tensor<f32>
+}
+
+// -----
+
+////////
+// TanhOp
+
+// CHECK-LABEL: func @fold_tanh
+func.func @fold_tanh() -> (tensor<f32>, tensor<f32>, tensor<f32>) {
+  // CHECK-DAG: [[NEG_SQRT_ONE_FIFTH:%.*]] = stablehlo.constant dense<-0.44721{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[ZERO:%.*]] = stablehlo.constant dense<0.0{{.*}}> : tensor<f32>
+  // CHECK-DAG: [[SQRT_ONE_FIFTH:%.*]] = stablehlo.constant dense<0.44721{{.*}}> : tensor<f32>
+  // CHECK:     return [[NEG_SQRT_ONE_FIFTH]], [[ZERO]], [[SQRT_ONE_FIFTH]]
+
+  %neg_log_phi = stablehlo.constant dense<-0.4812118250596034> : tensor<f32>
+  %zero = stablehlo.constant dense<0.0> : tensor<f32>
+  %log_phi = stablehlo.constant dense<0.4812118250596034> : tensor<f32>
+
+  %tanh_neg_log_phi = stablehlo.tanh %neg_log_phi : tensor<f32>
+  %tanh_zero = stablehlo.tanh %zero : tensor<f32>
+  %tanh_log_phi = stablehlo.tanh %log_phi : tensor<f32>
+
+  func.return %tanh_neg_log_phi, %tanh_zero, %tanh_log_phi : tensor<f32>, tensor<f32>, tensor<f32>
+}
+
+// -----
 
 ////////
 // SetDimensionSizeOp
@@ -748,6 +1160,19 @@
   // CHECK-NEXT: return [[RESULT0]]
   %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<10>>
   return %0 : tensor<?xf32, #stablehlo.bounds<10>>
+}
+
+// -----
+
+// Don't fold when washing away a bounded dimension, not safe to replace with
+// operand when types mismatch.
+// CHECK-LABEL: func.func @no_fold_set_dimension_size_bounded_input
+func.func @no_fold_set_dimension_size_bounded_input(%arg0: tensor<?x4xf32, #stablehlo.bounds<8, ?>>) -> tensor<8x4xf32> {
+  %c = stablehlo.constant dense<8> : tensor<i32>
+  // CHECK: [[RESULT0:%.+]] = stablehlo.set_dimension_size
+  // CHECK-NEXT: return [[RESULT0]]
+  %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<?x4xf32, #stablehlo.bounds<8, ?>>, tensor<i32>) -> tensor<8x4xf32>
+  return %0 : tensor<8x4xf32>
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -98,6 +98,14 @@
 
   // CHECK-NEXT: return [[ARG0]], [[R4]]
   return %3, %4 : tensor<3x3xi32>, tensor<3x3xi32>
+}
+
+// CHECK-LABEL: func.func @broadcast_in_dim_transpose_invert
+// CHECK-SAME:   ([[ARG0:%.+]]: tensor<1x2x3x4xf32>)
+func.func @broadcast_in_dim_transpose_invert(%arg0 : tensor<1x2x3x4xf32>) -> tensor<3x1x4x2xf32> {
+  // stablehlo.transpose %arg0, dims = [2, 0, 3, 1] : (tensor<1x2x3x4xf32>) -> tensor<3x1x4x2xf32>
+  %0 = stablehlo.broadcast_in_dim %arg0, dims = [1,3,0,2] : (tensor<1x2x3x4xf32>) -> tensor<3x1x4x2xf32>
+  return %0 : tensor<3x1x4x2xf32>
 }
 
 // CHECK-LABEL: func.func @broadcast_in_dim_nested
diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
@@ -24,6 +24,7 @@
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/Sequence.h"
 #include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/Debug.h"
 #include "mlir/Dialect/Arith/IR/Arith.h"
 #include "mlir/Dialect/Complex/IR/Complex.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
@@ -55,6 +56,8 @@
 // compilation, M_PI will not be defined.
 #define _USE_MATH_DEFINES
 
+#define DEBUG_TYPE "chlo-legalize-to-stablehlo"
+
 namespace mlir {
 namespace stablehlo {
 
@@ -198,6 +201,31 @@
       val);
 }
 
+// Broadcast using numpy-style broadcasting semantics.
+// This is only valid if the CHLO op has static shaped operands, and no
+// explicitly specified broadcast_dimensions.
+//
+// Asserts that input is ranked tensor type.
+Value numpyBroadcastIfNeeded(Value op, RankedTensorType opResultType,
+                             PatternRewriter& rewriter) {
+  RankedTensorType inputType = cast<RankedTensorType>(op.getType());
+  RankedTensorType broadcastedResultType =
+      opResultType.clone(inputType.getElementType());
+
+  // No broadcasting needed if input type matches broadcasted result type.
+  if (inputType == broadcastedResultType) return op;
+
+  // broadcast dims are the last dims for numpy style broadcasting.
+  int64_t inputRank = inputType.getRank();
+  int64_t resultRank = opResultType.getRank();
+  auto broadcastDimensions =
+      llvm::to_vector(llvm::seq<int64_t>(resultRank - inputRank, resultRank));
+  return stablehlo::BroadcastInDimOp::create(rewriter, op.getLoc(),
+                                             broadcastedResultType, op,
+                                             broadcastDimensions)
+      .getResult();
+}
+
 //===----------------------------------------------------------------------===//
 // Broadcasting Patterns.
 //===----------------------------------------------------------------------===//
@@ -215,24 +243,69 @@
     // Only rewrite for statically determinable non-broadcasting cases.
     auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());
     auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());
-    if (!lhsType || !rhsType) return failure();
-
-    // Requires rank broadcast.
-    if (lhsType.getRank() != rhsType.getRank()) return failure();
-
-    // Any dynamic dimension may require broadcasting and requires more
-    // analysis.
-    if (!lhsType.hasStaticShape() || !rhsType.hasStaticShape()) {
-      return failure();
-    }
-
-    if (!llvm::equal(lhsType.getShape(), rhsType.getShape())) {
-      return failure();
-    }
+    if (!lhsType || !rhsType || lhsType.getShape() != rhsType.getShape() ||
+        !lhsType.hasStaticShape() || !rhsType.hasStaticShape())
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected LHS and RHS to be ranked tensors with matching shapes that "
+          "are all static");
 
     rewriter.replaceOp(
         op, ValueRange{Adaptor::createOp(op, op.getType(),
                                          adaptor.getOperands(), rewriter)});
+    return success();
+  }
+};
+
+// Converts binary ops that statically determined to use default numpy
+// broadcasting to simple StableHLO broadcasting ops without shape dialect.
+template <typename ChloOpTy, typename HloOpTy, typename Adaptor>
+struct ConvertTrivialNumpyBroadcastBinaryOp final
+    : OpConversionPattern<ChloOpTy> {
+  using OpConversionPattern<ChloOpTy>::OpConversionPattern;
+
+  LogicalResult matchAndRewrite(
+      ChloOpTy op, typename ChloOpTy::Adaptor adaptor,
+      ConversionPatternRewriter& rewriter) const override {
+    // Only rewrite for statically determinable non-broadcasting cases.
+    auto lhsType = dyn_cast<RankedTensorType>(adaptor.getLhs().getType());
+    auto rhsType = dyn_cast<RankedTensorType>(adaptor.getRhs().getType());
+    if (!lhsType || !rhsType || !lhsType.hasStaticShape() ||
+        !rhsType.hasStaticShape())
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected LHS and RHS to be ranked tensor types with static "
+          "shape");
+
+    // Rely on CHLO type inference to figure out the proper broadcasted shape.
+    auto resultType = dyn_cast<RankedTensorType>(op.getResult().getType());
+    if (!resultType || !resultType.hasStaticShape())
+      return rewriter.notifyMatchFailure(
+          op, "expected result to be a ranked tensor type with static shape");
+
+    auto lhs = adaptor.getLhs();
+    auto rhs = adaptor.getRhs();
+    auto broadcastDimensions = adaptor.getBroadcastDimensions();
+    if (broadcastDimensions &&
+        !hlo::isLegalNumpyRankedBroadcast(lhs, rhs, *broadcastDimensions))
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected implicit broadcast_dimensions or numpy-style broadcasting");
+
+    LLVM_DEBUG(llvm::dbgs()
+               << "CHLO Decomposing " << op->getName() << " with broadcast "
+               << lhsType << " x " << rhsType << " -> " << resultType << "\n");
+
+    // If operands are static directly create stablehlo broadcasting ops.
+    // Use numpy-style broadcasting with using StableHLO broadcast ops,
+    // when user didn't specify broadcast_dimensions.
+    auto lhsBroadcast =
+        numpyBroadcastIfNeeded(adaptor.getLhs(), resultType, rewriter);
+    auto rhsBroadcast =
+        numpyBroadcastIfNeeded(adaptor.getRhs(), resultType, rewriter);
+    auto result = Adaptor::createOp(op, resultType,
+                                    {lhsBroadcast, rhsBroadcast}, rewriter);
+    rewriter.replaceOp(op, {result.getResult()});
     return success();
   }
 };
@@ -2416,6 +2489,8 @@
   // not have special attributes that need to be preserved.
   populateForBroadcastingBinaryOp<ConvertTrivialNonBroadcastBinaryOp>(
       context, patterns, 10);
+  populateForBroadcastingBinaryOp<ConvertTrivialNumpyBroadcastBinaryOp>(
+      context, patterns, 10);
   populateForBroadcastingBinaryOp<ConvertRankedDynamicBroadcastBinaryOp>(
       context, patterns, 5);
   patterns->add<ConvertConstantLikeOp, ConvertSelectOp>(context);
diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.td b/stablehlo/stablehlo/transforms/optimization/Passes.td
--- stablehlo/stablehlo/transforms/optimization/Passes.td
+++ stablehlo/stablehlo/transforms/optimization/Passes.td
@@ -23,14 +23,14 @@
          "explicit MLIR `MemoryEffects`. Notably, this means `func.call` ops "
          "will be assumed pure.">,
   Option<"foldOpElementLimit", "fold-op-element-limit", "int64_t",
-         /*default=*/"1",
+         /*default=*/"65536",
          "Folding an op into a constant can sometimes come at the cost of "
          "memory overhead. (This occurs if the op's inputs are reused, meaning "
          "that they can't be deleted after the op is folded to a constant, or "
-         "when folding operations like `iota` whose outputs take up more "
+         "when folding operations like `concat` whose outputs take up more "
          "memory than their inputs.) In such cases, this config option sets an "
          "upper limit on how many elements an op's result may have before the "
-         "op is no longer folded.">,
+         "op is no longer folded. Splat folds are exempt from this limit.">,
   Option<"optimizeFloat", "optimize-float", "bool", /*default=*/"true",
          "Allow float optimizations that, though mathematically equivalent, "
          "may result in slightly different quantization of floating-point "
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -21,6 +21,7 @@
 #include <numeric>
 #include <optional>
 #include <string>
+#include <type_traits>
 #include <utility>
 
 #include "llvm/ADT/APFloat.h"
@@ -58,6 +59,7 @@
 #include "mlir/Support/LogicalResult.h"
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
 #include "stablehlo/dialect/Base.h"
+#include "stablehlo/dialect/ChloOps.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/transforms/optimization/Passes.h"
 
@@ -87,6 +89,14 @@
       /*isUnsigned=*/isUnsigned);
 }
 
+template <typename T>
+APSInt getAPSInt(unsigned bitWidth, T value, bool isSigned) {
+  return APSInt({/*numBits=*/bitWidth, static_cast<uint64_t>(value),
+                 /*isSigned=*/isSigned,
+                 /*implicitTrunc=*/true},
+                /*isUnsigned=*/!isSigned);
+}
+
 APFloat getAPFloat(
     Type type, double value,
     llvm::RoundingMode roundingMode = llvm::RoundingMode::NearestTiesToEven) {
@@ -94,8 +104,8 @@
   if (!floatType) llvm::report_fatal_error("expected float type");
 
   APFloat result(value);
-  bool losesInfo = false;
-  result.convert(floatType.getFloatSemantics(), roundingMode, &losesInfo);
+  bool unusedLosesInfo = false;
+  result.convert(floatType.getFloatSemantics(), roundingMode, &unusedLosesInfo);
   return result;
 }
 
@@ -351,6 +361,190 @@
           "too many elements, fold "
           "limit is " +
               std::to_string(options.foldOpElementLimit));
+    return success();
+  }
+};
+
+namespace fold_unary {
+
+template <typename Impl, typename = void, typename... ArgTypes>
+struct FolderExistsHelper : std::false_type {};
+
+template <typename Impl, typename... ArgTypes>
+struct FolderExistsHelper<
+    Impl,
+    std::enable_if_t<sizeof(Impl::EvaluateOp(std::declval<ArgTypes>()...)) != 0,
+                     void>,
+    ArgTypes...> : std::true_type {};
+
+template <typename Impl, typename... ArgTypes>
+struct FolderExists : FolderExistsHelper<Impl, void, ArgTypes...> {};
+
+template <typename Impl, typename CanonicalType, typename = void>
+struct DirectFolderExists : std::false_type {};
+
+template <typename Impl, typename CanonicalType>
+struct DirectFolderExists<
+    Impl, CanonicalType,
+    std::enable_if_t<std::is_convertible_v<decltype(Impl::EvaluateOp(
+                                               std::declval<CanonicalType>())),
+                                           std::optional<CanonicalType>>,
+                     void>> : std::true_type {
+  static_assert(FolderExists<Impl, CanonicalType>::value);
+};
+
+template <typename Impl, typename CanonicalType, typename ComputeType,
+          typename ConversionFn, typename = void>
+struct ConvertingFolderExists : std::false_type {};
+
+template <typename Impl, typename CanonicalType, typename ComputeType,
+          typename ConversionFn>
+struct ConvertingFolderExists<
+    Impl, CanonicalType, ComputeType, ConversionFn,
+    std::enable_if_t<std::is_convertible_v<
+                         decltype(std::declval<ConversionFn>()(
+                             Impl::EvaluateOp(std::declval<ComputeType>()))),
+                         std::optional<CanonicalType>> &&
+                         !std::is_same_v<std::decay_t<CanonicalType>,
+                                         std::decay_t<ComputeType>>,
+                     void>> : std::true_type {
+  static_assert(FolderExists<Impl, ComputeType>::value);
+  static_assert(!DirectFolderExists<Impl, CanonicalType>::value);
+};
+
+}  // namespace fold_unary
+
+template <typename Impl, typename OpType>
+struct FoldUnaryOpPattern : public FoldOpRewritePattern<OpType> {
+  using FoldOpRewritePattern<OpType>::FoldOpRewritePattern;
+
+  template <
+      typename CanonicalType,
+      std::enable_if_t<
+          fold_unary::DirectFolderExists<Impl, CanonicalType>::value, int> = 0>
+  static std::optional<CanonicalType> FoldIfImplemented(CanonicalType operand) {
+    return Impl::EvaluateOp(operand);
+  }
+
+  template <typename CanonicalType, typename ComputeType,
+            typename ConversionFn =
+                std::optional<CanonicalType> (*)(std::optional<ComputeType>),
+            std::enable_if_t<
+                fold_unary::ConvertingFolderExists<
+                    Impl, CanonicalType, ComputeType, ConversionFn>::value,
+                int> = 0>
+  static std::optional<CanonicalType> FoldIfImplemented(
+      ComputeType operand,
+      ConversionFn&& convertResult = [](std::optional<ComputeType> result)
+          -> std::optional<CanonicalType> {
+        if (result == std::nullopt) return std::nullopt;
+        return CanonicalType(*result);
+      }) {
+    return convertResult(Impl::EvaluateOp(operand));
+  }
+
+  template <typename CanonicalType, typename ComputeType,
+            typename ConversionFn = std::nullptr_t,
+            std::enable_if_t<
+                !fold_unary::FolderExists<Impl, ComputeType>::value, int> = 0>
+  static std::nullopt_t FoldIfImplemented(
+      ComputeType operand, ConversionFn&& convertResult = nullptr) {
+    return std::nullopt;
+  }
+
+  struct FoldDispatch {
+    bool isSignedInt = false;
+
+    std::optional<APInt> operator()(APInt operand) const {
+      if constexpr (fold_unary::DirectFolderExists<Impl, APInt>::value) {
+        // Fold as a signedness-agnostic `APInt`.
+        return FoldIfImplemented<APInt>(operand);
+      } else if constexpr (fold_unary::DirectFolderExists<Impl,
+                                                          APSInt>::value) {
+        // Fold as a signedness-aware `APSInt`.
+        return FoldIfImplemented<APSInt>(
+            APSInt(std::move(operand), /*isUnsigned=*/!isSignedInt));
+      } else {
+        // Fold as a C++ primitive of type `int64_t` or `uint64_t`.
+        return isSignedInt ? FoldAsIntType<int64_t>(operand)
+                           : FoldAsIntType<uint64_t>(operand);
+      }
+    }
+
+    std::optional<APFloat> operator()(APFloat operand) const {
+      if constexpr (fold_unary::DirectFolderExists<Impl, APFloat>::value) {
+        // Fold as an `APFloat`.
+        return FoldIfImplemented<APFloat>(operand);
+      } else {
+        // Fold as a `double`.
+        return FoldAsDouble(operand);
+      }
+    }
+
+    template <typename ComputationDataType>
+    std::optional<APInt> FoldAsIntType(APInt operand) const {
+      const size_t bitWidth = operand.getBitWidth();
+
+      auto convertResult = [&](std::optional<ComputationDataType> result)
+          -> std::optional<APInt> {
+        if (result == std::nullopt) return std::nullopt;
+        return APInt(bitWidth, *result, isSignedInt);
+      };
+
+      ComputationDataType operandValue;
+      if constexpr (std::is_signed_v<ComputationDataType>) {
+        operandValue = static_cast<ComputationDataType>(operand.getSExtValue());
+      } else {
+        operandValue = static_cast<ComputationDataType>(operand.getZExtValue());
+      }
+
+      return FoldIfImplemented<APInt, ComputationDataType>(operandValue,
+                                                           convertResult);
+    }
+
+    std::optional<APFloat> FoldAsDouble(APFloat operand) const {
+      auto convertResult =
+          [&](std::optional<double> result) -> std::optional<APFloat> {
+        if (result == std::nullopt) return std::nullopt;
+        APFloat resultAsAPFloat(*result);
+        bool unusedLosesInfo;
+        resultAsAPFloat.convert(operand.getSemantics(),
+                                APFloat::rmNearestTiesToEven, &unusedLosesInfo);
+        return resultAsAPFloat;
+      };
+
+      APFloat operandCopy = operand;
+      bool unusedLosesInfo;
+      operandCopy.convert(APFloat::IEEEdouble(), APFloat::rmNearestTiesToEven,
+                          &unusedLosesInfo);
+      double operandValue = operandCopy.convertToDouble();
+
+      return FoldIfImplemented<APFloat, double>(operandValue, convertResult);
+    }
+  };
+
+  LogicalResult matchAndRewrite(OpType op,
+                                PatternRewriter& rewriter) const override {
+    auto elementType = op.getType().getElementType();
+
+    FailureOr<TypedAttr> result;
+    if (elementType.isUnsignedInteger()) {
+      result = foldUnaryOpIntOrFloat(rewriter, op,
+                                     FoldDispatch{/*isSignedInt=*/false});
+    } else if (elementType.isInteger()) {
+      // Types with unspecified signedness are treated as signed per StableHLO
+      // convention.
+      result = foldUnaryOpIntOrFloat(rewriter, op,
+                                     FoldDispatch{/*isSignedInt=*/true});
+    } else if (elementType.isFloat()) {
+      result = foldUnaryOpIntOrFloat(rewriter, op,
+                                     FoldDispatch{/*isSignedInt=*/false});
+    } else {
+      return failure();
+    }
+    if (failed(result)) return failure();
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(op, result.value());
     return success();
   }
 };
@@ -1014,6 +1208,11 @@
     // No need to verify static shape or dtype here since we aren't evaluating
     // dtype, just folding set_dim_size ops with no semantic meaning.
 
+    // Don't fold if the input is dynamic and we're washing away the bound.
+    if (op.getOperand().getType() != op.getType())
+      return rewriter.notifyMatchFailure(
+          op, "operand and result type must be the same");
+
     SplatElementsAttr cstSplatAttr;
     matchPattern(op.getSize(), m_Constant(&cstSplatAttr));
     if (!cstSplatAttr)
@@ -1031,50 +1230,6 @@
     rewriter.replaceAllOpUsesWith(op, op.getOperand());
     return success();
   }
-};
-
-struct FoldSignOpPattern : public ShapeOpRewritePattern<SignOp> {
-  using ShapeOpRewritePattern::ShapeOpRewritePattern;
-
-  LogicalResult matchAndRewrite(SignOp op,
-                                PatternRewriter& rewriter) const override {
-    if (failed(validateShapeFoldDtype(rewriter, op, op.getType())))
-      return failure();
-
-    auto elementType = op.getType().getElementType();
-    auto res = foldUnaryOpIntOrFloat(rewriter, op, FoldSign(elementType));
-    if (failed(res)) return failure();
-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
-    return success();
-  }
-
-  struct FoldSign {
-    FoldSign(Type elementType) : elementType(elementType) {}
-    Type elementType;
-    double result;
-    APFloat operator()(APFloat operand) {
-      if (operand.isNegative())
-        result = -1.0;
-      else if (operand.isZero())
-        result = 0.0;
-      else
-        result = 1.0;
-      return getAPFloat(elementType, result);
-    }
-
-    APInt operator()(APInt operand) {
-      // SignOp only supports signed integers.
-      APSInt signedInt = getAPSInt(elementType, operand.getSExtValue());
-      int64_t result;
-      if (signedInt.isNegative())
-        result = -1;
-      else if (signedInt.isZero())
-        result = 0;
-      else
-        result = 1;
-      return getAPSInt(elementType, result);
-    }
-  };
 };
 
 template <typename RangeType>
@@ -1128,15 +1283,21 @@
       return rewriter.notifyMatchFailure(
           op, "expected operand with static ranked tensor type");
 
-    ElementsAttr els;
+    DenseElementsAttr els;
     if (!matchPattern(operand, m_Constant(&els)))
       return rewriter.notifyMatchFailure(
           op, "expected constant integer or float operand");
 
+    // Short circuit on splat resizes
+    if (els.isSplat()) {
+      rewriter.replaceOpWithNewOp<ConstantOp>(op, els.resizeSplat(resultType));
+      return success();
+    }
+
     DenseElementsAttr resAttr;
-    if (auto data = els.tryGetValues<APInt>())
+    if (auto data = els.tryGetValues<APInt>(); succeeded(data))
       resAttr = sliceType(op, *data);
-    else if (auto data = els.tryGetValues<APFloat>())
+    else if (auto data = els.tryGetValues<APFloat>(); succeeded(data))
       resAttr = sliceType(op, *data);
     else
       return rewriter.notifyMatchFailure(op.getLoc(),
@@ -1147,6 +1308,27 @@
   }
 };
 
+// Pattern: dynamic_slice(splat_cst, start, end) -> resized_splat_cst
+struct FoldDynamicSliceOpPattern : public FoldOpRewritePattern<DynamicSliceOp> {
+  using FoldOpRewritePattern::FoldOpRewritePattern;
+
+  LogicalResult matchAndRewrite(DynamicSliceOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (failed(validateStaticShapeResult(rewriter, op, resultType)))
+      return failure();
+
+    SplatElementsAttr inputSplatAttr;
+    if (!matchPattern(op.getOperand(), m_Constant(&inputSplatAttr)) ||
+        !inputSplatAttr)
+      return rewriter.notifyMatchFailure(op, "Input must be a splat constant.");
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(
+        op, inputSplatAttr.resizeSplat(resultType));
+    return success();
+  }
+};
+
 struct FoldSubtractOpPattern final
     : ShapeOpRewritePattern<mlir::stablehlo::SubtractOp> {
   using ShapeOpRewritePattern::ShapeOpRewritePattern;
@@ -1163,31 +1345,169 @@
   }
 };
 
+struct FoldAbsOpPattern : public FoldUnaryOpPattern<FoldAbsOpPattern, AbsOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<APInt> EvaluateOp(APInt operand) {
+    return operand.abs();
+  }
+  static std::optional<APFloat> EvaluateOp(APFloat operand) {
+    return llvm::abs(operand);
+  }
+};
+
+struct FoldCosineOpPattern
+    : public FoldUnaryOpPattern<FoldCosineOpPattern, CosineOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return std::cos(operand);
+  }
+};
+
+struct FoldErfOpPattern
+    : public FoldUnaryOpPattern<FoldErfOpPattern, chlo::ErfOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return std::erf(operand);
+  }
+};
+
+struct FoldExpOpPattern : public FoldUnaryOpPattern<FoldExpOpPattern, ExpOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return std::exp(operand);
+  }
+};
+
+struct FoldLogOpPattern : public FoldUnaryOpPattern<FoldLogOpPattern, LogOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    if (operand <= 0.0) return std::nullopt;
+    return std::log(operand);
+  }
+};
+
+struct FoldLogisticOpPattern
+    : public FoldUnaryOpPattern<FoldLogisticOpPattern, LogisticOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return 1.0 / (1.0 + std::exp(-operand));
+  }
+};
+
+struct FoldNegOpPattern : public FoldUnaryOpPattern<FoldNegOpPattern, NegOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<APInt> EvaluateOp(APInt operand) { return -operand; }
+  static std::optional<APFloat> EvaluateOp(APFloat operand) { return -operand; }
+};
+
+struct FoldNotOpPattern : public FoldUnaryOpPattern<FoldNotOpPattern, NotOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<APInt> EvaluateOp(APInt operand) {
+    operand.flipAllBits();
+    return operand;
+  }
+};
+
+struct FoldRoundOpPattern
+    : public FoldUnaryOpPattern<FoldRoundOpPattern, RoundOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<APFloat> EvaluateOp(APFloat operand) {
+    operand.roundToIntegral(APFloat::rmNearestTiesToAway);
+    return operand;
+  }
+};
+
+struct FoldRoundNearestEvenOpPattern
+    : public FoldUnaryOpPattern<FoldRoundNearestEvenOpPattern,
+                                RoundNearestEvenOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<APFloat> EvaluateOp(APFloat operand) {
+    operand.roundToIntegral(APFloat::rmNearestTiesToEven);
+    return operand;
+  }
+};
+
+struct FoldRsqrtOpPattern
+    : public FoldUnaryOpPattern<FoldRsqrtOpPattern, RsqrtOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    if (operand <= 0.0) return std::nullopt;
+    return 1.0 / std::sqrt(operand);
+  }
+};
+
+struct FoldSignOpPattern
+    : public FoldUnaryOpPattern<FoldSignOpPattern, SignOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<APFloat> EvaluateOp(APFloat operand) {
+    if (operand.isNaN()) return std::nullopt;
+    if (operand.isZero()) return APFloat::getZero(operand.getSemantics());
+    return APFloat::getOne(operand.getSemantics(),
+                           /*Negative=*/operand.isNegative());
+  }
+
+  static std::optional<APSInt> EvaluateOp(APSInt operand) {
+    // SignOp only supports signed integers.
+    if (operand.isUnsigned()) return std::nullopt;
+
+    int sign;
+    if (operand.isNegative()) {
+      sign = -1;
+    } else if (operand.isZero()) {
+      sign = 0;
+    } else {
+      sign = +1;
+    }
+    return getAPSInt(operand.getBitWidth(), sign, /*isSigned=*/true);
+  }
+};
+
+struct FoldSineOpPattern
+    : public FoldUnaryOpPattern<FoldSineOpPattern, SineOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return std::sin(operand);
+  }
+};
+
 struct FoldSqrtOpPattern
-    : public FoldOpRewritePattern<mlir::stablehlo::SqrtOp> {
-  using FoldOpRewritePattern<mlir::stablehlo::SqrtOp>::FoldOpRewritePattern;
-
-  LogicalResult matchAndRewrite(mlir::stablehlo::SqrtOp op,
-                                PatternRewriter& rewriter) const final {
-    auto res = foldUnaryOpIntOrFloat(rewriter, op, FoldSqrt());
-    if (failed(res)) return failure();
-    rewriter.replaceOpWithNewOp<mlir::stablehlo::ConstantOp>(op, res.value());
-    return success();
-  }
-
-  struct FoldSqrt {
-    std::optional<APFloat> operator()(APFloat operand) {
-      if (operand.getSizeInBits(operand.getSemantics()) == 64)
-        return APFloat(std::sqrt(operand.convertToDouble()));
-
-      if (operand.getSizeInBits(operand.getSemantics()) == 32)
-        return APFloat(sqrtf(operand.convertToFloat()));
-      return std::nullopt;
-    }
-
-    // TODO: Enable int folding.
-    std::optional<APInt> operator()(APInt operand) { return std::nullopt; }
-  };
+    : public FoldUnaryOpPattern<FoldSqrtOpPattern, SqrtOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    if (operand < 0.0) return std::nullopt;
+    return std::sqrt(operand);
+  }
+};
+
+struct FoldTanOpPattern : public FoldUnaryOpPattern<FoldTanOpPattern, TanOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return std::tan(operand);
+  }
+};
+
+struct FoldTanhOpPattern
+    : public FoldUnaryOpPattern<FoldTanhOpPattern, TanhOp> {
+  using FoldUnaryOpPattern::FoldUnaryOpPattern;
+
+  static std::optional<double> EvaluateOp(double operand) {
+    return std::tanh(operand);
+  }
 };
 
 struct FoldIotaOpPattern : public FoldOpRewritePattern<IotaOp> {
@@ -1216,6 +1536,14 @@
       rewriter.replaceOpWithNewOp<ConstantOp>(
           op, DenseIntElementsAttr::get(resultType, values));
       return success();
+    }
+
+    // TODO: Support more iota folding, but doing so currently causes OOMs,
+    // so this pattern needs to be enabled more carefully.
+    if (outputSize != 1) {
+      return rewriter.notifyMatchFailure(
+          op, "expected output size to be 1, but got: " +
+                  std::to_string(outputSize));
     }
 
     int64_t sequences = 1;
@@ -1316,13 +1644,9 @@
 
     for (auto [inputValue, bodyArg] :
          llvm::zip_equal(op.getOperands(), body.getArguments())) {
-      auto inputConstantOp = inputValue.getDefiningOp<ConstantOp>();
-      if (!inputConstantOp)
-        return rewriter.notifyMatchFailure(op, "Input must be a constant.");
-
-      auto inputConstantAttr =
-          dyn_cast_or_null<DenseElementsAttr>(inputConstantOp.getValue());
-      if (!inputConstantAttr)
+      SplatElementsAttr constantSplatAttr;
+      if (!matchPattern(inputValue, m_Constant(&constantSplatAttr)) ||
+          !constantSplatAttr)
         return rewriter.notifyMatchFailure(op,
                                            "Input must be a splat constant.");
 
@@ -1332,7 +1656,7 @@
             op, "Could not get the shape of the body argument.");
 
       bodyArgConstantAttrs.push_back(DenseElementsAttr::get(
-          bodyArgShapedType, inputConstantAttr.getSplatValue<Attribute>()));
+          bodyArgShapedType, constantSplatAttr.getSplatValue<Attribute>()));
     }
 
     for (BlockArgument bodyArg : body.getArguments()) {
@@ -1570,11 +1894,25 @@
     PatternBenefit benefit) {
   populateStablehloShapeFolderPatterns(context, patterns, options, benefit);
 
-  patterns->add<FoldIotaOpPattern,                    //
+  patterns->add<FoldAbsOpPattern,                     //
+                FoldCosineOpPattern,                  //
+                FoldErfOpPattern,                     //
+                FoldExpOpPattern,                     //
+                FoldIotaOpPattern,                    //
+                FoldLogOpPattern,                     //
+                FoldLogisticOpPattern,                //
+                FoldNegOpPattern,                     //
+                FoldNotOpPattern,                     //
                 FoldReduceOpReducingZeroDims,         //
                 FoldReduceOpToConstantInitializer,    //
                 FoldReduceOpWithRedundantResults,     //
+                FoldRoundOpPattern,                   //
+                FoldRoundNearestEvenOpPattern,        //
+                FoldRsqrtOpPattern,                   //
+                FoldSineOpPattern,                    //
                 FoldSqrtOpPattern,                    //
+                FoldTanOpPattern,                     //
+                FoldTanhOpPattern,                    //
                 FoldTransposeOpPattern,               //
                 FoldWhileOpIfDeadAndPresumedPure,     //
                 FoldWhileOpPattern,                   //
@@ -1605,6 +1943,7 @@
   patterns->add<FoldConcatenateOpPattern>(context, options, benefit);
   patterns->add<FoldConvertOpPattern>(context, options, benefit);
   patterns->add<FoldDivOpPattern>(context, options, benefit);
+  patterns->add<FoldDynamicSliceOpPattern>(context, options, benefit);
   patterns->add<FoldGetDimensionSizeOpPattern>(context, options, benefit);
   patterns->add<FoldMaxOpPattern>(context, options, benefit);
   patterns->add<FoldMinOpPattern>(context, options, benefit);
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
@@ -325,8 +325,18 @@
 //////////////////////////////////
 // BroadcastInDimOp
 /////////////////////////////////
-
 // Used in DRR file.
+
+// Convert broadcast dimensions into permutation for transpose.
+DenseI64ArrayAttr getInvertedBroadcastDimensions(OpBuilder& b,
+                                                 ArrayRef<int64_t> dims) {
+  SmallVector<int64_t> permutation(dims.size());
+  for (auto i = 0; i < dims.size(); ++i) {
+    permutation[dims[i]] = i;
+  }
+  return b.getDenseI64ArrayAttr(permutation);
+}
+
 DenseI64ArrayAttr getMergedBroadcastDimensions(OpBuilder& b,
                                                ArrayRef<int64_t> dims,
                                                ArrayRef<int64_t> dimsParent) {
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -116,6 +116,8 @@
 
 def GetEmptyI64Array : NativeCodeCall<"$_builder.getDenseI64ArrayAttr({})">;
 
+def InvertBroadcastDims : NativeCodeCall<"getInvertedBroadcastDimensions($_builder, $0)">;
+
 def MergeBroadcastDims : NativeCodeCall<"getMergedBroadcastDimensions($_builder, $0, $1)">;
 
 def StableHLO_ConvertOpWithShape : NativeCodeCall<
@@ -193,7 +195,7 @@
 //          [if same numel & rank]
 def BroadcastInDimOp_ReplaceWithTranspose
   : Pat<(StableHLO_BroadcastInDimOp:$op $operand, $dims),
-        (StableHLO_TransposeOp $operand, $dims),
+        (StableHLO_TransposeOp $operand, (InvertBroadcastDims $dims)),
         [(NumberOfElementsEqual $op, $operand), (RankEqual $op, $operand)]>;
 
 ////////

