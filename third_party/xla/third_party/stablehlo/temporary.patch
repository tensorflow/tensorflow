diff --ruN a/stablehlo/CMakeLists.txt b/stablehlo/CMakeLists.txt
--- stablehlo/CMakeLists.txt
+++ stablehlo/CMakeLists.txt
@@ -13,135 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-cmake_minimum_required(VERSION 3.15.0)
 
-if(POLICY CMP0068)
-  cmake_policy(SET CMP0068 NEW)
-  set(CMAKE_BUILD_WITH_INSTALL_NAME_DIR ON)
-endif()
-
-if(POLICY CMP0075)
-  cmake_policy(SET CMP0075 NEW)
-endif()
-
-if(POLICY CMP0077)
-  cmake_policy(SET CMP0077 NEW)
-endif()
-
-# CMP0116: Ninja generators transform `DEPFILE`s from `add_custom_command()`
-# New in CMake 3.20. https://cmake.org/cmake/help/latest/policy/CMP0116.html
-if(POLICY CMP0116)
-  cmake_policy(SET CMP0116 OLD)
-endif()
+# This build of StableHLO is meant to be embedded in MLIR-HLO.
+# As a result, its root CMakeLists.txt is different from the original
+# CMakeLists.txt from https://github.com/openxla/stablehlo.
+# All other files of this build of StableHLO except for this one are the same
+# as the original files.
+# To get access to a standalone build of StableHLO, check out the
+# openxla/stablehlo repository.
 
 #-------------------------------------------------------------------------------
 # Options and settings
 #-------------------------------------------------------------------------------
-option(STABLEHLO_BUILD_EMBEDDED "Build StableHLO as part of another project" OFF)
-option(STABLEHLO_ENABLE_BINDINGS_PYTHON "Enables StableHLO Python bindings" OFF)
-option(STABLEHLO_ENABLE_STRICT_BUILD "Build StableHLO with strict warnings and warnings as errors" OFF)
 
-#-------------------------------------------------------------------------------
-# Project setup and globals
-#-------------------------------------------------------------------------------
-set(STABLEHLO_EXTERNAL_PROJECT_BUILD OFF)
-
-if(NOT (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR) AND NOT MLIR_BINARY_DIR)
-  # Building as part of LLVM via the external project mechanism.
-  set(STABLEHLO_EXTERNAL_PROJECT_BUILD ON)
-else()
-  # Building standalone.
-  project(stablehlo LANGUAGES CXX C)
-  set(CMAKE_C_STANDARD 11)
-  set(CMAKE_CXX_STANDARD 17)
-endif()
-
-# Build with ccache if the package is present
-set(LLVM_CCACHE_BUILD OFF CACHE BOOL "Set to ON for a ccache enabled build")
-if(LLVM_CCACHE_BUILD)
-  find_program(CCACHE_PROGRAM ccache)
-  if(CCACHE_PROGRAM)
-      set(LLVM_CCACHE_MAXSIZE "" CACHE STRING "Size of ccache")
-      set(LLVM_CCACHE_DIR "" CACHE STRING "Directory to keep ccached data")
-      set(LLVM_CCACHE_PARAMS "CCACHE_CPP2=yes CCACHE_HASHDIR=yes"
-          CACHE STRING "Parameters to pass through to ccache")
-
-      set(CCACHE_PROGRAM "${LLVM_CCACHE_PARAMS} ${CCACHE_PROGRAM}")
-      if (LLVM_CCACHE_MAXSIZE)
-        set(CCACHE_PROGRAM "CCACHE_MAXSIZE=${LLVM_CCACHE_MAXSIZE} ${CCACHE_PROGRAM}")
-      endif()
-      if (LLVM_CCACHE_DIR)
-        set(CCACHE_PROGRAM "CCACHE_DIR=${LLVM_CCACHE_DIR} ${CCACHE_PROGRAM}")
-      endif()
-      set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE ${CCACHE_PROGRAM})
-  else()
-    message(FATAL_ERROR "Unable to find the program ccache. Set LLVM_CCACHE_BUILD to OFF")
-  endif()
-endif()
-
-#-------------------------------------------------------------------------------
-# MLIR/LLVM Configuration
-#-------------------------------------------------------------------------------
-if (STABLEHLO_ENABLE_STRICT_BUILD)
-  set(LLVM_ENABLE_WARNINGS ON)
-  set(LLVM_ENABLE_WERROR ON)
-  set(LLVM_ENABLE_PEDANTIC ON)
-endif()
-
-# Find MLIR to install if we are building standalone. If building as part of
-# another project, let it handle the MLIR dependency. The dependent project
-# might use a bundled version of MLIR instead of installing, for instance.
-if(STABLEHLO_EXTERNAL_PROJECT_BUILD)
-  message(STATUS "Building StableHLO as an external LLVM project")
-  set(MLIR_MAIN_SRC_DIR ${LLVM_MAIN_SRC_DIR}/../mlir ) # --src-root
-  set(MLIR_INCLUDE_DIR ${MLIR_MAIN_SRC_DIR}/include ) # --includedir
-  set(MLIR_GENERATED_INCLUDE_DIR ${LLVM_BINARY_DIR}/tools/mlir/include)
-  include_directories(SYSTEM ${MLIR_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_GENERATED_INCLUDE_DIR})
-  include_directories(SYSTEM ${MLIR_TABLEGEN_OUTPUT_DIR})
-
-  set(BACKEND_PACKAGE_STRING "${PACKAGE_STRING}")
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_MAIN_SRC_DIR}/cmake/modules")
-elseif(NOT STABLEHLO_BUILD_EMBEDDED)
-  message(STATUS "Building StableHLO with an installed MLIR")
-  find_package(MLIR REQUIRED CONFIG)
-  message(STATUS "Using MLIRConfig.cmake in: ${MLIR_DIR}")
-  message(STATUS "Using LLVMConfig.cmake in: ${LLVM_DIR}")
-  set(LLVM_RUNTIME_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/bin)
-  set(LLVM_LIBRARY_OUTPUT_INTDIR ${CMAKE_BINARY_DIR}/lib)
-  list(APPEND CMAKE_MODULE_PATH "${MLIR_CMAKE_DIR}")
-  list(APPEND CMAKE_MODULE_PATH "${LLVM_CMAKE_DIR}")
-else()
-  message(STATUS "Building StableHLO embedded in another project")
-endif()
-
-if(LLVM_ENABLE_ZLIB)
-  find_package(ZLIB)
-endif()
-
-include(TableGen)
-include(AddLLVM)
-include(AddMLIR)
-include(HandleLLVMOptions)
-include_directories(${LLVM_INCLUDE_DIRS})
-include_directories(${MLIR_INCLUDE_DIRS})
-include_directories(${CMAKE_CURRENT_SOURCE_DIR})
-include_directories(${CMAKE_CURRENT_BINARY_DIR})
-link_directories(${LLVM_BUILD_LIBRARY_DIR})
-add_definitions(${LLVM_DEFINITIONS})
-
-#-------------------------------------------------------------------------------
-# Python configuration
-#-------------------------------------------------------------------------------
-
-if(STABLEHLO_ENABLE_BINDINGS_PYTHON)
-  if(NOT STABLEHLO_EXTERNAL_PROJECT_BUILD)
-    message(WARNING "StableHLO Python bindings are not supported in standalone mode")
-  endif()
-
-  include(MLIRDetectPythonEnv)
-  mlir_configure_python_dev_packages()
-endif()
+set(STABLEHLO_ENABLE_BINDINGS_PYTHON ${MHLO_ENABLE_BINDINGS_PYTHON})
 
 #-------------------------------------------------------------------------------
 # Directory setup
diff --ruN a/stablehlo/stablehlo/CMakeLists.txt b/stablehlo/stablehlo/CMakeLists.txt
--- stablehlo/stablehlo/CMakeLists.txt
+++ stablehlo/stablehlo/CMakeLists.txt
@@ -15,6 +15,7 @@
 add_subdirectory(api)
 add_subdirectory(conversions)
 add_subdirectory(dialect)
+add_subdirectory(experimental)
 add_subdirectory(integrations)
 add_subdirectory(reference)
 add_subdirectory(tests)
diff --ruN a/stablehlo/stablehlo/experimental/BUILD b/stablehlo/stablehlo/experimental/BUILD
--- stablehlo/stablehlo/experimental/BUILD
+++ stablehlo/stablehlo/experimental/BUILD
@@ -0,0 +1,117 @@
+load("//third_party/llvm/llvm-project/mlir:tblgen.bzl", "gentbl_cc_library")
+load("//tensorflow:tensorflow.google.bzl", "get_compatible_with_portable")
+load("//tensorflow/core/platform:rules_cc.bzl", "cc_library")
+
+package(
+    default_applicable_licenses = ["//third_party/stablehlo:license"],  # copybara:comment
+    default_visibility = ["//learning/brain/mlir:stablehlo_friends"],
+    licenses = ["notice"],
+)
+
+filegroup(
+    name = "stablehlo_experimental_filegroup",
+    srcs = glob(["**"]),
+)
+
+cc_library(
+    name = "experimental_base",
+    srcs = [
+        "dialect/Base.cpp",
+    ],
+    hdrs = [
+        "dialect/Base.h",
+    ],
+    compatible_with = get_compatible_with_portable(),
+    includes = ["../.."],
+    deps = [
+        "//third_party/llvm/llvm-project/llvm:Support",
+        "//third_party/llvm/llvm-project/mlir:IR",
+    ],
+)
+
+cc_library(
+    name = "experimental_stablehlo_ops",
+    srcs = [
+        "dialect/StablehloOps.cpp",
+    ],
+    hdrs = [
+        "dialect/StablehloOps.h",
+    ],
+    compatible_with = get_compatible_with_portable(),
+    includes = ["../.."],
+    deps = [
+        ":experimental_base",
+        "//third_party/llvm/llvm-project/llvm:Support",
+        "//third_party/llvm/llvm-project/mlir:FuncDialect",
+        "//third_party/llvm/llvm-project/mlir:IR",
+        "//third_party/llvm/llvm-project/mlir:Support",
+        "//third_party/stablehlo:stablehlo_ops",
+    ],
+)
+
+gentbl_cc_library(
+    name = "experimental_stablehlo_pass_inc_gen",
+    compatible_with = get_compatible_with_portable(),
+    tbl_outs = [
+        (
+            [
+                "-gen-pass-decls",
+            ],
+            "transforms/Passes.h.inc",
+        ),
+    ],
+    tblgen = "//third_party/llvm/llvm-project/mlir:mlir-tblgen",
+    td_file = "transforms/Passes.td",
+    deps = ["//third_party/llvm/llvm-project/mlir:PassBaseTdFiles"],
+)
+
+cc_library(
+    name = "experimental_stablehlo_passes",
+    srcs = [
+        "transforms/StablehloCanonicalizeDynamism.cpp",
+        "transforms/StablehloRefineShapes.cpp",
+    ],
+    hdrs = [
+        "transforms/Passes.h",
+    ],
+    compatible_with = get_compatible_with_portable(),
+    includes = ["../.."],
+    deps = [
+        ":experimental_stablehlo_ops",
+        ":experimental_stablehlo_pass_inc_gen",
+        "//third_party/llvm/llvm-project/llvm:Support",
+        "//third_party/llvm/llvm-project/mlir:FuncDialect",
+        "//third_party/llvm/llvm-project/mlir:IR",
+        "//third_party/llvm/llvm-project/mlir:InferTypeOpInterface",
+        "//third_party/llvm/llvm-project/mlir:Pass",
+        "//third_party/llvm/llvm-project/mlir:Support",
+        "//third_party/llvm/llvm-project/mlir:TransformUtils",
+        "//third_party/llvm/llvm-project/mlir:Transforms",
+        "//third_party/stablehlo:base",
+        "//third_party/stablehlo:chlo_ops",
+        "//third_party/stablehlo:stablehlo_ops",
+        "//third_party/stablehlo:stablehlo_ops_inc_gen",
+        "//third_party/stablehlo:stablehlo_type_inference",
+    ],
+)
+
+cc_binary(
+    name = "experimental-stablehlo-opt",
+    srcs = [
+        "tools/StablehloOptMain.cpp",
+    ],
+    compatible_with = get_compatible_with_portable(),
+    includes = ["../.."],
+    deps = [
+        ":experimental_stablehlo_passes",
+        "//third_party/llvm/llvm-project/mlir:AllExtensions",
+        "//third_party/llvm/llvm-project/mlir:AllPassesAndDialects",
+        "//third_party/llvm/llvm-project/mlir:MlirOptLib",
+        "//third_party/llvm/llvm-project/mlir:TosaDialect",
+        "//third_party/stablehlo:interpreter_ops",
+        "//third_party/stablehlo:register",
+        "//third_party/stablehlo:stablehlo_passes",
+        "//third_party/stablehlo:test_utils",
+        "//third_party/stablehlo:tosa_passes",
+    ],
+)
diff --ruN a/stablehlo/stablehlo/experimental/BUILD.bazel b/stablehlo/stablehlo/experimental/BUILD.bazel
--- stablehlo/stablehlo/experimental/BUILD.bazel
+++ stablehlo/stablehlo/experimental/BUILD.bazel
@@ -0,0 +1,113 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@llvm-project//mlir:tblgen.bzl", "gentbl_cc_library")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+cc_library(
+    name = "experimental_base",
+    srcs = [
+        "dialect/Base.cpp",
+    ],
+    hdrs = [
+        "dialect/Base.h",
+    ],
+    deps = [
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:IR",
+    ],
+)
+
+cc_library(
+    name = "experimental_stablehlo_ops",
+    srcs = [
+        "dialect/StablehloOps.cpp",
+    ],
+    hdrs = [
+        "dialect/StablehloOps.h",
+    ],
+    deps = [
+        ":experimental_base",
+        "//:stablehlo_ops",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+gentbl_cc_library(
+    name = "experimental_stablehlo_pass_inc_gen",
+    tbl_outs = [
+        (
+            [
+                "-gen-pass-decls",
+            ],
+            "transforms/Passes.h.inc",
+        ),
+    ],
+    tblgen = "@llvm-project//mlir:mlir-tblgen",
+    td_file = "transforms/Passes.td",
+    deps = ["@llvm-project//mlir:PassBaseTdFiles"],
+)
+
+cc_library(
+    name = "experimental_stablehlo_passes",
+    srcs = [
+        "transforms/StablehloCanonicalizeDynamism.cpp",
+        "transforms/StablehloRefineShapes.cpp",
+    ],
+    hdrs = [
+        "transforms/Passes.h",
+    ],
+    deps = [
+        ":experimental_stablehlo_ops",
+        ":experimental_stablehlo_pass_inc_gen",
+        "//:base",
+        "//:chlo_ops",
+        "//:stablehlo_ops",
+        "//:stablehlo_ops_inc_gen",
+        "//:stablehlo_type_inference",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:InferTypeOpInterface",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:TransformUtils",
+        "@llvm-project//mlir:Transforms",
+    ],
+)
+
+cc_binary(
+    name = "experimental-stablehlo-opt",
+    srcs = [
+        "tools/StablehloOptMain.cpp",
+    ],
+    deps = [
+        ":experimental_stablehlo_passes",
+        "//:interpreter_ops",
+        "//:register",
+        "//:stablehlo_passes",
+        "//:test_utils",
+        "//:tosa_passes",
+        "@llvm-project//mlir:AllExtensions",
+        "@llvm-project//mlir:AllPassesAndDialects",
+        "@llvm-project//mlir:MlirOptLib",
+        "@llvm-project//mlir:TosaDialect",
+    ],
+)
diff --ruN a/stablehlo/stablehlo/experimental/CMakeLists.txt b/stablehlo/stablehlo/experimental/CMakeLists.txt
--- stablehlo/stablehlo/experimental/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/CMakeLists.txt
@@ -0,0 +1,18 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_subdirectory(dialect)
+add_subdirectory(tests)
+add_subdirectory(tools)
+add_subdirectory(transforms)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.cpp b/stablehlo/stablehlo/experimental/dialect/Base.cpp
--- stablehlo/stablehlo/experimental/dialect/Base.cpp
+++ stablehlo/stablehlo/experimental/dialect/Base.cpp
@@ -0,0 +1,39 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/Base.h"
+
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext* context,
+                                    ArrayRef<int64_t> values) {
+  return DenseIntElementsAttr::get(
+      RankedTensorType::get({static_cast<int64_t>(values.size()) / 2, 2},
+                            IntegerType::get(context, 64)),
+      values);
+}
+
+DenseIntElementsAttr getPaddingAttr(Builder* builder,
+                                    ArrayRef<int64_t> values) {
+  return getPaddingAttr(builder->getContext(), values);
+}
+
+}  // namespace hlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/Base.h b/stablehlo/stablehlo/experimental/dialect/Base.h
--- stablehlo/stablehlo/experimental/dialect/Base.h
+++ stablehlo/stablehlo/experimental/dialect/Base.h
@@ -0,0 +1,35 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2022 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
+
+#include "llvm/ADT/ArrayRef.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+
+namespace mlir {
+namespace hlo {
+
+DenseIntElementsAttr getPaddingAttr(MLIRContext *context,
+                                    ArrayRef<int64_t> value);
+DenseIntElementsAttr getPaddingAttr(Builder *builder, ArrayRef<int64_t> value);
+
+}  // namespace hlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_BASE_H
diff --ruN a/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt b/stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
--- stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/dialect/CMakeLists.txt
@@ -0,0 +1,42 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+add_mlir_library(ExperimentalStablehloBase
+  PARTIAL_SOURCES_INTENDED
+  Base.cpp
+
+  LINK_LIBS PUBLIC
+  MLIRIR
+)
+
+add_mlir_dialect_library(ExperimentalStablehloOps
+  PARTIAL_SOURCES_INTENDED
+  StablehloOps.cpp
+
+  DEPENDS
+  StablehloOpsIncGen
+
+  LINK_LIBS PUBLIC
+  ExperimentalStablehloBase
+  MLIRFuncDialect
+  MLIRIR
+  MLIRSupport
+  StablehloOps
+)
+
+target_include_directories(ExperimentalStablehloOps INTERFACE
+  $<BUILD_INTERFACE:${STABLEHLO_SOURCE_DIR}>
+  $<BUILD_INTERFACE:${STABLEHLO_BINARY_DIR}>
+)
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp b/stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.cpp
@@ -0,0 +1,506 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+
+#include <optional>
+
+#include "llvm/ADT/ArrayRef.h"
+#include "llvm/ADT/STLExtras.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/Types.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+LogicalResult DynamicReduceWindowOpAdaptor::verify() {
+  // Before checking the constraints inherited from ReduceWindowOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2 * op_->getNumResults() + 5)
+    return op_.emitError("expects size(operands) = 2 * size(results) + 5");
+  if (op_->getNumResults() == 0)
+    return op_.emitError("expects size(results) > 0");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_reduce_window".
+    // called_computations carries the body.
+    if (attr.getName() != "api_version" &&
+        attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "called_computations")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_reduce_window")
+    return op_.emitError() << "expects @stablehlo.dynamic_reduce_window";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto numInputs = getInputs().size();
+  auto inputs = op_.getInputs().slice(0, numInputs);
+  auto initValues = op_.getInputs().slice(numInputs, numInputs);
+  auto windowDimensions = op_.getInputs()[op_.getInputs().size() - 5];
+  auto windowStrides = op_.getInputs()[op_.getInputs().size() - 4];
+  auto baseDilations = op_.getInputs()[op_.getInputs().size() - 3];
+  auto windowDilations = op_.getInputs()[op_.getInputs().size() - 2];
+  auto padding = op_.getInputs()[op_.getInputs().size() - 1];
+  auto results = op_.getResults();
+
+  // reduce_window_c1
+  // This constraint hold automatically thanks to the checks that we have
+  // performed above.
+
+  // reduce_window_i1
+  SmallVector<ShapedType> inputTypes;
+  for (auto [index, input] : llvm::enumerate(inputs)) {
+    auto inputType = input.getType().dyn_cast<ShapedType>();
+    inputTypes.push_back(inputType);
+    if (!inputType)
+      return op_.emitError()
+             << "expects inputs (e.g. operand #" << index << ") to be tensors";
+  }
+
+  // reduce_window_i2
+  SmallVector<ShapedType> initValueTypes;
+  for (auto [index, initValue] : llvm::enumerate(initValues)) {
+    auto initValueType = initValue.getType().dyn_cast<ShapedType>();
+    initValueTypes.push_back(initValueType);
+    if (!initValueType || !initValueType.hasRank() ||
+        initValueType.getRank() != 0)
+      return op_.emitError() << "expects init_values (e.g. operand #"
+                             << numInputs + index << ") "
+                             << "to be 0-dimensional tensors";
+  }
+
+  // reduce_window_i3...reduce_window_i7
+  auto checkRank = [&](StringRef name, int64_t index, Value dynamicAttr,
+                       int64_t expectedRank) -> LogicalResult {
+    auto type = dynamicAttr.getType().dyn_cast<ShapedType>();
+    if (!type || !type.hasRank() || type.getRank() != expectedRank ||
+        !type.getElementType().isIntOrIndex()) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to be a " << expectedRank << "-dimensional tensor "
+             << "of integer or index type";
+    }
+    return success();
+  };
+  if (failed(checkRank("window_dimensions", -5, windowDimensions, 1)) ||
+      failed(checkRank("window_strides", -4, windowStrides, 1)) ||
+      failed(checkRank("base_dilations", -3, baseDilations, 1)) ||
+      failed(checkRank("window_dilations", -2, windowDilations, 1)) ||
+      failed(checkRank("padding", -1, padding, 2)))
+    return failure();
+
+  // reduce_window_i7
+  auto paddingType = getPadding().getType().dyn_cast<ShapedType>();
+  if (!paddingType || !paddingType.hasRank() || paddingType.getRank() != 2 ||
+      paddingType.getDimSize(1) != 2 ||
+      !paddingType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects padding_type (operand #" << op_.getNumOperands() - 1
+           << ") to be a 2-dimensional tensor of integer or index type";
+
+  // reduce_window_c2
+  std::optional<ArrayRef<int64_t>> inputShape;
+  for (auto inputType : inputTypes) {
+    if (!inputType.hasRank()) continue;
+    if (!inputShape) inputShape = inputType.getShape();
+    if (failed(verifyCompatibleShape(inputType.getShape(), *inputShape)))
+      return op_.emitError() << "expects all inputs (operands 0.." << numInputs
+                             << ") to have compatible shapes";
+  }
+
+  // reduce_window_c3
+  for (auto [inputType, initValueType] :
+       llvm::zip(inputTypes, initValueTypes)) {
+    if (inputType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects inputs (operands 0.." << numInputs
+                             << ") and init_values (operands " << numInputs
+                             << ".." << numInputs * 2 << ") to have pairwise "
+                             << "the same element types";
+  }
+
+  // reduce_window_c4...reduce_window_c12
+  // In this range, we only verify the constraints with even numbers.
+  // Verifying the constraints with odd numbers would require knowing the
+  // actual values of window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+  auto checkShape = [&](StringRef name, int64_t index, Value dynamicAttr,
+                        ArrayRef<int64_t> expectedShape) -> LogicalResult {
+    auto type = dynamicAttr.getType().cast<ShapedType>();
+    if (type.getShape() != expectedShape) {
+      if (index < 0) index += op_->getNumOperands();
+      return op_.emitError()
+             << "expects " << name << " (operand #" << index << ") "
+             << "to have shape [" << expectedShape << "]";
+    }
+    return success();
+  };
+  if (inputShape) {
+    auto inputRank = static_cast<int64_t>(inputShape->size());
+    if (failed(checkShape("window_dimensions", -5, windowDimensions,
+                          {inputRank})) ||
+        failed(checkShape("window_strides", -4, windowStrides, {inputRank})) ||
+        failed(checkShape("base_dilations", -3, baseDilations, {inputRank})) ||
+        failed(
+            checkShape("window_dilations", -2, windowDilations, {inputRank})) ||
+        failed(checkShape("padding", -1, padding, {inputRank, 2})))
+      return failure();
+  }
+
+  // reduce_window_c13
+  if (op_.getCalledComputations().size() != 1)
+    return op_.emitError() << "expects called_computations to have 1 element";
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  if (!bodyFunc)
+    return op_.emitError() << "expects called_computations to refer to "
+                           << "a function that exists within a parent module";
+
+  // reduce_window_c13
+  SmallVector<Type> expectedBodyInputs;
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  llvm::append_range(expectedBodyInputs, initValueTypes);
+  SmallVector<Type> expectedBodyOutputs;
+  llvm::append_range(expectedBodyOutputs, initValueTypes);
+  auto expectedBodyType = FunctionType::get(
+      op_.getContext(), expectedBodyInputs, expectedBodyOutputs);
+  if (bodyFunc.getFunctionType() != expectedBodyType)
+    return op_.emitError() << "expects body to have type " << expectedBodyType;
+
+  // reduce_window_c14
+  SmallVector<ShapedType> resultTypes;
+  std::optional<ArrayRef<int64_t>> resultShape;
+  for (auto result : results) {
+    auto resultType = result.getType().dyn_cast<ShapedType>();
+    resultTypes.push_back(resultType);
+    if (!resultType) return op_.emitError() << "expects results to be tensors";
+
+    if (!resultType.hasRank()) continue;
+    if (!resultShape) resultShape = resultType.getShape();
+    if (failed(verifyCompatibleShape(resultType.getShape(), *resultShape)))
+      return op_.emitError() << "expects all results to have compatible shapes";
+  }
+
+  // reduce_window_c15
+  // Verifying this constraint would require knowing the actual values of
+  // window_dimensions, window_strides, etc.
+  // While we certainly can try to check whether they are constants and
+  // verify them in that case, that seems like too much at this point.
+
+  // reduce_window_c16
+  for (auto [resultType, initValueType] :
+       llvm::zip(resultTypes, initValueTypes)) {
+    if (resultType.getElementType() != initValueType.getElementType())
+      return op_.emitError() << "expects results and init_values (operands "
+                             << numInputs << ".." << numInputs * 2 << ") "
+                             << "to have pairwise the same element types";
+  }
+
+  return success();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInputs() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(0, numInputs);
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getInitValues() {
+  auto numInputs = (op_.getInputs().size() - 5) / 2;
+  return op_.getInputs().slice(numInputs, numInputs);
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDimensions() {
+  return op_.getInputs()[op_.getInputs().size() - 5]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowStrides() {
+  return op_.getInputs()[op_.getInputs().size() - 4]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getBaseDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 3]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getWindowDilations() {
+  return op_.getInputs()[op_.getInputs().size() - 2]
+      .cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicReduceWindowOpAdaptor::getPadding() {
+  return op_.getInputs()[op_.getInputs().size() - 1]
+      .cast<TypedValue<ShapedType>>();
+}
+
+Region& DynamicReduceWindowOpAdaptor::getBody() {
+  auto bodyAttr = op_.getCalledComputations()[0].cast<FlatSymbolRefAttr>();
+  auto bodyFunc =
+      op_->getParentOfType<ModuleOp>().lookupSymbol<func::FuncOp>(bodyAttr);
+  return bodyFunc.getBody();
+}
+
+ValueRange DynamicReduceWindowOpAdaptor::getResults() {
+  return op_.getResults();
+}
+
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_reduce_window") return {};
+  return DynamicReduceWindowOpAdaptor(op);
+}
+
+LogicalResult DynamicRngBitGeneratorOpAdaptor::verify() {
+  // Before checking the constraints inherited from RngBitGeneratorOp,
+  // make sure that the operands and the attributes of the underlying custom
+  // call make sense.
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_rng_bit_generator".
+    // rng_algorithm comes from the operation.
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name" &&
+        attr.getName() != "rng_algorithm")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return op_.emitError() << "expects @stablehlo.dynamic_rng_bit_generator";
+  if (!op_->hasAttr("rng_algorithm"))
+    return op_.emitError() << "expects an rng_algorithm";
+
+  // Unpack operands and attributes of the underlying custom call into
+  // operation-specific inputs.
+  auto rngAlgorithmAttr = op_->getAttr("rng_algorithm");
+  auto initialState = op_.getInputs()[0];
+  auto outputShape = op_.getInputs()[1];
+  auto outputState = op_.getResults()[0];
+  auto output = op_.getResults()[1];
+
+  // dynamic_rng_bit_generator_i1
+  if (!rngAlgorithmAttr.isa<RngAlgorithmAttr>())
+    return op_.emitError()
+           << "expects a #stablehlo<rng_algorithm ...> rng_algorithm";
+
+  // dynamic_rng_bit_generator_i2
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto initialStateType = initialState.getType().dyn_cast<ShapedType>();
+  if (!initialStateType || !initialStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects initial_state (operand #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_i3
+  auto outputShapeType = outputShape.getType().dyn_cast<ShapedType>();
+  if (!outputShapeType || !outputShapeType.hasRank() ||
+      outputShapeType.getRank() != 1 ||
+      !outputShapeType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects output_shape (operand #1) "
+           << "to be a 1-dimensional tensor of integer or index type";
+
+  // dynamic_rng_bit_generator_o1
+  // TODO(#643): Clarify supported types for RngBitGeneratorOp.
+  auto outputStateType = outputState.getType().dyn_cast<ShapedType>();
+  if (!outputStateType || !outputStateType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output_state (result #0) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_o2
+  auto outputType = output.getType().dyn_cast<ShapedType>();
+  if (!outputType || !outputType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects output (result #1) "
+           << "to be a tensor of integer or floating-point type";
+
+  // dynamic_rng_bit_generator_c1
+  if (!hlo::isCompatibleForHloTypeInference(initialStateType, outputStateType))
+    return op_.emitError()
+           << "expects initial_state (operand #0) and output_state (result #0) "
+           << "to have compatible shapes";
+
+  // dynamic_rng_bit_generator_c2
+  // TODO(#486): Verify rng_algorithm in RngBitGeneratorOp.
+
+  // dynamic_rng_bit_generator_c3
+  if (!hlo::isCompatibleForHloTypeInference(outputShape, outputType))
+    return op_.emitError() << "expects output (result #1) to have shape  "
+                           << "compatible with output_shape (operand #2)";
+
+  return success();
+}
+
+RngAlgorithm DynamicRngBitGeneratorOpAdaptor::getRngAlgorithm() {
+  return op_->getAttr("rng_algorithm").cast<RngAlgorithmAttr>().getValue();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getInitialState() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputShape() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutputState() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicRngBitGeneratorOpAdaptor::getOutput() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_rng_bit_generator")
+    return {};
+  return DynamicRngBitGeneratorOpAdaptor(op);
+}
+
+LogicalResult DynamicTopKOpAdaptor::verify() {
+  if (op_->getNumOperands() != 2)
+    return op_.emitError("expects size(operands) = 2");
+  if (op_->getNumResults() != 2)
+    return op_.emitError("expects size(results) = 2");
+  for (const auto& attr : op_->getAttrs()) {
+    // api_version and backend_config have default values.
+    // call_target_name should be "stablehlo.dynamic_top_k".
+    if (attr.getName() != "api_version" && attr.getName() != "backend_config" &&
+        attr.getName() != "call_target_name")
+      return op_.emitError()
+             << attr.getName() << " is not a supported attribute";
+  }
+  if (!op_.getBackendConfig().empty())
+    return op_.emitError() << "expects an empty backend_config";
+  if (op_.getCallTargetName() != "stablehlo.dynamic_top_k")
+    return op_.emitError() << "expects @stablehlo.dynamic_top_k";
+
+  auto operand = op_.getInputs()[0];
+  auto k = op_.getInputs()[1];
+  auto values = op_.getResults()[0];
+  auto indices = op_.getResults()[1];
+
+  // dynamic_top_k_i1
+  auto operandType = operand.getType().dyn_cast<ShapedType>();
+  if (!operandType || !operandType.hasRank() || operandType.getRank() < 1 ||
+      !operandType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects operand #0 "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_i2
+  auto kType = k.getType().dyn_cast<ShapedType>();
+  if (!kType || !kType.hasRank() ||
+      kType.getRank() != 0 || !kType.getElementType().isIntOrIndex())
+    return op_.emitError()
+           << "expects k (operand #1) "
+           << "to be a 0-dimensional tensor of integer or index type";
+
+  // dynamic_top_k_o1
+  auto valuesType = values.getType().dyn_cast<ShapedType>();
+  if (!valuesType || !valuesType.hasRank() || valuesType.getRank() < 1 ||
+      !valuesType.getElementType().isIntOrFloat())
+    return op_.emitError()
+           << "expects values (result #0) "
+           << "to be a tensor of integer or floating-point type "
+           << "of rank at least 1";
+
+  // dynamic_top_k_o2
+  auto indicesType = indices.getType().dyn_cast<ShapedType>();
+  if (!indicesType || !indicesType.hasRank() || indicesType.getRank() < 1 ||
+      !indicesType.getElementType().isSignlessInteger(32))
+    return op_.emitError() << "expects indices (result #1) "
+                           << "to be a tensor of si32 of rank at least 1";
+
+  // dynamic_top_k_c1
+  auto operandLastDim = operandType.getRank() - 1;
+  SmallVector<int64_t> expectedValuesShape(operandType.getShape());
+  expectedValuesShape[operandLastDim] =
+      valuesType.getDimSize(valuesType.getRank() - 1);
+  if (failed(verifyCompatibleShape(expectedValuesShape, valuesType.getShape())))
+    return op_.emitError() << "expects the values shape to match the operand "
+                              "shape in all but the last dimension";
+
+  // dynamic_top_k_c2
+  if (valuesType.getElementType() != operandType.getElementType())
+    return op_.emitError()
+           << "expects the values element type to be the same as the operand "
+           << "element type";
+
+  // dynamic_top_k_c3
+  if (!operandType.isDynamicDim(operandLastDim) &&
+      !valuesType.isDynamicDim(operandLastDim) &&
+      operandType.getDimSize(operandLastDim) <
+          valuesType.getDimSize(operandLastDim))
+    return op_.emitError() << "expects the values last dimension to have size "
+                              "at least as large "
+                           << "as operand last dimension";
+
+  // dynamic_top_k_c4
+  if (failed(
+          verifyCompatibleShape(indicesType.getShape(), valuesType.getShape())))
+    return op_.emitError()
+           << "expects the indices shape to match the values shape";
+
+  return success();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getOperand() {
+  return op_.getInputs()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getK() {
+  return op_.getInputs()[1].cast<TypedValue<ShapedType>>();
+}
+
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getValues() {
+  return op_.getResults()[0].cast<TypedValue<ShapedType>>();
+}
+
+TypedValue<ShapedType> DynamicTopKOpAdaptor::getIndices() {
+  return op_.getResults()[1].cast<TypedValue<ShapedType>>();
+}
+
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(
+    CustomCallOp op) {
+  if (op.getCallTargetName() != "stablehlo.dynamic_top_k") return {};
+  return DynamicTopKOpAdaptor(op);
+}
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/dialect/StablehloOps.h b/stablehlo/stablehlo/experimental/dialect/StablehloOps.h
--- stablehlo/stablehlo/experimental/dialect/StablehloOps.h
+++ stablehlo/stablehlo/experimental/dialect/StablehloOps.h
@@ -0,0 +1,230 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+#define STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
+
+// This file supports XLA-specific experiments with the StableHLO opset.
+// These experiments are not yet ready to be upstreamed to openxla/stablehlo
+// and are incubating towards the respective StableHLO RFCs.
+//
+// Custom calls (which are the implementation vehicle of these experiments)
+// don't have compatibility guarantees within the StableHLO process, but
+// the StableHLO team at Google provides out-of-band guarantees for these
+// custom calls, with the same compatibility window as StableHLO upstream.
+
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
+#include "mlir/IR/Value.h"
+#include "mlir/IR/ValueRange.h"
+#include "mlir/Support/LogicalResult.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/Base.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+// The DynamicReduceWindowOp experiment provides a dynamic version of
+// ReduceWindowOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicReduceWindowOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_reduce_window` custom call.
+// This custom call has the following operands which represent a dynamic version
+// of operands and attributes of ReduceWindowOp:
+//   * [0:N]   => inputs
+//   * [N:2*N] => init_values
+//   * [-5]    => window_dimensions
+//   * [-4]    => window_strides
+//   * [-3]    => base_dilations
+//   * [-2]    => window_dilations
+//   * [-1]    => padding
+// Additionally, to represent the body of DynamicReduceWindowOp, the custom call
+// has a satellite function attached to the custom call via called_computations.
+//
+// Semantics of DynamicReduceWindowOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#reduce_window
+// with the following exceptions:
+//   1) All tensor constants, i.e. window_dimensions, window_strides,
+//      base_dilations, window_dilations and padding, become tensors of
+//      integer type.
+//   2) As a result, some of the constraints can no longer be validated
+//      statically. However, this operation still expects these constraints
+//      to hold dynamically, and if they don't hold, the behavior is undefined.
+class DynamicReduceWindowOpAdaptor {
+ public:
+  DynamicReduceWindowOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::ReduceWindowOp, except that all the
+  // std::optional<DenseIntElementsAttr> attributes have turned into values.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  ValueRange getInputs();
+  ValueRange getInitValues();
+  TypedValue<ShapedType> getWindowDimensions();
+  TypedValue<ShapedType> getWindowStrides();
+  TypedValue<ShapedType> getBaseDilations();
+  TypedValue<ShapedType> getWindowDilations();
+  TypedValue<ShapedType> getPadding();
+  Region& getBody();
+  ValueRange getResults();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicReduceWindowOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_reduce_window".
+std::optional<DynamicReduceWindowOpAdaptor> getDynamicReduceWindowOp(
+    CustomCallOp op);
+
+// The DynamicRngBitGeneratorOp experiment provides a dynamic version of
+// RngBitGeneratorOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicRngBitGeneratorOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator` custom call.
+// This custom call has the regular operand of RngBitGeneratorOp plus an
+// additional `output_shape` operand that determines the shape of the output:
+//   * [0] => initial_state
+//   * [1] => output_shape
+//
+// Semantics of DynamicRngBitGeneratorOp are inherited from semantics of
+// https://github.com/openxla/stablehlo/blob/main/docs/spec.md#rng_bit_generator
+// extended with an additional input (I3) and an additional constraint (C3):
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `rng_algorithm` | enum of `DEFAULT`, `THREE_FRY`, and `PHILOX` |
+// | (I2)  | `initial_state` | 1-dimensional tensor of type `ui64`          |
+// | (I3)  | `output_shape`  | 1-dimensional tensor of integer type         |
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `output_state` | 1-dimensional tensor of type `ui64`      |
+// | `output`       | tensor of integer or floating-point type |
+//
+// #### Constraints
+//
+// * (C1) `type(initial_state) = type(output_state)`.
+// * (C2) `size(initial_state)` is defined as:
+//   * implementation-defined if `rng_algorithm = DEFAULT`.
+//   * `2` if `rng_algorithm = THREE_FRY`.
+//   * `2` or `3` if `rng_algorithm = PHILOX`.
+// * (C3) `shape(output) = output_shape`.
+class DynamicRngBitGeneratorOpAdaptor {
+ public:
+  DynamicRngBitGeneratorOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // Same accessors as for stablehlo::RngBitGeneratorOp, extended with the
+  // additional `output_shape` operand.
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  RngAlgorithm getRngAlgorithm();
+  TypedValue<ShapedType> getInitialState();
+  TypedValue<ShapedType> getOutputShape();
+  TypedValue<ShapedType> getOutputState();
+  TypedValue<ShapedType> getOutput();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicRngBitGeneratorOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_rng_bit_generator".
+std::optional<DynamicRngBitGeneratorOpAdaptor> getDynamicRngBitGeneratorOp(
+    CustomCallOp op);
+
+// The DynamicTopKOp experiment provides a dynamic version of
+// TopKOp. Once the dynamism RFC is figured out, we expect to have an
+// upstream representation for this notion.
+//
+// Within this experiment, DynamicTopKOp is represented via the
+// `stablehlo.custom_call @stablehlo.dynamic_top_k` custom call.
+// This custom call has the regular operand of TopKOp plus an
+// additional `k` operand that determines the shape of the output.
+//
+// Semantics of DynamicTopKOp are inherited from semantics of Chlo.TopKOp.
+//
+// #### Inputs
+//
+// | Label | Name            | Type                                         |
+// |-------|-----------------|----------------------------------------------|
+// | (I1)  | `operand`       | tensor of integer or floating-point type     |
+// | (I2)  | `k`             | 0-dimensional tensor of integer or index type|
+//
+// #### Outputs
+//
+// | Name           | Type                                     |
+// |----------------|------------------------------------------|
+// | `values`       | tensor of integer or floating-point type |
+// | `indices`      | tensor of si32 type                      |
+//
+// #### Constraints
+//
+// * (C1) `shape(values)[:-1] = shape(operand)[:-1]`
+// * (C2) `element_type(values) = element_type(operand)`
+// * (C3) `shape(values)[-1] <= shape(operand)[-1]`
+// * (C4) `shape(indices) = shape(values)`
+class DynamicTopKOpAdaptor {
+ public:
+  DynamicTopKOpAdaptor(CustomCallOp op) : op_(op) {}
+  operator Operation*() { return op_; }
+  Operation* operator->() { return op_; }
+
+  // These accessors assume that the operation is well-formed (i.e. that it
+  // can pass verification).
+  TypedValue<ShapedType> getOperand();
+  TypedValue<ShapedType> getK();
+  TypedValue<ShapedType> getValues();
+  TypedValue<ShapedType> getIndices();
+
+  // Verifies the constraints documented above.
+  // Emits errors if errors are detected.
+  LogicalResult verify();
+
+ private:
+  CustomCallOp op_;
+};
+
+// Wraps a custom call in a DynamicTopKOpAdaptor.
+// Fails if the call_target_name of the custom call doesn't match
+// "stablehlo.dynamic_top_k".
+std::optional<DynamicTopKOpAdaptor> getDynamicTopKOp(CustomCallOp op);
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_DIALECT_STABLEHLO_OPS_H
diff --ruN a/stablehlo/stablehlo/experimental/tests/BUILD.bazel b/stablehlo/stablehlo/experimental/tests/BUILD.bazel
--- stablehlo/stablehlo/experimental/tests/BUILD.bazel
+++ stablehlo/stablehlo/experimental/tests/BUILD.bazel
@@ -0,0 +1,58 @@
+# Copyright 2023 The StableHLO Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+load("@bazel_skylib//rules:expand_template.bzl", "expand_template")
+load("@llvm-project//llvm:lit_test.bzl", "lit_test", "package_path")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+# Equivalent of configure_lit_site_cfg from CMakeLists.txt.
+expand_template(
+    name = "lit_site_cfg_py_gen",
+    testonly = True,
+    out = "lit.site.cfg.py",
+    substitutions = {
+        "@LIT_SITE_CFG_IN_HEADER@": "# Autogenerated, do not edit.",
+        "@LLVM_TOOLS_DIR@": package_path("@llvm-project//llvm:BUILD"),
+        "\"@STABLEHLO_TOOLS_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+        "\"@STABLEHLO_SOURCE_DIR@\"": "os.path.join(os.environ['TEST_SRCDIR'], 'stablehlo')",
+    },
+    template = "lit.site.cfg.py.in",
+)
+
+# Equivalent of add_lit_testsuite from CMakeLists.txt.
+[
+    lit_test(
+        name = "%s.test" % src,
+        size = "small",
+        srcs = [src],
+        data = [
+            "lit.cfg.py",
+            "lit.site.cfg.py",
+            "//:stablehlo-opt",
+            "//:stablehlo-translate",
+            "@llvm-project//llvm:FileCheck",
+            "@llvm-project//llvm:not",
+        ] + glob(["%s.bc" % src]),
+        tags = ["stablehlo_tests"],
+    )
+    for src in glob(["**/*.mlir"])
+]
+
+test_suite(
+    name = "experimental_stablehlo_tests",
+    tags = ["experimental_stablehlo_tests"],
+)
diff --ruN a/stablehlo/stablehlo/experimental/tests/CMakeLists.txt b/stablehlo/stablehlo/experimental/tests/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tests/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tests/CMakeLists.txt
@@ -0,0 +1,29 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+configure_lit_site_cfg(
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.site.cfg.py.in
+  ${CMAKE_CURRENT_BINARY_DIR}/lit.site.cfg.py
+  MAIN_CONFIG
+  ${CMAKE_CURRENT_SOURCE_DIR}/lit.cfg.py
+)
+add_lit_testsuite(check-experimental-stablehlo-tests "Running the experimental/tests/ suite"
+  ${CMAKE_CURRENT_BINARY_DIR}
+  DEPENDS
+  FileCheck
+  experimental-stablehlo-opt
+  stablehlo-translate
+)
+add_dependencies(check-stablehlo-quick check-experimental-stablehlo-tests)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.cfg.py b/stablehlo/stablehlo/experimental/tests/lit.cfg.py
--- stablehlo/stablehlo/experimental/tests/lit.cfg.py
+++ stablehlo/stablehlo/experimental/tests/lit.cfg.py
@@ -0,0 +1,42 @@
+"""Lit configuration to drive test in this repo."""
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# -*- Python -*-
+# pylint: disable=undefined-variable
+
+import os
+
+import lit.formats
+from lit.llvm import llvm_config
+
+# Populate Lit configuration with the minimal required metadata.
+# Some metadata is populated in lit.site.cfg.py.in.
+config.name = 'STABLEHLO_TESTS_SUITE'
+config.test_format = lit.formats.ShTest(not llvm_config.use_lit_shell)
+config.suffixes = ['.mlir']
+config.test_source_root = os.path.dirname(__file__)
+
+# Make LLVM and StableHLO tools available in RUN directives
+tools = [
+  'FileCheck',
+  'experimental-stablehlo-opt',
+  'stablehlo-translate',
+  'not',
+]
+tool_dirs = [
+  config.llvm_tools_dir,
+  config.stablehlo_tools_dir,
+]
+llvm_config.add_tool_substitutions(tools, tool_dirs)
diff --ruN a/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in b/stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
--- stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
+++ stablehlo/stablehlo/experimental/tests/lit.site.cfg.py.in
@@ -0,0 +1,21 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+@LIT_SITE_CFG_IN_HEADER@
+
+import lit.llvm
+lit.llvm.initialize(lit_config, config)
+config.llvm_tools_dir = "@LLVM_TOOLS_DIR@"
+config.stablehlo_tools_dir = "@STABLEHLO_TOOLS_DIR@"
+lit_config.load_config(config, "@STABLEHLO_SOURCE_DIR@" + "/stablehlo/experimental/tests/lit.cfg.py")
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_canonicalize_dynamism.mlir
@@ -0,0 +1,344 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-canonicalize-dynamism --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_static_result_type
+func.func @dynamic_reduce_window_success_static_result_type(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<2x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = dense<[2, 1]> : tensor<2xi64>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = dense<[3, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_dimensions = dense<[2, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_strides = dense<[4, 1]> : tensor<2xi64>
+  //          CHECK-SAME: } : (tensor<3x2xf32>, tensor<f32>) -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %5 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_success_dynamic_result_type
+func.func @dynamic_reduce_window_success_dynamic_result_type(%arg0: tensor<?x2xf32>, %arg1: tensor<f32>) -> tensor<?x2xf32> {
+  //           CHECK-NOT: stablehlo.dynamic_reduce_window
+  //               CHECK: "stablehlo.reduce_window"(%arg0, %arg1) ({
+  //          CHECK-NEXT:   ^[[BB:bb.*]](%[[ARG2:arg.*]]: tensor<f32>, %[[ARG3:arg.*]]: tensor<f32>):
+  //          CHECK-NEXT:     %[[VAL1:.*]] = stablehlo.add %arg2, %arg3 : tensor<f32>
+  //          CHECK-NEXT:     stablehlo.return %[[VAL1]] : tensor<f32>
+  //          CHECK-NEXT: }) {
+  //          CHECK-SAME:   base_dilations = dense<[2, 1]> : tensor<2xi64>,
+  // CHECK-SAME{LITERAL}:   padding = dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>,
+  //          CHECK-SAME:   window_dilations = dense<[3, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_dimensions = dense<[2, 1]> : tensor<2xi64>,
+  //          CHECK-SAME:   window_strides = dense<[4, 1]> : tensor<2xi64>
+  //          CHECK-SAME: } : (tensor<?x2xf32>, tensor<f32>) -> tensor<?x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<?x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<?x2xf32>
+  func.return %5 : tensor<?x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_reduce_window.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dimensions(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %arg2, %0, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_strides
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_strides(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %arg2, %1, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_base_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_base_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %arg2, %2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_window_dilations
+func.func @dynamic_reduce_window_inapplicable_dynamic_window_dilations(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %arg2, %3) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_reduce_window_inapplicable_dynamic_padding
+func.func @dynamic_reduce_window_inapplicable_dynamic_padding(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>, %arg2: tensor<2x2xi64>) -> tensor<2x2xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %arg2) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<2x2xf32>
+  func.return %4 : tensor<2x2xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_success
+func.func @dynamic_rng_bit_generator_success(%arg0: tensor<2xui64>) -> tensor<1x4xf32> {
+  // CHECK-NOT: stablehlo.dynamic_rng_bit_generator
+  // CHECK: stablehlo.rng_bit_generator %arg0, algorithm = DEFAULT : (tensor<2xui64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// TODO(burmako): Implement tests for verification failures for dynamic_rng_bit_generator.
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_shape(%arg0: tensor<2xui64>, %arg1: tensor<2xi64>) -> tensor<1x4xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %arg1) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<1x4xf32>)
+  return %1#1 : tensor<1x4xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type
+func.func @dynamic_rng_bit_generator_inapplicable_dynamic_output_type(%arg0: tensor<2xui64>) -> tensor<?x?xf32> {
+  // CHECK: stablehlo.dynamic_rng_bit_generator
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<2xui64>, tensor<?x?xf32>)
+  return %1#1 : tensor<?x?xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_success
+func.func @dynamic_top_k_success(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: chlo.top_k
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// CHECK-LABEL: func @dynamic_top_k_failure_k_mismatch
+func.func @dynamic_top_k_failure_k_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // CHECK: @stablehlo.dynamic_top_k
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_not_float
+func.func @dynamic_top_k_error_operand_not_float(%arg0: tensor<16xcomplex<f64>>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xcomplex<f64>>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_operand_unranked
+func.func @dynamic_top_k_error_operand_unranked(%arg0: tensor<*xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<*xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I1
+// CHECK-LABEL: func @dynamic_top_k_error_scalar_operand
+func.func @dynamic_top_k_error_scalar_operand(%arg0: tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects operand #0 to be a tensor of integer or floating-point type of rank at least 1}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<f32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_integer
+func.func @dynamic_top_k_error_k_not_integer(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3.> : tensor<f32>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<f32>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k I2
+// CHECK-LABEL: func @dynamic_top_k_error_k_not_scalar
+func.func @dynamic_top_k_error_k_not_scalar(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects k (operand #1) to be a 0-dimensional tensor of integer or index type}}
+  %k = stablehlo.constant dense<3> : tensor<1xui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<1xui64>) -> (tensor<3xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O1
+// CHECK-LABEL: func @dynamic_top_k_error_values_not_float
+func.func @dynamic_top_k_error_values_not_float(%arg0: tensor<16xf32>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>) {
+  // expected-error@+2{{expects values (result #0) to be a tensor of integer or floating-point type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xcomplex<f64>>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xcomplex<f64>>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k O2
+// CHECK-LABEL: func @dynamic_top_k_error_indices_not_i32
+func.func @dynamic_top_k_error_indices_not_i32(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<3xi64>) {
+  // expected-error@+2{{expects indices (result #1) to be a tensor of si32}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<3xi64>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<3xi64>
+}
+
+// -----
+
+// dynamic_top_k C1
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_rank
+func.func @dynamic_top_k_error_values_bad_rank(%arg0: tensor<16xf32>) -> (tensor<3x4xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values shape to match the operand shape in all but the last dimension}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3x4xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3x4xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C2
+// CHECK-LABEL: func @dynamic_top_k_error_values_bad_element_type
+func.func @dynamic_top_k_error_values_bad_element_type(%arg0: tensor<16xf32>) -> (tensor<3xf64>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values element type to be the same as the operand element type}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf64>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<3xf64>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C3
+// CHECK-LABEL: func @dynamic_top_k_error_values_last_dim_too_large
+func.func @dynamic_top_k_error_values_last_dim_too_large(%arg0: tensor<16xf32>) -> (tensor<17xf32>, tensor<3xi32>) {
+  // expected-error@+2{{expects the values last dimension to have size at least as large as operand last dimension}}
+  %k = stablehlo.constant dense<17> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<17xf32>, tensor<3xi32>)
+  return %1#0, %1#1 : tensor<17xf32>, tensor<3xi32>
+}
+
+// -----
+
+// dynamic_top_k C4
+// CHECK-LABEL: func @dynamic_top_k_error_indices_shape_mismatch
+func.func @dynamic_top_k_error_indices_shape_mismatch(%arg0: tensor<16xf32>) -> (tensor<3xf32>, tensor<4xi32>) {
+  // expected-error@+2{{expects the indices shape to match the values shape}}
+  %k = stablehlo.constant dense<3> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<3xf32>, tensor<4xi32>)
+  return %1#0, %1#1 : tensor<3xf32>, tensor<4xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/experimental/tests/stablehlo_refine_shapes.mlir
@@ -0,0 +1,42 @@
+// RUN: experimental-stablehlo-opt --experimental-stablehlo-refine-shapes --split-input-file --verify-diagnostics %s | FileCheck %s
+
+// CHECK-LABEL: @main
+func.func @main(%arg0: tensor<3x2xf32>, %arg1: tensor<f32>) -> tensor<*xf32> {
+  // CHECK: stablehlo.dynamic_reduce_window{{.*}} -> tensor<2x2xf32>
+  %0 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %1 = stablehlo.constant dense<[4, 1]> : tensor<2xi64>
+  %2 = stablehlo.constant dense<[2, 1]> : tensor<2xi64>
+  %3 = stablehlo.constant dense<[3, 1]> : tensor<2xi64>
+  %4 = stablehlo.constant dense<[[2, 1], [0, 0]]> : tensor<2x2xi64>
+  %5 = stablehlo.custom_call @stablehlo.dynamic_reduce_window(%arg0, %arg1, %0, %1, %2, %3, %4) {
+    called_computations = [@dynamic_reduce_window0]
+  } : (tensor<3x2xf32>, tensor<f32>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2x2xi64>) -> tensor<*xf32>
+  func.return %5 : tensor<*xf32>
+}
+
+func.func private @dynamic_reduce_window0(%arg0: tensor<f32>, %arg1: tensor<f32>) -> tensor<f32> {
+  %0 = stablehlo.add %arg0, %arg1 : tensor<f32>
+  func.return %0 : tensor<f32>
+}
+
+// -----
+
+// CHECK-LABEL: @refine_dynamic_rng_bit_generator
+func.func @refine_dynamic_rng_bit_generator(%arg0: tensor<2xui64>) -> (tensor<?xui64>, tensor<*xf32>) {
+  // CHECK: stablehlo.dynamic_rng_bit_generator{{.*}} -> (tensor<2xui64>, tensor<1x4xf32>)
+  %0 = stablehlo.constant dense<[1, 4]> : tensor<2xi64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_rng_bit_generator(%arg0, %0) {
+    rng_algorithm = #stablehlo<rng_algorithm DEFAULT>
+  } : (tensor<2xui64>, tensor<2xi64>) -> (tensor<?xui64>, tensor<*xf32>)
+  func.return %1#0, %1#1 : tensor<?xui64>, tensor<*xf32>
+}
+
+// -----
+
+// CHECK-LABEL: func @refine_dynamic_top_k
+func.func @refine_dynamic_top_k(%arg0: tensor<16xf32>) -> (tensor<?xf32>, tensor<?xi32>) {
+  // CHECK: stablehlo.dynamic_top_k{{.*}} -> (tensor<4xf32>, tensor<4xi32>)
+  %k = stablehlo.constant dense<4> : tensor<ui64>
+  %1:2 = stablehlo.custom_call @stablehlo.dynamic_top_k(%arg0, %k) : (tensor<16xf32>, tensor<ui64>) -> (tensor<?xf32>, tensor<?xi32>)
+  return %1#0, %1#1 : tensor<?xf32>, tensor<?xi32>
+}
diff --ruN a/stablehlo/stablehlo/experimental/tools/CMakeLists.txt b/stablehlo/stablehlo/experimental/tools/CMakeLists.txt
--- stablehlo/stablehlo/experimental/tools/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/tools/CMakeLists.txt
@@ -0,0 +1,41 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_OPTIONAL_SOURCES
+  StablehloOptMain.cpp
+)
+
+# stablehlo-opt
+get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)
+get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)
+get_property(extension_libs GLOBAL PROPERTY MLIR_EXTENSION_LIBS)
+set(LIBS
+        ${dialect_libs}
+        ${conversion_libs}
+        ${extension_libs}
+        ExperimentalStablehloPasses
+        MLIROptLib
+        StablehloRegister
+        StablehloTestUtils
+        StablehloPasses
+        InterpreterOps
+        StablehloTOSATransforms
+        )
+add_llvm_executable(experimental-stablehlo-opt StablehloOptMain.cpp)
+llvm_update_compile_flags(experimental-stablehlo-opt)
+target_link_libraries(experimental-stablehlo-opt PRIVATE ${LIBS})
+
+mlir_check_all_link_libraries(experimental-stablehlo-opt)
+
diff --ruN a/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp b/stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
--- stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
+++ stablehlo/stablehlo/experimental/tools/StablehloOptMain.cpp
@@ -0,0 +1,46 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlir/Dialect/Tosa/IR/TosaOps.h"
+#include "mlir/Dialect/Tosa/Transforms/Passes.h"
+#include "mlir/InitAllDialects.h"
+#include "mlir/InitAllExtensions.h"
+#include "mlir/InitAllPasses.h"
+#include "mlir/Tools/mlir-opt/MlirOptMain.h"
+#include "stablehlo/conversions/tosa/transforms/Passes.h"
+#include "stablehlo/dialect/Register.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+#include "stablehlo/reference/InterpreterOps.h"
+#include "stablehlo/tests/TestUtils.h"
+#include "stablehlo/transforms/Passes.h"
+
+int main(int argc, char **argv) {
+  mlir::registerAllPasses();
+  mlir::hlo::registerAllTestPasses();
+  mlir::stablehlo::registerPassPipelines();
+  mlir::stablehlo::registerPasses();
+  mlir::stablehlo::experimental::registerPasses();
+  mlir::tosa::registerStablehloLegalizeToTosaPassPass();
+  mlir::tosa::registerStablehloPrepareForTosaPassPass();
+
+  mlir::DialectRegistry registry;
+  mlir::registerAllDialects(registry);
+  mlir::registerAllExtensions(registry);
+  mlir::stablehlo::registerAllDialects(registry);
+  registry.insert<mlir::stablehlo::interpreter::InterpreterDialect>();
+
+  return failed(
+      mlir::MlirOptMain(argc, argv, "Experimental StableHLO optimizer driver\n", registry));
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt b/stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
--- stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
+++ stablehlo/stablehlo/experimental/transforms/CMakeLists.txt
@@ -0,0 +1,38 @@
+# Copyright 2023 The StableHLO Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set(LLVM_TARGET_DEFINITIONS Passes.td)
+mlir_tablegen(Passes.h.inc -gen-pass-decls)
+add_public_tablegen_target(ExperimentalPassesIncGen)
+
+add_mlir_dialect_library(ExperimentalStablehloPasses
+  PARTIAL_SOURCES_INTENDED
+  StablehloCanonicalizeDynamism.cpp
+  StablehloRefineShapes.cpp
+
+  DEPENDS
+  ExperimentalPassesIncGen
+
+  LINK_LIBS PUBLIC
+  ChloOps
+  MLIRFuncDialect
+  MLIRIR
+  MLIRInferTypeOpInterface
+  MLIRSupport
+  MLIRTransformUtils
+  ExperimentalStablehloOps
+  StablehloBase
+  StablehloTypeInference
+  StablehloOps
+)
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.h b/stablehlo/stablehlo/experimental/transforms/Passes.h
--- stablehlo/stablehlo/experimental/transforms/Passes.h
+++ stablehlo/stablehlo/experimental/transforms/Passes.h
@@ -0,0 +1,37 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+#define STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
+
+#include <memory>
+
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+  
+#define GEN_PASS_DECL_STABLEHLOCANONICALIZEDYNAMISMPASS
+#define GEN_PASS_DECL_STABLEHLOREFINESHAPESPASS
+#define GEN_PASS_REGISTRATION
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
+
+#endif  // STABLEHLO_EXPERIMENTAL_TRANSFORMS_PASSES_H
diff --ruN a/stablehlo/stablehlo/experimental/transforms/Passes.td b/stablehlo/stablehlo/experimental/transforms/Passes.td
--- stablehlo/stablehlo/experimental/transforms/Passes.td
+++ stablehlo/stablehlo/experimental/transforms/Passes.td
@@ -0,0 +1,31 @@
+/* Copyright 2023 The StableHLO Authors.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+include "mlir/Pass/PassBase.td"
+
+def StablehloCanonicalizeDynamismPass : Pass<"experimental-stablehlo-canonicalize-dynamism", "func::FuncOp"> {
+  let summary = "(Experimental) Canonicalizes dynamic StableHLO ops into static ops.";
+  let description = [{
+    Experimental version of the --stablehlo-canonicalize-dynamism pass.
+  }];
+  let dependentDialects = ["mlir::chlo::ChloDialect"];
+}
+
+def StablehloRefineShapesPass : Pass<"experimental-stablehlo-refine-shapes", "ModuleOp"> {
+  let summary = "(Experimental) Refines shapes across a StableHLO program.";
+  let description = [{
+    Experimental version of the --stablehlo-refine-shapes pass.
+  }];
+}
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloCanonicalizeDynamism.cpp
@@ -0,0 +1,441 @@
+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+   Copyright 2023 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "llvm/ADT/DenseSet.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/SmallVector.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Matchers.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOCANONICALIZEDYNAMISMPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+struct CanonicalizeCustomCallOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<ShapedTypeComponents> refinements;
+    if (failed(hlo::getShapeRefinements(op.getLoc(), op, refinements)))
+      return rewriter.notifyMatchFailure(op, "expected valid refinements");
+    auto indicesAttr =
+        op->getAttr("indices_of_shape_operands").cast<DenseIntElementsAttr>();
+    DenseSet<int64_t> indices(indicesAttr.value_begin<int64_t>(),
+                              indicesAttr.value_end<int64_t>());
+
+    // Discard the indices_of_shape_operands attribute.
+    // We rely on the verification logic implemented in getShapeRefinements to
+    // make sure that its value is consistent with the result types.
+    // In the future, when we upgrade indices_of_shape_operands from an
+    // experiment to a full-fledged StableHLO feature, this logic will be moved
+    // to a proper verifier.
+    SmallVector<NamedAttribute> newAttrs;
+    for (auto attr : op->getAttrs()) {
+      if (attr.getName() == "indices_of_shape_operands") continue;
+      if (attr.getName() == "operand_layouts") {
+        // Drop the operand_layouts that correspond to indices_of_shape_operands
+        ArrayAttr operandLayouts = op.getOperandLayoutsAttr();
+        SmallVector<Attribute> newOperandLayouts;
+        for (unsigned i = 0; i < operandLayouts.size(); ++i) {
+          if (indices.contains(i)) continue;
+          newOperandLayouts.push_back(operandLayouts[i]);
+        }
+        attr = NamedAttribute(attr.getName(),
+                              rewriter.getArrayAttr(newOperandLayouts));
+      }
+      newAttrs.push_back(attr);
+    }
+
+    // Discard the operands that correspond to indices_of_shape_operands.
+    // We rely on the verification logic implemented in getShapeRefinements to
+    // make sure that: 1) these operands are static, 2) the values of these
+    // operands are consistent with the result types.
+    SmallVector<Value> newOperands;
+    auto resultIndex = 0;
+    for (auto& operand : op->getOpOperands()) {
+      if (indices.contains(operand.getOperandNumber())) {
+        auto resultType =
+            op->getResult(resultIndex).getType().dyn_cast<ShapedType>();
+        if (!resultType || !resultType.hasStaticShape())
+          return rewriter.notifyMatchFailure(op,
+                                             "expected static result types");
+        ++resultIndex;
+        continue;
+      }
+      newOperands.push_back(operand.get());
+    }
+    rewriter.replaceOpWithNewOp<CustomCallOp>(op, op.getResultTypes(),
+                                              newOperands, newAttrs);
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicBroadcastInDimOpPattern
+    : public OpRewritePattern<DynamicBroadcastInDimOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicBroadcastInDimOp op,
+                                PatternRewriter& rewriter) const override {
+    // This pattern discards the output_dimensions operand as well as the
+    // known_expanding_dimensions and known_nonexpanding_dimensions attributes.
+    // We rely on the verifier to make sure that their values are consistent
+    // with the result type.
+    if (!op.getOperand().getType().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static operand type");
+    if (!succeeded(hlo::matchInts(op.getOutputDimensions())))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static output_dimensions");
+    if (!op.getType().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static result type");
+    rewriter.replaceOpWithNewOp<BroadcastInDimOp>(
+        op, op.getType(), op.getOperand(), op.getBroadcastDimensions());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicConvOpPattern
+    : public OpRewritePattern<DynamicConvOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicConvOp op,
+                                PatternRewriter& rewriter) const override {
+    // ConvolutionOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> padding;
+    if (!succeeded(hlo::matchInts(op.getDPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto paddingAttr = DenseIntElementsAttr::get(
+        RankedTensorType::get({static_cast<int64_t>(padding.size()) / 2, 2},
+                              rewriter.getI64Type()),
+        padding);
+    rewriter.replaceOpWithNewOp<ConvolutionOp>(
+        op, op.getType(), op.getLhs(), op.getRhs(), op.getWindowStridesAttr(),
+        paddingAttr, op.getLhsDilationAttr(), op.getRhsDilationAttr(),
+        op.getWindowReversalAttr(), op.getDimensionNumbers(),
+        op.getFeatureGroupCount(), op.getBatchGroupCount(),
+        op.getPrecisionConfigAttr());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicGatherOpPattern
+    : public OpRewritePattern<DynamicGatherOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicGatherOp op,
+                                PatternRewriter& rewriter) const override {
+    // GatherOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> sliceSizes;
+    if (!succeeded(hlo::matchInts(op.getSliceSizes(), sliceSizes)))
+      return rewriter.notifyMatchFailure(op, "expected static slice_sizes");
+    rewriter.replaceOpWithNewOp<GatherOp>(
+        op, op.getType(), op.getOperand(), op.getStartIndices(),
+        op.getDimensionNumbersAttr(), rewriter.getI64TensorAttr(sliceSizes),
+        op.getIndicesAreSortedAttr());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicIotaOpPattern
+    : public OpRewritePattern<DynamicIotaOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicIotaOp op,
+                                PatternRewriter& rewriter) const override {
+    // This pattern discards the output_shape operand. We rely on the verifier
+    // to make sure that its value is consistent with result type.
+    SmallVector<int64_t> outputShape;
+    if (!succeeded(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getType().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static result type");
+    rewriter.replaceOpWithNewOp<IotaOp>(op, op.getType(),
+                                        op.getIotaDimension());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicPadOpPattern : public OpRewritePattern<DynamicPadOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicPadOp op,
+                                PatternRewriter& rewriter) const override {
+    // PadOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> edgePaddingLow, edgePaddingHigh, interiorPadding;
+    if (!succeeded(hlo::matchInts(op.getEdgePaddingLow(), edgePaddingLow)))
+      return rewriter.notifyMatchFailure(op, "expected static low");
+    if (!succeeded(hlo::matchInts(op.getEdgePaddingHigh(), edgePaddingHigh)))
+      return rewriter.notifyMatchFailure(op, "expected static high");
+    if (!succeeded(hlo::matchInts(op.getInteriorPadding(), interiorPadding)))
+      return rewriter.notifyMatchFailure(op, "expected static interior");
+    rewriter.replaceOpWithNewOp<PadOp>(
+        op, op.getType(), op.getOperand(), op.getPaddingValue(),
+        rewriter.getI64TensorAttr(edgePaddingLow),
+        rewriter.getI64TensorAttr(edgePaddingHigh),
+        rewriter.getI64TensorAttr(interiorPadding));
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // ReduceWindowOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op, "expected static window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op, "expected static base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected static window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected static padding");
+    auto newOp = rewriter.create<ReduceWindowOp>(
+        op->getLoc(), op->getResultTypes(), op.getInputs(), op.getInitValues(),
+        rewriter.getI64TensorAttr(windowDimensions),
+        rewriter.getI64TensorAttr(windowStrides),
+        rewriter.getI64TensorAttr(baseDilations),
+        rewriter.getI64TensorAttr(windowDilations),
+        hlo::getPaddingAttr(&rewriter, padding));
+
+    // Inline the called computation into newOp.
+    // This is somewhat annoying because we also have to rewrite the original
+    // func::ReturnOp into stablehlo::ReturnOp.
+    rewriter.cloneRegionBefore(op.getBody(), newOp.getBody(),
+                               newOp.getBody().end());
+    auto funcReturnOp =
+        cast<func::ReturnOp>(newOp.getBody().front().getTerminator());
+    rewriter.setInsertionPointToEnd(&newOp.getBody().front());
+    rewriter.replaceOpWithNewOp<stablehlo::ReturnOp>(
+        funcReturnOp, funcReturnOp.getOperands());
+    rewriter.replaceOp(op, newOp->getResults());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicReshapeOpPattern
+    : public OpRewritePattern<DynamicReshapeOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicReshapeOp op,
+                                PatternRewriter& rewriter) const override {
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getType().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static result type");
+    rewriter.replaceOpWithNewOp<ReshapeOp>(op, op.getType(), op.getOperand());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // This pattern ignores and discards the output_shape operand. We rely on
+    // the verifier to make sure that its value is consistent with result type.
+    if (!succeeded(hlo::matchInts(op.getOutputShape())))
+      return rewriter.notifyMatchFailure(op, "expected static output_shape");
+    if (!op.getOutput().getType().cast<ShapedType>().hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "expected static output type");
+    rewriter.replaceOpWithNewOp<RngBitGeneratorOp>(
+        op, op->getResultTypes(), op.getRngAlgorithm(), op.getInitialState());
+    return success();
+  }
+};
+
+struct CanonicalizeDynamicTopKOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(impl, "expected constant k");
+
+    // We rely on many of the properties checked by verification.
+    auto valuesType = op.getValues().getType().cast<ShapedType>();
+    auto valuesLastDimSize = valuesType.getShape()[valuesType.getRank() - 1];
+    if (hlo::isDynamicDimSize(valuesLastDimSize) ||
+        valuesLastDimSize != k[0])
+      return rewriter.notifyMatchFailure(
+          op,
+          "expected value of k to match the values last dimension size of "
+          "static values type (result #0)");
+
+    rewriter.replaceOpWithNewOp<chlo::TopKOp>(
+        op, op->getResultTypes(), op.getOperand(), k[0]);
+    return success();
+  }
+};
+
+struct CanonicalizeRealDynamicSliceOpToDynamicSliceOpPattern
+    : public OpRewritePattern<RealDynamicSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(RealDynamicSliceOp op,
+                                PatternRewriter& rewriter) const override {
+    // DynamicSliceOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+
+    // This rewrite only works for unit strides because DynamicSliceOp
+    // doesn't support strides (i.e. it implicitly has unit strides).
+    SmallVector<int64_t> strides;
+    if (!succeeded(hlo::matchInts(op.getStrides(), strides)))
+      return rewriter.notifyMatchFailure(op, "expected static strides");
+    if (!llvm::all_of(strides, [&](int64_t stride) { return stride == 1; }))
+      return rewriter.notifyMatchFailure(op, "expected unit strides");
+
+    // Check that slice sizes are fully static (DynamicSliceOp style).
+    // To detect that, we check whether `limit_indices` is defined as
+    // `start_indices + constant` or `constant + start_indices`.
+    DenseIntElementsAttr sliceSizesAttr;
+    auto m_startIndices = matchers::m_Val(op.getStartIndices());
+    if (!matchPattern(
+            op.getLimitIndices(),
+            m_Op<AddOp>(m_startIndices, m_Constant(&sliceSizesAttr))) &&
+        !matchPattern(op.getLimitIndices(),
+                      m_Op<AddOp>(m_Constant(&sliceSizesAttr), m_startIndices)))
+      return rewriter.notifyMatchFailure(
+          op, "expected limit indices equal to start indices plus constant");
+
+    // RealDynamicSliceOp can take tensors of integer or index element types.
+    // DynamicSliceOp::slice_sizes only supports i64 element type.
+    // Adapt accordingly in order to be compatible with DynamicSliceOp.
+    SmallVector<int64_t> sliceSizes;
+    for (auto element : sliceSizesAttr.getValues<APInt>()) {
+      sliceSizes.push_back(element.getSExtValue());
+    }
+
+    // RealDynamicSliceOp::start_indices is a 1-dimensional tensor.
+    // DynamicSliceOp::start_indices is a vararg of 0-dimensional tensors.
+    // Adapt accordingly in order to be compatible with DynamicSliceOp.
+    SmallVector<Value> startIndices;
+    for (auto i = 0; i < static_cast<int64_t>(sliceSizes.size()); ++i) {
+      auto startIndexElementType =
+          op.getStartIndices().getType().getElementType();
+      auto startIndex1DType = RankedTensorType::get({1}, startIndexElementType);
+      auto startIndex1D = rewriter.create<SliceOp>(
+          op.getLoc(), startIndex1DType, op.getStartIndices(),
+          rewriter.getI64TensorAttr(i), rewriter.getI64TensorAttr(i + 1),
+          rewriter.getI64TensorAttr(1));
+      auto startIndex0DType = RankedTensorType::get({}, startIndexElementType);
+      auto startIndex0D = rewriter.create<ReshapeOp>(
+          op.getLoc(), startIndex0DType, startIndex1D);
+      startIndices.push_back(startIndex0D);
+    }
+
+    rewriter.replaceOpWithNewOp<DynamicSliceOp>(
+        op, op.getType(), op.getOperand(), startIndices,
+        rewriter.getI64TensorAttr(sliceSizes));
+    return success();
+  }
+};
+
+struct CanonicalizeRealDynamicSliceOpToSliceOpPattern
+    : public OpRewritePattern<RealDynamicSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(RealDynamicSliceOp op,
+                                PatternRewriter& rewriter) const override {
+    // SliceOp supports dynamic shapes for operands and results, so we
+    // don't check for that here unlike in some other patterns in this pass.
+    SmallVector<int64_t> startIndices, limitIndices, strides;
+    if (!succeeded(hlo::matchInts(op.getStartIndices(), startIndices)))
+      return rewriter.notifyMatchFailure(op, "expected static start");
+    if (!succeeded(hlo::matchInts(op.getLimitIndices(), limitIndices)))
+      return rewriter.notifyMatchFailure(op, "expected static limit");
+    if (!succeeded(hlo::matchInts(op.getStrides(), strides)))
+      return rewriter.notifyMatchFailure(op, "expected static strides");
+    rewriter.replaceOpWithNewOp<SliceOp>(
+        op, op.getType(), op.getOperand(),
+        rewriter.getI64TensorAttr(startIndices),
+        rewriter.getI64TensorAttr(limitIndices),
+        rewriter.getI64TensorAttr(strides));
+    return success();
+  }
+};
+
+struct StablehloCanonicalizeDynamismPass
+    : public impl::StablehloCanonicalizeDynamismPassBase<
+          StablehloCanonicalizeDynamismPass> {
+  using StablehloCanonicalizeDynamismPassBase::
+      StablehloCanonicalizeDynamismPassBase;
+
+  void runOnOperation() override {
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 2;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    patterns.add<CanonicalizeCustomCallOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicBroadcastInDimOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicConvOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicGatherOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicIotaOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicPadOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicReshapeOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<CanonicalizeDynamicTopKOpPattern>(&getContext());
+    patterns.add<CanonicalizeRealDynamicSliceOpToDynamicSliceOpPattern>(
+        &getContext());
+    patterns.add<CanonicalizeRealDynamicSliceOpToSliceOpPattern>(&getContext());
+    if (failed(applyPatternsAndFoldGreedily(getOperation(), std::move(patterns),
+                                            config))) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir
diff --ruN a/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp b/stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
--- stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
+++ stablehlo/stablehlo/experimental/transforms/StablehloRefineShapes.cpp
@@ -0,0 +1,1293 @@
+/* Copyright 2022 The StableHLO Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <utility>
+
+#include "llvm/ADT/APInt.h"
+#include "llvm/ADT/APSInt.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/ADT/STLFunctionalExtras.h"
+#include "llvm/ADT/SmallSet.h"
+#include "llvm/ADT/SmallVector.h"
+#include "llvm/ADT/StringRef.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/FormatVariadic.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
+#include "mlir/IR/BuiltinTypes.h"
+#include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Matchers.h"
+#include "mlir/IR/OpDefinition.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Value.h"
+#include "mlir/Interfaces/InferTypeOpInterface.h"
+#include "mlir/Support/LogicalResult.h"
+#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
+#include "stablehlo/dialect/Base.h"
+#include "stablehlo/dialect/ChloOps.h"
+#include "stablehlo/dialect/StablehloOps.h"
+#include "stablehlo/dialect/TypeInference.h"
+#include "stablehlo/experimental/dialect/StablehloOps.h"
+#include "stablehlo/experimental/transforms/Passes.h"
+
+namespace mlir {
+namespace stablehlo {
+namespace experimental {
+
+#define GEN_PASS_DEF_STABLEHLOREFINESHAPESPASS
+#include "stablehlo/experimental/transforms/Passes.h.inc"
+
+namespace {
+
+// DenseElementsAttr can be constructed from ArrayRef<APInt> but not from
+// ArrayRef<APSInt>. This helper bridges the gap.
+DenseIntElementsAttr getTensorAttr(ShapedType type, ArrayRef<APSInt> values) {
+  SmallVector<APInt> supportedValues(values);
+  return DenseIntElementsAttr::get(type, supportedValues);
+}
+
+APSInt getAPSInt(Type type, uint64_t value) {
+  unsigned numBits;
+  bool isUnsigned;
+  if (auto integerType = type.dyn_cast<IntegerType>()) {
+    numBits = integerType.getWidth();
+    // Signless types are treated as signed, per StableHLO convention.
+    isUnsigned = integerType.isUnsignedInteger();
+  } else {
+    llvm::report_fatal_error("expected integer type");
+  }
+  return APSInt({/*numBits=*/numBits, value},
+                /*isUnsigned=*/isUnsigned);
+}
+
+// The patterns below implement partial evaluation of shape computations which
+// is a critical part of implementing type refinement for ops like
+// dynamic_broadcast_in_dim, dynamic_iota and dynamic_reshape whose shape
+// depends on the value of their shape operands.
+
+template <typename OpType, typename FuncType>
+LogicalResult evalElementwise(PatternRewriter& rewriter, OpType op,
+                              FuncType fn) {
+  auto resultType = op.getType();
+  if (!resultType.hasRank() ||
+      !resultType.getElementType().template isa<IntegerType>())
+    return rewriter.notifyMatchFailure(op,
+                                       "expected integer result tensor type");
+
+  SmallVector<APSInt> result;
+  if constexpr (OpType::template hasTrait<OpTrait::OneOperand>()) {
+    SmallVector<APSInt> operand;
+    if (failed(hlo::matchInts(op.getOperand(), operand)))
+      return rewriter.notifyMatchFailure(op, "expected constant operand");
+    for (const auto& operandEl : operand) {
+      result.push_back(fn(operandEl));
+    }
+  } else if constexpr (OpType::template hasTrait<
+                           OpTrait::NOperands<2>::Impl>()) {
+    SmallVector<APSInt> lhs, rhs;
+    if (failed(hlo::matchInts(op.getLhs(), lhs)) ||
+        failed(hlo::matchInts(op.getRhs(), rhs)))
+      return rewriter.notifyMatchFailure(op, "expected constant operands");
+    for (auto [lhsEl, rhsEl] : llvm::zip(lhs, rhs)) {
+      result.push_back(fn(lhsEl, rhsEl));
+    }
+  } else if constexpr (OpType::template hasTrait<
+                           OpTrait::NOperands<3>::Impl>()) {
+    SmallVector<APSInt> x, y, z;
+    if (failed(hlo::matchInts(op->getOperand(0), x)) ||
+        failed(hlo::matchInts(op->getOperand(1), y)) ||
+        failed(hlo::matchInts(op->getOperand(2), z)))
+      return rewriter.notifyMatchFailure(op, "expected constant operands");
+    for (auto [xEl, yEl, zEl] : llvm::zip(x, y, z)) {
+      result.push_back(fn(xEl, yEl, zEl));
+    }
+  } else {
+    llvm::report_fatal_error("unsupported number of operands");
+  }
+
+  rewriter.replaceOpWithNewOp<ConstantOp>(op,
+                                          getTensorAttr(resultType, result));
+  return success();
+}
+
+struct EvalAddOpPattern : public OpRewritePattern<AddOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(AddOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op,
+                           [&](APSInt lhs, APSInt rhs) { return lhs + rhs; });
+  }
+};
+
+struct EvalAndOpPattern : public OpRewritePattern<AndOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(AndOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (!resultType.getElementType().isInteger(1))
+      return rewriter.notifyMatchFailure(op, "expected boolean element type");
+
+    return evalElementwise(rewriter, op, [&](APSInt lhsInt, APSInt rhsInt) {
+      return getAPSInt(resultType.getElementType(), lhsInt != 0 && rhsInt != 0);
+    });
+  }
+};
+
+struct EvalBroadcastInDimOpPattern : public OpRewritePattern<BroadcastInDimOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(BroadcastInDimOp op,
+                                PatternRewriter& rewriter) const override {
+    auto operandType = op.getOperand().getType();
+    if (!operandType.hasRank() || operandType.getRank() != 0)
+      return rewriter.notifyMatchFailure(op, "expected 0-dimensional type");
+
+    SmallVector<APSInt> operand;
+    if (failed(hlo::matchInts(op.getOperand(), operand)))
+      return rewriter.notifyMatchFailure(op, "expected constant operands");
+    auto scalar = operand[0];
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(
+        op, getTensorAttr(op.getType(), scalar));
+    return success();
+  }
+};
+
+struct EvalClampOpPattern : public OpRewritePattern<ClampOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ClampOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op,
+                           [&](APSInt min, APSInt operand, APSInt max) {
+                             if (operand < min) return min;
+                             if (max < operand) return max;
+                             return operand;
+                           });
+  }
+};
+
+struct EvalCompareOpPattern : public OpRewritePattern<CompareOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CompareOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    return evalElementwise(rewriter, op, [&](APSInt lhs, APSInt rhs) {
+      bool result;
+      switch (op.getComparisonDirection()) {
+        case ComparisonDirection::EQ:
+          result = lhs == rhs;
+          break;
+        case ComparisonDirection::NE:
+          result = lhs != rhs;
+          break;
+        case ComparisonDirection::GE:
+          result = lhs >= rhs;
+          break;
+        case ComparisonDirection::GT:
+          result = lhs > rhs;
+          break;
+        case ComparisonDirection::LE:
+          result = lhs <= rhs;
+          break;
+        case ComparisonDirection::LT:
+          result = lhs < rhs;
+          break;
+      }
+      return getAPSInt(resultType.getElementType(), result);
+    });
+  }
+};
+
+struct EvalConcatenateOpPattern : public OpRewritePattern<ConcatenateOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ConcatenateOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (!resultType.hasRank() || op.getDimension() != 0)
+      return rewriter.notifyMatchFailure(op, "expected dimension = 0");
+
+    SmallVector<APSInt> result;
+    for (Value operand : op->getOperands()) {
+      if (failed(hlo::matchInts(operand, result)))
+        return rewriter.notifyMatchFailure(op, "expected constant operands");
+    }
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(op,
+                                            getTensorAttr(resultType, result));
+    return success();
+  }
+};
+
+struct EvalConvertOpPattern : public OpRewritePattern<ConvertOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ConvertOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (!resultType.getElementType().isa<IntegerType>())
+      return rewriter.notifyMatchFailure(op,
+                                         "expected integer result tensor type");
+    auto resultBitWidth = resultType.getElementType().getIntOrFloatBitWidth();
+    return evalElementwise(rewriter, op, [&](APSInt operand) {
+      return operand.extOrTrunc(resultBitWidth);
+    });
+  }
+};
+
+struct EvalDivOpPattern : public OpRewritePattern<DivOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DivOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op,
+                           [&](APSInt lhs, APSInt rhs) { return lhs / rhs; });
+  }
+};
+
+struct EvalGetDimensionSizeOpPattern
+    : public OpRewritePattern<GetDimensionSizeOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(GetDimensionSizeOp op,
+                                PatternRewriter& rewriter) const override {
+    auto operandType = op.getOperand().getType();
+    if (!operandType.hasRank())
+      return rewriter.notifyMatchFailure(op, "expected ranked operand");
+    if (operandType.isDynamicDim(op.getDimension()))
+      return rewriter.notifyMatchFailure(op, "expected static dimension");
+
+    auto result = operandType.getDimSize(op.getDimension());
+    rewriter.replaceOpWithNewOp<ConstantOp>(
+        op, DenseIntElementsAttr::get<int32_t>(op.getType(), result));
+    return success();
+  }
+};
+
+struct EvalMaxOpPattern : public OpRewritePattern<MaxOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(MaxOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op, [&](APSInt lhs, APSInt rhs) {
+      return lhs >= rhs ? lhs : rhs;
+    });
+  }
+};
+
+struct EvalMinOpPattern : public OpRewritePattern<MinOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(MinOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op, [&](APSInt lhs, APSInt rhs) {
+      return lhs <= rhs ? lhs : rhs;
+    });
+  }
+};
+
+struct EvalMulOpPattern : public OpRewritePattern<MulOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(MulOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op,
+                           [&](APSInt lhs, APSInt rhs) { return lhs * rhs; });
+  }
+};
+
+struct EvalRemOpPattern : public OpRewritePattern<RemOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(RemOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op,
+                           [&](APSInt lhs, APSInt rhs) { return lhs % rhs; });
+  }
+};
+
+struct EvalReshapeOpPattern : public OpRewritePattern<ReshapeOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ReshapeOp op,
+                                PatternRewriter& rewriter) const override {
+    DenseIntElementsAttr attr;
+    if (!matchPattern(op.getOperand(), m_Constant(&attr)))
+      return rewriter.notifyMatchFailure(op, "expected constant operand");
+    rewriter.replaceOpWithNewOp<ConstantOp>(op, attr.reshape(op.getType()));
+    return success();
+  }
+};
+
+struct EvalSelectOpPattern : public OpRewritePattern<SelectOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(SelectOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<APSInt> pred, onTrue, onFalse;
+    if (failed(hlo::matchInts(op.getPred(), pred)) ||
+        failed(hlo::matchInts(op.getOnTrue(), onTrue)) ||
+        failed(hlo::matchInts(op.getOnFalse(), onFalse)))
+      return rewriter.notifyMatchFailure(op, "expected constant operands");
+
+    SmallVector<APSInt> result;
+    for (auto [predEl, onTrueEl, onFalseEl] :
+         llvm::zip(pred, onTrue, onFalse)) {
+      result.push_back(predEl != 0 ? onTrueEl : onFalseEl);
+    }
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(
+        op, getTensorAttr(op.getType(), result));
+    return success();
+  }
+};
+
+struct EvalSignOpPattern : public OpRewritePattern<SignOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(SignOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (!resultType.getElementType().isa<IntegerType>())
+      return rewriter.notifyMatchFailure(op,
+                                         "expected integer result tensor type");
+    return evalElementwise(rewriter, op, [&](APSInt operand) {
+      int64_t result;
+      if (operand.isNegative())
+        result = -1;
+      else if (operand.isZero())
+        result = 0;
+      else
+        result = 1;
+      return getAPSInt(resultType.getElementType(), result);
+    });
+  }
+};
+
+struct EvalSliceOpPattern : public OpRewritePattern<SliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(SliceOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    if (!resultType.hasRank() || resultType.getRank() != 1)
+      return rewriter.notifyMatchFailure(op, "expected 1-dimensional type");
+
+    SmallVector<APSInt> operand;
+    if (failed(hlo::matchInts(op.getOperand(), operand)))
+      return rewriter.notifyMatchFailure(op, "expected constant operand");
+
+    int64_t start = op.getStartIndices().getValues<int64_t>()[0];
+    int64_t limit = op.getLimitIndices().getValues<int64_t>()[0];
+    int64_t stride = op.getStrides().getValues<int64_t>()[0];
+    SmallVector<APSInt> result;
+    for (auto i = start; i < limit; i += stride) {
+      result.push_back(operand[i]);
+    }
+
+    rewriter.replaceOpWithNewOp<ConstantOp>(op,
+                                            getTensorAttr(resultType, result));
+    return success();
+  }
+};
+
+struct EvalSubtractOpPattern : public OpRewritePattern<SubtractOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(SubtractOp op,
+                                PatternRewriter& rewriter) const override {
+    return evalElementwise(rewriter, op,
+                           [&](APSInt lhs, APSInt rhs) { return lhs - rhs; });
+  }
+};
+
+// The patterns below implement shape refinement of individual ops.
+// In a nutshell, they use the upstream type inference infrastructure and a
+// StableHLO-specific extension to refine return types based on potentially
+// refined operands.
+
+// Refines the values using the given types.
+// Tricky implementation details:
+//   1) Need to support partial shape refinements, e.g. if just a single
+//      dimension size out of an entire tensor type got refined. This is done
+//      via inferMostSpecificType.
+//   2) Need to signal propagation of the refined shapes across the
+//      StableHLO program. Different callers of this function have different
+//      propagation needs, so this function doesn't signal anything on its own
+//      and leaves that to the callers.
+LogicalResult refineValues(PatternRewriter& rewriter, Operation* op,
+                           ValueRange values, TypeRange types) {
+  if (values.size() != types.size())
+    return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+      diag << "refineValues failed for " << types << ": expected "
+           << values.size() << " types, got " << types.size();
+    });
+
+  // Check whether `types` contain any new information with respect to existing
+  // return types. Even if just a single dimension size out of an entire tensor
+  // type got updated, using `inferMostSpecificType` ensures that we don't
+  // miss that.
+  bool needsRefinement = false;
+  SmallVector<Type> refinedTypes;
+  for (auto it : llvm::zip(values.getTypes(), types)) {
+    // Cannot use structured bindings to simplify this because capturing
+    // structured bindings in a lambda is a C++ 20 extension.
+    auto currentType = std::get<0>(it);
+    auto refinement = std::get<1>(it);
+    auto refinedType = hlo::inferMostSpecificType(
+        /*location=*/{}, {currentType, refinement});
+    if (failed(refinedType))
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "inferMostSpecificType failed for " << currentType << " and "
+             << refinement;
+      });
+    refinedTypes.push_back(*refinedType);
+    needsRefinement |= (currentType != *refinedType);
+  }
+  if (!needsRefinement)
+    return rewriter.notifyMatchFailure(op, "doesn't need refinement");
+
+  for (auto it : llvm::zip(values, refinedTypes)) {
+    // Cannot use structured bindings to simplify this because capturing
+    // structured bindings in a lambda is a C++ 20 extension.
+    auto value = std::get<0>(it);
+    auto refinedType = std::get<1>(it);
+    if (value.getType() == refinedType) continue;
+
+    // Check whether the users of this value are ready for the type of the
+    // value to be refined.
+    for (Operation* user : value.getUsers()) {
+      // CHLO and StableHLO ops are designed to support type refinements of
+      // their operands and results. Any operand type in these ops can change
+      // within what's supported by `inferMostSpecificType` without breaking
+      // verification of the op.
+      if (isa<chlo::ChloDialect, StablehloDialect>(user->getDialect()))
+        continue;
+
+      // Simply changing operand type of `func.return` won't work because
+      // that won't update the FunctionType of the enclosing `func.func`.
+      // Nonetheless, we still want to support these ops because they are widely
+      // used in StableHLO programs (although the plan of record is to replace
+      // `func.return` ops in StableHLO programs with `stablehlo.return`:
+      // https://github.com/openxla/stablehlo/issues/425).
+      if (isa<func::ReturnOp>(user)) continue;
+
+      // Unlike in TensorFlow's type inference pass, here we work only with
+      // allowlisted ops to focus our support on well-defined semantics of
+      // StableHLO programs.
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "unsupported refinement: tried to refine " << value.getType()
+             << " to " << refinedType << " for user " << user;
+      });
+    }
+
+    // Happy path: simply call setType here because most of our users are
+    // fine with that.
+    auto unrefinedType = value.getType();
+    value.setType(refinedType);
+
+    // Special case: for `func.return`, guard the refinement with a cast
+    // and leave propagation of the refined return type to a dedicated pattern.
+    auto isFuncReturn = [](OpOperand& use) -> bool {
+      return isa<func::ReturnOp>(use.getOwner());
+    };
+    if (llvm::none_of(value.getUses(), isFuncReturn)) continue;
+    rewriter.setInsertionPointAfter(op);
+    auto castToUnrefinedType = rewriter.create<UnrealizedConversionCastOp>(
+        op->getLoc(), unrefinedType, value);
+    value.replaceUsesWithIf(castToUnrefinedType.getOutputs()[0], isFuncReturn);
+  }
+
+  return success();
+}
+
+// Refines the return types of the given operation using the given types.
+// This function also signals PatternRewriter that it needs to visit all the
+// users of this op if any updates to its results have happened during execution
+// of the function.
+LogicalResult refineReturnTypes(PatternRewriter& rewriter, Operation* op,
+                                ArrayRef<Type> types) {
+  if (failed(refineValues(rewriter, op, op->getResults(), types)))
+    return failure();
+
+  // This `replaceOpWithIf` call doesn't actually change the IR, but
+  // it does ask the rewriter to visit all the users of this op. There is no
+  // upstream API to achieve this directly, but if it's introduced in the
+  // future, we could use it here.
+  rewriter.replaceOpWithIf(op, op->getResults(),
+                           [](OpOperand& use) { return false; });
+  return success();
+}
+
+// Refines the return types of the given operation using the given types.
+// Tricky implementation details:
+//   1) `types` can include non-shaped types. If there are tuple types,
+//      then they are first flattened into non-tuple types using in-order
+//      traversal, and only then we apply the refinements. If there are other
+//      types, then the corresponding refinements must be completely empty.
+//   2) Encodings are not supported. In principle, TypeExtensions should be
+//      supportable, but this needs careful thinking through. Given that no one
+//      asked for support for bounded dynamism in this pass yet, this is left
+//      for future work.
+// This function also signals PatternRewriter that it needs to visit all the
+// users of this op if any updates to its results have happened during execution
+// of the function.
+LogicalResult refineReturnTypes(PatternRewriter& rewriter, Operation* op,
+                                ArrayRef<ShapedTypeComponents> refinements) {
+  SmallVector<Type> flattenedTypes;
+  hlo::flattenTupleTypes(op->getResultTypes(), flattenedTypes);
+  auto flattenedSize = flattenedTypes.size();
+  if (flattenedSize != refinements.size())
+    return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+      diag << "refineReturnTypes failed: expected " << flattenedSize
+           << " refinements, got " << refinements.size();
+    });
+
+  SmallVector<Type> flattenedRefinedTypes;
+  for (auto it : llvm::zip(flattenedTypes, refinements)) {
+    // Cannot use structured bindings to simplify this because capturing
+    // structured bindings in a lambda is a C++ 20 extension.
+    ShapedType currentType = std::get<0>(it).dyn_cast<ShapedType>();
+    ShapedTypeComponents refinement = std::get<1>(it);
+    auto failWithReason = [&](StringRef reason) {
+      return rewriter.notifyMatchFailure(op, [&](Diagnostic& diag) {
+        diag << "refineTypes failed: refining " << currentType
+             << "with refinement: {";
+        if (refinement.hasRank()) {
+          diag << "shape = [" << refinement.getDims() << "]";
+          if (refinement.getAttribute())
+            diag << "attribute = " << refinement.getAttribute();
+        } else {
+          diag << "hasRank = false";
+        }
+        diag << ", elementType = " << refinement.getElementType();
+        diag << "} failed: " << reason;
+      });
+    };
+
+    // If the current type is not a shaped type, then the refinement must
+    // be completely empty.
+    if (!currentType) {
+      if (refinement.hasRank() || refinement.getElementType() ||
+          refinement.getAttribute())
+        return failWithReason("unsupported refinement");
+      flattenedRefinedTypes.push_back(currentType);
+      continue;
+    }
+
+    // If the refinement has an element type, then it must be the same as
+    // the current element type.
+    Type currentElementType = currentType.getElementType();
+    if (refinement.getElementType() &&
+        currentElementType != refinement.getElementType())
+      return failWithReason("expected compatible element types");
+
+    // If neither the current type nor the refinement are ranked, then there's
+    // nothing to refine, and we return the current type.
+    bool hasRank = currentType.hasRank() || refinement.hasRank();
+    if (!hasRank) {
+      flattenedRefinedTypes.push_back(currentType);
+      continue;
+    }
+
+    // If either the current type or the refinement have encodings, then
+    // we fail. Encodings are left for future work.
+    Attribute currentEncoding = nullptr;
+    if (auto currentRankedType = currentType.dyn_cast<RankedTensorType>()) {
+      currentEncoding = currentRankedType.getEncoding();
+    }
+    Attribute refinedEncoding = refinement.getAttribute();
+    if (currentEncoding || refinedEncoding)
+      return failWithReason("expected compatible encodings");
+
+    // If both the current type and the refinement have shapes, use the shape
+    // from the refinement. Otherwise, pick whatever is available.
+    // Make sure that the resulting type is compatible with the current type
+    // to avoid creating invalid code.
+    auto refinedShape =
+        refinement.hasRank() ? refinement.getDims() : currentType.getShape();
+    auto refinedType = RankedTensorType::get(refinedShape, currentElementType);
+    if (!hlo::isCompatibleForHloTypeInference(currentType, refinedType))
+      return failWithReason("expected compatible shapes");
+    flattenedRefinedTypes.push_back(refinedType);
+  }
+
+  SmallVector<Type> refinedTypes;
+  if (failed(hlo::unflattenTupleTypes(op->getResultTypes(),
+                                      flattenedRefinedTypes, refinedTypes)))
+    return failure();
+  return refineReturnTypes(rewriter, op, refinedTypes);
+}
+
+// Refines the return type of the given operation using the given shape.
+// This function also signals PatternRewriter that it needs to visit all the
+// users of this op if any updates to its results have happened during execution
+// of the function.
+template <typename OpType>
+LogicalResult refineReturnShape(PatternRewriter& rewriter, OpType op,
+                                ArrayRef<int64_t> shape) {
+  return refineReturnTypes(rewriter, op, ShapedTypeComponents(shape));
+}
+
+// Refines the return type of the given operation using the given shape.
+// This function also signals PatternRewriter that it needs to visit all the
+// users of this op if any updates to its results have happened during execution
+// of the function.
+template <typename OpType>
+LogicalResult refineReturnShape(PatternRewriter& rewriter, OpType op,
+                                Value shapeValue) {
+  // At the moment, we only support refining return types using fully static
+  // shape values which serves the current use cases well.
+  // Support for partially static shape values is left for future work.
+  SmallVector<int64_t> shape;
+  if (failed(hlo::matchInts(shapeValue, shape)))
+    return rewriter.notifyMatchFailure(op, "expected constant output shape");
+  return refineReturnShape(rewriter, op, shape);
+}
+
+struct RefineAllGatherOpPattern : public OpRewritePattern<AllGatherOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(AllGatherOp op,
+                                PatternRewriter& rewriter) const override {
+    auto operandType = op.getOperand().getType();
+    if (!operandType.hasRank())
+      return rewriter.notifyMatchFailure(op, "expected ranked operand type");
+
+    // This represents the cross_replica_and_partition process grouping strategy
+    // that requires num_partitions to compute shardCount. Since we don't know
+    // num_partitions at this point, we error out.
+    if (op.getChannelHandle() && !op.getUseGlobalDeviceIds())
+      return rewriter.notifyMatchFailure(op, "unsupported strategy");
+    DenseIntElementsAttr replicaGroups = op.getReplicaGroups();
+    auto shardCount = replicaGroups.getType().getDimSize(1);
+
+    SmallVector<int64_t> refinement(operandType.getShape());
+    if (!operandType.isDynamicDim(op.getAllGatherDim()))
+      refinement[op.getAllGatherDim()] *= shardCount;
+    return refineReturnShape(rewriter, op, refinement);
+  }
+};
+
+struct RefineBitcastConvertOpPattern
+    : public OpRewritePattern<BitcastConvertOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(BitcastConvertOp op,
+                                PatternRewriter& rewriter) const override {
+    auto operandType = op.getOperand().getType();
+    if (!operandType.hasRank())
+      return rewriter.notifyMatchFailure(op, "expected ranked operand type");
+
+    // If bit widths of the operand and the result are different, then
+    // operand and result shapes have different ranks.
+    // This complicates the logic quite a bit and is not needed to pass the
+    // current tests, so we leave this for future work.
+    auto resultType = op.getType();
+    auto getBitWidthFn = [](ShapedType type) {
+      auto elementType = type.getElementType();
+      if (auto complexType = elementType.dyn_cast<ComplexType>())
+        return complexType.getElementType().getIntOrFloatBitWidth();
+      return elementType.getIntOrFloatBitWidth();
+    };
+
+    if (getBitWidthFn(operandType) != getBitWidthFn(resultType))
+      return rewriter.notifyMatchFailure(op, "unsupported bit width");
+
+    return refineReturnShape(rewriter, op, operandType.getShape());
+  }
+};
+
+struct RefineConvertOpPattern : public OpRewritePattern<ConvertOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ConvertOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<ShapedTypeComponents> inferredReturnShapes;
+    if (failed(hlo::inferConvertOp(
+            /*location=*/{}, op.getOperand(), inferredReturnShapes)))
+      return rewriter.notifyMatchFailure(op, "inferConvertOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnShapes);
+  }
+};
+
+struct RefineConvolutionOpPattern : public OpRewritePattern<ConvolutionOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ConvolutionOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<ShapedTypeComponents> inferredReturnShapes;
+    if (failed(hlo::inferConvolutionOp(
+            /*location=*/{}, op.getLhs().getType(), op.getRhs().getType(),
+            op.getWindowStrides(), op.getPadding(), op.getLhsDilation(),
+            op.getRhsDilation(), op.getWindowReversal(),
+            op.getDimensionNumbers().getInputBatchDimension(),
+            op.getDimensionNumbers().getInputFeatureDimension(),
+            op.getDimensionNumbers().getInputSpatialDimensions(),
+            op.getDimensionNumbers().getKernelInputFeatureDimension(),
+            op.getDimensionNumbers().getKernelOutputFeatureDimension(),
+            op.getDimensionNumbers().getKernelSpatialDimensions(),
+            op.getDimensionNumbers().getOutputBatchDimension(),
+            op.getDimensionNumbers().getOutputFeatureDimension(),
+            op.getDimensionNumbers().getOutputSpatialDimensions(),
+            op.getFeatureGroupCount(), op.getBatchGroupCount(),
+            op.getPrecisionConfig(), inferredReturnShapes)))
+      return rewriter.notifyMatchFailure(op, "inferConvolutionOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnShapes);
+  }
+};
+
+struct RefineCustomCallOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<ShapedTypeComponents> refinements;
+    if (failed(hlo::getShapeRefinements(op.getLoc(), op, refinements)))
+      return rewriter.notifyMatchFailure(op, "expected valid refinements");
+    return refineReturnTypes(rewriter, op, refinements);
+  }
+};
+
+struct RefineDotGeneralOpPattern : public OpRewritePattern<DotGeneralOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DotGeneralOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<ShapedTypeComponents> inferredReturnShapes;
+    if (failed(hlo::inferDotGeneralOp(
+            /*location=*/{}, op.getLhs().getType(), op.getRhs().getType(),
+            op.getDotDimensionNumbersAttr().getLhsBatchingDimensions(),
+            op.getDotDimensionNumbersAttr().getRhsBatchingDimensions(),
+            op.getDotDimensionNumbersAttr().getLhsContractingDimensions(),
+            op.getDotDimensionNumbersAttr().getRhsContractingDimensions(),
+            op.getPrecisionConfig(), inferredReturnShapes)))
+      return rewriter.notifyMatchFailure(op, "inferDotGeneralOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnShapes);
+  }
+};
+
+struct RefineDynamicBroadcastInDimOpPattern
+    : public OpRewritePattern<DynamicBroadcastInDimOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicBroadcastInDimOp op,
+                                PatternRewriter& rewriter) const override {
+    return refineReturnShape(rewriter, op, op.getOutputDimensions());
+  }
+};
+
+struct RefineDynamicConvOpPattern : public OpRewritePattern<DynamicConvOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicConvOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<int64_t> padding;
+    if (failed(hlo::matchInts(op.getDPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant d_padding");
+    if (op.getPadding().has_value())
+      return rewriter.notifyMatchFailure(op, "expected empty padding");
+    auto paddingType = RankedTensorType::get(
+        op.getDPadding().getType().getShape(), rewriter.getIntegerType(64));
+    auto paddingAttr = DenseIntElementsAttr::get(paddingType, padding);
+
+    SmallVector<ShapedTypeComponents> inferredReturnShapes;
+    if (failed(hlo::inferConvolutionOp(
+            /*location=*/{}, op.getLhs().getType(), op.getRhs().getType(),
+            op.getWindowStrides(), paddingAttr, op.getLhsDilation(),
+            op.getRhsDilation(), op.getWindowReversal(),
+            op.getDimensionNumbers().getInputBatchDimension(),
+            op.getDimensionNumbers().getInputFeatureDimension(),
+            op.getDimensionNumbers().getInputSpatialDimensions(),
+            op.getDimensionNumbers().getKernelInputFeatureDimension(),
+            op.getDimensionNumbers().getKernelOutputFeatureDimension(),
+            op.getDimensionNumbers().getKernelSpatialDimensions(),
+            op.getDimensionNumbers().getOutputBatchDimension(),
+            op.getDimensionNumbers().getOutputFeatureDimension(),
+            op.getDimensionNumbers().getOutputSpatialDimensions(),
+            op.getFeatureGroupCount(), op.getBatchGroupCount(),
+            op.getPrecisionConfig(), inferredReturnShapes)))
+      return rewriter.notifyMatchFailure(op, "inferConvolutionOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnShapes);
+  }
+};
+
+struct RefineDynamicIotaOpPattern : public OpRewritePattern<DynamicIotaOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicIotaOp op,
+                                PatternRewriter& rewriter) const override {
+    return refineReturnShape(rewriter, op, op.getOutputShape());
+  }
+};
+
+struct RefineDynamicPadOpPattern : public OpRewritePattern<DynamicPadOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicPadOp op,
+                                PatternRewriter& rewriter) const override {
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> edgePaddingLow, edgePaddingHigh, interiorPadding;
+    if (failed(hlo::matchInts(op.getEdgePaddingLow(), edgePaddingLow)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant edge_padding_low");
+    if (failed(hlo::matchInts(op.getEdgePaddingHigh(), edgePaddingHigh)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant edge_padding_high");
+    if (failed(hlo::matchInts(op.getInteriorPadding(), interiorPadding)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant interior_padding");
+
+    SmallVector<Type> inferredReturnTypes;
+    if (failed(hlo::inferPadOp(
+            /*location=*/{}, op.getOperand().getType(),
+            op.getPaddingValue().getType(),
+            rewriter.getI64TensorAttr(edgePaddingLow),
+            rewriter.getI64TensorAttr(edgePaddingHigh),
+            rewriter.getI64TensorAttr(interiorPadding), inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferPadOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
+struct RefineDynamicReduceWindowOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicReduceWindowOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicReduceWindowOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    SmallVector<int64_t> windowDimensions, windowStrides, baseDilations,
+        windowDilations, padding;
+    if (failed(hlo::matchInts(op.getWindowDimensions(), windowDimensions)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dimensions");
+    if (failed(hlo::matchInts(op.getWindowStrides(), windowStrides)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_strides");
+    if (failed(hlo::matchInts(op.getBaseDilations(), baseDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant base_dilations");
+    if (failed(hlo::matchInts(op.getWindowDilations(), windowDilations)))
+      return rewriter.notifyMatchFailure(op,
+                                         "expected constant window_dilations");
+    if (failed(hlo::matchInts(op.getPadding(), padding)))
+      return rewriter.notifyMatchFailure(op, "expected constant padding");
+
+    SmallVector<ShapedTypeComponents> inferredReturnTypes;
+    if (failed(hlo::inferReduceWindowOp(
+            /*location=*/{}, op.getInputs(), op.getInitValues(),
+            rewriter.getI64TensorAttr(windowDimensions),
+            rewriter.getI64TensorAttr(windowStrides),
+            rewriter.getI64TensorAttr(baseDilations),
+            rewriter.getI64TensorAttr(windowDilations),
+            hlo::getPaddingAttr(&rewriter, padding), inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReduceWindowOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
+struct RefineDynamicReshapeOpPattern
+    : public OpRewritePattern<DynamicReshapeOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(DynamicReshapeOp op,
+                                PatternRewriter& rewriter) const override {
+    return refineReturnShape(rewriter, op, op.getOutputShape());
+  }
+};
+
+struct RefineDynamicRngBitGeneratorOpPattern
+    : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicRngBitGeneratorOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicRngBitGeneratorOpAdaptor op = *maybeOp;
+
+    // At the moment, we only support refining return types using fully static
+    // shape values which serves the current use cases well.
+    // Support for partially static shape values is left for future work.
+    auto initialStateType = op.getInitialState().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape;
+    if (failed(hlo::matchInts(op.getOutputShape(), outputShape)))
+      return rewriter.notifyMatchFailure(op, "expected constant output_shape");
+
+    // We only need to refine the shape of `output` (the second result).
+    // The shape of `output_state` (the first result) is determined by the shape
+    // of `initial_state`, so we ignore it and provide an empty refinement.
+    return refineReturnTypes(rewriter, op, {{initialStateType}, {outputShape}});
+  }
+};
+
+struct RefineDynamicTopKOpPattern : public OpRewritePattern<CustomCallOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(CustomCallOp impl,
+                                PatternRewriter& rewriter) const override {
+    auto maybeOp = getDynamicTopKOp(impl);
+    if (!maybeOp || failed(maybeOp->verify())) return failure();
+    DynamicTopKOpAdaptor op = *maybeOp;
+
+    auto operandType = op.getOperand().getType().cast<ShapedType>();
+    SmallVector<int64_t> outputShape(operandType.getShape());
+    SmallVector<int64_t> k;
+    if (failed(hlo::matchInts(op.getK(), k)))
+      return rewriter.notifyMatchFailure(op, "expected constant k");
+
+    outputShape[operandType.getRank() - 1] = k[0];
+    return refineReturnTypes(rewriter, op, {{outputShape}, {outputShape}});
+  }
+};
+
+struct RefineInferTypeOpInterfacePattern
+    : public OpInterfaceRewritePattern<InferTypeOpInterface> {
+  explicit RefineInferTypeOpInterfacePattern(MLIRContext* context)
+      : OpInterfaceRewritePattern(context, /*benefit=*/0) {}
+  LogicalResult matchAndRewrite(InferTypeOpInterface op,
+                                PatternRewriter& rewriter) const override {
+    // Unlike in TensorFlow's type inference pass, here we work only with
+    // allowlisted ops to focus our support on well-defined semantics of
+    // StableHLO programs.
+    if (!isa<chlo::ChloDialect, StablehloDialect>(op->getDialect()))
+      return rewriter.notifyMatchFailure(op, "unsupported dialect");
+
+    // For the ops that implement InferTypeOpInterface, we reinfer their return
+    // types and see what happens.
+    // Operands of these ops might have been refined elsewhere (e.g. someone
+    // might have updated argument types of a function) or earlier during this
+    // pass, and this might enable refinement opportunities downstream.
+    SmallVector<Type> inferredReturnTypes;
+    if (failed(op.inferReturnTypes(getContext(), /*location=*/{},
+                                   op->getOperands(), op->getAttrDictionary(),
+                                   op->getPropertiesStorage(), op->getRegions(),
+                                   inferredReturnTypes)))
+      return rewriter.notifyMatchFailure(op, "inferReturnTypes failed");
+    return refineReturnTypes(rewriter, op, inferredReturnTypes);
+  }
+};
+
+struct RefineRealDynamicSliceOpPattern
+    : public OpRewritePattern<RealDynamicSliceOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(RealDynamicSliceOp op,
+                                PatternRewriter& rewriter) const override {
+    // Alternative #1: All attributes are fully static (SliceOp style).
+    SmallVector<int64_t> startIndices, limitIndices, strides;
+    if (succeeded(hlo::matchInts(op.getStartIndices(), startIndices)) &&
+        succeeded(hlo::matchInts(op.getLimitIndices(), limitIndices)) &&
+        succeeded(hlo::matchInts(op.getStrides(), strides))) {
+      SmallVector<Type> inferredReturnTypes;
+      if (failed(hlo::inferSliceOp(/*location=*/{}, op.getOperand().getType(),
+                                   rewriter.getI64TensorAttr(startIndices),
+                                   rewriter.getI64TensorAttr(limitIndices),
+                                   rewriter.getI64TensorAttr(strides),
+                                   inferredReturnTypes)))
+        return rewriter.notifyMatchFailure(op, "inferSliceOp failed");
+      return refineReturnTypes(rewriter, op, inferredReturnTypes);
+    }
+
+    // Alternative #2: Slice sizes are fully static (DynamicSliceOp style).
+    // To detect that, we check whether `limit_indices` is defined as
+    // `start_indices + constant` or `constant + start_indices`.
+    DenseIntElementsAttr sliceSizesAttr;
+    auto m_startIndices = matchers::m_Val(op.getStartIndices());
+    if (matchPattern(
+            op.getLimitIndices(),
+            m_Op<AddOp>(m_startIndices, m_Constant(&sliceSizesAttr))) ||
+        matchPattern(
+            op.getLimitIndices(),
+            m_Op<AddOp>(m_Constant(&sliceSizesAttr), m_startIndices))) {
+      SmallVector<int64_t> strides;
+      if (!succeeded(hlo::matchInts(op.getStrides(), strides)) ||
+          !llvm::all_of(strides, [&](int64_t stride) { return stride == 1; }))
+        return rewriter.notifyMatchFailure(op, "expected unit strides");
+
+      // RealDynamicSliceOp::start_indices is a 1-dimensional tensor.
+      // DynamicSliceOp::start_indices is a vararg of 0-dimensional tensors.
+      // Adapt accordingly in order to be compatible with inferDynamicSliceOp.
+      auto startIndicesElementType =
+          op.getStartIndices().getType().getElementType();
+      SmallVector<Type> startIndicesTypes(
+          sliceSizesAttr.size(),
+          RankedTensorType::get({}, startIndicesElementType));
+
+      // RealDynamicSliceOp can take tensors of integer or index element types.
+      // DynamicSliceOp::slice_sizes only supports i64 element type.
+      // Adapt accordingly in order to be compatible with inferDynamicSliceOp.
+      SmallVector<int64_t> sliceSizes;
+      for (auto element : sliceSizesAttr.getValues<APInt>()) {
+        sliceSizes.push_back(element.getSExtValue());
+      }
+
+      SmallVector<ShapedTypeComponents> inferredReturnTypes;
+      if (failed(hlo::inferDynamicSliceOp(
+              op.getLoc(), op.getOperand().getType(), startIndicesTypes,
+              rewriter.getI64TensorAttr(sliceSizes), inferredReturnTypes)))
+        return rewriter.notifyMatchFailure(op, "inferDynamicSliceOp failed");
+      return refineReturnTypes(rewriter, op, inferredReturnTypes);
+    }
+
+    return rewriter.notifyMatchFailure(
+        op,
+        "expected either fully static attributes (SliceOp style) "
+        "or static sliceSizes (DynamicSliceOp style)");
+  }
+};
+
+struct RefineReduceScatterOpPattern : public OpRewritePattern<ReduceScatterOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ReduceScatterOp op,
+                                PatternRewriter& rewriter) const override {
+    auto operandType = op.getOperand().getType();
+    if (!operandType.hasRank())
+      return rewriter.notifyMatchFailure(op, "expected ranked operand type");
+
+    // This represents the cross_replica_and_partition process grouping strategy
+    // that requires num_partitions to compute shardCount. Since we don't know
+    // num_partitions at this point, we error out.
+    if (op.getChannelHandle() && !op.getUseGlobalDeviceIds())
+      return rewriter.notifyMatchFailure(op, "unsupported strategy");
+    DenseIntElementsAttr replicaGroups = op.getReplicaGroups();
+    auto shardCount = replicaGroups.getType().getDimSize(1);
+
+    SmallVector<int64_t> refinement(operandType.getShape());
+    if (!operandType.isDynamicDim(op.getScatterDimension()))
+      refinement[op.getScatterDimension()] /= shardCount;
+    return refineReturnShape(rewriter, op, refinement);
+  }
+};
+
+struct RefineRngOpPattern : public OpRewritePattern<RngOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(RngOp op,
+                                PatternRewriter& rewriter) const override {
+    return refineReturnShape(rewriter, op, op.getShape());
+  }
+};
+
+struct RefineUniformQuantizeOpPattern
+    : public OpRewritePattern<UniformQuantizeOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(UniformQuantizeOp op,
+                                PatternRewriter& rewriter) const override {
+    SmallVector<ShapedTypeComponents> inferredReturnShapes;
+    if (failed(hlo::inferUniformQuantizeOp(
+            /*location=*/{}, op.getOperand(), inferredReturnShapes)))
+      return rewriter.notifyMatchFailure(op, "inferConvertOp failed");
+    return refineReturnTypes(rewriter, op, inferredReturnShapes);
+  }
+};
+
+struct RefineWhileOpPattern : public OpRewritePattern<WhileOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(WhileOp op,
+                                PatternRewriter& rewriter) const override {
+    // Push the potentially refined operand types into the nested regions.
+    // This can lead to refinements of the return types of the body (but not
+    // of the cond since it always returns tensor<i1>), but the key insight here
+    // is that the enclosing while op doesn't care about these refinements
+    // (because its return types are equal to its operand types).
+    // If we end up with incompatibilities between while's return types and
+    // body's return types, the verifier will tell us about that. This means
+    // that the original program wasn't well-formed. TODO(burmako): Implement
+    // better error reporting for this case.
+    // This serves the current use cases well, so the implementation of more
+    // sophisticated refinement algorithm is left for future work.
+    rewriter.startRootUpdate(op);
+    auto condStatus = refineValues(rewriter, op, op.getCond().getArguments(),
+                                   op.getOperandTypes());
+    auto bodyStatus = refineValues(rewriter, op, op.getBody().getArguments(),
+                                   op.getOperandTypes());
+    if (succeeded(condStatus) || succeeded(bodyStatus)) {
+      rewriter.finalizeRootUpdate(op);
+      return success();
+    } else {
+      rewriter.cancelRootUpdate(op);
+      return failure();
+    }
+  }
+};
+
+struct UpdateFunctionTypePattern : public OpRewritePattern<func::ReturnOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(func::ReturnOp op,
+                                PatternRewriter& rewriter) const override {
+    // Check whether any of the values returned by `func.return` are casts
+    // which convert more specific type to less specific type.
+    // Such ops are produced by the algorithm behind this pass to avoid
+    // bringing the enclosing `func.func` op into an inconsistent state when
+    // refining individual ops. This pattern cleans this up.
+    bool needsUpdate = false;
+    SmallVector<Type> updatedResultTypes(op.getOperandTypes());
+    llvm::SmallSet<UnrealizedConversionCastOp, 4> castsToReplace;
+    for (auto [i, operand] : llvm::enumerate(op.getOperands())) {
+      auto cast =
+          dyn_cast_or_null<UnrealizedConversionCastOp>(operand.getDefiningOp());
+      if (!cast || cast.getInputs().size() != 1 ||
+          cast.getOutputs().size() != 1)
+        continue;
+
+      // Only proceed if the type that we're casting from is more specific
+      // than the type that we're casting to.
+      auto sourceType = cast.getInputs()[0].getType();
+      auto destType = cast.getOutputs()[0].getType();
+      auto mostSpecificType = hlo::inferMostSpecificType(
+          /*location=*/{}, {sourceType, destType});
+      if (failed(mostSpecificType) || destType == *mostSpecificType) continue;
+
+      // If the source type of the cast is more specific than the target type,
+      // then we conclude that the cast is redundant (i.e. needs to be removed)
+      // and that the return type of the function needs an update.
+      needsUpdate = true;
+      updatedResultTypes[i] = sourceType;
+
+      // Insert into set and continue iterating.
+      // ReturnOp may point to same value more than once.
+      castsToReplace.insert(cast);
+    }
+    if (!needsUpdate)
+      return rewriter.notifyMatchFailure(op, "doesn't need update");
+
+    // Replace CastOps with more specific operands than results.
+    for (auto cast : castsToReplace)
+      rewriter.replaceOp(cast, cast->getOperands());
+
+    // If the type of the enclosing `func.func` needs an update, we simply
+    // call setType. We can afford this simplicity because our algorithm
+    // currently supports only one function per module.
+    auto func = cast<func::FuncOp>(op->getParentOp());
+    func.setType(
+        rewriter.getFunctionType(func.getArgumentTypes(), updatedResultTypes));
+    return success();
+  }
+};
+
+struct UpdateRegionTypePattern : public OpRewritePattern<ReturnOp> {
+  using OpRewritePattern::OpRewritePattern;
+  LogicalResult matchAndRewrite(ReturnOp op,
+                                PatternRewriter& rewriter) const override {
+    if (!isa<CaseOp, IfOp>(op->getParentOp()))
+      return rewriter.notifyMatchFailure(op, "unsupported region");
+
+    bool needsUpdate = false;
+    SmallVector<Type> updatedResultTypes(op.getOperandTypes());
+    for (auto [regionType, refinedType] : llvm::zip(
+             op->getParentOp()->getResultTypes(), op->getOperandTypes())) {
+      auto mostSpecificType = hlo::inferMostSpecificType(
+          /*location=*/{}, {regionType, refinedType});
+      if (failed(mostSpecificType) || regionType == *mostSpecificType) continue;
+      needsUpdate = true;
+    }
+    if (!needsUpdate)
+      return rewriter.notifyMatchFailure(op, "doesn't need update");
+
+    rewriter.updateRootInPlace(op->getParentOp(), [&]() { return; });
+    return success();
+  }
+};
+
+struct StablehloRefineShapesPass
+    : public impl::StablehloRefineShapesPassBase<StablehloRefineShapesPass> {
+  using StablehloRefineShapesPassBase::StablehloRefineShapesPassBase;
+
+  void runOnOperation() override {
+    // Only one function per module is supported at the moment to avoid the need
+    // to think about iterative type inference algorithms.
+    // Current use cases are served well by inlining multiple functions into
+    // a single function, so we leave native support for multiple functions to
+    // future work.
+    // To enable modules that contain CustomCallOp::called_computations,
+    // we allow multiple functions, in which case we only refine the main
+    // function called "main", assuming that the called computations will have
+    // static shapes. Lifting this assumption and expanding refinement to
+    // multiple functions is left for future work.
+    ModuleOp module = getOperation();
+    auto funcs = llvm::to_vector(module.getOps<func::FuncOp>());
+    if (funcs.empty()) return;
+    func::FuncOp func;
+    if (funcs.size() == 1) {
+      func = funcs[0];
+    } else {
+      func = module.lookupSymbol<func::FuncOp>("main");
+    }
+    if (!func) {
+      module.emitOpError()
+          << "must have no more than one function or a `main`"
+          << " function to clearly identify which function will be refined";
+      return signalPassFailure();
+    }
+
+    // Similarly, only one block per function is supported at the moment.
+    // At the StableHLO level, functions are expected to only have one block,
+    // so supporting more is out of scope for this pass.
+    if (!func.getRegion().hasOneBlock()) {
+      func.emitOpError() << "must have exactly one block";
+      return signalPassFailure();
+    }
+
+    // The algorithm behind this pass consists of a single traversal of the
+    // function. This is sufficient because we only support one function per
+    // program at the moment.
+    // TODO(#1048): Find out why .maxIterations = 1 no longer works.
+    // There have been recent refactors to applyPatternsAndFoldGreedily
+    // upstream, and that might be the reason.
+    GreedyRewriteConfig config;
+    config.useTopDownTraversal = true;
+    config.enableRegionSimplification = true;
+    config.maxIterations = 2;
+    config.maxNumRewrites = GreedyRewriteConfig::kNoLimit;
+    config.strictMode = GreedyRewriteStrictness::AnyOp;
+
+    RewritePatternSet patterns(&getContext());
+    patterns.add<EvalAddOpPattern>(&getContext());
+    patterns.add<EvalAndOpPattern>(&getContext());
+    patterns.add<EvalBroadcastInDimOpPattern>(&getContext());
+    patterns.add<EvalClampOpPattern>(&getContext());
+    patterns.add<EvalCompareOpPattern>(&getContext());
+    patterns.add<EvalConcatenateOpPattern>(&getContext());
+    patterns.add<EvalConvertOpPattern>(&getContext());
+    patterns.add<EvalDivOpPattern>(&getContext());
+    patterns.add<EvalGetDimensionSizeOpPattern>(&getContext());
+    patterns.add<EvalMaxOpPattern>(&getContext());
+    patterns.add<EvalMinOpPattern>(&getContext());
+    patterns.add<EvalMulOpPattern>(&getContext());
+    patterns.add<EvalRemOpPattern>(&getContext());
+    patterns.add<EvalReshapeOpPattern>(&getContext());
+    patterns.add<EvalSelectOpPattern>(&getContext());
+    patterns.add<EvalSignOpPattern>(&getContext());
+    patterns.add<EvalSliceOpPattern>(&getContext());
+    patterns.add<EvalSubtractOpPattern>(&getContext());
+    patterns.add<RefineAllGatherOpPattern>(&getContext());
+    patterns.add<RefineBitcastConvertOpPattern>(&getContext());
+    patterns.add<RefineConvertOpPattern>(&getContext());
+    patterns.add<RefineConvolutionOpPattern>(&getContext());
+    patterns.add<RefineCustomCallOpPattern>(&getContext());
+    patterns.add<RefineDotGeneralOpPattern>(&getContext());
+    patterns.add<RefineDynamicBroadcastInDimOpPattern>(&getContext());
+    patterns.add<RefineDynamicConvOpPattern>(&getContext());
+    patterns.add<RefineDynamicIotaOpPattern>(&getContext());
+    patterns.add<RefineDynamicPadOpPattern>(&getContext());
+    patterns.add<RefineDynamicReduceWindowOpPattern>(&getContext());
+    patterns.add<RefineDynamicReshapeOpPattern>(&getContext());
+    patterns.add<RefineDynamicRngBitGeneratorOpPattern>(&getContext());
+    patterns.add<RefineDynamicTopKOpPattern>(&getContext());
+    patterns.add<RefineInferTypeOpInterfacePattern>(&getContext());
+    patterns.add<RefineRealDynamicSliceOpPattern>(&getContext());
+    patterns.add<RefineReduceScatterOpPattern>(&getContext());
+    patterns.add<RefineRngOpPattern>(&getContext());
+    patterns.add<RefineUniformQuantizeOpPattern>(&getContext());
+    patterns.add<RefineWhileOpPattern>(&getContext());
+    patterns.add<UpdateFunctionTypePattern>(&getContext());
+    patterns.add<UpdateRegionTypePattern>(&getContext());
+    if (failed(
+            applyPatternsAndFoldGreedily(func, std::move(patterns), config))) {
+      return signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+}  // namespace experimental
+}  // namespace stablehlo
+}  // namespace mlir

