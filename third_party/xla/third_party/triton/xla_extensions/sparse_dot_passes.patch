diff --git a/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp b/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp
--- a/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp
+++ b/lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp
@@ -277,6 +277,89 @@ struct TritonDotPattern : public OpConve
   }
 };
 
+struct TritonSparseDotPattern
+    : public OpConversionPattern<triton::gpu::SparseDotOp> {
+  using OpConversionPattern<triton::gpu::SparseDotOp>::OpConversionPattern;
+
+  LogicalResult matchAndRewrite(
+      triton::gpu::SparseDotOp op, OpAdaptor adaptor,
+      ConversionPatternRewriter &rewriter) const override {
+    RankedTensorType origType = op.getType().cast<RankedTensorType>();
+    auto origShape = origType.getShape();
+    auto typeConverter = getTypeConverter<TritonGPUTypeConverter>();
+    int numWarps = typeConverter->getNumWarps();
+    int threadsPerWarp = typeConverter->getThreadsPerWarp();
+    int numCTAs = typeConverter->getNumCTAs();
+
+    auto rank = origShape.size();
+    auto numElements = product<int64_t>(origShape);
+    SmallVector<unsigned> retSizePerThread(rank, 1);
+    if (numElements / (numWarps * threadsPerWarp) >= 4) {
+      retSizePerThread[rank - 1] = 2;
+      retSizePerThread[rank - 2] = 2;
+    }
+    if (numElements / (numWarps * threadsPerWarp) >= 16) {
+      retSizePerThread[rank - 1] = 4;
+      retSizePerThread[rank - 2] = 4;
+    }
+    SmallVector<unsigned> retOrder(rank);
+    for (unsigned i = 0; i < rank; ++i)
+      retOrder[i] = rank - 1 - i;
+    Attribute dEncoding = triton::gpu::BlockedEncodingAttr::get(
+        getContext(), origShape, retSizePerThread, retOrder, numWarps,
+        threadsPerWarp, numCTAs);
+    RankedTensorType retType =
+        RankedTensorType::get(origShape, origType.getElementType(), dEncoding);
+
+    // a & b must be of smem layout
+    auto aType = adaptor.getA().getType().cast<RankedTensorType>();
+    auto bType = adaptor.getB().getType().cast<RankedTensorType>();
+    Type aEltType = aType.getElementType();
+    Type bEltType = bType.getElementType();
+    Attribute aEncoding = aType.getEncoding();
+    Attribute bEncoding = bType.getEncoding();
+    if (!aEncoding || !bEncoding)
+      return failure();
+    Value a = adaptor.getA();
+    Value b = adaptor.getB();
+    Value c = adaptor.getC();
+    if (!aEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {
+      Attribute encoding = triton::gpu::DotOperandEncodingAttr::get(
+          getContext(), 0, dEncoding, aEltType);
+      auto dstType =
+          RankedTensorType::get(aType.getShape(), aEltType, encoding);
+      a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), dstType, a);
+    }
+    if (!bEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {
+      Attribute encoding = triton::gpu::DotOperandEncodingAttr::get(
+          getContext(), 1, dEncoding, bEltType);
+      auto dstType =
+          RankedTensorType::get(bType.getShape(), bEltType, encoding);
+      b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), dstType, b);
+    }
+    c = rewriter.create<triton::gpu::ConvertLayoutOp>(c.getLoc(), retType, c);
+
+    // aMeta must be of smem layout
+    auto aMetaType = adaptor.getAMeta().getType().cast<RankedTensorType>();
+    Attribute aMetaEncoding = aMetaType.getEncoding();
+    if (!aMetaEncoding) return failure();
+    Value aMeta = adaptor.getAMeta();
+    if (!aMetaEncoding.isa<triton::gpu::SparseDotMetaEncodingAttr>()) {
+      Attribute encoding =
+          triton::gpu::SparseDotMetaEncodingAttr::get(getContext(), dEncoding);
+      auto dstType = RankedTensorType::get(
+          aMetaType.getShape(), aMetaType.getElementType(), encoding);
+      aMeta = rewriter.create<triton::gpu::ConvertLayoutOp>(aMeta.getLoc(),
+                                                            dstType, aMeta);
+    }
+
+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SparseDotOp>(
+                      op, retType, a, b, c, aMeta),
+                  adaptor.getAttributes());
+    return success();
+  }
+};
+
 struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {
   using OpConversionPattern::OpConversionPattern;
 
@@ -550,6 +633,7 @@ void populateTritonPatterns(TritonGPUTyp
       GenericOpPattern<triton::AtomicRMWOp>, GenericOpPattern<ReturnOp>,
       GenericOpPattern<triton::CallOp>, TritonFuncOpPattern>(typeConverter,
                                                              context);
+  patterns.insert<TritonSparseDotPattern>(typeConverter, context);
 }
 
 //
@@ -788,6 +872,12 @@ public:
                  IntegerAttr::get(
                      i32_ty, llvm::APInt(32, computeCapability.getValue())));
 
+    // Only transform sparse dot op with undefined layout.
+    target.addDynamicallyLegalOp<triton::gpu::SparseDotOp>(
+        [](triton::gpu::SparseDotOp op) {
+          return op.getAMeta().getType().getEncoding() != nullptr;
+        });
+
     if (failed(applyPartialConversion(mod, target, std::move(patterns))))
       return signalPassFailure();
 
diff --git a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
--- a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
@@ -42,8 +42,9 @@ static int getMMAVersionSafe(int compute
   return 0;
 }
 
+template <typename DotType>
 SmallVector<unsigned>
-warpsPerTileV2(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps) {
+warpsPerTileV2(DotType dotOp, const ArrayRef<int64_t> shape, int numWarps) {
   auto rank = shape.size();
   // Early exit for batched matmul
   if (rank == 3)
@@ -56,14 +57,14 @@ warpsPerTileV2(tt::DotOp dotOp, const Ar
   auto slices = multiRootGetSlice(dotOp, {filter}, {filter});
   bool hasChainedDot = false;
   for (Operation *op : slices) {
-    if (isa<tt::DotOp>(op) && (op != dotOp)) {
-      auto chainedDot = cast<tt::DotOp>(op);
+    if (isa<DotType>(op) && (op != dotOp)) {
+      auto chainedDot = cast<DotType>(op);
       auto resTy = chainedDot.getResult().getType();
       if (resTy.getRank() != rank) {
         continue;
       }
       if (auto mmaEncoding =
-              resTy.getEncoding().dyn_cast<NvidiaMmaEncodingAttr>()) {
+              resTy.getEncoding().template dyn_cast<NvidiaMmaEncodingAttr>()) {
         return ttg::getWarpsPerCTA(mmaEncoding);
       }
       hasChainedDot = true;
@@ -101,12 +102,13 @@ warpsPerTileV2(tt::DotOp dotOp, const Ar
   return ret;
 }
 
-SmallVector<unsigned, 2>
-warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,
-               const SmallVector<unsigned, 3> &instrShape) {
+template <typename DotType>
+SmallVector<unsigned, 2> warpsPerTileV3(
+    DotType dotOp, const ArrayRef<int64_t> shape, int numWarps,
+    const SmallVector<unsigned, 3> &instrShape) {
   SetVector<Operation *> slices;
   mlir::getForwardSlice(dotOp.getResult(), &slices);
-  if (llvm::find_if(slices, [](Operation *op) { return isa<tt::DotOp>(op); }) !=
+  if (llvm::find_if(slices, [](Operation *op) { return isa<DotType>(op); }) !=
       slices.end())
     return {(unsigned)numWarps, 1};
 
@@ -175,9 +177,10 @@ public:
       : mlir::RewritePattern(tt::DotOp::getOperationName(), 2, context),
         computeCapability(computeCapability) {}
 
-  static SmallVector<unsigned, 3>
-  getWarpsPerTile(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int version,
-                  int numWarps, const SmallVector<unsigned, 3> &instrShape) {
+  template <typename DotType>
+  static SmallVector<unsigned, 3> getWarpsPerTile(
+      DotType dotOp, const ArrayRef<int64_t> shape, int version, int numWarps,
+      const SmallVector<unsigned, 3> &instrShape) {
     switch (version) {
     case 2:
       return warpsPerTileV2(dotOp, shape, numWarps);
@@ -337,6 +340,98 @@ public:
     return success();
   }
 };
+
+class SparseBlockedToMMA : public mlir::RewritePattern {
+ public:
+  using SparseDotOp = mlir::triton::gpu::SparseDotOp;
+  using SparseDotMetaEncodingAttr =
+      mlir::triton::gpu::SparseDotMetaEncodingAttr;
+
+  SparseBlockedToMMA(mlir::MLIRContext *context, int computeCapability)
+      : mlir::RewritePattern(SparseDotOp::getOperationName(), 2, context),
+        computeCapability(computeCapability) {}
+
+  mlir::LogicalResult matchAndRewrite(
+      mlir::Operation *op, mlir::PatternRewriter &rewriter) const override {
+    auto dotOp = cast<SparseDotOp>(op);
+    auto ctx = op->getContext();
+    Value a = dotOp.getA();
+    Value b = dotOp.getB();
+
+    // Check data-types and SM compatibility
+    RankedTensorType oldRetType = dotOp.getType();
+    if (!oldRetType.getEncoding() ||
+        oldRetType.getEncoding().isa<ttg::NvidiaMmaEncodingAttr>())
+      return failure();
+
+    assert(computeCapability >= 80 &&
+           "SparseDot is supported on Ampere and higher");
+    int versionMajor = computeCapability < 90 ? 2 : 3;
+
+    // get MMA encoding for the given number of warps
+    auto retShapePerCTA = ttg::getShapePerCTA(oldRetType);
+    auto mod = op->getParentOfType<mlir::ModuleOp>();
+    int numWarps = ttg::TritonGPUDialect::getNumWarps(mod);
+    auto CTALayout = ttg::getCTALayout(oldRetType.getEncoding());
+
+    auto instrShape =
+        mmaVersionToInstrShape(versionMajor, retShapePerCTA,
+                               a.getType().cast<TensorOrMemDesc>(), numWarps);
+    auto warpsPerTile = BlockedToMMA::getWarpsPerTile(
+        dotOp, retShapePerCTA, versionMajor, numWarps, instrShape);
+    ttg::NvidiaMmaEncodingAttr mmaEnc =
+        ttg::NvidiaMmaEncodingAttr::get(ctx, versionMajor, /*versionMinor=*/0,
+                                        warpsPerTile, CTALayout, instrShape);
+    auto newRetType = RankedTensorType::get(
+        oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);
+
+    // convert accumulator
+    auto oldAcc = dotOp.getOperand(2);
+    auto newAcc = rewriter.create<ttg::ConvertLayoutOp>(oldAcc.getLoc(),
+                                                        newRetType, oldAcc);
+
+    if (versionMajor == 2) {
+      // convert A operand
+      auto oldAType = a.getType().cast<RankedTensorType>();
+      auto newAEncoding = ttg::DotOperandEncodingAttr::get(
+          ctx, 0, mmaEnc, oldAType.getElementType());
+      auto newAType = RankedTensorType::get(
+          oldAType.getShape(), oldAType.getElementType(), newAEncoding);
+      a = rewriter.create<ttg::ConvertLayoutOp>(a.getLoc(), newAType, a);
+
+      // convert B operand
+      auto oldBType = b.getType().cast<RankedTensorType>();
+      auto newBEncoding = ttg::DotOperandEncodingAttr::get(
+          ctx, 1, mmaEnc, oldBType.getElementType());
+      auto newBType = RankedTensorType::get(
+          oldBType.getShape(), oldBType.getElementType(), newBEncoding);
+      b = rewriter.create<ttg::ConvertLayoutOp>(b.getLoc(), newBType, b);
+    } else {
+      a = BlockedToMMA::getMMAv3Operand(a, rewriter, 0);
+      b = BlockedToMMA::getMMAv3Operand(b, rewriter, 1);
+    }
+
+    // convert metadata
+    Value meta = dotOp.getAMeta();
+    auto oldMetaType = meta.getType().cast<RankedTensorType>();
+    auto newMetaType = RankedTensorType::get(
+        oldMetaType.getShape(), oldMetaType.getElementType(),
+        SparseDotMetaEncodingAttr::get(ctx, mmaEnc));
+    meta =
+        rewriter.create<ttg::ConvertLayoutOp>(meta.getLoc(), newMetaType, meta);
+
+    // convert dot instruction
+    auto newDot = rewriter.create<SparseDotOp>(dotOp.getLoc(), newRetType, a, b,
+                                               newAcc, meta);
+
+    rewriter.replaceOpWithNewOp<ttg::ConvertLayoutOp>(op, oldRetType,
+                                                      newDot.getResult());
+    return success();
+  }
+
+ private:
+  int computeCapability;
+};
 } // namespace
 
 static Value promoteOperand(OpBuilder &builder, Location loc, Value operand,
@@ -397,6 +491,7 @@ public:
 
     mlir::RewritePatternSet patterns(context);
     patterns.add<::BlockedToMMA>(context, computeCapability);
+    patterns.add<::SparseBlockedToMMA>(context, computeCapability);
     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {
       signalPassFailure();
     }
diff --git a/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp b/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp
--- a/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp
@@ -47,6 +47,10 @@ struct PipelinedOpInfo {
   bool loadIsMMAV3 = false;
 };
 
+bool isDotOp(Operation* op) {
+  return isa<tt::DotOp, ttg::SparseDotOp>(op);
+}
+
 } // namespace
 
 static bool isMMAv3Dot(Operation *op) {
@@ -165,22 +169,28 @@ getSharedEncIfAllUsersAreDotEnc(Value val) {
     } else {
       if (!isa<ttg::LocalLoadOp, ttg::ConvertLayoutOp>(user))
         return std::nullopt;
-      auto dotOpEnc = user->getResult(0)
-                          .getType()
-                          .cast<TensorOrMemDesc>()
-                          .getEncoding()
-                          .dyn_cast<ttg::DotOperandEncodingAttr>();
-      if (!dotOpEnc)
+      auto enc =
+          user->getResult(0).getType().cast<TensorOrMemDesc>().getEncoding();
+      if (isa<ttg::DotOperandEncodingAttr>(enc)) {
+        auto srcTy = val.getType().cast<TensorOrMemDesc>();
+        auto CTALayout = ttg::getCTALayout(srcTy.getEncoding());
+        auto order = ttg::getOrder(srcTy.getEncoding());
+        unsigned bitWidth = srcTy.getElementType().getIntOrFloatBitWidth();
+        tempAttr = ttg::SharedEncodingAttr::get(
+            val.getContext(), cast<ttg::DotOperandEncodingAttr>(enc),
+            srcTy.getShape(), ttg::getOrder(srcTy.getEncoding()),
+            ttg::getCTALayout(srcTy.getEncoding()),
+            srcTy.getElementType().getIntOrFloatBitWidth(),
+            /*needTrans=*/false);
+      } else if (isa<ttg::SparseDotMetaEncodingAttr>(enc)) {
+        auto srcTy = val.getType().cast<TensorOrMemDesc>();
+        tempAttr = ttg::SharedEncodingAttr::get(
+            val.getContext(), /*vec=*/1, /*perPhase=*/1, /*maxPhase=*/1,
+            ttg::getOrder(srcTy.getEncoding()),
+            ttg::getCTALayout(srcTy.getEncoding()));
+      } else {
         return std::nullopt;
-      auto srcTy = val.getType().cast<TensorOrMemDesc>();
-      auto CTALayout = ttg::getCTALayout(srcTy.getEncoding());
-      auto order = ttg::getOrder(srcTy.getEncoding());
-      unsigned bitWidth = srcTy.getElementType().getIntOrFloatBitWidth();
-      tempAttr = ttg::SharedEncodingAttr::get(
-          val.getContext(), dotOpEnc, srcTy.getShape(),
-          ttg::getOrder(srcTy.getEncoding()),
-          ttg::getCTALayout(srcTy.getEncoding()),
-          srcTy.getElementType().getIntOrFloatBitWidth(), /*needTrans=*/false);
+      }
     }
     // Check that the shared encodings needed by the users are compatible.
     if (!tempAttr || (attr != nullptr && attr != tempAttr))
@@ -313,7 +323,7 @@ loadOpsToDistanceAndUse(scf::ForOp forOp) {
       };
 
   for (Operation &op : forOp.getBody()->without_terminator()) {
-    if (!isa<tt::DotOp>(op))
+    if (!isDotOp(&op))
       continue;
     dfs(&op, 0, &op);
   }
@@ -391,7 +401,8 @@ collectOpsToPipeline(scf::ForOp forOp,
   // loads.
   for (auto &[loadOp, distAndUse] : loadOpToDistAndUse) {
     PipelinedOpInfo loadInfo;
-    if (auto dot = dyn_cast<tt::DotOp>(distAndUse.second)) {
+    if (isDotOp(distAndUse.second)) {
+      auto dot = dyn_cast<tt::DotOp>(distAndUse.second);
       if (loadIsMMAv3(loadOp)) {
         loadInfo.loadIsMMAV3 = true;
         loadInfo.sharedEncoding =
@@ -410,7 +421,7 @@ collectOpsToPipeline(scf::ForOp forOp,
         // The codegen bug is caught by an assertion, so if you think you've
         // fixed it, feel free to delete this code and see if the assert still
         // fails.  :)
-        if (!loadInfo.sharedEncoding) {
+        if (dot && !loadInfo.sharedEncoding) {
           if (auto dotEnc = dot.getResult()
                                 .getType()
                                 .getEncoding()
@@ -788,7 +799,7 @@ bool mlir::triton::preProcessLoopAndGetSchedule(
     int useStage = opToInfo[info.use].stage;
     int numBuffers = useStage - defStage;
 
-    if (hasMMAV3 && isa<tt::DotOp>(info.use)) {
+    if (hasMMAV3 && isDotOp(info.use)) {
       // For MMAv3, we need an extra buffer as this is assumed in the wgmma
       // pipelining post-processing.
       numBuffers++;
diff --git a/lib/Dialect/TritonGPU/Transforms/ReduceDataDuplication.cpp b/lib/Dialect/TritonGPU/Transforms/ReduceDataDuplication.cpp
--- a/lib/Dialect/TritonGPU/Transforms/ReduceDataDuplication.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/ReduceDataDuplication.cpp
@@ -36,6 +36,10 @@ public:
       auto srcEncoding = srcType.getEncoding();
       if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())
         return;
+      if (dstType.getEncoding().isa<triton::gpu::SparseDotMetaEncodingAttr>()) {
+        replaceSparseMetaEncoding(cvtOp);
+        return;
+      }
       auto dstDotOp =
           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();
       if (!dstDotOp)
@@ -74,6 +78,27 @@ public:
       cvtOp.erase();
     });
   }
+
+ private:
+  void replaceSparseMetaEncoding(triton::gpu::ConvertLayoutOp cvtOp) {
+    auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();
+    auto srcEncoding = srcType.getEncoding();
+    auto sharedLayout = triton::gpu::SharedEncodingAttr::get(
+        cvtOp.getContext(), 8, 1, 1, triton::gpu::getOrder(srcEncoding),
+        triton::gpu::getCTALayout(srcEncoding));
+
+    auto dstType = cvtOp.getType().cast<RankedTensorType>();
+    auto tmpType = triton::MemDescType::get(
+        dstType.getShape(), dstType.getElementType(), sharedLayout);
+
+    OpBuilder builder(cvtOp);
+    auto tmp = builder.create<triton::gpu::LocalAllocOp>(
+        cvtOp.getLoc(), tmpType, cvtOp.getSrc());
+    auto newConvert = builder.create<triton::gpu::LocalLoadOp>(
+        cvtOp.getLoc(), dstType, tmp);
+    cvtOp.replaceAllUsesWith(newConvert.getResult());
+    cvtOp.erase();
+  }
 };
 
 std::unique_ptr<Pass> mlir::triton::gpu::createReduceDataDuplicationPass() {
diff --git a/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp b/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp
--- a/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp
+++ b/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp
@@ -45,7 +45,7 @@ public:
       return;
     ModuleOp mod = getOperation();
     mod.walk([&](Operation *op) {
-      if (!isa<tt::DotOp, ttng::DotAsyncOp>(op))
+      if (!isa<tt::DotOp, ttng::DotAsyncOp, ttg::SparseDotOp>(op))
         return WalkResult::advance();
       OpBuilder builder(op);
       auto a = op->getOperand(0);
@@ -83,7 +83,7 @@ private:
     static DenseSet<std::pair<Operation *, unsigned>> trace;
     auto op = operand.getDefiningOp();
     // avoid redundant insertion
-    if (op && isa<tt::DotOp, ttng::DotAsyncOp>(op))
+    if (op && isa<tt::DotOp, ttng::DotAsyncOp, ttg::SparseDotOp>(op))
       return false;
     // reach convertlayout
     if (op && isa<ttg::LocalAllocOp>(op) &&
