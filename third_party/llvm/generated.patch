Auto generated patch. Do not edit or delete it, even if empty.
diff -ruN --strip-trailing-cr a/clang/lib/AST/ItaniumMangle.cpp b/clang/lib/AST/ItaniumMangle.cpp
--- a/clang/lib/AST/ItaniumMangle.cpp
+++ b/clang/lib/AST/ItaniumMangle.cpp
@@ -1405,16 +1405,6 @@
     //   - a template template parameter with arguments
     // In all of these cases, we should have no prefix.
     if (NestedNameSpecifier *Prefix = qualifier->getPrefix()) {
-      if (const auto *DTST =
-              dyn_cast<DependentTemplateSpecializationType>(type)) {
-        Out << "srN";
-        TemplateName Template = getASTContext().getDependentTemplateName(
-            {Prefix, DTST->getDependentTemplateName().getName(),
-             /*HasTemplateKeyword=*/true});
-        mangleTemplatePrefix(Template);
-        mangleTemplateArgs(Template, DTST->template_arguments());
-        break;
-      }
       mangleUnresolvedPrefix(Prefix,
                              /*recursive=*/true);
     } else {
@@ -2618,7 +2608,8 @@
         cast<DependentTemplateSpecializationType>(Ty);
     TemplateName Template = getASTContext().getDependentTemplateName(
         DTST->getDependentTemplateName());
-    mangleTemplatePrefix(Template);
+    const DependentTemplateStorage &S = DTST->getDependentTemplateName();
+    mangleSourceName(S.getName().getIdentifier());
     mangleTemplateArgs(Template, DTST->template_arguments());
     break;
   }
diff -ruN --strip-trailing-cr a/clang/lib/Sema/SemaExprCXX.cpp b/clang/lib/Sema/SemaExprCXX.cpp
--- a/clang/lib/Sema/SemaExprCXX.cpp
+++ b/clang/lib/Sema/SemaExprCXX.cpp
@@ -1929,8 +1929,9 @@
     }
     return true;
   }
-
-  return S.CheckAllocationAccess(StartLoc, Range, NamingClass, Decl, Diagnose);
+  Sema::AccessResult Accessible =
+      S.CheckAllocationAccess(StartLoc, Range, NamingClass, Decl, Diagnose);
+  return Accessible == Sema::AR_inaccessible;
 }
 
 /// Select the correct "usual" deallocation function to use from a selection of
diff -ruN --strip-trailing-cr a/clang/test/CodeGenCXX/bug135668.cpp b/clang/test/CodeGenCXX/bug135668.cpp
--- a/clang/test/CodeGenCXX/bug135668.cpp
+++ b/clang/test/CodeGenCXX/bug135668.cpp
@@ -0,0 +1,38 @@
+// RUN: %clang_cc1 %s -triple arm64-apple-macosx -emit-llvm -fcxx-exceptions -fexceptions -std=c++23 -o - | FileCheck  %s
+
+class TestClass {
+  public:
+   TestClass();
+   int field = 0;
+   friend class Foo;
+   static void * operator new(unsigned long size);
+  private:
+   static void operator delete(void *p);
+ };
+
+class Foo {
+public:
+  int test_method();
+};
+
+int Foo::test_method() {
+  TestClass *obj = new TestClass() ;
+  return obj->field;
+}
+
+// CHECK-LABEL: define noundef i32 @_ZN3Foo11test_methodEv
+// CHECK: [[THIS_ADDR:%.*]] = alloca ptr, align 8
+// CHECK: [[OBJ:%.*]] = alloca ptr, align 8
+// CHECK: store ptr %this, ptr [[THIS_ADDR]], align 8
+// CHECK: [[THIS1:%.*]] = load ptr, ptr [[THIS_ADDR]], align 8
+// CHECK: [[ALLOCATION:%.*]] = call noundef ptr @_ZN9TestClassnwEm(i64 noundef 4)
+// CHECK: [[INITIALIZEDOBJ:%.*]] = invoke noundef ptr @_ZN9TestClassC1Ev(ptr noundef nonnull align 4 dereferenceable(4) [[ALLOCATION]])
+// CHECK-NEXT:  to label %[[INVOKE_CONT:.*]] unwind label %[[LPAD:.*]]
+// CHECK: [[INVOKE_CONT]]:
+// CHECK: store ptr [[ALLOCATION]], ptr [[OBJ]], align 8
+// CHECK: [[OBJPTR:%.*]] = load ptr, ptr [[OBJ]], align 8
+// CHECK: [[FIELDPTR:%.*]] = getelementptr inbounds nuw %class.TestClass, ptr [[OBJPTR]], i32 0, i32 0
+// CHECK: [[FIELD:%.*]] = load i32, ptr [[FIELDPTR]], align 4
+// CHECK: ret i32 [[FIELD]]
+// CHECK: [[LPAD]]:
+// CHECK: call void @_ZN9TestClassdlEPv(ptr noundef [[ALLOCATION]]) #3
diff -ruN --strip-trailing-cr a/clang/test/CodeGenCXX/mangle-template.cpp b/clang/test/CodeGenCXX/mangle-template.cpp
--- a/clang/test/CodeGenCXX/mangle-template.cpp
+++ b/clang/test/CodeGenCXX/mangle-template.cpp
@@ -416,3 +416,20 @@
   template enable_if<true> raw_hash_set<int>::AbslHashValue<HashStateBase>();
   // CHECH: @_ZN39unresolved_template_specialization_type12raw_hash_setIiE13AbslHashValueINS_13HashStateBaseEEENS_9enable_ifIXsrNT_11is_hashableIiEE5valueEEEv
 } // namespace unresolved_template_specialization_type
+
+namespace GH133610 {
+  template <class T> struct A {
+    template <class V> struct B { int MEM; };
+  };
+
+  struct D {};
+  struct C: public A<int>::B<D> {};
+
+  template <class T, class U, class V>
+  auto k(T t, U u, V v) -> decltype (t.U::template B<V>::MEM) { return {}; }
+
+  void t() {
+    k( C(), A<int>(), D() );
+  }
+  // CHECK: @_ZN8GH1336101kINS_1CENS_1AIiEENS_1DEEEDtdtfp_sr1U1BIT1_EE3MEMET_T0_S5_
+} // namespace GH133610
diff -ruN --strip-trailing-cr a/clang/test/SemaCXX/bug135668.cpp b/clang/test/SemaCXX/bug135668.cpp
--- a/clang/test/SemaCXX/bug135668.cpp
+++ b/clang/test/SemaCXX/bug135668.cpp
@@ -0,0 +1,25 @@
+// RUN: %clang_cc1 -triple arm64-apple-macosx -Wall -fsyntax-only -verify %s -std=c++26 -fexceptions -fcxx-exceptions
+// expected-no-diagnostics
+
+// This test makes sure that we don't erroneously consider an accessible operator
+// delete to be inaccessible, and then discard the entire new expression.
+
+class TestClass {
+public:
+  TestClass();
+  int field = 0;
+  friend class Foo;
+  static void * operator new(unsigned long size);
+private:
+  static void operator delete(void *p);
+};
+
+class Foo {
+public:
+  int test_method();
+};
+
+int Foo::test_method() {
+  TestClass *obj = new TestClass() ;
+  return obj->field;
+}
diff -ruN --strip-trailing-cr a/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp b/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
--- a/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
+++ b/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp
@@ -25183,7 +25183,7 @@
     return SDValue();
 
   auto *Ld = dyn_cast<LoadSDNode>(Extract->getOperand(0));
-  if (!Ld || Ld->getExtensionType() || !Ld->isSimple())
+  if (!Ld || !ISD::isNormalLoad(Ld) || !Ld->isSimple())
     return SDValue();
 
   // Allow targets to opt-out.
diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp b/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp
--- a/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp
+++ b/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp
@@ -1141,8 +1141,6 @@
         break;
       [[fallthrough]];
     case Intrinsic::abs:
-    case Intrinsic::uadd_sat:
-    case Intrinsic::usub_sat:
       WorkList.push_back(FactOrCheck::getInstFact(DT.getNode(&BB), &I));
       break;
     }
@@ -1893,26 +1891,13 @@
         AddFact(CmpInst::ICMP_SGE, CB.Inst, X);
         continue;
       }
+
       if (auto *MinMax = dyn_cast<MinMaxIntrinsic>(CB.Inst)) {
         Pred = ICmpInst::getNonStrictPredicate(MinMax->getPredicate());
         AddFact(Pred, MinMax, MinMax->getLHS());
         AddFact(Pred, MinMax, MinMax->getRHS());
         continue;
       }
-      if (auto *USatI = dyn_cast<SaturatingInst>(CB.Inst)) {
-        switch (USatI->getIntrinsicID()) {
-        default:
-          llvm_unreachable("Unexpected intrinsic.");
-        case Intrinsic::uadd_sat:
-          AddFact(ICmpInst::ICMP_UGE, USatI, USatI->getLHS());
-          AddFact(ICmpInst::ICMP_UGE, USatI, USatI->getRHS());
-          break;
-        case Intrinsic::usub_sat:
-          AddFact(ICmpInst::ICMP_ULE, USatI, USatI->getLHS());
-          break;
-        }
-        continue;
-      }
     }
 
     Value *A = nullptr, *B = nullptr;
diff -ruN --strip-trailing-cr a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
--- a/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
+++ b/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp
@@ -1888,6 +1888,7 @@
     LoadEntriesToVectorize.clear();
     IsGraphTransformMode = false;
     GatheredLoadsEntriesFirst.reset();
+    CompressEntryToData.clear();
     ExternalUses.clear();
     ExternalUsesAsOriginalScalar.clear();
     for (auto &Iter : BlocksSchedules) {
@@ -4307,6 +4308,11 @@
   /// The index of the first gathered load entry in the VectorizeTree.
   std::optional<unsigned> GatheredLoadsEntriesFirst;
 
+  /// Maps compress entries to their mask data for the final codegen.
+  SmallDenseMap<const TreeEntry *,
+                std::tuple<SmallVector<int>, VectorType *, unsigned, bool>>
+      CompressEntryToData;
+
   /// This POD struct describes one external user in the vectorized tree.
   struct ExternalUser {
     ExternalUser(Value *S, llvm::User *U, const TreeEntry &E, int L)
@@ -7576,6 +7582,8 @@
             return Res.takeVector();
           };
           auto GetNumOperands = [](const TreeEntry *TE) {
+            if (TE->State == TreeEntry::SplitVectorize)
+              return TE->getNumOperands();
             if (auto *CI = dyn_cast<CallInst>(TE->getMainOp()); CI)
               return CI->arg_size();
             return TE->getNumOperands();
@@ -13411,6 +13419,8 @@
             *TLI, [](Value *) { return true; }, IsMasked, InterleaveFactor,
             CompressMask, LoadVecTy);
         assert(IsVectorized && "Expected to be vectorized");
+        CompressEntryToData.try_emplace(E, CompressMask, LoadVecTy,
+                                        InterleaveFactor, IsMasked);
         Align CommonAlignment;
         if (IsMasked)
           CommonAlignment = computeCommonAlignment<LoadInst>(VL);
@@ -17972,10 +17982,6 @@
       if (E->State == TreeEntry::Vectorize) {
         NewLI = Builder.CreateAlignedLoad(VecTy, PO, LI->getAlign());
       } else if (E->State == TreeEntry::CompressVectorize) {
-        bool IsMasked;
-        unsigned InterleaveFactor;
-        SmallVector<int> CompressMask;
-        VectorType *LoadVecTy;
         SmallVector<Value *> Scalars(E->Scalars.begin(), E->Scalars.end());
         if (!E->ReorderIndices.empty()) {
           SmallVector<int> Mask(E->ReorderIndices.begin(),
@@ -17985,11 +17991,8 @@
         SmallVector<Value *> PointerOps(Scalars.size());
         for (auto [I, V] : enumerate(Scalars))
           PointerOps[I] = cast<LoadInst>(V)->getPointerOperand();
-        [[maybe_unused]] bool IsVectorized = isMaskedLoadCompress(
-            Scalars, PointerOps, E->ReorderIndices, *TTI, *DL, *SE, *AC, *DT,
-            *TLI, [](Value *) { return true; }, IsMasked, InterleaveFactor,
-            CompressMask, LoadVecTy);
-        assert(IsVectorized && "Expected to be vectorized");
+        auto [CompressMask, LoadVecTy, InterleaveFactor, IsMasked] =
+            CompressEntryToData.at(E);
         Align CommonAlignment;
         if (IsMasked)
           CommonAlignment = computeCommonAlignment<LoadInst>(E->Scalars);
@@ -18423,8 +18426,14 @@
   // need to rebuild it.
   EntryToLastInstruction.clear();
   // All blocks must be scheduled before any instructions are inserted.
-  for (auto &BSIter : BlocksSchedules) {
+  for (auto &BSIter : BlocksSchedules)
     scheduleBlock(BSIter.second.get());
+  // Cache last instructions for the nodes to avoid side effects, which may
+  // appear during vectorization, like extra uses, etc.
+  for (const std::unique_ptr<TreeEntry> &TE : VectorizableTree) {
+    if (TE->isGather())
+      continue;
+    (void)getLastInstructionInBundle(TE.get());
   }
 
   if (ReductionRoot)
diff -ruN --strip-trailing-cr a/llvm/test/CodeGen/AArch64/pr135821.ll b/llvm/test/CodeGen/AArch64/pr135821.ll
--- a/llvm/test/CodeGen/AArch64/pr135821.ll
+++ b/llvm/test/CodeGen/AArch64/pr135821.ll
@@ -0,0 +1,27 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
+; RUN: llc < %s -mtriple=aarch64-unknown-linux-gnu  | FileCheck %s
+
+define <4 x float> @f(ptr  %0) {
+; CHECK-LABEL: f:
+; CHECK:       // %bb.0:
+; CHECK-NEXT:    sub sp, sp, #32
+; CHECK-NEXT:    str x30, [sp, #16] // 8-byte Folded Spill
+; CHECK-NEXT:    .cfi_def_cfa_offset 32
+; CHECK-NEXT:    .cfi_offset w30, -16
+; CHECK-NEXT:    ldr q1, [x0, #56]!
+; CHECK-NEXT:    ldr d0, [x0, #16]
+; CHECK-NEXT:    mov v1.d[1], v0.d[0]
+; CHECK-NEXT:    str q1, [sp] // 16-byte Folded Spill
+; CHECK-NEXT:    bl use
+; CHECK-NEXT:    ldr q0, [sp] // 16-byte Folded Reload
+; CHECK-NEXT:    ldr x30, [sp, #16] // 8-byte Folded Reload
+; CHECK-NEXT:    add sp, sp, #32
+; CHECK-NEXT:    ret
+  %2 = getelementptr inbounds nuw i8, ptr %0, i64 56
+  %3 = load <6 x float>, ptr %2, align 4
+  %4 = shufflevector <6 x float> %3, <6 x float> poison, <4 x i32> <i32 0, i32 1, i32 4, i32 5>
+  tail call void @use(ptr %2)
+  ret <4 x float> %4
+}
+
+declare void @use(ptr)
diff -ruN --strip-trailing-cr a/llvm/test/Transforms/ConstraintElimination/uadd-usub-sat.ll b/llvm/test/Transforms/ConstraintElimination/uadd-usub-sat.ll
--- a/llvm/test/Transforms/ConstraintElimination/uadd-usub-sat.ll
+++ b/llvm/test/Transforms/ConstraintElimination/uadd-usub-sat.ll
@@ -1,43 +0,0 @@
-; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
-; RUN: opt -passes=constraint-elimination -S %s | FileCheck %s
-
-declare i64 @llvm.uadd.sat.i64(i64, i64)
-declare i64 @llvm.usub.sat.i64(i64, i64)
-
-define i1 @uadd_sat_uge(i64 %a, i64 %b) {
-; CHECK-LABEL: define i1 @uadd_sat_uge(
-; CHECK-SAME: i64 [[A:%.*]], i64 [[B:%.*]]) {
-; CHECK-NEXT:    [[ADD_SAT:%.*]] = call i64 @llvm.uadd.sat.i64(i64 [[A]], i64 [[B]])
-; CHECK-NEXT:    [[CMP:%.*]] = and i1 true, true
-; CHECK-NEXT:    ret i1 [[CMP]]
-;
-  %add.sat = call i64 @llvm.uadd.sat.i64(i64 %a, i64 %b)
-  %cmp1 = icmp uge i64 %add.sat, %a
-  %cmp2 = icmp uge i64 %add.sat, %b
-  %cmp = and i1 %cmp1, %cmp2
-  ret i1 %cmp
-}
-
-define i1 @usub_sat_ule_lhs(i64 %a, i64 %b) {
-; CHECK-LABEL: define i1 @usub_sat_ule_lhs(
-; CHECK-SAME: i64 [[A:%.*]], i64 [[B:%.*]]) {
-; CHECK-NEXT:    [[SUB_SAT:%.*]] = call i64 @llvm.usub.sat.i64(i64 [[A]], i64 [[B]])
-; CHECK-NEXT:    ret i1 true
-;
-  %sub.sat = call i64 @llvm.usub.sat.i64(i64 %a, i64 %b)
-  %cmp = icmp ule i64 %sub.sat, %a
-  ret i1 %cmp
-}
-
-; Negative test
-define i1 @usub_sat_not_ule_rhs(i64 %a, i64 %b) {
-; CHECK-LABEL: define i1 @usub_sat_not_ule_rhs(
-; CHECK-SAME: i64 [[A:%.*]], i64 [[B:%.*]]) {
-; CHECK-NEXT:    [[SUB_SAT:%.*]] = call i64 @llvm.usub.sat.i64(i64 [[A]], i64 [[B]])
-; CHECK-NEXT:    [[CMP:%.*]] = icmp ule i64 [[SUB_SAT]], [[B]]
-; CHECK-NEXT:    ret i1 [[CMP]]
-;
-  %sub.sat = call i64 @llvm.usub.sat.i64(i64 %a, i64 %b)
-  %cmp = icmp ule i64 %sub.sat, %b
-  ret i1 %cmp
-}
diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/AArch64/masked-loads-side-effects-after-vec.ll b/llvm/test/Transforms/SLPVectorizer/AArch64/masked-loads-side-effects-after-vec.ll
--- a/llvm/test/Transforms/SLPVectorizer/AArch64/masked-loads-side-effects-after-vec.ll
+++ b/llvm/test/Transforms/SLPVectorizer/AArch64/masked-loads-side-effects-after-vec.ll
@@ -0,0 +1,48 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S --passes=slp-vectorizer -mtriple=aarch64-unknown-linux-gnu < %s | FileCheck %s
+
+declare noalias ptr @malloc()
+
+define void @test() {
+; CHECK-LABEL: define void @test() {
+; CHECK-NEXT:    [[TMP1:%.*]] = call dereferenceable_or_null(16) ptr @malloc()
+; CHECK-NEXT:    [[TMP2:%.*]] = load volatile ptr, ptr null, align 8
+; CHECK-NEXT:    [[TMP3:%.*]] = load <15 x i8>, ptr [[TMP1]], align 1
+; CHECK-NEXT:    [[TMP4:%.*]] = shufflevector <15 x i8> [[TMP3]], <15 x i8> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
+; CHECK-NEXT:    store <8 x i8> [[TMP4]], ptr [[TMP2]], align 1
+; CHECK-NEXT:    ret void
+;
+  %1 = call dereferenceable_or_null(16) ptr @malloc()
+  %2 = load volatile ptr, ptr null, align 8
+  %3 = load i8, ptr %1, align 1
+  store i8 %3, ptr %2, align 1
+  %4 = getelementptr i8, ptr %1, i64 2
+  %5 = load i8, ptr %4, align 1
+  %6 = getelementptr i8, ptr %2, i64 1
+  store i8 %5, ptr %6, align 1
+  %7 = getelementptr i8, ptr %1, i64 4
+  %8 = load i8, ptr %7, align 1
+  %9 = getelementptr i8, ptr %2, i64 2
+  store i8 %8, ptr %9, align 1
+  %10 = getelementptr i8, ptr %1, i64 6
+  %11 = load i8, ptr %10, align 1
+  %12 = getelementptr i8, ptr %2, i64 3
+  store i8 %11, ptr %12, align 1
+  %13 = getelementptr i8, ptr %1, i64 8
+  %14 = load i8, ptr %13, align 1
+  %15 = getelementptr i8, ptr %2, i64 4
+  store i8 %14, ptr %15, align 1
+  %16 = getelementptr i8, ptr %1, i64 10
+  %17 = load i8, ptr %16, align 1
+  %18 = getelementptr i8, ptr %2, i64 5
+  store i8 %17, ptr %18, align 1
+  %19 = getelementptr i8, ptr %1, i64 12
+  %20 = load i8, ptr %19, align 1
+  %21 = getelementptr i8, ptr %2, i64 6
+  store i8 %20, ptr %21, align 1
+  %22 = getelementptr i8, ptr %1, i64 14
+  %23 = load i8, ptr %22, align 1
+  %24 = getelementptr i8, ptr %2, i64 7
+  store i8 %23, ptr %24, align 1
+  ret void
+}
diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll b/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll
--- a/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll
+++ b/llvm/test/Transforms/SLPVectorizer/X86/entry-no-bundle-but-extra-use-on-vec.ll
@@ -0,0 +1,91 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S --passes=slp-vectorizer -mtriple=x86_64-generic-linux-gnu < %s | FileCheck %s
+
+define void @test(ptr %nExp, float %0, i1 %cmp, float %1) {
+; CHECK-LABEL: define void @test(
+; CHECK-SAME: ptr [[NEXP:%.*]], float [[TMP0:%.*]], i1 [[CMP:%.*]], float [[TMP1:%.*]]) {
+; CHECK-NEXT:  [[ENTRY:.*]]:
+; CHECK-NEXT:    [[TMP2:%.*]] = insertelement <4 x float> <float 0.000000e+00, float 0x7FF8000000000000, float poison, float poison>, float [[TMP1]], i32 2
+; CHECK-NEXT:    [[TMP3:%.*]] = insertelement <4 x float> [[TMP2]], float [[TMP0]], i32 3
+; CHECK-NEXT:    br i1 [[CMP]], label %[[IF_THEN:.*]], label %[[IF_END:.*]]
+; CHECK:       [[IF_THEN]]:
+; CHECK-NEXT:    [[TMP4:%.*]] = load float, ptr [[NEXP]], align 4
+; CHECK-NEXT:    [[TMP5:%.*]] = shufflevector <4 x float> [[TMP3]], <4 x float> poison, <2 x i32> <i32 3, i32 3>
+; CHECK-NEXT:    [[TMP6:%.*]] = insertelement <2 x float> [[TMP5]], float [[TMP4]], i32 0
+; CHECK-NEXT:    [[TMP7:%.*]] = fmul <2 x float> [[TMP6]], zeroinitializer
+; CHECK-NEXT:    [[TMP8:%.*]] = fmul <2 x float> [[TMP5]], zeroinitializer
+; CHECK-NEXT:    [[TMP9:%.*]] = insertelement <4 x float> <float poison, float 0.000000e+00, float 0.000000e+00, float poison>, float [[TMP1]], i32 3
+; CHECK-NEXT:    [[TMP10:%.*]] = shufflevector <2 x float> [[TMP8]], <2 x float> poison, <4 x i32> <i32 0, i32 poison, i32 poison, i32 poison>
+; CHECK-NEXT:    [[TMP11:%.*]] = shufflevector <4 x float> [[TMP9]], <4 x float> [[TMP10]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
+; CHECK-NEXT:    br label %[[IF_END]]
+; CHECK:       [[IF_END]]:
+; CHECK-NEXT:    [[TMP12:%.*]] = phi <4 x float> [ [[TMP11]], %[[IF_THEN]] ], [ [[TMP3]], %[[ENTRY]] ]
+; CHECK-NEXT:    [[TMP13:%.*]] = phi <2 x float> [ [[TMP8]], %[[IF_THEN]] ], [ zeroinitializer, %[[ENTRY]] ]
+; CHECK-NEXT:    [[TMP14:%.*]] = phi <2 x float> [ zeroinitializer, %[[IF_THEN]] ], [ <float 0x7FF8000000000000, float 1.000000e+00>, %[[ENTRY]] ]
+; CHECK-NEXT:    [[TMP15:%.*]] = phi <2 x float> [ [[TMP7]], %[[IF_THEN]] ], [ zeroinitializer, %[[ENTRY]] ]
+; CHECK-NEXT:    [[TMP16:%.*]] = shufflevector <2 x float> [[TMP14]], <2 x float> <float poison, float 0.000000e+00>, <2 x i32> <i32 1, i32 3>
+; CHECK-NEXT:    [[TMP17:%.*]] = fmul <2 x float> [[TMP15]], [[TMP16]]
+; CHECK-NEXT:    [[TMP18:%.*]] = fmul <2 x float> [[TMP13]], [[TMP14]]
+; CHECK-NEXT:    [[TMP19:%.*]] = fmul <4 x float> [[TMP12]], zeroinitializer
+; CHECK-NEXT:    [[CALL25:%.*]] = load volatile ptr, ptr null, align 8
+; CHECK-NEXT:    [[TMP20:%.*]] = fadd <2 x float> [[TMP18]], [[TMP17]]
+; CHECK-NEXT:    [[TMP21:%.*]] = fmul <2 x float> [[TMP20]], zeroinitializer
+; CHECK-NEXT:    [[TMP22:%.*]] = fadd <2 x float> [[TMP21]], zeroinitializer
+; CHECK-NEXT:    [[TMP23:%.*]] = fmul <4 x float> [[TMP19]], zeroinitializer
+; CHECK-NEXT:    [[TMP24:%.*]] = fadd <4 x float> [[TMP19]], zeroinitializer
+; CHECK-NEXT:    [[TMP25:%.*]] = shufflevector <4 x float> [[TMP23]], <4 x float> [[TMP24]], <4 x i32> <i32 0, i32 5, i32 6, i32 7>
+; CHECK-NEXT:    [[TMP26:%.*]] = call <4 x float> @llvm.vector.insert.v4f32.v2f32(<4 x float> <float 0.000000e+00, float 1.000000e+00, float poison, float poison>, <2 x float> [[TMP22]], i64 2)
+; CHECK-NEXT:    [[TMP27:%.*]] = fadd <4 x float> [[TMP25]], [[TMP26]]
+; CHECK-NEXT:    store <4 x float> [[TMP27]], ptr [[CALL25]], align 4
+; CHECK-NEXT:    ret void
+;
+entry:
+  br i1 %cmp, label %if.then, label %if.end
+
+if.then:
+  %div.i41 = fmul float %0, 0.000000e+00
+  %2 = load float, ptr %nExp, align 4
+  %div.1.i.i = fmul float %2, 0.000000e+00
+  %div.2.i.i = fmul float %0, 0.000000e+00
+  br label %if.end
+
+if.end:
+  %3 = phi float [ %1, %if.then ], [ %0, %entry ]
+  %4 = phi float [ 0.000000e+00, %if.then ], [ %1, %entry ]
+  %5 = phi float [ 0.000000e+00, %if.then ], [ 0x7FF8000000000000, %entry ]
+  %6 = phi float [ 0.000000e+00, %if.then ], [ 1.000000e+00, %entry ]
+  %fa.sroa.9.0 = phi float [ %div.2.i.i, %if.then ], [ 0.000000e+00, %entry ]
+  %fa.sroa.7.0 = phi float [ %div.1.i.i, %if.then ], [ 0.000000e+00, %entry ]
+  %fa.sroa.0.0 = phi float [ %div.i41, %if.then ], [ 0.000000e+00, %entry ]
+  %mul.1.i.i58 = fmul float %fa.sroa.7.0, %6
+  %mul.2.i.i60 = fmul float %fa.sroa.9.0, %6
+  %mul.1.i.i.i63 = fmul float %fa.sroa.0.0, %5
+  %mul.2.i.i.i65 = fmul float %fa.sroa.0.0, 0.000000e+00
+  %mul.i66 = fmul float %fa.sroa.0.0, 0.000000e+00
+  %add.1.i.i = fadd float %mul.1.i.i58, %mul.1.i.i.i63
+  %add.2.i.i = fadd float %mul.2.i.i60, %mul.2.i.i.i65
+  %mul.1.i.i74 = fmul float %add.1.i.i, 0.000000e+00
+  %mul.2.i.i76 = fmul float %add.2.i.i, 0.000000e+00
+  %mul.i.i.i78 = fmul float %mul.i66, 0.000000e+00
+  %add.1.i.i85 = fadd float %mul.1.i.i74, 0.000000e+00
+  %add.2.i.i86 = fadd float %mul.2.i.i76, 0.000000e+00
+  %mul.i.i.i97 = fmul float %5, 0.000000e+00
+  %mul.1.i.i.i99 = fmul float %4, 0.000000e+00
+  %mul.2.i.i.i101 = fmul float %3, 0.000000e+00
+  %add.i.i103 = fadd float %mul.i.i.i97, 0.000000e+00
+  %add.1.i.i104 = fadd float %mul.1.i.i.i99, 0.000000e+00
+  %add.2.i.i105 = fadd float %mul.2.i.i.i101, 0.000000e+00
+  %add = fadd float %mul.i.i.i78, 0.000000e+00
+  %add.i = fadd float %add.i.i103, 1.000000e+00
+  %add.1.i = fadd float %add.1.i.i104, %add.1.i.i85
+  %add.2.i = fadd float %add.2.i.i105, %add.2.i.i86
+  %call25 = load volatile ptr, ptr null, align 8
+  store float %add, ptr %call25, align 4
+  %__trans_tmp_29.sroa.5.0.call25.sroa_idx = getelementptr i8, ptr %call25, i64 4
+  store float %add.i, ptr %__trans_tmp_29.sroa.5.0.call25.sroa_idx, align 4
+  %__trans_tmp_29.sroa.6.0.call25.sroa_idx = getelementptr i8, ptr %call25, i64 8
+  store float %add.1.i, ptr %__trans_tmp_29.sroa.6.0.call25.sroa_idx, align 4
+  %__trans_tmp_29.sroa.7.0.call25.sroa_idx = getelementptr i8, ptr %call25, i64 12
+  store float %add.2.i, ptr %__trans_tmp_29.sroa.7.0.call25.sroa_idx, align 4
+  ret void
+}
diff -ruN --strip-trailing-cr a/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll b/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll
--- a/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll
+++ b/llvm/test/Transforms/SLPVectorizer/X86/split-node-num-operands.ll
@@ -0,0 +1,121 @@
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
+; RUN: opt -S --passes=slp-vectorizer -mtriple=x86_64-unknown-linux-gnu -mattr=+avx -slp-threshold=-1000 < %s | FileCheck %s
+
+define i64 @Foo(ptr align 8 dereferenceable(344) %0, i64 %1) {
+; CHECK-LABEL: define i64 @Foo(
+; CHECK-SAME: ptr align 8 dereferenceable(344) [[TMP0:%.*]], i64 [[TMP1:%.*]]) #[[ATTR0:[0-9]+]] {
+; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[TMP0]], i64 104
+; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[TMP0]], i64 112
+; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[TMP0]], i64 24
+; CHECK-NEXT:    [[TMP6:%.*]] = load i64, ptr [[TMP3]], align 8
+; CHECK-NEXT:    [[TMP7:%.*]] = load i64, ptr [[TMP4]], align 8
+; CHECK-NEXT:    [[TMP8:%.*]] = load i64, ptr [[TMP5]], align 8
+; CHECK-NEXT:    [[TMP9:%.*]] = load i64, ptr [[TMP0]], align 8
+; CHECK-NEXT:    [[TMP10:%.*]] = insertelement <2 x i64> poison, i64 [[TMP6]], i32 0
+; CHECK-NEXT:    [[TMP11:%.*]] = insertelement <2 x i64> [[TMP10]], i64 [[TMP9]], i32 1
+; CHECK-NEXT:    [[TMP12:%.*]] = insertelement <2 x i64> poison, i64 [[TMP7]], i32 0
+; CHECK-NEXT:    [[TMP13:%.*]] = insertelement <2 x i64> [[TMP12]], i64 [[TMP8]], i32 1
+; CHECK-NEXT:    [[TMP14:%.*]] = insertelement <2 x i64> poison, i64 0, i32 0
+; CHECK-NEXT:    [[TMP15:%.*]] = insertelement <2 x i64> <i64 0, i64 poison>, i64 [[TMP1]], i32 1
+; CHECK-NEXT:    br label %[[BB16:.*]]
+; CHECK:       [[BB16]]:
+; CHECK-NEXT:    [[TMP17:%.*]] = phi <2 x i64> [ [[TMP11]], [[TMP2:%.*]] ], [ zeroinitializer, %[[TMP25:.*]] ]
+; CHECK-NEXT:    [[TMP18:%.*]] = phi <2 x i64> [ [[TMP13]], [[TMP2]] ], [ [[TMP29:%.*]], %[[TMP25]] ]
+; CHECK-NEXT:    switch i32 0, label %[[BB19:.*]] [
+; CHECK-NEXT:      i32 0, label %[[TMP25]]
+; CHECK-NEXT:    ]
+; CHECK:       [[BB19]]:
+; CHECK-NEXT:    [[TMP20:%.*]] = shufflevector <2 x i64> [[TMP18]], <2 x i64> poison, <4 x i32> <i32 1, i32 poison, i32 poison, i32 poison>
+; CHECK-NEXT:    [[TMP21:%.*]] = insertelement <4 x i64> [[TMP20]], i64 0, i32 1
+; CHECK-NEXT:    [[TMP22:%.*]] = insertelement <4 x i64> [[TMP21]], i64 0, i32 2
+; CHECK-NEXT:    [[TMP23:%.*]] = shufflevector <4 x i64> [[TMP22]], <4 x i64> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 2>
+; CHECK-NEXT:    [[TMP24:%.*]] = shufflevector <2 x i64> [[TMP14]], <2 x i64> [[TMP18]], <2 x i32> <i32 0, i32 2>
+; CHECK-NEXT:    br label %[[TMP25]]
+; CHECK:       [[TMP25]]:
+; CHECK-NEXT:    [[TMP26:%.*]] = phi <2 x i64> [ [[TMP17]], %[[BB19]] ], [ zeroinitializer, %[[BB16]] ]
+; CHECK-NEXT:    [[TMP27:%.*]] = phi <4 x i64> [ [[TMP23]], %[[BB19]] ], [ zeroinitializer, %[[BB16]] ]
+; CHECK-NEXT:    [[TMP28:%.*]] = phi <2 x i64> [ [[TMP24]], %[[BB19]] ], [ [[TMP15]], %[[BB16]] ]
+; CHECK-NEXT:    [[TMP29]] = shufflevector <2 x i64> [[TMP18]], <2 x i64> <i64 0, i64 poison>, <2 x i32> <i32 2, i32 1>
+; CHECK-NEXT:    br i1 false, label %[[DOTLOOPEXIT206:.*]], label %[[BB16]]
+; CHECK:       [[_LOOPEXIT206:.*:]]
+; CHECK-NEXT:    switch i32 0, label %[[BB32:.*]] [
+; CHECK-NEXT:      i32 0, [[DOTCONT174:label %.*]]
+; CHECK-NEXT:      i32 1, label %[[BB30:.*]]
+; CHECK-NEXT:    ]
+; CHECK:       [[BB30]]:
+; CHECK-NEXT:    [[TMP31:%.*]] = shufflevector <4 x i64> [[TMP27]], <4 x i64> <i64 0, i64 0, i64 poison, i64 0>, <4 x i32> <i32 4, i32 5, i32 2, i32 7>
+; CHECK-NEXT:    br [[DOTCONT174]]
+; CHECK:       [[BB32]]:
+; CHECK-NEXT:    [[TMP33:%.*]] = insertelement <4 x i64> [[TMP27]], i64 0, i32 1
+; CHECK-NEXT:    [[TMP34:%.*]] = insertelement <4 x i64> [[TMP33]], i64 0, i32 2
+; CHECK-NEXT:    [[TMP35:%.*]] = shufflevector <4 x i64> [[TMP34]], <4 x i64> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 2>
+; CHECK-NEXT:    [[TMP36:%.*]] = insertelement <2 x i64> [[TMP28]], i64 0, i32 0
+; CHECK-NEXT:    br [[DOTCONT174]]
+; CHECK:       [[_CONT174:.*:]]
+; CHECK-NEXT:    [[TMP37:%.*]] = phi <2 x i64> [ [[TMP26]], %[[BB32]] ], [ zeroinitializer, %[[BB30]] ], [ [[TMP26]], %[[DOTLOOPEXIT206]] ]
+; CHECK-NEXT:    [[TMP38:%.*]] = phi <4 x i64> [ [[TMP35]], %[[BB32]] ], [ [[TMP31]], %[[BB30]] ], [ [[TMP27]], %[[DOTLOOPEXIT206]] ]
+; CHECK-NEXT:    [[TMP39:%.*]] = phi <2 x i64> [ [[TMP36]], %[[BB32]] ], [ zeroinitializer, %[[BB30]] ], [ [[TMP28]], %[[DOTLOOPEXIT206]] ]
+; CHECK-NEXT:    ret i64 0
+;
+  %3 = getelementptr i8, ptr %0, i64 104
+  %4 = getelementptr i8, ptr %0, i64 112
+  %5 = getelementptr i8, ptr %0, i64 24
+  %6 = load i64, ptr %3, align 8
+  %7 = load i64, ptr %4, align 8
+  %8 = load i64, ptr %5, align 8
+  %9 = load i64, ptr %0, align 8
+  br label %10
+
+10:
+  %11 = phi i64 [ %9, %2 ], [ 0, %18 ]
+  %12 = phi i64 [ %8, %2 ], [ %12, %18 ]
+  %13 = phi i64 [ %7, %2 ], [ 0, %18 ]
+  %14 = phi i64 [ %6, %2 ], [ 0, %18 ]
+  switch i32 0, label %15 [
+  i32 0, label %18
+  ]
+
+15:
+  %16 = tail call i64 @llvm.umin.i64(i64 0, i64 0)
+  %17 = tail call i64 @llvm.umax.i64(i64 0, i64 0)
+  br label %18
+
+18:
+  %19 = phi i64 [ %17, %15 ], [ 0, %10 ]
+  %20 = phi i64 [ %16, %15 ], [ 0, %10 ]
+  %21 = phi i64 [ %11, %15 ], [ 0, %10 ]
+  %22 = phi i64 [ %12, %15 ], [ 0, %10 ]
+  %23 = phi i64 [ %13, %15 ], [ %1, %10 ]
+  %24 = phi i64 [ %14, %15 ], [ 0, %10 ]
+  br i1 false, label %.loopexit206, label %10
+
+.loopexit206:
+  switch i32 0, label %26 [
+  i32 0, label %.cont174
+  i32 1, label %25
+  ]
+
+25:
+  br label %.cont174
+
+26:
+  %27 = tail call i64 @llvm.umin.i64(i64 0, i64 0)
+  %28 = tail call i64 @llvm.umax.i64(i64 0, i64 0)
+  br label %.cont174
+
+.cont174:
+  %.sroa.139.1 = phi i64 [ %28, %26 ], [ %19, %25 ], [ %19, %.loopexit206 ]
+  %.sroa.133.1 = phi i64 [ %27, %26 ], [ 0, %25 ], [ %20, %.loopexit206 ]
+  %.sroa.81.1 = phi i64 [ %23, %26 ], [ 0, %25 ], [ %23, %.loopexit206 ]
+  %.sroa.75.1 = phi i64 [ %24, %26 ], [ 0, %25 ], [ %24, %.loopexit206 ]
+  %.sroa.21.1 = phi i64 [ %21, %26 ], [ 0, %25 ], [ %21, %.loopexit206 ]
+  %.sroa.15.1 = phi i64 [ %22, %26 ], [ 0, %25 ], [ %22, %.loopexit206 ]
+  %29 = phi i64 [ %28, %26 ], [ 0, %25 ], [ %19, %.loopexit206 ]
+  %30 = phi i64 [ %27, %26 ], [ 0, %25 ], [ %20, %.loopexit206 ]
+  ret i64 0
+}
+
+declare i64 @llvm.umax.i64(i64, i64)
+
+declare i64 @llvm.umin.i64(i64, i64)
+
