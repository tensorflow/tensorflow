 *******************************************************************************
 Copyright 2022-2023 Arm Limited and affiliates.
 SPDX-License-Identifier: Apache-2.0

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 *******************************************************************************

diff --git a/src/cpu/aarch64/acl_convolution_utils.cpp b/src/cpu/aarch64/acl_convolution_utils.cpp
index c46d697575..1792d0af96 100644
--- a/src/cpu/aarch64/acl_convolution_utils.cpp
+++ b/src/cpu/aarch64/acl_convolution_utils.cpp
@@ -212,6 +212,65 @@ status_t acl_init_conf(acl_conv_conf_t &acp, memory_desc_t &src_md,
                 arm_compute::QuantizationInfo(1.0f / scales[0], 0));
     }
 
+    // WeightFormat::ANY tells ACL we can handle any format
+    acp.weights_info = arm_compute::WeightsInfo(
+            false, kw, kh, oc, false, arm_compute::WeightFormat::ANY);
+
+    // Get the format that the ACL kernel will expect the weights to be
+    // in (if a kernel exists). Note that these are referred to as fixed format
+    // kernels, because they require one specific weights format
+    arm_compute::WeightFormat expected_weight_format;
+    ACL_CHECK_VALID(arm_compute::NEGEMMConvolutionLayer::has_opt_impl(
+            expected_weight_format, &acp.src_info, &acp.wei_info,
+            acp.with_bias ? &acp.bia_info : nullptr,
+            &acp.dst_info, acp.padstride_info, acp.weights_info,
+            acp.dilation_info, acp.act_info, acp.fast_math));
+
+    // Set weights info to the one returned by has_opt_impl
+    acp.weights_info.set_weight_format(expected_weight_format);
+
+    // has_opt_impl may return a non fast math kernel, even if we requested one
+    acp.fast_math
+            = arm_compute::is_fixed_format_fast_math(expected_weight_format);
+
+    // Map OIHW used in ACL WeightFormat to the logical dimensions of the memory descriptor
+    dim_t O_dim = 0;
+    dim_t I_dim = 1;
+    dim_t H_dim = 2;
+    dim_t W_dim = 3;
+
+    if (!is_nspc) {
+        // We can try to support NCHW by swapping IHW around, note that this
+        // requires weights_md.dims[I_dim] % block_by != 0 (see next block)
+        O_dim = 0;
+        I_dim = 3;
+        H_dim = 1;
+        W_dim = 2;
+    }
+
+    // We can't currently support nchw and block_by != 1. If this is the case,
+    // try a non fast math kernel, which currently have no blocking
+    int block_by = arm_compute::block_by(acp.weights_info.weight_format());
+    if (!is_nspc && weights_md.dims[I_dim] % block_by != 0 && acp.fast_math) {
+        acp.fast_math = false;
+        acp.weights_info.set_weight_format(arm_compute::WeightFormat::ANY);
+        ACL_CHECK_VALID(arm_compute::NEGEMMConvolutionLayer::has_opt_impl(
+                expected_weight_format, &acp.src_info,
+                &acp.wei_info,
+                acp.with_bias ? &acp.bia_info : nullptr,
+                &acp.dst_info, acp.padstride_info, acp.weights_info,
+                acp.dilation_info, acp.act_info, acp.fast_math));
+        acp.weights_info.set_weight_format(expected_weight_format);
+        block_by = arm_compute::block_by(expected_weight_format);
+        // This shouldn't happen, because non-fastmath have no blocking, but
+        // guard against it because it would silently return incorrect results
+        if (weights_md.dims[I_dim] % block_by != 0)
+            return status::unimplemented;
+    }
+
+    acl_utils::reorder_to_weight_format(acp.wei_info, weights_md,
+            expected_weight_format, I_dim, O_dim, {W_dim, H_dim}, {});
+
     return status::success;
 }
 
@@ -219,6 +278,7 @@ status_t init_conf_gemm(acl_conv_conf_t &acp, memory_desc_t &src_md,
         memory_desc_t &weights_md, memory_desc_t &dst_md,
         memory_desc_t &bias_md, const convolution_desc_t &cd,
         const primitive_attr_t &attr) {
+    acp.is_indirect = false;
 
     // General Compute Library checks, memory tags are also set there
     CHECK(acl_init_conf(acp, src_md, weights_md, dst_md, bias_md, cd, attr));
@@ -244,11 +304,13 @@ status_t init_conf_indirect_gemm(acl_conv_conf_t &acp, memory_desc_t &src_md,
         memory_desc_t &weights_md, memory_desc_t &dst_md,
         memory_desc_t &bias_md, const convolution_desc_t &cd,
         const primitive_attr_t &attr) {
+    acp.is_indirect = true;
+    auto math_mode = get_fpmath_mode();
     // Indirect convolution results in slowdown for low thread count or 1x1
     // kernels, so fall back to GEMM-based convolution in these cases
     if (one_of(true, weights_md.dims[2] == 1, // kh
                 weights_md.dims[3] == 1, // kw
-                dnnl_get_max_threads() < 28)) {
+                (!math_mode && dnnl_get_max_threads() < 28))) {
         return status::unimplemented;
     }
 
@@ -275,6 +337,7 @@ status_t init_conf_wino(acl_conv_conf_t &acp, memory_desc_t &src_md,
         memory_desc_t &weights_md, memory_desc_t &dst_md,
         memory_desc_t &bias_md, const convolution_desc_t &cd,
         const primitive_attr_t &attr) {
+    acp.is_indirect = false;
 
     // Under these conditions, fallback to faster GEMM-based convolution
     // unless the user explicitly specifies Winograd algorithm
diff --git a/src/cpu/aarch64/acl_convolution_utils.hpp b/src/cpu/aarch64/acl_convolution_utils.hpp
index 3e56245faf..44dc8eecbf 100644
--- a/src/cpu/aarch64/acl_convolution_utils.hpp
+++ b/src/cpu/aarch64/acl_convolution_utils.hpp
@@ -43,6 +43,7 @@ struct acl_conv_conf_t {
     // If this is true, the result of the convolution goes into a temporarily
     // allocated ACL tensor to be accumulated into the oneDNN dst during postops
     bool use_dst_acc;
+    bool is_indirect;
     arm_compute::TensorInfo src_info;
     arm_compute::TensorInfo wei_info;
     arm_compute::TensorInfo bia_info;
diff --git a/src/cpu/aarch64/acl_indirect_gemm_convolution.hpp b/src/cpu/aarch64/acl_indirect_gemm_convolution.hpp
index bcf031a771..4ddc8cf910 100644
--- a/src/cpu/aarch64/acl_indirect_gemm_convolution.hpp
+++ b/src/cpu/aarch64/acl_indirect_gemm_convolution.hpp
@@ -41,6 +41,7 @@ struct acl_indirect_gemm_resource_t : public resource_t {
         acl_obj_->bia_tensor.allocator()->init(acp.bia_info);
 
         // clang-format off
+        arm_compute::experimental::PostOpList<arm_compute::ITensorInfo *> empty_post_ops = arm_compute::experimental::PostOpList<arm_compute::ITensorInfo *> {};
         acl_obj_->conv.configure(
             &acl_obj_->src_tensor,
             &acl_obj_->wei_tensor,
@@ -50,7 +51,9 @@ struct acl_indirect_gemm_resource_t : public resource_t {
                                     acp.dilation_info,
                                     acp.act_info,
                                     acp.fast_math,
-                                    1));
+                                    1,
+                                    empty_post_ops,
+                                    acp.weights_info));
         // clang-format on
 
         return status::success;
diff --git a/src/cpu/aarch64/acl_inner_product.hpp b/src/cpu/aarch64/acl_inner_product.hpp
index c5e507085f..163ff066e6 100644
--- a/src/cpu/aarch64/acl_inner_product.hpp
+++ b/src/cpu/aarch64/acl_inner_product.hpp
@@ -45,6 +45,7 @@ struct acl_ip_conf_t {
     arm_compute::TensorInfo bia_info;
     arm_compute::TensorInfo dst_info;
     arm_compute::FullyConnectedLayerInfo fc_info;
+    arm_compute::WeightsInfo weights_info;
 };
 struct acl_ip_resource_t : public resource_t {
     acl_ip_resource_t() : acl_ip_obj_(utils::make_unique<acl_ip_obj_t>()) {}
@@ -64,7 +65,8 @@ struct acl_ip_resource_t : public resource_t {
             &acl_ip_obj_->wei_tensor,
             aip.with_bias ? &acl_ip_obj_->bia_tensor : nullptr,
             &acl_ip_obj_->dst_tensor,
-            aip.fc_info);
+            aip.fc_info,
+            aip.weights_info);
         // clang-format on
 
         return status::success;
@@ -156,8 +158,8 @@ struct acl_inner_product_fwd_t : public primitive_t {
                 src_shape = (src_tag == nc) ? arm_compute::TensorShape(ic, n)
                                             : arm_compute::TensorShape(n, ic);
 
-                wei_shape = (wei_tag == io) ? arm_compute::TensorShape(oc, ic)
-                                            : arm_compute::TensorShape(ic, oc);
+                // For fixed format kernels weight shape is always io
+                wei_shape = arm_compute::TensorShape(oc, ic);
             }
             if (is_4d) {
                 src_shape = (src_tag == nhwc)
@@ -166,7 +168,8 @@ struct acl_inner_product_fwd_t : public primitive_t {
 
                 // ACL requires the weights to be in 2D flattened shape
                 const int flattened_ic = is_4d ? ic * kh * kw : ic;
-                wei_shape = arm_compute::TensorShape(flattened_ic, oc);
+                // For fixed format kernels weights shape is always io
+                wei_shape = arm_compute::TensorShape(oc, flattened_ic);
             }
 
             arm_compute::DataLayout src_layout = (src_tag == nhwc)
@@ -183,6 +186,9 @@ struct acl_inner_product_fwd_t : public primitive_t {
             aip.wei_info = arm_compute::TensorInfo(
                     wei_shape, 1, arm_compute::DataType::F32, wei_layout);
 
+            aip.weights_info = arm_compute::WeightsInfo(
+                    false, 1, 1, is_4d ? ic * kh *kw : ic, false, arm_compute::WeightFormat::ANY);
+
             aip.dst_info
                     = arm_compute::TensorInfo(arm_compute::TensorShape(oc, n),
                             1, arm_compute::DataType::F32);
@@ -194,15 +200,7 @@ struct acl_inner_product_fwd_t : public primitive_t {
                     1, arm_compute::DataType::F32);
 
             aip.fc_info.weights_trained_layout = wei_layout;
-            if (is_2d && wei_tag != src_tag) {
-                // weights are already transposed
-                aip.fc_info.transpose_weights = false;
-
-                if (desc()->prop_kind == dnnl_forward_training) {
-                    aip.wei_info.set_are_values_constant(false);
-                    aip.fc_info.are_weights_reshaped = true;
-                }
-            }
+            aip.fc_info.transpose_weights = false;
 
             // Fast math mode
             auto math_mode = get_fpmath_mode();
@@ -214,6 +212,80 @@ struct acl_inner_product_fwd_t : public primitive_t {
                     aip.fc_info.activation_info));
             aip.use_dst_acc = post_ops.has_sum();
 
+            arm_compute::WeightFormat expected_weight_format;
+            auto acl_st = arm_compute::NEFullyConnectedLayer::has_opt_impl(
+                expected_weight_format,
+                &aip.src_info,
+                &aip.wei_info,
+                aip.with_bias ? &aip.bia_info : nullptr,
+                &aip.dst_info,
+                aip.fc_info,
+                aip.weights_info);
+            if(acl_st.error_code() != arm_compute::ErrorCode::OK) {
+                return status::unimplemented;
+            }
+
+            aip.weights_info.set_weight_format(expected_weight_format);
+
+            int interleaved_by = arm_compute::interleave_by(expected_weight_format);
+            int block_by = arm_compute::block_by(expected_weight_format);
+            bool is_fast_math_kernel = arm_compute::is_fixed_format_fast_math(expected_weight_format);
+
+            if(!is_fast_math_kernel) {
+                // FP32 kernel might be faster for some cases then BF16
+                aip.fc_info.enable_fast_math = false;
+            }
+
+            memory_desc_t want_wei_md = weights_md_;
+
+            int ic_multiply = ic;
+            if(is_4d) {
+                ic_multiply = ic * kh * kw;
+
+                // Since we are flattening dimensions the memory descriptor
+                // should also be for 2D
+                want_wei_md.ndims = 2;
+
+                want_wei_md.dims[1] = ic_multiply;
+                want_wei_md.padded_dims[1] = ic_multiply;
+                want_wei_md.format_desc.blocking.strides[1] = 1;
+
+                want_wei_md.dims[0] = oc;
+                want_wei_md.padded_dims[0] = want_wei_md.padded_dims[1];
+                want_wei_md.padded_dims[0] = oc;
+            }
+
+            want_wei_md.format_desc.blocking.strides[1] = interleaved_by * block_by;
+            if(want_wei_md.dims[1] % block_by != 0) {
+                want_wei_md.padded_dims[1] = utils::div_up(want_wei_md.dims[1], block_by) * block_by;
+            }
+            want_wei_md.format_desc.blocking.strides[0] = interleaved_by * want_wei_md.padded_dims[1];
+
+            if(oc % interleaved_by != 0) {
+                int padded_dim = utils::div_up(oc, interleaved_by) * interleaved_by;
+                want_wei_md.padded_dims[0] = padded_dim;
+            }
+
+            int data_type_size = memory_desc_wrapper(want_wei_md).data_type_size();
+            acl_utils::update_strides_y_and_z(
+                aip.wei_info,
+                want_wei_md.format_desc.blocking.strides[0] * data_type_size,
+                want_wei_md.format_desc.blocking.strides[1] * data_type_size);
+
+            want_wei_md.format_desc.blocking.inner_nblks = (block_by > 1) + 1;
+            want_wei_md.format_desc.blocking.inner_idxs[0] = 0;
+            want_wei_md.format_desc.blocking.inner_blks[0] = interleaved_by;
+            if(block_by > 1) {
+                want_wei_md.format_desc.blocking.inner_idxs[1] = 1;
+                want_wei_md.format_desc.blocking.inner_blks[1] = block_by;
+	        }
+
+            if(is_fast_math_kernel) {
+                want_wei_md.data_type = dnnl_bf16;
+            }
+
+            weights_md_ = want_wei_md;
+
             // clang-format off
             // Validate fully connected layer manually to check for return status
             ACL_CHECK_VALID(arm_compute::NEFullyConnectedLayer::validate(
diff --git a/src/cpu/aarch64/acl_utils.cpp b/src/cpu/aarch64/acl_utils.cpp
index 79ea775d6d..3424d5dacc 100644
--- a/src/cpu/aarch64/acl_utils.cpp
+++ b/src/cpu/aarch64/acl_utils.cpp
@@ -157,6 +157,28 @@ status_t tensor_info(
     return status::success;
 }
 
+status_t update_strides_y_and_z(
+    arm_compute::TensorInfo &info, const int y, const int z) {
+
+    arm_compute::TensorShape shape = info.tensor_shape();
+    arm_compute::Strides old_strides_in_bytes = info.strides_in_bytes();
+
+    arm_compute::Strides new_strides_in_bytes;
+    for(size_t i = 0; i < shape.num_dimensions(); ++i) {
+        new_strides_in_bytes.set(i, old_strides_in_bytes[i]);
+    }
+
+    // set y
+    new_strides_in_bytes.set(1, y);
+    // set z
+    new_strides_in_bytes.set(2, z);
+
+    info.init(info.tensor_shape(), info.num_channels(), info.data_type(),
+            new_strides_in_bytes, info.offset_first_element_in_bytes(), info.total_size());
+
+    return status::success;
+}
+
 status_t insert_singleton_dimension(arm_compute::TensorInfo &ti, size_t dim_i) {
 
     // Max 6 dims in ACL, so we can't insert another
@@ -261,6 +283,75 @@ int reorder_dimensions_by_stride(std::vector<memory_desc_t *> permuted_mds,
     return reordered_dims;
 }
 
+void reorder_to_weight_format(arm_compute::TensorInfo &info, memory_desc_t &md,
+        arm_compute::WeightFormat wf, dim_t I_dim, dim_t O_dim,
+        std::vector<dim_t> spatial_dims, std::vector<dim_t> batch_dims) {
+
+    md.format_kind = format_kind::blocked;
+    md.format_desc.blocking = blocking_desc_t {};
+    const int interleaved_by = arm_compute::interleave_by(wf);
+    const int block_by = arm_compute::block_by(wf);
+
+    // I dimension becomes densest (apart from blocking)
+    md.format_desc.blocking.strides[I_dim] = interleaved_by * block_by;
+    md.padded_dims[I_dim] = utils::rnd_up(md.dims[I_dim], block_by);
+
+    // Then any spatial dimensions (e.g. HW)
+    dim_t ldb = interleaved_by * md.padded_dims[I_dim];
+    for (dim_t sd : spatial_dims) {
+        md.format_desc.blocking.strides[sd] = ldb;
+        ldb *= md.padded_dims[sd];
+    }
+
+    // O dim (which was the innermost) becomes the outermost (apart from batching)
+    md.format_desc.blocking.strides[O_dim] = ldb;
+    md.padded_dims[O_dim] = utils::rnd_up(md.dims[O_dim], interleaved_by);
+
+    // Update the batch dimensions, starting with stride of the innermost batch
+    const dim_t innermost_batch_stride
+            = md.padded_dims[I_dim] * md.padded_dims[O_dim];
+    dim_t batch_stride = innermost_batch_stride;
+    for (dim_t bd : batch_dims) {
+        md.format_desc.blocking.strides[bd] = batch_stride;
+        batch_stride *= md.padded_dims[bd];
+    }
+
+    // Weights can only be blocked if they are also interleaved
+    if (interleaved_by > 1) {
+        md.format_desc.blocking.inner_nblks = 1 + (block_by > 1);
+
+        md.format_desc.blocking.inner_idxs[0] = O_dim;
+        md.format_desc.blocking.inner_blks[0] = interleaved_by;
+        if (block_by > 1) {
+            md.format_desc.blocking.inner_idxs[1] = I_dim;
+            md.format_desc.blocking.inner_blks[1] = block_by;
+        }
+    }
+
+    if (arm_compute::is_fixed_format_fast_math(wf)) {
+        md.data_type = dnnl_bf16;
+        info.set_data_type(arm_compute::DataType::BFLOAT16);
+    }
+
+    // The data layout is now determined by the manually set strides
+    info.set_data_layout(arm_compute::DataLayout::UNKNOWN);
+
+    // x is ignored in fixed format kernels
+    // y is the leading dimension of b (ldb) in the GEMM d = a*b + c
+    //   This is the stride of O_dim in the md
+    // z is the batch dimension (not strictly needed if there's only 1 batch)
+    //   i.e. how much do I need to stride to get to the next matmul (ignoring
+    //   the interleaving). Note that we use the innermost_batch_stride
+    //   because all the batched dimensions are collapsed (as required by ACL).
+    arm_compute::Strides new_strides_in_bytes = info.strides_in_bytes();
+    new_strides_in_bytes.set(1, ldb * info.element_size());
+    new_strides_in_bytes.set(2, innermost_batch_stride * info.element_size());
+
+    info.init(info.tensor_shape(), info.num_channels(), info.data_type(),
+            new_strides_in_bytes, info.offset_first_element_in_bytes(),
+            memory_desc_wrapper(md).size());
+}
+
 } // namespace acl_utils
 
 } // namespace aarch64
diff --git a/src/cpu/aarch64/acl_utils.hpp b/src/cpu/aarch64/acl_utils.hpp
index 28693bb167..141b2974a2 100644
--- a/src/cpu/aarch64/acl_utils.hpp
+++ b/src/cpu/aarch64/acl_utils.hpp
@@ -62,6 +62,9 @@ status_t tensor_info(arm_compute::TensorInfo &info, const memory_desc_t &md);
 status_t tensor_info(
         arm_compute::TensorInfo &info, const memory_desc_wrapper &md);
 
+// Update y and z strides in arm_compute::TensorInfo
+status_t update_strides_y_and_z(arm_compute::TensorInfo &info, const int y, const int z);
+
 // Insert a dimension of size 1 at the index dim_i of TensorInfo
 status_t insert_singleton_dimension(arm_compute::TensorInfo &ti, size_t dim_i);
 
@@ -74,6 +77,28 @@ status_t insert_singleton_dimension(arm_compute::TensorInfo &ti, size_t dim_i);
 int reorder_dimensions_by_stride(std::vector<memory_desc_t *> permuted_mds,
         std::vector<const memory_desc_t *> mds);
 
+// Reorder a memory_desc_t and set the strides on a arm_compute::TensorInfo to
+// match an arm_compute::WeightFormat. You are required to specify how various
+// logical dimensions in oneDNN correspond to logical dimensions in arm_compute.
+// info  TensorInfo where the strides will be changed to match the reordering
+// md    memory descriptor where the stride and padded dimensions will be
+//       changed or reordering
+// wf    Describes the memory format/layout of the weights
+// I_dim The logical dimension of md corresponding to the input channel of
+//       a convolution or the K dimension in a matmul
+// O_dim The logical dimension of md corresponding to the output channel of a
+//       convolution or the N dimension in a matmul
+// spatial_dims The logical dimensions of md corresponding to the spatial
+//              dimensions of the weights (H, W, D for example). These will be
+//              the next densest after the inner blocks and the input channel.
+// batch_dims The logical dimensions of md related to the batch in a batched
+//            matmul, ordered from innermost to outermost. ACL calls these
+//            the multi_stride_b. These will become the outermost (least dense)
+//            dimensions and will be collapsed.
+void reorder_to_weight_format(arm_compute::TensorInfo &info, memory_desc_t &md,
+        arm_compute::WeightFormat wf, dim_t I_dim, dim_t O_dim,
+        std::vector<dim_t> spatial_dims, std::vector<dim_t> batch_dims = {});
+
 // Logs a custom 'info' line describing an unsupported case
 #define LOG_ACL_UNSUPPORTED(msg) \
     do { \
diff --git a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
index 679baec3a4..853277e37b 100644
--- a/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
+++ b/src/cpu/aarch64/matmul/acl_matmul_utils.cpp
@@ -66,15 +66,12 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
 
     // Transpose A (src) or B (wei)
     amp.is_transA = helper.transA() == 'T';
-    amp.is_transB = helper.transB() == 'T';
+    amp.is_transB = false;
+
     if (amp.is_transA)
         amp.src_acc_info = arm_compute::TensorInfo(
                 arm_compute::TensorShape(M, K, 1, src_batch), 1,
                 arm_compute::DataType::F32);
-    if (amp.is_transB)
-        amp.wei_acc_info = arm_compute::TensorInfo(
-                arm_compute::TensorShape(K, N, wei_batch), 1,
-                arm_compute::DataType::F32);
 
     amp.src_info = arm_compute::TensorInfo(
             arm_compute::TensorShape(K, M, 1, src_batch), 1,
@@ -103,6 +100,140 @@ status_t init_conf_matmul(acl_matmul_conf_t &amp, memory_desc_t &src_md,
         ACL_CHECK_VALID(arm_compute::NETranspose::validate(
                 &amp.wei_acc_info, &amp.wei_info));
 
+    arm_compute::WeightFormat expected_weight_format;
+
+    amp.gemm_info.set_fixed_format(true);
+    amp.gemm_info.set_weight_format(arm_compute::WeightFormat::ANY);
+
+    auto acl_st = arm_compute::NEGEMM::has_opt_impl(
+        expected_weight_format,
+        &amp.src_info,
+        &amp.wei_info,
+        nullptr,
+        &amp.dst_info,
+        amp.alpha,
+        0.0f,
+        amp.gemm_info);
+
+    if(acl_st.error_code() != arm_compute::ErrorCode::OK) {
+        return status::unimplemented;
+    }
+
+    amp.gemm_info.set_weight_format(expected_weight_format);
+
+    memory_desc_t want_wei_md = wei_md;
+
+    // We need to transpose second to last dimension and use blocking
+    // as returned by interleave by from expecting strides
+    int interleaved_by = arm_compute::interleave_by(expected_weight_format);
+    int block_by = arm_compute::block_by(expected_weight_format);
+    bool is_fast_math_kernel = arm_compute::is_fixed_format_fast_math(expected_weight_format);
+    if(!is_fast_math_kernel) {
+        amp.gemm_info.set_fast_math(false);
+    }
+
+    int blocked_first_dimension = -1;
+    int blocked_second_dimension = -1;
+
+    // Assume that interleaved by is X and blocked by is Y
+    switch(want_wei_md.ndims) {
+        case 2: {
+           // For 2D case the format that we need to pass is BaXb and
+           // when doing fast mode BAXbYa
+           want_wei_md.format_desc.blocking.strides[0] = interleaved_by * block_by;
+            // check to see whether we need to pad
+            if(want_wei_md.dims[0] % block_by != 0) {
+                want_wei_md.padded_dims[0] = utils::div_up(want_wei_md.dims[0], block_by) * block_by;
+            }
+            want_wei_md.format_desc.blocking.strides[1] = interleaved_by * want_wei_md.padded_dims[0];
+            if(want_wei_md.dims[1] % interleaved_by != 0) {
+                want_wei_md.padded_dims[1] = utils::div_up(want_wei_md.dims[1], interleaved_by) * interleaved_by;
+            }
+
+            acl_utils::update_strides_y_and_z(
+                amp.wei_info,
+                want_wei_md.format_desc.blocking.strides[1] * wei_d.data_type_size(),
+                want_wei_md.format_desc.blocking.strides[0] * wei_d.data_type_size());
+
+            blocked_first_dimension = 1;
+            blocked_second_dimension = 0;
+
+            break;
+        }
+
+        case 3: {
+           // For 3D case the format we need to pass is aCbXc and
+           // when doing fast mode is aCBXcYb
+           want_wei_md.format_desc.blocking.strides[1] = interleaved_by*block_by;
+           if(want_wei_md.dims[1] % block_by != 0) {
+               want_wei_md.padded_dims[1] = utils::div_up(want_wei_md.dims[1], block_by) * block_by;
+           }
+           want_wei_md.format_desc.blocking.strides[2] = interleaved_by * want_wei_md.padded_dims[1];
+           if(want_wei_md.dims[2] % interleaved_by != 0) {
+                want_wei_md.padded_dims[2] = utils::div_up(want_wei_md.dims[2], interleaved_by) * interleaved_by;
+           }
+           want_wei_md.format_desc.blocking.strides[0] = want_wei_md.padded_dims[2] * want_wei_md.padded_dims[1];
+
+           acl_utils::update_strides_y_and_z(
+                amp.wei_info,
+                want_wei_md.format_desc.blocking.strides[2] * wei_d.data_type_size(),
+                want_wei_md.format_desc.blocking.strides[0] * wei_d.data_type_size());
+
+           blocked_first_dimension = 2;
+           blocked_second_dimension = 1;
+
+           break;
+        }
+
+        case 4: {
+            // For 4D case the format we need to pass is abDcXd and
+            // when doing fast mode is abDCxdYc
+            int D_padded = want_wei_md.dims[3];
+            if(D_padded % interleaved_by != 0) {
+                D_padded = utils::div_up(D_padded, interleaved_by) * interleaved_by;
+                want_wei_md.padded_dims[3] = D_padded;
+            }
+
+            int C_padded = want_wei_md.dims[2];
+            if(C_padded % block_by != 0) {
+                C_padded = utils::div_up(C_padded, block_by) * block_by;
+                want_wei_md.padded_dims[2] = C_padded;
+            }
+
+            want_wei_md.format_desc.blocking.strides[0] = want_wei_md.dims[1]*D_padded*C_padded;
+            want_wei_md.format_desc.blocking.strides[1] = D_padded*C_padded;
+            want_wei_md.format_desc.blocking.strides[2] = interleaved_by*block_by;
+            want_wei_md.format_desc.blocking.strides[3] = interleaved_by*C_padded;
+
+            acl_utils::update_strides_y_and_z(
+                amp.wei_info,
+                want_wei_md.format_desc.blocking.strides[3] * wei_d.data_type_size(),
+                want_wei_md.format_desc.blocking.strides[1] * wei_d.data_type_size());
+
+            blocked_first_dimension = 3;
+            blocked_second_dimension = 2;
+
+            break;
+        }
+
+        default:
+            return status::unimplemented;
+    }
+
+    want_wei_md.format_desc.blocking.inner_nblks = (block_by > 1) + 1;
+    want_wei_md.format_desc.blocking.inner_idxs[0] = blocked_first_dimension;
+    want_wei_md.format_desc.blocking.inner_blks[0] = interleaved_by;
+    if(block_by > 1) {
+        want_wei_md.format_desc.blocking.inner_idxs[1] = blocked_second_dimension;
+        want_wei_md.format_desc.blocking.inner_blks[1] = block_by;
+    }
+
+    if(is_fast_math_kernel) {
+        want_wei_md.data_type = dnnl_bf16;
+    }
+
+    wei_md = want_wei_md;
+
     return status::success;
 }
 
