diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/unary.mlir b/stablehlo/stablehlo/conversions/tosa/tests/unary.mlir
--- stablehlo/stablehlo/conversions/tosa/tests/unary.mlir
+++ stablehlo/stablehlo/conversions/tosa/tests/unary.mlir
@@ -123,8 +123,7 @@
 
 // CHECK-LABEL: @transpose
 func.func @transpose(%arg0: tensor<1x2x3xf32>) -> tensor<3x2x1xf32> {
-  // CHECK: %[[VAR0:.*]] = "tosa.const"() <{value = dense<[2, 1, 0]> : tensor<3xi32>}> : () -> tensor<3xi32>
-  // CHECK: %[[VAR1:.*]] = tosa.transpose %arg0, %[[VAR0]]
+  // CHECK: %[[VAR0:.*]] = tosa.transpose %arg0 {perms = array<i32: 2, 1, 0>}
   %0 = "stablehlo.transpose"(%arg0) {permutation = array<i64: 2, 1, 0>} : (tensor<1x2x3xf32>) -> tensor<3x2x1xf32>
   return %0 : tensor<3x2x1xf32>
 }
diff --ruN a/stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp b/stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp
--- stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp
+++ stablehlo/stablehlo/conversions/tosa/transforms/StablehloLegalizeToTosa.cpp
@@ -458,13 +458,10 @@
     }
 
     auto perms = op.getPermutation();
-    auto type = RankedTensorType::get({static_cast<int64_t>(perms.size())},
-                                      rewriter.getI32Type());
     std::vector<int32_t> perms_int32(perms.begin(), perms.end());
-    auto constOp = rewriter.create<tosa::ConstOp>(
-        op->getLoc(), type, DenseIntElementsAttr::get(type, perms_int32));
-    rewriter.replaceOpWithNewOp<tosa::TransposeOp>(op, op.getType(),
-                                                   op.getOperand(), constOp);
+    rewriter.replaceOpWithNewOp<tosa::TransposeOp>(
+        op, op.getType(), op.getOperand(),
+        rewriter.getDenseI32ArrayAttr(perms_int32));
     return success();
   }
 };
diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp
--- stablehlo/stablehlo/dialect/Base.cpp
+++ stablehlo/stablehlo/dialect/Base.cpp
@@ -680,6 +680,7 @@
           {{bf16, bf16, f32, 1}, KnownDotAlgorithm::BF16_BF16_F32},
           {{bf16, bf16, f32, 3}, KnownDotAlgorithm::BF16_BF16_F32_X3},
           {{bf16, bf16, f32, 6}, KnownDotAlgorithm::BF16_BF16_F32_X6},
+          {{bf16, bf16, f32, 9}, KnownDotAlgorithm::BF16_BF16_F32_X9},
           {{tf32, tf32, f32, 1}, KnownDotAlgorithm::TF32_TF32_F32},
           {{tf32, tf32, f32, 3}, KnownDotAlgorithm::TF32_TF32_F32_X3},
           {{f32, f32, f32, 1}, KnownDotAlgorithm::F32_F32_F32},
diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h
--- stablehlo/stablehlo/dialect/Base.h
+++ stablehlo/stablehlo/dialect/Base.h
@@ -260,6 +260,7 @@
   TF32_TF32_F32_X3 = 10,
   F32_F32_F32 = 11,
   F64_F64_F64 = 12,
+  BF16_BF16_F32_X9 = 13,
 };
 
 FailureOr<KnownDotAlgorithm> getKnownDotAlgorithm(
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_simplification.mlir
@@ -924,6 +924,15 @@
   // CHECK: %[[RES:.+]] = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<7x2xf32>
   // CHECK: return %[[RES]]
   return %0 : tensor<7x2xf32>
+}
+
+// Can't do anything with the dynamic shape, but shouldn't crash.
+// CHECK-LABEL: @dynamic_pad
+func.func @dynamic_pad(%arg0: tensor<?x2x3xi1>, %arg1: tensor<i1>) -> tensor<?x2x1xi1> {
+  %0 = stablehlo.pad %arg0, %arg1, low = [0, 0, -1], high = [0, 0, -1], interior = [0, 0, 0] : (tensor<?x2x3xi1>, tensor<i1>) -> tensor<?x2x1xi1>
+  // CHECK-NEXT: %[[RES:.+]] = stablehlo.pad %arg0, %arg1, low = [0, 0, -1], high = [0, 0, -1], interior = [0, 0, 0] : (tensor<?x2x3xi1>, tensor<i1>) -> tensor<?x2x1xi1>
+  // CHECK-NEXT: return %[[RES]]
+  return %0 : tensor<?x2x1xi1>
 }
 
 // -----
@@ -1908,6 +1917,19 @@
 
 // -----
 
+// CHECK-LABEL: @side_effecting_custom_call
+func.func @side_effecting_custom_call(%arg0: tensor<0xf32>) -> (tensor<0xf32>, tensor<0xf32>) {
+  // CHECK:      %[[CST:.*]] = stablehlo.constant dense<> : tensor<0xf32>
+  // CHECK-NEXT: %[[CC:.*]] = stablehlo.custom_call @foo(%arg0) {api_version = 0 : i32, has_side_effect = true} : (tensor<0xf32>) -> tensor<0xf32>
+  %0 = stablehlo.custom_call @foo(%arg0) {api_version = 0 : i32, has_side_effect = true} : (tensor<0xf32>) -> tensor<0xf32>
+  // CHECK-NOT:  stablehlo.custom_call{{.*}}has_side_effect = false
+  %1 = stablehlo.custom_call @foo(%arg0) {api_version = 0 : i32, has_side_effect = false} : (tensor<0xf32>) -> tensor<0xf32>
+  // CHECK: return %[[CC]], %[[CST]]
+  return %0, %1 : tensor<0xf32>, tensor<0xf32>
+}
+
+// -----
+
 /////////
 // Generic Shape Ops
 
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_compatibility_expander_locations.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_compatibility_expander_locations.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_compatibility_expander_locations.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_compatibility_expander_locations.mlir
@@ -0,0 +1,43 @@
+// RUN: stablehlo-opt %s -stablehlo-compatibility-expander=target=1.8.0 --mlir-print-debuginfo | FileCheck %s
+// RUN: stablehlo-opt %s -stablehlo-compatibility-expander=target=1.8.0 --mlir-print-debuginfo | stablehlo-translate --serialize --target=1.8.0
+
+// Test that FileLineColRange locations are converted to FileLineColLoc
+// locations, including in nested location contexts, block args, module op, etc.
+// Ex: loc("file.mlir":2:21 to :30) ==> loc("file.mlir":2:21)
+
+#loc3 = loc("file.mlir":2:21 to :30)
+module {
+  func.func @main(%arg0: tensor<i32> loc("file.mlir":2:21 to :30)) -> tensor<i32> {
+    %c = stablehlo.constant dense<1> : tensor<i32> loc(#loc4)
+    %0 = stablehlo.add %arg0, %c : tensor<i32> loc(#loc5)
+    return %0 : tensor<i32> loc(#loc6)
+  } loc(#loc9)
+} loc(#loc)
+#loc = loc("file.mlir":0:0 to :3)
+#loc1 = loc("file.mlir":1:1 to :2)
+#loc2 = loc("file.mlir":2:19 to :20)
+#loc4 = loc("file.mlir":2:8 to :10)
+#loc5 = loc("file.mlir":4:10 to :12)
+#loc6 = loc("file.mlir":3:3 to :5)
+#loc7 = loc("WrappedLocation.call"(#loc1))
+#loc8 = loc("WrappedLocation.callsite"(#loc2))
+#loc9 = loc(callsite(#loc7 at #loc8))
+
+// CHECK:      #[[LOC3:.*]] = loc("file.mlir":2:21)
+// CHECK-NEXT: module {
+// CHECK-NEXT:   func.func @main{{.*}}tensor<i32> loc("file.mlir":2:21)
+// CHECK-NEXT:     stablehlo.constant {{.*}} loc(#[[LOC4:.*]])
+// CHECK-NEXT:     stablehlo.add {{.*}} : tensor<i32> loc(#[[LOC5:.*]])
+// CHECK-NEXT:     return {{.*}} loc(#[[LOC6:.*]])
+// CHECK-NEXT:   } loc(#[[LOC9:.*]])
+// CHECK-NEXT: } loc(#[[LOC:.*]])
+// CHECK-NEXT: #[[LOC]]     = loc("file.mlir":0:0)
+// CHECK-NEXT: #[[LOC1:.*]] = loc("file.mlir":1:1)
+// CHECK-NEXT: #[[LOC2:.*]] = loc("file.mlir":2:19)
+// CHECK-NEXT: #[[LOC4]]    = loc("file.mlir":2:8)
+// CHECK-NEXT: #[[LOC5]]    = loc("file.mlir":4:10)
+// CHECK-NEXT: #[[LOC6]]    = loc("file.mlir":3:3)
+// CHECK-NEXT: #[[LOC7:.*]] = loc("WrappedLocation.call"(#[[LOC1]]))
+// CHECK-NEXT: #[[LOC8:.*]] = loc("WrappedLocation.callsite"(#[[LOC2]]))
+// CHECK-NEXT: #[[LOC9]] = loc(callsite(#[[LOC7]] at #[[LOC8]]))
+
diff --ruN a/stablehlo/stablehlo/transforms/Passes.td b/stablehlo/stablehlo/transforms/Passes.td
--- stablehlo/stablehlo/transforms/Passes.td
+++ stablehlo/stablehlo/transforms/Passes.td
@@ -55,7 +55,7 @@
   }];
 }
 
-def StablehloCompatibilityExpanderPass : Pass<"stablehlo-compatibility-expander", "mlir::func::FuncOp"> {
+def StablehloCompatibilityExpanderPass : Pass<"stablehlo-compatibility-expander", "mlir::ModuleOp"> {
   let summary = "Compatibility expander for StableHLO operations.";
 
   let description = [{
diff --ruN a/stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp b/stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp
--- stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp
+++ stablehlo/stablehlo/transforms/StablehloCompatibilityExpander.cpp
@@ -11,33 +11,43 @@
 ==============================================================================*/
 
 #include <fcntl.h>
+#include <stdbool.h>
 
 #include <algorithm>
 #include <cassert>
 #include <cstdint>
 #include <iterator>
+#include <optional>
 #include <utility>
 
 #include "llvm/ADT/APFloat.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/Debug.h"
 #include "llvm/Support/ErrorHandling.h"
 #include "llvm/Support/MathExtras.h"
 #include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/AttrTypeSubElements.h"
+#include "mlir/IR/Attributes.h"
 #include "mlir/IR/Builders.h"
 #include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinOps.h"
 #include "mlir/IR/BuiltinTypeInterfaces.h"
 #include "mlir/IR/BuiltinTypes.h"
 #include "mlir/IR/Diagnostics.h"
+#include "mlir/IR/Location.h"
 #include "mlir/IR/PatternMatch.h"
+#include "mlir/IR/Region.h"
 #include "mlir/Rewrite/FrozenRewritePatternSet.h"
 #include "mlir/Support/LLVM.h"
-#include "mlir/Transforms/DialectConversion.h"
 #include "mlir/Transforms/GreedyPatternRewriteDriver.h"
 #include "stablehlo/dialect/StablehloOps.h"
 #include "stablehlo/dialect/Version.h"
-#include "stablehlo/transforms/PassUtils.h"
+#include "stablehlo/transforms/PassUtils.h"  // IWYU pragma: keep
 #include "stablehlo/transforms/Passes.h"
+
+#define DEBUG_TYPE "compat-passes"
 
 namespace mlir {
 namespace stablehlo {
@@ -167,7 +177,7 @@
 // Converts a `GatherOp` with batching dims to a `GatherOp` without batching
 // dims, such that each batching dim becomes a collapsed slice dim with a
 // corresponding `IotaOp` concatenated to the start indices.
-class GatherWithBatchingDimsExpander : public OpRewritePattern<GatherOp> {
+struct GatherWithBatchingDimsExpander : public OpRewritePattern<GatherOp> {
   using OpRewritePattern<GatherOp>::OpRewritePattern;
 
   LogicalResult matchAndRewrite(GatherOp op,
@@ -213,7 +223,7 @@
 // Converts a `ScatterOp` with batching dims to a `ScatterOp` without batching
 // dims, such that each batching dim becomes an inserted window dim with a
 // corresponding `IotaOp` concatenated to the scatter indices.
-class ScatterWithBatchingDimsExpander : public OpRewritePattern<ScatterOp> {
+struct ScatterWithBatchingDimsExpander : public OpRewritePattern<ScatterOp> {
   using OpRewritePattern<ScatterOp>::OpRewritePattern;
 
   LogicalResult matchAndRewrite(ScatterOp op,
@@ -262,6 +272,43 @@
   }
 };
 
+// FileLineColRange locations are a forward incompatibility in upstream MLIR.
+// This pattern removes the precise start/end range information and converts
+// all FileLineColRange locations to forward compatible FileLineColLoc
+// locations.
+struct FileLineColRangeToLoc : public OpRewritePattern<ModuleOp> {
+  using OpRewritePattern<ModuleOp>::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(ModuleOp op,
+                                PatternRewriter &rewriter) const override {
+    bool changed = false;
+    mlir::AttrTypeReplacer replacer;
+    replacer.addReplacement([&](FileLineColLoc flcLoc)
+                                -> std::optional<Location> {
+      // Skip if it's actually a FileLineColLoc
+      if (isStrictFileLineColLoc(flcLoc)) return flcLoc;
+
+      // Replace FileLineColRange with FileLineColLoc
+      changed = true;
+      auto newFlcLoc = FileLineColLoc::get(
+          flcLoc.getFilename(), flcLoc.getStartLine(), flcLoc.getStartColumn());
+      LLVM_DEBUG(llvm::dbgs()
+                 << "Rewriting FLC " << flcLoc << " -> " << newFlcLoc << "\n");
+      return newFlcLoc;
+    });
+
+    // Call this on the module to update all locations in the module.
+    // This should be safe since this pass is declared as a ModuleOp level pass
+    // in the pass TD file, so no async issues.
+    replacer.recursivelyReplaceElementsIn(op,
+                                          /*replaceAttrs=*/false,
+                                          /*replaceLocs=*/true,
+                                          /*replaceTypes=*/false);
+
+    return success(changed);
+  }
+};
+
 //===----------------------------------------------------------------------===//
 // Pass
 //===----------------------------------------------------------------------===//
@@ -282,6 +329,7 @@
     auto targetVersion = validateTargetVersion(targetVersionOption);
 
     config.useTopDownTraversal = true;
+
     RewritePatternSet patterns_(context);
     populateStablehloCompatibilityExpanderPatterns(&patterns_, context,
                                                    targetVersion);
@@ -290,9 +338,13 @@
   }
 
   void runOnOperation() override {
-    auto func = getOperation();
-    if (failed(applyPatternsGreedily(func, patterns, config))) {
-      func.emitError(
+    auto module = getOperation();
+
+    // Apply to both the module and its children
+    if (failed(
+            applyOpPatternsGreedily(module.getOperation(), patterns, config)) ||
+        failed(applyPatternsGreedily(module, patterns, config))) {
+      module.emitError(
           "Failed to converge StableHLOCompatibilityExpanderPass in ")
           << config.maxIterations << " iterations";
       signalPassFailure();
@@ -321,6 +373,12 @@
   if (targetVersion < vhlo::Version(1, 4, 0))
     patterns->add<TanOp_ComplexElementType_CompatiblityExpander,
                   TanOp_CompatiblityExpander>(context);
+
+  // MLIR Upstream FileLineColRange introduced ~v1.8.4
+  // Conservatively use 1.9.0 since StableHLO passes require major versions for
+  // incompats.
+  if (targetVersion < vhlo::Version(1, 9, 0))
+    patterns->add<FileLineColRangeToLoc>(context);
 }
 
 }  // namespace stablehlo
diff --ruN a/stablehlo/stablehlo/transforms/optimization/Passes.h b/stablehlo/stablehlo/transforms/optimization/Passes.h
--- stablehlo/stablehlo/transforms/optimization/Passes.h
+++ stablehlo/stablehlo/transforms/optimization/Passes.h
@@ -50,6 +50,13 @@
                                           MLIRContext *context,
                                           bool foldFloat = false,
                                           PatternBenefit benefit = 1);
+
+/// Some workloads in XLA import StableHLO from HLO. Since there are a few
+/// differences in HLO (no implicit captures, lots of tuples, etc.), this
+/// set of patterns brings the imported HLO back to a more canonical form
+/// without applying a full set of graph simplifications.
+void populateStablehloHloImportCanonicalizationPatterns(
+    MLIRContext *context, RewritePatternSet *patterns);
 }  // namespace stablehlo
 }  // namespace mlir
 
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplification.cpp
@@ -934,8 +934,12 @@
     auto padVal = op.getPaddingValue();
 
     auto resultTy = cast<RankedTensorType>(op.getType());
-
-    if (cast<ShapedType>(operand.getType()).getNumElements() != 0)
+    auto operandTy = cast<RankedTensorType>(operand.getType());
+
+    if (!operandTy.hasStaticShape())
+      return rewriter.notifyMatchFailure(op, "operand shape is dynamic");
+
+    if (operandTy.getNumElements() != 0)
       return rewriter.notifyMatchFailure(op, "operand is not empty tensor");
 
     if (resultTy.hasStaticShape()) {
@@ -1399,6 +1403,12 @@
       return rewriter.notifyMatchFailure(op, "not stablehlo");
     if (isa<ConstantOp>(op))
       return rewriter.notifyMatchFailure(op, "op is empty constant");
+
+    // Skip ops that have memory effects, similar to XLA's zero extent
+    // simplification, replacing these doesn't save any computation.
+    auto effectInterface = dyn_cast<MemoryEffectOpInterface>(op);
+    if (effectInterface && !effectInterface.hasNoEffect())
+      return rewriter.notifyMatchFailure(op, "op has memory effect");
 
     // If the result is a zero-extent tensor, replace the whole op with an empty
     // constant.
@@ -1528,6 +1538,12 @@
             DynamicReshapeOpIsStatic, DynamicIotaIsStatic>(context);
 }
 
+void populateStablehloHloImportCanonicalizationPatterns(
+    MLIRContext *context, RewritePatternSet *patterns) {
+  patterns->add<TupleIsRepacking, TupleIsUnpacked, WhileOpImplicitCapture>(
+      context);
+}
+
 std::unique_ptr<Pass> createStablehloAggressiveSimplificationPass(
     GreedyRewriteConfig config) {
   return std::make_unique<StablehloAggressiveSimplificationPass>(config);
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveSimplificationPatterns.td
@@ -411,7 +411,7 @@
 // GetTupleElementOp
 
 // Pattern: get_tuple_element(tuple(X_0, X_1, ...), i) -> X_i
-def : Pat<(StableHLO_GetTupleElementOp (StableHLO_TupleOp:$tuple $operands), $idx),
+def TupleIsUnpacked : Pat<(StableHLO_GetTupleElementOp (StableHLO_TupleOp:$tuple $operands), $idx),
           (GetOperandN $tuple, $idx)>;
 
 ////////

