diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h
--- stablehlo/stablehlo/dialect/StablehloOps.h
+++ stablehlo/stablehlo/dialect/StablehloOps.h
@@ -24,6 +24,7 @@
 #include "mlir/Dialect/Quant/IR/QuantTypes.h"
 #include "mlir/Dialect/Shape/IR/Shape.h"
 #include "mlir/IR/Attributes.h"
+#include "mlir/IR/Block.h"
 #include "mlir/IR/Builders.h"
 #include "mlir/IR/BuiltinAttributes.h"
 #include "mlir/IR/BuiltinTypes.h"
@@ -33,6 +34,7 @@
 #include "mlir/IR/MLIRContext.h"
 #include "mlir/IR/OpDefinition.h"
 #include "mlir/IR/Operation.h"
+#include "mlir/IR/Region.h"
 #include "mlir/IR/TensorEncoding.h"
 #include "mlir/IR/TypeUtilities.h"
 #include "mlir/IR/Types.h"
@@ -174,6 +176,21 @@
                     const llvm::ArrayRef<Type> &elementTypes, int64_t dimension,
                     bool isStable, ComparisonDirection direction);
 
+template <typename OpTy>
+void buildReduceBody(Type elementType, Region &body, OpBuilder &builder) {
+  OpBuilder::InsertionGuard guard(builder);
+  Block *block = builder.createBlock(&body);
+
+  // Block arguments are scalars of the given element type.
+  Type type = RankedTensorType::get(/*shape=*/{}, elementType);
+  Location loc = body.getLoc();
+  block->addArguments({type, type}, {loc, loc});
+
+  auto reducer =
+      builder.create<OpTy>(loc, block->getArgument(0), block->getArgument(1));
+  builder.create<stablehlo::ReturnOp>(loc, reducer.getResult());
+}
+
 }  // end namespace stablehlo
 }  // end namespace mlir
 
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -1161,7 +1161,8 @@
 //===----------------------------------------------------------------------===//
 
 def StableHLO_InfeedOp : StableHLO_Op<"infeed", [
-      MemoryEffects<[MemRead<StableHLO_InfeedResource>]>,
+      MemoryEffects<[MemRead<StableHLO_InfeedResource>,
+                     MemWrite<StableHLO_InfeedResource>]>,
     ]> {
   let summary = "Infeed operation";
   let description = [{
@@ -1188,7 +1189,8 @@
 
 def StableHLO_OutfeedOp : StableHLO_Op<"outfeed", [
       DeclareOpInterfaceMethods<InferTypeOpInterface>,
-      MemoryEffects<[MemWrite<StableHLO_OutfeedResource>]>,
+      MemoryEffects<[MemRead<StableHLO_OutfeedResource>,
+                     MemWrite<StableHLO_OutfeedResource>]>,
     ]> {
   let summary = "Outfeed operation";
   let description = [{
@@ -1214,7 +1216,8 @@
 
 def StableHLO_SendOp : StableHLO_Op<"send", [
       DeclareOpInterfaceMethods<InferTypeOpInterface>,
-      MemoryEffects<[MemWrite<StableHLO_SendResource>]>,
+      MemoryEffects<[MemRead<StableHLO_SendResource>,
+                     MemWrite<StableHLO_SendResource>]>,
     ]> {
   let summary = "Send operation";
   let description = [{
@@ -1243,7 +1246,8 @@
 }
 
 def StableHLO_RecvOp : StableHLO_Op<"recv", [
-      MemoryEffects<[MemRead<StableHLO_RecvResource>]>
+      MemoryEffects<[MemRead<StableHLO_RecvResource>,
+                     MemWrite<StableHLO_RecvResource>]>,
     ]> {
   let summary = "Recv operation";
   let description = [{
diff --ruN a/stablehlo/stablehlo/dialect/TypeInference.cpp b/stablehlo/stablehlo/dialect/TypeInference.cpp
--- stablehlo/stablehlo/dialect/TypeInference.cpp
+++ stablehlo/stablehlo/dialect/TypeInference.cpp
@@ -879,7 +879,8 @@
 
   auto replicaIds = replicaGroups.getValues<int64_t>();
 
-  llvm::SmallSet<int64_t, 8> replicaIdsSeen;
+  // Large programs can have many replicas, use a set with efficient lookup.
+  llvm::DenseSet<int64_t> replicaIdsSeen;
   for (int64_t replicaId : replicaIds) {
     // Replica groups are stored in a 2D tensor. If the op supports non-uniform
     // groups, null replica IDs are stored as -1.
@@ -1841,6 +1842,7 @@
                                  /*allGroupsMustHaveSameSize=*/true,
                                  /*useGlobalDeviceIds=*/false, splitCount)))
     return failure();
+
   for (const Value& operand : operands) {
     auto operandType = cast<RankedTensorType>(operand.getType());
 
@@ -3562,6 +3564,19 @@
                                 DenseIntElementsAttr replicaGroups,
                                 int64_t channelId, bool useGlobalDeviceIds,
                                 ValueRange results) {
+  // all_gather_i3, all_gather_c2, all_gather_c4
+  if (failed(verifyReplicaGroups(location, replicaGroups,
+                                 /*allGroupsMustHaveSameSize=*/true,
+                                 useGlobalDeviceIds,
+                                 /*expectedGroupSize=*/std::nullopt)))
+    return failure();
+
+  // all_gather_c5
+  if (useGlobalDeviceIds && channelId < 0)
+    return emitOptionalError(
+        location,
+        "channel_id cannot be negative when useGlobalDeviceIds is set");
+
   for (const auto& [operand, result] : llvm::zip(operands, results)) {
     auto operandType = cast<RankedTensorType>(operand.getType());
     auto resultType = cast<RankedTensorType>(result.getType());
@@ -3576,19 +3591,6 @@
       return emitOptionalError(
           location,
           "dimension size of operand at 'all_gather_dim' cannot be zero");
-
-    // all_gather_i3, all_gather_c2, all_gather_c4
-    if (failed(verifyReplicaGroups(location, replicaGroups,
-                                   /*allGroupsMustHaveSameSize=*/true,
-                                   useGlobalDeviceIds,
-                                   /*expectedGroupSize=*/std::nullopt)))
-      return failure();
-
-    // all_gather_c5
-    if (useGlobalDeviceIds && channelId < 0)
-      return emitOptionalError(
-          location,
-          "channel_id cannot be negative when useGlobalDeviceIds is set");
 
     // all_gather_c6
     if (resultType.getRank() != operandType.getRank())
@@ -3788,7 +3790,7 @@
         "but instead it is of rank ", replicaGroupType.getRank());
 
   auto replicaIds = replicaGroups.getValues<int64_t>();
-  llvm::SmallSet<int64_t, 8> replicaIdsSeen;
+  llvm::DenseSet<int64_t> replicaIdsSeen;
   for (int64_t replicaId : replicaIds) {
     // collective_broadcast_c2
     // We only check that is is not negative, as it is impossible
diff --ruN a/stablehlo/stablehlo/integrations/c/StablehloDialectApi.cpp b/stablehlo/stablehlo/integrations/c/StablehloDialectApi.cpp
--- stablehlo/stablehlo/integrations/c/StablehloDialectApi.cpp
+++ stablehlo/stablehlo/integrations/c/StablehloDialectApi.cpp
@@ -78,10 +78,11 @@
 
 MlirLogicalResult stablehloSerializePortableArtifactFromModule(
     MlirModule moduleStr, MlirStringRef targetVersion,
-    MlirStringCallback callback, void *userData) {
+    MlirStringCallback callback, void *userData, bool allowOtherDialects) {
   mlir::detail::CallbackOstream stream(callback, userData);
   if (failed(mlir::stablehlo::serializePortableArtifact(
-          unwrap(moduleStr), unwrap(targetVersion), stream)))
+          unwrap(moduleStr), unwrap(targetVersion), stream,
+          allowOtherDialects)))
     return mlirLogicalResultFailure();
   return mlirLogicalResultSuccess();
 }
diff --ruN a/stablehlo/stablehlo/integrations/c/StablehloDialectApi.h b/stablehlo/stablehlo/integrations/c/StablehloDialectApi.h
--- stablehlo/stablehlo/integrations/c/StablehloDialectApi.h
+++ stablehlo/stablehlo/integrations/c/StablehloDialectApi.h
@@ -92,7 +92,8 @@
 stablehloSerializePortableArtifactFromModule(MlirModule moduleStr,
                                              MlirStringRef targetVersion,
                                              MlirStringCallback callback,
-                                             void* userData);
+                                             void* userData,
+                                             bool allowOtherDialects = false);
 
 // Read a StableHLO program from a portable artifact, returning the module as
 // MLIR bytecode. Note, this bytecode returned is not a portable artifact,
diff --ruN a/stablehlo/stablehlo/integrations/python/StablehloApi.cpp b/stablehlo/stablehlo/integrations/python/StablehloApi.cpp
--- stablehlo/stablehlo/integrations/python/StablehloApi.cpp
+++ stablehlo/stablehlo/integrations/python/StablehloApi.cpp
@@ -102,20 +102,22 @@
   //
   m.def(
       "serialize_portable_artifact",
-      [](MlirModule module, std::string_view target) -> nb::bytes {
+      [](MlirModule module, std::string_view target,
+         bool allowOtherDialects) -> nb::bytes {
         StringWriterHelper accumulator;
         if (mlirLogicalResultIsFailure(
                 stablehloSerializePortableArtifactFromModule(
                     module, toMlirStringRef(target),
                     accumulator.getMlirStringCallback(),
-                    accumulator.getUserData()))) {
+                    accumulator.getUserData(), allowOtherDialects))) {
           throw nb::value_error("failed to serialize module");
         }
 
         std::string serialized = accumulator.toString();
         return nb::bytes(serialized.data(), serialized.size());
       },
-      nb::arg("module"), nb::arg("target"));
+      nb::arg("module"), nb::arg("target"),
+      nb::arg("allow_other_dialects") = false);
 
   m.def(
       "deserialize_portable_artifact",
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_convert_to_signless.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_convert_to_signless.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_convert_to_signless.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_convert_to_signless.mlir
@@ -9,7 +9,7 @@
   %3 = builtin.unrealized_conversion_cast %2 : memref<i16> to memref<ui16>
   %4 = bufferization.to_tensor %3 : memref<ui16> to tensor<ui16>
   %5 = builtin.unrealized_conversion_cast %4 : tensor<ui16> to tensor<i16>
-  %6 = bufferization.to_memref %5 : tensor<i16> to memref<i16>
+  %6 = bufferization.to_buffer %5 : tensor<i16> to memref<i16>
   %7 = builtin.unrealized_conversion_cast %6 : memref<i16> to memref<ui16>
   func.return %7 : memref<ui16>
 }

