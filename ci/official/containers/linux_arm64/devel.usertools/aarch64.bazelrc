# This bazelrc can build a CPU-supporting TF package.

# Convenient cache configurations
# Use a cache directory mounted to /tf/cache. Very useful!
build:sigbuild_local_cache --disk_cache=/tf/cache
# Use the public-access TF DevInfra cache (read only)
build:sigbuild_remote_cache --remote_cache="https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/manylinux2014" --remote_upload_local_results=false
# Change the value of CACHEBUSTER when upgrading the toolchain, or when testing
# different compilation methods. E.g. for a PR to test a new CUDA version, set
# the CACHEBUSTER to the PR number.
build --action_env=CACHEBUSTER=20220325

# Use Python 3.X as installed in container image
build --action_env PYTHON_BIN_PATH="/usr/local/bin/python3"
build --python_path="/usr/local/bin/python3"

# Build TensorFlow v2
build --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1

# Prevent double-compilation of some TF code, ref. b/183279666 (internal)
# > TF's gen_api_init_files has a genrule to run the core TensorFlow code
# > on the host machine. If we don't have --distinct_host_configuration=false,
# > the core TensorFlow code will be built once for the host and once for the
# > target platform.
# See also https://docs.bazel.build/versions/master/guide.html#build-configurations-and-cross-compilation
build --distinct_host_configuration=false

# Store performance profiling log in the mounted artifact directory.
# The profile can be viewed by visiting chrome://tracing in a Chrome browser.
# See https://docs.bazel.build/versions/main/skylark/performance.html#performance-profiling
build --profile=/tf/pkg/profile.json.gz

# Use the rebuilt gcc toolchain to compile for manylinux2014
build --crosstool_top="@ml2014_aarch64_config_aarch64//crosstool:toolchain"
test --crosstool_top="@ml2014_aarch64_config_aarch64//crosstool:toolchain"

build --copt="-mtune=generic" --copt="-march=armv8-a" --copt="-O3"

# Test-related settings below this point.
test --build_tests_only --keep_going --test_output=errors --verbose_failures=true
test --test_timeout=500,900,-1,-1
# Give only the list of failed tests at the end of the log
test --test_summary=short

# "nonpip" tests are regular py_test tests.
# Pass --config=nonpip to run the same suite of tests. If you want to run just
# one test for investigation, you don't need --config=nonpip; just run the
# bazel test invocation as normal.
test:nonpip_filters --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310
test:nonpip_filters --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310
test:nonpip_filters --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium
test:nonpip --config=nonpip_filters -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...

# "pip tests" run a similar suite of tests the "nonpip" tests, but do something
# odd to attempt to validate the quality of the pip package. The wheel is
# installed into a virtual environment, and then that venv is used to run all
# bazel tests with a special flag "--define=no_tensorflow_py_deps=true", which
# drops all the bazel dependencies for each py_test; this makes all the tests
# use the wheel's TensorFlow installation instead of the one made available
# through bazel. This must be done in a different root directory, //bazel_pip/...,
# because "import tensorflow" run from the root directory would instead import
# the folder instead of the venv package.
# 
# Pass --config=pip to run the same suite of tests. If you want to run just one
# test for investigation, you'll need --config=pip_venv instead, and then you
# can specify whichever target you want.
test:pip_venv --action_env PYTHON_BIN_PATH="/bazel_pip/bin/python3"
test:pip_venv --action_env PYTHON_LIB_PATH="/bazel_pip/lib/python3/site-packages"
test:pip_venv --python_path="/bazel_pip/bin/python3"
test:pip_venv --define=no_tensorflow_py_deps=true
test:pip --config=pip_venv
# Yes, we don't exclude the gpu tests on pip for some reason.
test:pip_filters --test_tag_filters=-nopip,-no_pip,-no_oss,-oss_serial,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310
test:pip_filters --build_tag_filters=-nopip,-no_pip,-no_oss,-oss_serial,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310
test:pip_filters --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium
test:pip --config=pip_filters -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/lite/... -//tensorflow/tools/toolchains/...

# For building libtensorflow archives
test:libtensorflow_test -- //tensorflow/tools/lib_package:libtensorflow_test //tensorflow/tools/lib_package:libtensorflow_java_test
build:libtensorflow_build -- //tensorflow/tools/lib_package:libtensorflow.tar.gz //tensorflow/tools/lib_package:libtensorflow_jni.tar.gz //tensorflow/java:libtensorflow.jar //tensorflow/java:libtensorflow-src.jar //tensorflow/tools/lib_package:libtensorflow_proto.zip

# For continuous builds
test:pycpp_filters --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310
test:pycpp_filters --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310
test:pycpp_filters --test_lang_filters=cc,py --flaky_test_attempts=3 --test_size_filters=small,medium
test:pycpp --config=pycpp_filters -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/core/tpu/... -//tensorflow/lite/... -//tensorflow/tools/toolchains/...
