{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOEk20_fG3B9"
   },
   "source": [
    "# TensorFlow Neural Machine Translation on Cloud TPUs\n",
    "\n",
    "This tutorial demonstrates how to translate text using a LSTM Network from one language to another (from English to German in this case). We will work with a dataset that contains pairs of English-German phrases. Given a sequence of words in English, we train a model to predict the German equivalent in the sequence.\n",
    "\n",
    "Note: Enable TPU acceleration to execute this notebook faster. In Colab: Runtime > Change runtime type > Hardware acclerator > TPU. If running locally make sure TensorFlow version >= 1.11.\n",
    "\n",
    "This tutorial includes runnable code implemented using [tf.keras](https://www.tensorflow.org/programmers_guide/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "7njQiFR4GvVu",
    "outputId": "b8940325-961e-4306-888f-040450b7c979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n",
      "--2019-08-11 12:27:55--  http://www.manythings.org/anki/deu-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6cc4, ...\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4541707 (4.3M) [application/zip]\n",
      "Saving to: ‘deu-eng.zip’\n",
      "\n",
      "deu-eng.zip         100%[===================>]   4.33M  15.2MB/s    in 0.3s    \n",
      "\n",
      "2019-08-11 12:27:55 (15.2 MB/s) - ‘deu-eng.zip’ saved [4541707/4541707]\n",
      "\n",
      "Archive:  deu-eng.zip\n",
      "  inflating: deu.txt                 \n",
      "  inflating: _about.txt              \n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!wget http://www.manythings.org/anki/deu-eng.zip\n",
    "!unzip deu-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "VDKbicZCG1u4",
    "outputId": "3c69b962-89cc-44dd-e1cc-d2c3f6550ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\tHallo!\n",
      "Hi.\tGrüß Gott!\n",
      "Run!\tLauf!\n",
      "Wow!\tPotzdonner!\n",
      "Wow!\tDonnerwetter!\n",
      "Fire!\tFeuer!\n",
      "Help!\tHilfe!\n",
      "Help!\tZu Hülf!\n",
      "Stop!\tStopp!\n",
      "Wait!\tWarte!\n"
     ]
    }
   ],
   "source": [
    "!head deu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UA_LkfdIDco"
   },
   "source": [
    "### Importing TensorFlow and other libraries\n",
    "\n",
    "Here, apart from `Tensorflow`, we will also be importing the helper functions from the `tf.keras` library to help with cleaning and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YrUkkyKmE6_1"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, RepeatVector, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZfpxwubIc5R"
   },
   "source": [
    "### Extracting lines from dataset and into array\n",
    "\n",
    "Here, we can examine how the dataset is structures. The English-German dataset comprises of an English and German phrase separted by a tab `\\t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "jVuZy_JTF8Oo",
    "outputId": "3b9aae8e-9fef-427d-95c3-12e996e8c958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi.' 'Hallo!']\n",
      " ['Hi.' 'Grüß Gott!']\n",
      " ['Run!' 'Lauf!']\n",
      " ['Wow!' 'Potzdonner!']\n",
      " ['Wow!' 'Donnerwetter!']]\n"
     ]
    }
   ],
   "source": [
    "deu_eng = open('./deu.txt', mode='rt', encoding='utf-8')\n",
    "deu_eng = deu_eng.read()\n",
    "deu_eng = deu_eng.strip().split('\\n')\n",
    "deu_eng = [i.split('\\t') for i in deu_eng]\n",
    "deu_eng = array(deu_eng)\n",
    "deu_eng = deu_eng[:50000, :]\n",
    "print (deu_eng[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mqGMosWJIP7"
   },
   "source": [
    "### Removing punctuation\n",
    "\n",
    "We will be removing punctuation from the phrases and converting them to lowercase. We will not be creating embeddings for punctuations or uppercase characters as it adds to the complexity of the NMT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "blqRhSTmJF_A",
    "outputId": "7e996abd-17b6-430a-f254-ec0862f166c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi' 'hallo']\n",
      " ['hi' 'grüß gott']\n",
      " ['run' 'lauf']\n",
      " ['wow' 'potzdonner']\n",
      " ['wow' 'donnerwetter']]\n"
     ]
    }
   ],
   "source": [
    "deu_eng[:, 0] = [s.translate((str.maketrans('', '', string.punctuation))) for s in deu_eng[:, 0]]\n",
    "deu_eng[:, 1] = [s.translate((str.maketrans('', '', string.punctuation))) for s in deu_eng[:, 1]]\n",
    "\n",
    "for i in range(len(deu_eng)):\n",
    "    deu_eng[i, 0] = deu_eng[i, 0].lower()\n",
    "    deu_eng[i, 1] = deu_eng[i, 1].lower()\n",
    "\n",
    "print (deu_eng[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqWZWUAwJcjE"
   },
   "source": [
    "### Tokenising the phrases\n",
    "\n",
    "Tokenisation is the process of taking a sequence and chopping it up into smaller pieces called `tokens`. For example, suppose we have a sentence \n",
    "\n",
    "`\"Bob returned home after the party\"`\n",
    "\n",
    "The tokenised sentence will return an array with the tokens:\n",
    "\n",
    "`[\"Bob\", \"returned\", \"home\", \"after\", \"the\", \"party\"]`\n",
    "\n",
    "In this section, we will be breaking up the phrases into tokenised sequences that comprises of numbers for each unique word. For instance, the word \"good\" may have the value of 32 while the word \"boy\" may have the value of 46. Supposing the phrase is \"good boy\", the tokenised sequence is `[32, 46]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LWlZwu6PJa9p",
    "outputId": "7a68603d-f177-4ef0-8f0b-c610a828b865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 6352\n",
      "German vocabulary size: 10678\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    \n",
    "    return tokenizer\n",
    "    \n",
    "eng_tokenizer = tokenize(deu_eng[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_sequence_length = 8\n",
    "print ('English vocabulary size: {}'.format(eng_vocab_size))\n",
    "\n",
    "deu_tokenizer = tokenize(deu_eng[:, 1])\n",
    "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
    "deu_sequence_length = 8\n",
    "print ('German vocabulary size: {}'.format(deu_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bv6jcJsvK0AR"
   },
   "source": [
    "### Convert lines into sequences as input for the NMT model\n",
    "\n",
    "We will now be using our Tokeniser to create tokenised sequences of the original English and German phrases from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM20OwUtKs4e"
   },
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, sequence_length, lines):\n",
    "    sequence = tokenizer.texts_to_sequences(lines)\n",
    "    sequence = pad_sequences(sequence, sequence_length, padding=\"post\") # 0s after the actual sequence\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m2kHwXOBLFEX"
   },
   "source": [
    "### Splitting the dataset into training and testing sets\n",
    "\n",
    "We will dividing our English-German dataset into two parts - training and testing.\n",
    "We will be using the 80-20 split so that our model generalises well on the dataset while still having enough instances to test the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AuGjDhZeLFqY",
    "outputId": "b502294f-37f6-4730-db5b-7851820fd041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 8) (40000, 8)\n",
      "(10000, 8) (10000, 8)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(deu_eng, test_size=.2, random_state=12)\n",
    "\n",
    "x_train = encode_sequences(deu_tokenizer, deu_sequence_length, train[:, 1])\n",
    "y_train = encode_sequences(eng_tokenizer, eng_sequence_length, train[:, 0])\n",
    "\n",
    "x_test = encode_sequences(deu_tokenizer, deu_sequence_length, test[:, 1])\n",
    "y_test = encode_sequences(eng_tokenizer, eng_sequence_length, test[:, 0])\n",
    "\n",
    "print (x_train.shape, y_train.shape)\n",
    "print (x_test.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4nWQSW7LNTp"
   },
   "source": [
    "### Training on a TPU\n",
    "\n",
    "In order to connect to a TPU, we can follow 4 easy steps:\n",
    "\n",
    "1. Connect to a TPU instance\n",
    "2. Initialise a parallelly-distributed training `strategy`\n",
    "3. Build our NMT model under the `strategy`\n",
    "4. Train the model on a TPU\n",
    "\n",
    "For more details on training on TPUs for free, feel free to check out [this](https://medium.com/@mail.rishabh.anand/tpu-training-made-easy-with-colab-3b73b920878f) article that covers the process in great detail.\n",
    "<br>\n",
    "Here, we can see that we have around 11 instances that are free for us to use. You won't always find 11 but that shouldn't be much of a problem as long as we have access to a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "mkwlNRk1LLkz",
    "outputId": "99c0f784-ba1b-4134-fc20-ff1ace935221"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint # for pretty printing our device stats\n",
    "\n",
    "# Connecting to TPU instance\n",
    "if 'COLAB_TPU_ADDR' not in os.environ:\n",
    "    print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
    "else:\n",
    "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "    print ('TPU address is', tpu_address)\n",
    "\n",
    "    with tf.Session(tpu_address) as session:\n",
    "        devices = session.list_devices()\n",
    "\n",
    "    print('TPU devices:')\n",
    "    pprint.pprint(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising a training strategy\n",
    "\n",
    "Here, we will be initialising a parallelly-distributed training strategy. All this does is makes our code executable on a TPU. It also makes our general purpose `tf.keras` model compatible for TPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "a5WySFFxL391",
    "outputId": "92c671a5-85fa-4f9e-eec6-67c99c4b55a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 12:28:12.760934 139874475276160 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialising a parallelly-distributed training strategy\n",
    "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
    "strategy = tf.contrib.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "pwSP8GeTMCp-",
    "outputId": "5b4e00ef-eb51-42ab-b038-330a128eef86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 12:28:21.273466 139874475276160 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0811 12:28:21.343526 139874475276160 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0811 12:28:21.661533 139874475276160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 8, 512)            5467136   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 8, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8, 6352)           3258576   \n",
      "=================================================================\n",
      "Total params: 12,924,112\n",
      "Trainable params: 12,924,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building our model under that strategy\n",
    "\n",
    "in_vocab = deu_vocab_size\n",
    "out_vocab = eng_vocab_size\n",
    "units = 512\n",
    "in_timesteps = deu_sequence_length\n",
    "out_timesteps = eng_sequence_length\n",
    "\n",
    "with strategy.scope():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "\n",
    "    rms = RMSprop(lr=0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=rms)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "B33H_fwvMn6f",
    "outputId": "7dcabdf2-859c-4ee1-adc0-388542e5ba96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0811 12:28:36.859959 139874475276160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py:411: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 13s 26ms/step - loss: 2.8356\n",
      "Epoch 2/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 2.1901\n",
      "Epoch 3/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 1.8496\n",
      "Epoch 4/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 1.6087\n",
      "Epoch 5/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 1.4174\n",
      "Epoch 6/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 1.2542\n",
      "Epoch 7/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 1.1097\n",
      "Epoch 8/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.9866\n",
      "Epoch 9/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.8774\n",
      "Epoch 10/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.7793\n",
      "Epoch 11/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6960\n",
      "Epoch 12/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.6268\n",
      "Epoch 13/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5663\n",
      "Epoch 14/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.5100\n",
      "Epoch 15/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4621\n",
      "Epoch 16/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.4206\n",
      "Epoch 17/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3858\n",
      "Epoch 18/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3553\n",
      "Epoch 19/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.3283\n",
      "Epoch 20/30\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.3040\n",
      "Epoch 21/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2841\n",
      "Epoch 22/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2655\n",
      "Epoch 23/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.2518\n",
      "Epoch 24/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.2388\n",
      "Epoch 25/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.2233\n",
      "Epoch 26/30\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2113\n",
      "Epoch 27/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.2008\n",
      "Epoch 28/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.1921\n",
      "Epoch 29/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.1819\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.1738\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train.reshape(y_train.shape[0], y_train.shape[1], 1), epochs=30, steps_per_epoch=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4YwTfTROVv4"
   },
   "source": [
    "### Checking the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "2S6V9YHoOLk5",
    "outputId": "254d4e69-c151-4231-989b-98299ca275c1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sparse Categorical Loss')\n",
    "plt.legend(['train'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LiqzrayKOYoW"
   },
   "source": [
    "### Running our model on testing dataset\n",
    "\n",
    "Remember the the testing dataset from earlier? We will be using it to test how good our model is at translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "iIJ5J5w0O25p",
    "outputId": "6fa7fd0f-fd78-4d1a-a57f-215647afa7b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10  305    7   28  277    0    0    0]\n",
      " [   2   80    1    0    0    0    0    0]\n",
      " [  87   31   70   70    0    0    0    0]\n",
      " [   2   24    7   80    0    0    0    0]\n",
      " [  47   16 1456    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Getting the predictions from the testing dataset\n",
    "preds = model.predict_classes(x_test.reshape(x_test.shape[0], x_test.shape[1]))\n",
    "print (preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRiYw1miOo47"
   },
   "outputs": [],
   "source": [
    "# A function to convert a sequence back into words\n",
    "def convert_words(n, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvW2uwKNOttE"
   },
   "outputs": [],
   "source": [
    "pred_texts = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        word = convert_words(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (word == convert_words(i[j-1], eng_tokenizer)) or (word == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(word)\n",
    "        else:\n",
    "            if (word == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(word)\n",
    "                \n",
    "    pred_texts.append(' '.join(temp))    \n",
    "    \n",
    "pred_df = pd.DataFrame({'actual': test[:, 0], 'prediction': pred_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z-lVxIZ19Zh-",
    "outputId": "c43a90cc-2fc6-4033-d257-dfa0d47c0104"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he wanted to be rich</td>\n",
       "      <td>he wanted to be rich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love tom</td>\n",
       "      <td>i love tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>let us go home</td>\n",
       "      <td>lets go home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i love driving</td>\n",
       "      <td>i like to love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is my dictionary</td>\n",
       "      <td>thats my dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi tom good morning</td>\n",
       "      <td>send tom a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why is she so popular</td>\n",
       "      <td>why is she so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ill show you my room</td>\n",
       "      <td>ill show you my room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>did tom oversleep</td>\n",
       "      <td>did tom respond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>keep up the good work</td>\n",
       "      <td>just a  work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>what happened</td>\n",
       "      <td>what happened that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>toms weak</td>\n",
       "      <td>toms is weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>take tom home</td>\n",
       "      <td>take tom home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>school is out</td>\n",
       "      <td>school is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>it is pitch dark</td>\n",
       "      <td>it already</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dogs are smart</td>\n",
       "      <td>dogs is are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>im allowing you to go</td>\n",
       "      <td>im sorry to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is she here yet</td>\n",
       "      <td>is she here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i know its a surprise</td>\n",
       "      <td>i know a  minute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tom did do that</td>\n",
       "      <td>tom has this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>can i go to work</td>\n",
       "      <td>may i go to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>thats not my car</td>\n",
       "      <td>this not my car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tom is undressing</td>\n",
       "      <td>tom is now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>does tom have one</td>\n",
       "      <td>does tom have one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>you must study hard</td>\n",
       "      <td>you must study hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>it cant be helped</td>\n",
       "      <td>this time to sense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tom is ridiculous</td>\n",
       "      <td>tom is ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i meet a lot of people</td>\n",
       "      <td>i read many lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>earth is a planet</td>\n",
       "      <td>the make is a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>you wont be shot</td>\n",
       "      <td>you wont be wont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>i accept your terms</td>\n",
       "      <td>i accept your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>stop tom</td>\n",
       "      <td>stop tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>tom seems touched</td>\n",
       "      <td>tom seems uneasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>see you around tom</td>\n",
       "      <td>talk tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>tom didnt object</td>\n",
       "      <td>tom wasnt liked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>stay here with tom</td>\n",
       "      <td>stay with tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>youre humming</td>\n",
       "      <td>youre humming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>i must buy one</td>\n",
       "      <td>i must buy one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>i wont say anything</td>\n",
       "      <td>i wont say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>i have another job now</td>\n",
       "      <td>i have a job now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>he must be over fifty</td>\n",
       "      <td>he must be the crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>im a hairdresser</td>\n",
       "      <td>i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>tom tried his luck</td>\n",
       "      <td>tom must youre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>open your mouth wide</td>\n",
       "      <td>up on different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>youre untalented</td>\n",
       "      <td>youre untalented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>i didnt notice it</td>\n",
       "      <td>i didnt notice it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>she rode a camel</td>\n",
       "      <td>they made a big</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>trust me he said</td>\n",
       "      <td>tell one said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>tom grabbed his bag</td>\n",
       "      <td>tom  his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>what was tom like</td>\n",
       "      <td>how was tom that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>it looks like snow</td>\n",
       "      <td>it looks like snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>tom works slowly</td>\n",
       "      <td>tom took slowly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>i love rock</td>\n",
       "      <td>i like rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>our tv isnt working</td>\n",
       "      <td>our tv is out of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>youre a racist</td>\n",
       "      <td>youre a genius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>i have one question</td>\n",
       "      <td>i asked nobody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>i find swimming fun</td>\n",
       "      <td>swimming is fun for me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>tom usually wins</td>\n",
       "      <td>tom appears wins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>he deserves the prize</td>\n",
       "      <td>he the  it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>ill explain later</td>\n",
       "      <td>ill will it later</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      actual                 prediction\n",
       "0       he wanted to be rich    he wanted to be rich   \n",
       "1                 i love tom            i love tom     \n",
       "2             let us go home          lets go home     \n",
       "3             i love driving         i like to love    \n",
       "4      this is my dictionary   thats my dictionary     \n",
       "5        hi tom good morning            send tom a     \n",
       "6      why is she so popular          why is she so    \n",
       "7       ill show you my room    ill show you my room   \n",
       "8          did tom oversleep       did tom respond     \n",
       "9      keep up the good work           just a  work    \n",
       "10             what happened    what happened that     \n",
       "11                 toms weak          toms is weak     \n",
       "12             take tom home         take tom home     \n",
       "13             school is out            school is      \n",
       "14          it is pitch dark           it already      \n",
       "15            dogs are smart           dogs is are     \n",
       "16     im allowing you to go        im sorry to you    \n",
       "17           is she here yet           is she here     \n",
       "18     i know its a surprise        i know a  minute   \n",
       "19           tom did do that          tom has this     \n",
       "20          can i go to work            may i go to    \n",
       "21          thats not my car        this not my car    \n",
       "22         tom is undressing            tom is now     \n",
       "23         does tom have one      does tom have one    \n",
       "24       you must study hard    you must study hard    \n",
       "25         it cant be helped     this time to sense    \n",
       "26         tom is ridiculous     tom is ridiculous     \n",
       "27    i meet a lot of people        i read many lot    \n",
       "28         earth is a planet          the make is a    \n",
       "29          you wont be shot       you wont be wont    \n",
       "...                      ...                        ...\n",
       "9970     i accept your terms         i accept your     \n",
       "9971                stop tom             stop tom      \n",
       "9972       tom seems touched      tom seems uneasy     \n",
       "9973      see you around tom             talk tom      \n",
       "9974        tom didnt object       tom wasnt liked     \n",
       "9975      stay here with tom         stay with tom     \n",
       "9976           youre humming        youre humming      \n",
       "9977          i must buy one         i must buy one    \n",
       "9978     i wont say anything            i wont say     \n",
       "9979  i have another job now        i have a job now   \n",
       "9980   he must be over fifty    he must be the crazy   \n",
       "9981        im a hairdresser                 i am      \n",
       "9982      tom tried his luck        tom must youre     \n",
       "9983    open your mouth wide        up on different    \n",
       "9984        youre untalented     youre untalented      \n",
       "9985       i didnt notice it      i didnt notice it    \n",
       "9986        she rode a camel        they made a big    \n",
       "9987        trust me he said         tell one said     \n",
       "9988     tom grabbed his bag              tom  his     \n",
       "9989       what was tom like       how was tom that    \n",
       "9990      it looks like snow     it looks like snow    \n",
       "9991        tom works slowly       tom took slowly     \n",
       "9992             i love rock           i like rock     \n",
       "9993     our tv isnt working        our tv is out of   \n",
       "9994          youre a racist        youre a genius     \n",
       "9995     i have one question        i asked nobody     \n",
       "9996     i find swimming fun  swimming is fun for me   \n",
       "9997        tom usually wins      tom appears wins     \n",
       "9998   he deserves the prize             he the  it    \n",
       "9999       ill explain later      ill will it later    \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "neural machine translation on cloud tpus",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
