{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple transfer learning with an Inception v3 architecture model.\n",
    "\n",
    "This example shows how to take a Inception v3 architecture model trained on\n",
    "ImageNet images, and train a new top layer that can recognize other classes of\n",
    "images.\n",
    "\n",
    "The top layer receives as input a 2048-dimensional vector for each image. We\n",
    "train a softmax layer on top of this representation. Assuming the softmax layer\n",
    "contains N labels, this corresponds to learning N + 2048*N model parameters\n",
    "corresponding to the learned biases and weights.\n",
    "\n",
    "Here's an example, which assumes you have a folder containing class-named\n",
    "subfolders, each full of images for each label. The example folder flower_photos\n",
    "should have a structure like this:\n",
    "```\n",
    "~/flower_photos/daisy/photo1.jpg\n",
    "~/flower_photos/daisy/photo2.jpg\n",
    "...\n",
    "~/flower_photos/rose/anotherphoto77.jpg\n",
    "...\n",
    "~/flower_photos/sunflower/somepicture.jpg\n",
    "```\n",
    "The subfolder names are important, since they define what label is applied to\n",
    "each image, but the filenames themselves don't matter. Once your images are\n",
    "prepared, you can run the training with a command like this:\n",
    "```\n",
    "bazel build third_party/tensorflow/examples/image_retraining:retrain && \\\n",
    "bazel-bin/third_party/tensorflow/examples/image_retraining/retrain \\\n",
    "--image_dir ~/flower_photos\n",
    "```\n",
    "\n",
    "You can replace the image_dir argument with any folder containing subfolders of\n",
    "images. The label for each image is taken from the name of the subfolder it's\n",
    "in.\n",
    "\n",
    "This produces a new model file that can be loaded and run by any TensorFlow\n",
    "program, for example the label_image sample code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.client import graph_util\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.platform import gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all parameters that are tied to the particular model architecture\n",
    "# we're using for Inception v3. These include things like tensor names and their\n",
    "# sizes. If you want to adapt this script to work with another model, you will\n",
    "# need to update these to reflect the values in the network you're using.\n",
    "# pylint: disable=line-too-long\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "# pylint: enable=line-too-long\n",
    "BOTTLENECK_TENSOR_NAME = 'pool_3/_reshape'\n",
    "BOTTLENECK_TENSOR_SIZE = 2048\n",
    "MODEL_INPUT_WIDTH = 299\n",
    "MODEL_INPUT_HEIGHT = 299\n",
    "MODEL_INPUT_DEPTH = 3\n",
    "JPEG_DATA_TENSOR_NAME = 'DecodeJpeg/contents'\n",
    "RESIZED_INPUT_TENSOR_NAME = 'ResizeBilinear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Since IPython Notebook is not a big fan of the ```tf.app.flags.FLAGS``` argument, we create our own synthetic class with all the parameters inside of it and and swap it into of the retrain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import retrain as nnrt\n",
    "class retrain_parameters():\n",
    "    image_dir = './'\n",
    "    model_dir = './' # \"\"\"Path to classify_image_graph_def.pb, \"\"\"\n",
    "                      #         \"\"\"imagenet_synset_to_human_label_map.txt, and \"\"\"\n",
    "                      #         \"\"\"imagenet_2012_challenge_label_map_proto.pbtxt.\"\"\"\n",
    "    output_graph =  '/tmp/output_graph.pb'\n",
    "    output_labels = '/tmp/output_labels.txt' # \"\"\"Where to save the trained graph's labels.\"\"\")\n",
    "    # Details of the training configuration.\n",
    "    how_many_training_steps = 4000 # \"\"\"How many training steps to run before ending.\"\"\"\n",
    "    learning_rate = 0.01 # \"\"\"How large a learning rate to use when training.\"\"\")\n",
    "    \n",
    "    testing_percentage = 10 # \"\"\"What percentage of images to use as a test set.\"\"\"\n",
    "    validation_percentage = 10 #\"\"\"What percentage of images to use as a validation set.\"\"\")\n",
    "    \n",
    "    eval_step_interval = 10 # \"\"\"How often to evaluate the training results.\"\"\")\n",
    "    train_batch_size = 100 # \"\"\"How many images to train on at a time.\"\"\")\n",
    "    test_batch_size = 500 # \"\"\"How many images to test on at a time. This\"\"\"\n",
    "                          #      \"\"\" test set is only used infrequently to verify\"\"\"\n",
    "                          #      \"\"\" the overall accuracy of the model.\"\"\")\n",
    "    validation_batch_size = 100\n",
    "    # \"\"\"How many images to use in an evaluation batch. This validation set is\"\"\"\n",
    "    # \"\"\" used much more often than the test set, and is an early indicator of\"\"\"\n",
    "    # \"\"\" how accurate the model is during training.\"\"\")\n",
    "\n",
    "    \n",
    "    bottleneck_dir = '/tmp/bottleneck' # \"\"\"Path to cache bottleneck layer values as files.\"\"\")\n",
    "    final_tensor_name = 'final_result' #\"\"\"The name of the output classification layer in the retrained graph.\"\"\")\n",
    "\n",
    "    # Controls the distortions used during training.\n",
    "    flip_left_right = False # \"\"\"Whether to randomly flip half of the training images horizontally.\"\"\")\n",
    "    random_crop = 0 # \"\"\"A percentage determining how much of a margin to randomly crop off the training images.\"\"\")\n",
    "    random_scale = 0 # \"\"\"A percentage determining how much to randomly scale up the size of the training images by.\"\"\")\n",
    "    random_brightness = 0 # \"\"\"A percentage determining how much to randomly multiply the training image input pixels up or down by.\"\"\")\n",
    "FLAGS = retrain_parameters()\n",
    "nnrt.FLAGS = FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "    \"\"\"Download and extract model tar file.\n",
    "\n",
    "    If the pretrained model we're using doesn't already exist, this function\n",
    "    downloads it from the TensorFlow.org website and unpacks it into a directory.\n",
    "    \"\"\"\n",
    "    dest_directory = FLAGS.model_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = DATA_URL.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' %\n",
    "                           (filename,\n",
    "                            float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.request.urlretrieve(DATA_URL,\n",
    "                                                 filepath,\n",
    "                                                 reporthook=_progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "def create_inception_graph():\n",
    "    \"\"\"\"Creates a graph from saved GraphDef file and returns a Graph object.\n",
    "\n",
    "    Returns:\n",
    "    Graph holding the trained Inception network.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        with gfile.FastGFile(\n",
    "            os.path.join(FLAGS.model_dir, 'classify_image_graph_def.pb'), 'r') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            _ = tf.import_graph_def(graph_def, name='')\n",
    "    return sess.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the pre-trained graph.\n",
    "\n",
    "nnrt.maybe_download_and_extract()\n",
    "graph = nnrt.create_inception_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Look at the folder structure, and create lists of all the images.\n",
    "image_lists = nnrt.create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n",
    "                               FLAGS.validation_percentage)\n",
    "class_count = len(image_lists.keys())\n",
    "if class_count == 0:\n",
    "    print('No valid folders of images found at ' + FLAGS.image_dir)\n",
    "    \n",
    "if class_count == 1:\n",
    "    print('Only one valid folder of images found at ' + FLAGS.image_dir +\n",
    "          ' - multiple classes are needed for classification.')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See if the command-line flags mean we're applying any distortions.\n",
    "do_distort_images = nnrt.should_distort_images(\n",
    "  FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n",
    "  FLAGS.random_brightness)\n",
    "ground_truth_tensor_name = 'ground_truth'\n",
    "distorted_image_name = 'distorted_image'\n",
    "distorted_jpeg_data_tensor_name = 'distorted_jpeg_data'\n",
    "sess = tf.Session()\n",
    "\n",
    "if do_distort_images:\n",
    "    # We will be applying distortions, so set upthe operations we'll need.\n",
    "    nnrt.add_input_distortions(FLAGS.flip_left_right, FLAGS.random_crop,\n",
    "                          FLAGS.random_scale, FLAGS.random_brightness,\n",
    "                          distorted_jpeg_data_tensor_name, distorted_image_name)\n",
    "else:\n",
    "    # We'll make sure we've calculated the 'bottleneck' image summaries and\n",
    "    # cached them on disk.\n",
    "    nnrt.cache_bottlenecks(sess, image_lists, FLAGS.image_dir, FLAGS.bottleneck_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the new layer that we'll be training.\n",
    "train_step, cross_entropy = nnrt.add_final_training_ops(\n",
    "  graph, len(image_lists.keys()), FLAGS.final_tensor_name,\n",
    "  ground_truth_tensor_name)\n",
    "\n",
    "# Set up all our weights to their initial default values.\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the operations we need to evaluate the accuracy of our new layer.\n",
    "evaluation_step = nnrt.add_evaluation_step(graph, FLAGS.final_tensor_name,\n",
    "                                    ground_truth_tensor_name)\n",
    "\n",
    "# Get some layers we'll need to access during training.\n",
    "bottleneck_tensor = graph.get_tensor_by_name(nnrt.ensure_name_has_port(\n",
    "  BOTTLENECK_TENSOR_NAME))\n",
    "ground_truth_tensor = graph.get_tensor_by_name(nnrt.ensure_name_has_port(\n",
    "  ground_truth_tensor_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the training for as many cycles as requested on the command line.\n",
    "for i in range(FLAGS.how_many_training_steps):\n",
    "    # Get a catch of input bottleneck values, either calculated fresh every time\n",
    "    # with distortions applied, or from the cache stored on disk.\n",
    "    if do_distort_images:\n",
    "      train_bottlenecks, train_ground_truth = nnrt.get_random_distorted_bottlenecks(\n",
    "          sess, graph, image_lists, FLAGS.train_batch_size, 'training',\n",
    "          FLAGS.image_dir, distorted_jpeg_data_tensor_name,\n",
    "          distorted_image_name)\n",
    "    else:\n",
    "      train_bottlenecks, train_ground_truth = nnrt.get_random_cached_bottlenecks(\n",
    "          sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "          FLAGS.bottleneck_dir, FLAGS.image_dir)\n",
    "    # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "    # step.\n",
    "    sess.run(train_step,\n",
    "             feed_dict={bottleneck_tensor: train_bottlenecks,\n",
    "                        ground_truth_tensor: train_ground_truth})\n",
    "    # Every so often, print out how well the graph is training.\n",
    "    is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
    "    if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "      train_accuracy, cross_entropy_value = sess.run(\n",
    "          [evaluation_step, cross_entropy],\n",
    "          feed_dict={bottleneck_tensor: train_bottlenecks,\n",
    "                     ground_truth_tensor: train_ground_truth})\n",
    "      print('%s: Step %d: Train accuracy = %.1f%%' % (datetime.now(), i,\n",
    "                                                      train_accuracy * 100))\n",
    "      print('%s: Step %d: Cross entropy = %f' % (datetime.now(), i,\n",
    "                                                 cross_entropy_value))\n",
    "      validation_bottlenecks, validation_ground_truth = (\n",
    "          nnrt.get_random_cached_bottlenecks(\n",
    "              sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
    "              FLAGS.bottleneck_dir, FLAGS.image_dir))\n",
    "      validation_accuracy = sess.run(\n",
    "          evaluation_step,\n",
    "          feed_dict={bottleneck_tensor: validation_bottlenecks,\n",
    "                     ground_truth_tensor: validation_ground_truth})\n",
    "      print('%s: Step %d: Validation accuracy = %.1f%%' %\n",
    "            (datetime.now(), i, validation_accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've completed all our training, so run a final test evaluation on\n",
    "# some new images we haven't used before.\n",
    "test_bottlenecks, test_ground_truth = get_random_cached_bottlenecks(\n",
    "  sess, image_lists, FLAGS.test_batch_size, 'testing',\n",
    "  FLAGS.bottleneck_dir, FLAGS.image_dir)\n",
    "test_accuracy = sess.run(\n",
    "  evaluation_step,\n",
    "  feed_dict={bottleneck_tensor: test_bottlenecks,\n",
    "             ground_truth_tensor: test_ground_truth})\n",
    "print('Final test accuracy = %.1f%%' % (test_accuracy * 100))\n",
    "\n",
    "# Write out the trained graph and labels with the weights stored as constants.\n",
    "output_graph_def = graph_util.convert_variables_to_constants(\n",
    "  sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n",
    "with gfile.FastGFile(FLAGS.output_graph, 'w') as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
    "    f.write('\\n'.join(image_lists.keys()) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
