// RUN: tf-mlir-translate -split-input-file -hlo-text-to-lhlo -optimize-xla-hlo=false %s | FileCheck %s

HloModule TestModule

// CHECK-LABEL: func @TestComputation

FusedComputation {
  // CHECK: tensor_load %arg0 {minor_to_major = dense<[0, 1]> : tensor<2xindex>}
  x = f32[3, 2]{0,1} parameter(0)
  ROOT y = f32[3, 2]{0,1} add(x, x)
}

ENTRY TestComputation {
  x = f32[3, 2]{0,1} parameter(0)
  ROOT y = f32[3, 2]{0,1} fusion(x), kind=kLoop, calls=FusedComputation
}

// -----

HloModule ScatterModule

update_s32 (lhs: s32[], rhs: s32[]) -> s32[] {
  lhs = s32[] parameter(0)
  ROOT rhs = s32[] parameter(1)
}

// CHECK-LABEL: func @main
// CHECK: "lmhlo.scatter"
// CHECK: ^bb0(%[[ARG5:.*]]: tensor<i32>, %[[ARG6:.*]]: tensor<i32>):
// CHECK:  "mhlo.return"(%[[ARG6]])
// CHECK: indices_are_sorted = false
// CHECK: index_vector_dim = 1 : i64
// CHECK: inserted_window_dims = dense<0> : tensor<1xi64>
// CHECK: scatter_dims_to_operand_dims = dense<0> : tensor<1xi64>
// CHECK: update_window_dims = dense<1> : tensor<1xi64>
// CHECK: unique_indices = false
// CHECK: (memref<3x3xi32>, memref<2xi32>, memref<2x3xi32>, memref<3x3xi32>) -> ()
ENTRY main {
  operand = s32[3,3] parameter(0)
  indices = s32[2] parameter(1)
  updates = s32[2,3] parameter(2)
  ROOT scatter_op = s32[3,3] scatter(operand, indices, updates),
      to_apply=update_s32,
      update_window_dims={1},
      inserted_window_dims={0},
      scatter_dims_to_operand_dims={0},
      index_vector_dim=1
}

// -----

HloModule SelectAndScatter

%ge_F32 (lhs.5: f32[], rhs.6: f32[]) -> pred[] {
  %lhs.5 = f32[] parameter(0)
  %rhs.6 = f32[] parameter(1)
  ROOT %compare.7 = pred[] compare(f32[] %lhs.5, f32[] %rhs.6), direction=GE
}

%add_F32 (lhs.9: f32[], rhs.10: f32[]) -> f32[] {
  %lhs.9 = f32[] parameter(0)
  %rhs.10 = f32[] parameter(1)
  ROOT %add.11 = f32[] add(f32[] %lhs.9, f32[] %rhs.10)
}

// CHECK-LABEL: module
// CHECK: global_memref "private" constant @[[$GLOBAL:.*]] : memref<f32> = dense<0.000000e+00>
// CHECK-LABEL: func @main
// CHECK: %[[GLOBAL_MEMREF:.*]] = get_global_memref @[[$GLOBAL]] : memref<f32>
// CHECK: "lmhlo.select_and_scatter"(%{{.*}}, %{{.*}}, %[[GLOBAL_MEMREF]], %{{.*}})
// CHECK: ^bb0(%[[ARG0:.*]]: tensor<f32>, %[[ARG1:.*]]: tensor<f32>):
// CHECK: %[[COMPARE:.*]] = "mhlo.compare"(%[[ARG0]], %[[ARG1]]) {comparison_direction = "GE"}
// CHECK: "mhlo.return"(%[[COMPARE]]) : (tensor<i1>) -> ()
// CHECK: ^bb0(%[[ARG2:.*]]: tensor<f32>, %[[ARG3:.*]]: tensor<f32>):
// CHECK: %[[ADD:.*]] = mhlo.add %[[ARG2]], %[[ARG3]]
// CHECK: "mhlo.return"(%[[ADD]]) : (tensor<f32>) -> ()
// CHECK: padding = dense<0> : tensor<1xi64>
// CHECK: window_dimensions = dense<3> : tensor<1xi64>
// CHECK: window_strides = dense<3> : tensor<1xi64>
// CHECK: (memref<6xf32>, memref<2xf32>, memref<f32>, memref<6xf32>) -> ()
ENTRY main () -> f32[6] {
  %operand = f32[6]{0} parameter(0)
  %source = f32[2]{0} parameter(1)
  %init = f32[] constant(0)
  ROOT %select-and-scatter.12 = f32[6]{0} select-and-scatter(f32[6]{0} %operand, f32[2]{0} %source, f32[] %init), window={size=3 stride=3}, select=%ge_F32, scatter=%add_F32
}

// -----

HloModule SliceToDynamic

// CHECK-LABEL: func @main
// CHECK: "lmhlo.custom_call"
// CHECK: backend_config = "", call_target_name = "SliceToDynamic"
// CHECK-SAME: operand_segment_sizes = dense<[4, 1]> : vector<2xi32>
// CHECK: (memref<2x2x2xi32>, memref<i32>, memref<i32>, memref<i32>, memref<2x2x2xi32>) -> ()
ENTRY main {
  %param = s32[2,2,2] parameter(0)
  %static = s32[] parameter(1)
  %dynamic = s32[] parameter(2)
  ROOT %custom-call = s32[2,<=2, 2] custom-call(s32[2,2,2] %param,
                                                  s32[] %static,
                                                  s32[] %dynamic,
                                                  s32[] %static),
                                      custom_call_target="SliceToDynamic",
                                      backend_config=""
}

// -----

HloModule Cholesky

// CHECK-LABEL: func @main
// CHECK: "lmhlo_gpu.cholesky"
// CHECK-SAME: is_lower = true
ENTRY main {
  %param = f32[3,3] parameter(0)
  ROOT %custom-call = (f32[3,3], f32[3], s32[]) custom-call(f32[3,3] %param),
                                custom_call_target="__cusolver$cholesky",
                                operand_layout_constraints={f32[3,3]},
                                backend_config="{\"lower\":true}"
}

// -----

HloModule Gemm

// CHECK-LABEL: func @main
// CHECK: "lmhlo_gpu.gemm"
// CHECK-SAME: algorithm = 7 : i64
// CHECK-SAME: alpha_imag = 0.000000e+00 : f64
// CHECK-SAME: alpha_real = 1.000000e+00 : f64
// CHECK-SAME: batch_size = 1 : i64
// CHECK-SAME: lhs_batching_dimensions = dense<> : tensor<0xi64>
// CHECK-SAME: lhs_contracting_dimensions = dense<1> : tensor<1xi64>
// CHECK-SAME: rhs_batching_dimensions = dense<> : tensor<0xi64>
// CHECK-SAME: rhs_contracting_dimensions = dense<0> : tensor<1xi64>
// CHECK: (memref<2x2xf32>, memref<2x2xf32>, memref<2x2xf32>) -> ()
ENTRY main {
  %A = f32[2,2]{1,0} parameter(0)
  %B = f32[2,2]{1,0} parameter(1)
  ROOT %sgemm = f32[2,2]{1,0} custom-call(f32[2,2]{1,0} %A, f32[2,2]{1,0} %B),
                              custom_call_target="__cublas$gemm",
  backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"7\"}"
}

// -----

HloModule GemmBias

// CHECK-LABEL: func @main
// CHECK: "lmhlo_gpu.gemm_bias"
// CHECK-SAME: algorithm = 0 : i64
// CHECK-SAME: alpha_imag = 0.000000e+00 : f64
// CHECK-SAME: alpha_real = 1.000000e+00 : f64
// CHECK-SAME: batch_size = 1 : i64
// CHECK-SAME: beta = 1.000000e+00 : f64
// CHECK-SAME: lhs_batching_dimensions = dense<> : tensor<0xi64>
// CHECK-SAME: lhs_contracting_dimensions = dense<1> : tensor<1xi64>
// CHECK-SAME: rhs_batching_dimensions = dense<> : tensor<0xi64>
// CHECK-SAME: rhs_contracting_dimensions = dense<0> : tensor<1xi64>
// CHECK: (memref<1x1xf32>, memref<1x4xf32>, memref<1x4xf32>, memref<1x4xf32>)
ENTRY main {
  %A = f32[1,1]{1,0} parameter(0)
  %B = f32[1,4]{1,0} parameter(1)
  %C = f32[1,4]{1,0} parameter(2)
  ROOT %sgemm_add = f32[1,4]{1,0} custom-call(f32[1,1]{0,1} %A, f32[1,4]{1,0} %B, f32[1,4]{1,0} %C),
                                  custom_call_target="__cublas$gemm",
  backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"0\"}"
}

// -----

HloModule GemmBias

// CHECK-LABEL: func @main
// CHECK: "lmhlo_gpu.gemm_bias"
// CHECK-SAME: algorithm = 0 : i64
// CHECK-SAME: alpha_imag = 0.000000e+00 : f64
// CHECK-SAME: alpha_real = 1.000000e+00 : f64
// CHECK-SAME: batch_size = 1 : i64
// CHECK-SAME: beta = 1.000000e+00 : f64
// CHECK-SAME: lhs_batching_dimensions = dense<> : tensor<0xi64>
// CHECK-SAME: lhs_contracting_dimensions = dense<1> : tensor<1xi64>
// CHECK-SAME: rhs_batching_dimensions = dense<> : tensor<0xi64>
// CHECK-SAME: rhs_contracting_dimensions = dense<0> : tensor<1xi64>
// CHECK: (memref<1x1xf32>, memref<1x4xf32>, memref<1x4xf32>, memref<1x4xf32>)
ENTRY main {
  %A = f32[1,1]{1,0} parameter(0)
  %B = f32[1,4]{1,0} parameter(1)
  %C = f32[1,4]{1,0} parameter(2)
  ROOT %sgemm_add = f32[1,4]{1,0} custom-call(f32[1,1]{0,1} %A, f32[1,4]{1,0} %B, f32[1,4]{1,0} %C),
                                  custom_call_target="__cublas$gemm",
  backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"0\"}"
}

// -----

HloModule AllReduce

// Test all-reduce
add {
  lhs = f32[] parameter(0)
  rhs = f32[] parameter(1)
  ROOT add = f32[] add(lhs, rhs)
}

// CHECK-LABEL: func @test_all_reduce
// CHECK-SAME:  ([[INPUT:%.*]]: memref<8xf32>
%test_all_reduce {
  input = f32[8] parameter(0)
  // CHECK:  "lmhlo.all_reduce"([[INPUT]], {{.*}})
  // CHECK:  ^bb0([[ARG0:%.*]]: tensor<f32>, [[ARG1:%.*]]: tensor<f32>):
  // CHECK:    [[ADD:%.*]] = mhlo.add [[ARG0]], [[ARG1]]
  // CHECK:    "mhlo.return"([[ADD]]) : (tensor<f32>) -> ()
  // CHECK:  }) {
  // CHECK-SAME:  channel_id = {handle = 1 : i64, type = 0 : i64}
  // CHECK-SAME:  replica_groups = dense<{{\[\[}}0, 1, 2, 3], [5, 6, 7, 8]]> : tensor<2x4xi64>
  ROOT result = f32[8] all-reduce(input), channel_id=1, replica_groups={{0,1,2,3}, {5,6,7,8}}, to_apply=add
}
