# RUN: tf-mlir-translate -graphdef-to-mlir -tf-enable-shape-inference-on-import=false %s -tf-output-arrays=func_call -o - | FileCheck %s

node {
  name: "x"
  op: "VarHandleOp"
  device: "/CPU:0"
  attr {
    key: "container"
    value {
      s: "a"
    }
  }
  attr {
    key: "dtype"
    value {
      type: DT_INT64
    }
  }
  attr {
    key: "shape"
    value {
      shape {
      }
    }
  }
  attr {
    key: "shared_name"
    value {
      s: "x"
    }
  }
}
node {
  name: "func_call"
  op: "test_func_name"
  input: "x"
  input: "x"
  attr {
    key: "_disable_call_shape_inference"
    value {
      b: true
    }
  }
}
library {
  function {
    signature {
      name: "test_func_name"
      input_arg {
        name: "a_0"
        type: DT_RESOURCE
      }
      input_arg {
        name: "a_1"
        type: DT_RESOURCE
      }
      output_arg {
        name: "a"
        type: DT_RESOURCE
      }
    }
    resource_arg_unique_id {
      key: 0
      value: 0
    }
    resource_arg_unique_id {
      key: 1
      value: 0
    }
    ret {
      key: "a"
      value: "a_0"
    }
    attr {
      key: "_disable_call_shape_inference"
      value {
        b: true
      }
    }
  }
}

# Check that the `resource_arg_unique_id` for each argument is propagated to the
# `tf._resource_arg_unique_id` argument attribute of the function
# @test_func_name0.

# CHECK:  func @main
# CHECK:    tf_executor.graph
# CHECK:      "tf.VarHandleOp"()
# CHECK:      "tf.LegacyCall"
# CHECK-SAME:   <{_disable_call_shape_inference = true, f = @test_func_name0}> {device = ""}
# CHECK:      tf_executor.fetch
# CHECK:    return
# CHECK:  func private @test_func_name0
# CHECK-SAME:   tf._resource_arg_unique_id = 0
# CHECK-SAME:   tf._resource_arg_unique_id = 0
# CHECK:    tf_executor.graph
# CHECK:      tf_executor.fetch
# CHECK:    return
