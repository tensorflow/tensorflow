# RUN: tf-mlir-translate -graphdef-to-mlir -tf-enable-shape-inference-on-import=false -mlir-print-debuginfo %s -o - | FileCheck %s

node {
  name: "PartitionedCall"
  op: "PartitionedCall"
  attr {
    key: "Tin"
    value {
      list {
      }
    }
  }
  attr {
    key: "Tout"
    value {
      list {
        type: DT_UINT8
      }
    }
  }
  attr {
    key: "_gradient_op_type"
    value {
      s: "PartitionedCall-15"
    }
  }
  attr {
    key: "config"
    value {
      s: ""
    }
  }
  attr {
    key: "config_proto"
    value {
      s: "\n\007\n\003GPU\020\000\n\007\n\003CPU\020\0012\002J\0008\001"
    }
  }
  attr {
    key: "executor_type"
    value {
      s: ""
    }
  }
  attr {
    key: "f"
    value {
      func {
        name: "__inference_uint_const_14"
      }
    }
  }
}
library {
  function {
    signature {
      name: "__inference_uint_const_14"
      output_arg {
        name: "identity"
        type: DT_UINT8
      }
    }
    node_def {
      name: "Const"
      op: "Const"
      attr {
        key: "dtype"
        value {
          type: DT_UINT8
        }
      }
      attr {
        key: "value"
        value {
          tensor {
            dtype: DT_UINT8
            tensor_shape {
            }
            int_val: 5
          }
        }
      }
    }
    node_def {
      name: "Identity"
      op: "Identity"
      input: "Const:output:0"
      attr {
        key: "T"
        value {
          type: DT_UINT8
        }
      }
    }
    ret {
      key: "identity"
      value: "Identity:output:0"
    }
  }
}
versions {
  producer: 29
  min_consumer: 12
}

# CHECK: func @main
# CHECK: "tf.PartitionedCall"()
# CHECK-SAME: f = @[[FUNCTION:[A-Za-z0-9_]*]]
# CHECK: func @[[FUNCTION]]() -> tensor<ui8>
# CHECK: return {{.*}} : tensor<ui8>
