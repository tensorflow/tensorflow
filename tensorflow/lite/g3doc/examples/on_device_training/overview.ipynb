{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "On-Device Training in TensorFlow Lite",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "id": "a1b42e5b",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nWetWWd_ns"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ],
      "id": "g_nWetWWd_ns"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2pHVBk_seED1"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "id": "2pHVBk_seED1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7vSdG6sAIQn"
      },
      "source": [
        "# On-Device Training in TensorFlow Lite"
      ],
      "id": "M7vSdG6sAIQn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwc5GKHBASdc"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/examples/on_device_training/overview\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/examples/on_device_training/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "id": "fwc5GKHBASdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ee074e4"
      },
      "source": [
        "**Note:** This API is new and only available via `pip install tf-nightly`. It will be available in TensorFlow version 2.7.\n",
        "\n",
        "To use TensorFlow Lite, a developer needs to prepare a TensorFlow model, use the converter to convert it to TensorFlow Lite model format, and run the model with the TensorFlow Lite runtime on device. This is true for inference use cases, and a similar flow can be applied to training too.\n",
        "\n",
        "The following code illustrates the high-level flow of preparing a TensorFlow training model, converting it to TensorFlow Lite model and running in TensorFlow Lite runtime for a training use case.\n",
        "\n",
        "The implementation is based on the [Keras classification example](https://www.tensorflow.org/tutorials/keras/classification) in the TensorFlow official guide page."
      ],
      "id": "9ee074e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaWdLA3fQDK2"
      },
      "source": [
        "## Setup"
      ],
      "id": "UaWdLA3fQDK2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJ35RWsQHG2"
      },
      "source": [
        "!pip uninstall -y tensorflow keras\n",
        "!pip install tf-nightly"
      ],
      "id": "uZJ35RWsQHG2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j4MGqyKQEo4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "id": "9j4MGqyKQEo4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_omifE5JKpt4"
      },
      "source": [
        "## Classify images of clothing\n",
        "\n",
        "This CoLab trains a neural network model to classify images of clothing, like sneakers and shirts.\n",
        "\n",
        "Here, 60,000 images are used to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:\n",
        "\n",
        "Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_images` and `train_labels` arrays are the *training set*â€”the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays."
      ],
      "id": "_omifE5JKpt4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDo-K3Mq63iK"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "id": "NDo-K3Mq63iK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN1O10csLcVs"
      },
      "source": [
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:"
      ],
      "id": "jN1O10csLcVs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnwN4QoB7BN7"
      },
      "source": [
        "train_images.shape"
      ],
      "id": "CnwN4QoB7BN7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0yf3e-zLdUW"
      },
      "source": [
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ],
      "id": "p0yf3e-zLdUW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l610mvhA695s"
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "id": "l610mvhA695s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the *training set* and the *testing set* be preprocessed in the same way:"
      ],
      "id": "Wz7l27Lz9S1P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3OkzF7O7Hr8"
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ],
      "id": "w3OkzF7O7Hr8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgqvO8Eh7NFx"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "id": "NgqvO8Eh7NFx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2N6hPEP-Ay"
      },
      "source": [
        "### TensorFlow Model for Training\n",
        "\n",
        "Instead of converting a single TensorFlow model or tf.function to a TensorFlow Lite model with a single entry point, we can convert multiple tf.function(s) into a TensorFlow Lite model. To be able to do that, we're extending the TensorFlow Lite's converter & runtime to handle multiple signatures.\n",
        "\n",
        "Preparing a TensorFlow Model. The code constructs a tf.module with 4 tf.functions:\n",
        "*   train function trains the model with training data.\n",
        "*   infer function invokes the inference.\n",
        "*   save function saves the trainable weights into the file system.\n",
        "*   restore function loads the trainable weights from the file system.\n",
        "\n",
        "The weights will be serialized as a TensorFlow version one checkpoint file format."
      ],
      "id": "FN2N6hPEP-Ay"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8577c80"
      },
      "source": [
        "IMG_SIZE = 28\n",
        "\n",
        "\n",
        "class Model(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    self.model.compile(\n",
        "        optimizer='sgd',\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'])\n",
        "    self._LOSS_FN = tf.keras.losses.CategoricalCrossentropy()\n",
        "    self._OPTIM = tf.optimizers.SGD()\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
        "      tf.TensorSpec([None, 10], tf.float32),\n",
        "  ])\n",
        "  def train(self, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "      prediction = self.model(x)\n",
        "      loss = self._LOSS_FN(prediction, y)\n",
        "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self._OPTIM.apply_gradients(\n",
        "        zip(gradients, self.model.trainable_variables))\n",
        "    result = {\"loss\": loss}\n",
        "    for grad in gradients:\n",
        "      result[grad.name] = grad\n",
        "    return result\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32)])\n",
        "  def predict(self, x):\n",
        "    return {\n",
        "        \"output\": self.model(x)\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def save(self, checkpoint_path):\n",
        "    tensor_names = [weight.name for weight in self.model.weights]\n",
        "    tensors_to_save = [weight.read_value() for weight in self.model.weights]\n",
        "    tf.raw_ops.Save(\n",
        "        filename=checkpoint_path, tensor_names=tensor_names,\n",
        "        data=tensors_to_save, name='save')\n",
        "    return {\n",
        "        \"checkpoint_path\": checkpoint_path\n",
        "    }\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def restore(self, checkpoint_path):\n",
        "    restored_tensors = {}\n",
        "    for var in self.model.weights:\n",
        "      restored = tf.raw_ops.Restore(\n",
        "          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\n",
        "          name='restore')\n",
        "      var.assign(restored)\n",
        "      restored_tensors[var.name] = restored\n",
        "    return restored_tensors"
      ],
      "id": "d8577c80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8YUTIvzMVw5"
      },
      "source": [
        "### Converting to TensorFlow Lite format."
      ],
      "id": "A8YUTIvzMVw5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwsDUEKFMYtq"
      },
      "source": [
        "# Export the TensorFlow model to the saved model\n",
        "SAVED_MODEL_DIR = \"saved_model\"\n",
        "m= Model()\n",
        "tf.saved_model.save(\n",
        "    m,\n",
        "    SAVED_MODEL_DIR,\n",
        "    signatures={\n",
        "        'train':\n",
        "            m.train.get_concrete_function(),\n",
        "        'infer':\n",
        "            m.predict.get_concrete_function(),\n",
        "        'save':\n",
        "            m.save.get_concrete_function(),\n",
        "        'restore':\n",
        "            m.restore.get_concrete_function(),\n",
        "    })\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
        "]\n",
        "converter.experimental_enable_resource_variables = True\n",
        "tflite_model = converter.convert()"
      ],
      "id": "WwsDUEKFMYtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-mXb38wM6Nf"
      },
      "source": [
        "### Executing in TensorFlow Lite runtime.\n",
        "\n",
        "TensorFlow Lite's Interpreter capability will be extended to support multiple signatures too. Developers can choose to invoke restoring, training, saving and inferring signatures separately.\n"
      ],
      "id": "B-mXb38wM6Nf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNX2vqXd2-HM"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "train = interpreter.get_signature_runner(\"train\")\n",
        "infer = interpreter.get_signature_runner(\"infer\")\n",
        "save = interpreter.get_signature_runner(\"save\")\n",
        "restore = interpreter.get_signature_runner(\"restore\")"
      ],
      "id": "qNX2vqXd2-HM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fauhoadfg842"
      },
      "source": [
        "On Android, TensorFlow Lite on-device training can be performed using either Java or C++ APIs. In this document, we describe how the above TensorFlow Lite model will work with Java API."
      ],
      "id": "fauhoadfg842"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jhWXwVCO09F"
      },
      "source": [
        "### Training with data set\n",
        "\n",
        "Training can be done with the `train` signature method."
      ],
      "id": "5jhWXwVCO09F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Diwn1MmkNVeX"
      },
      "source": [
        "# Generate the training labels\n",
        "processed_train_labels = []\n",
        "for i in range(len(train_images)):\n",
        "  train_label = [0,0,0,0,0,0,0,0,0,0]\n",
        "  train_label[train_labels[i]] = 1\n",
        "  processed_train_labels.append(train_label)\n",
        "\n",
        "# Run training for a few steps\n",
        "NUM_EPOCHS = 100\n",
        "for i in range(NUM_EPOCHS):\n",
        "  train(\n",
        "      x=tf.constant(train_images, shape=(len(train_images), IMG_SIZE, IMG_SIZE), dtype=tf.float32),\n",
        "      y=tf.constant(processed_train_labels, shape=(len(train_images), 10), dtype=tf.float32))"
      ],
      "id": "Diwn1MmkNVeX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBYb5JEnPeOR"
      },
      "source": [
        "In Java, you'll use the `Interpreter` class to load a model and drive model training tasks.\n",
        "The following example shows how to run the training procedure by using the `runSignature` method:\n",
        "\n",
        "```\n",
        "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
        "    int NUM_EPOCHS = 100;\n",
        "    int NUM_TRAININGS = 60000;\n",
        "    float[][][] trainImages = new float[NUM_TRAININGS][28][28];\n",
        "    float[][] trainLabels = new float[NUM_TRAININGS][10];\n",
        "\n",
        "    // Fill the data values.\n",
        "\n",
        "    // Run training for a few steps.\n",
        "    for (int i = 0; i < NUM_EPOCHS; ++i) {\n",
        "        Map<String, Object> inputs = new HashMap<>();\n",
        "        inputs.put(\"x\", trainImages);\n",
        "        inputs.put(\"y\", trainLabels);\n",
        "        Map<String, Object> outputs = new HashMap<>();\n",
        "        FloatBuffer loss = FloatBuffer.allocate(1);\n",
        "        outputs.put(\"loss\", loss);\n",
        "        interpreter.runSignature(inputs, outputs, \"train\");\n",
        "    }\n",
        "\n",
        "    // Do the other stuffs..\n",
        "}\n",
        "```"
      ],
      "id": "jBYb5JEnPeOR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDIi0_RlPb2n"
      },
      "source": [
        "### Exporting the trained weights to the checkpoint file\n",
        "\n",
        "The checkpoint file can be generated through the `save` signature method."
      ],
      "id": "UDIi0_RlPb2n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgeX9XiXOyex"
      },
      "source": [
        "# Export the trained weights to /tmp/model.ckpt\n",
        "save(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ],
      "id": "sgeX9XiXOyex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlZN-rhR_Ag"
      },
      "source": [
        "In Java, you can store the trained weight as a checkpoint format into the internal storage of the application. Training tasks usually perform at the idle time (e.g., at night time) in the background process occasionally.\n",
        "\n",
        "```\n",
        "try (Interpreter interpreter = new Interpreter(modelBuffer)) {\n",
        "    // Conduct the training jobs.\n",
        "\n",
        "    // Export the trained weights as a checkpoint file.\n",
        "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
        "    Map<String, Object> inputs = new HashMap<>();\n",
        "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
        "    Map<String, Object> outputs = new HashMap<>();\n",
        "    interpreter.runSignature(inputs, outputs, \"save\");\n",
        "}\n",
        "```"
      ],
      "id": "vvlZN-rhR_Ag"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSDydMyOQfL5"
      },
      "source": [
        "### Restoring the trained weights from the checkpoint file\n",
        "\n",
        "The exported checkpoint file can be restored through the `restore` signature method."
      ],
      "id": "SSDydMyOQfL5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yIZoLveRZgp"
      },
      "source": [
        "another_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "\n",
        "train = another_interpreter.get_signature_runner(\"train\")\n",
        "infer = another_interpreter.get_signature_runner(\"infer\")\n",
        "save = another_interpreter.get_signature_runner(\"save\")\n",
        "restore = another_interpreter.get_signature_runner(\"restore\")\n",
        "\n",
        "# Restore the trained weights from /tmp/model.ckpt\n",
        "restore(checkpoint_path=np.array(\"/tmp/model.ckpt\", dtype=np.string_))"
      ],
      "id": "5yIZoLveRZgp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9I_-gjdSnGn"
      },
      "source": [
        "In Java, you can restore the serialized trained weights from the file, stored at the internal storage. When the application restarts, the trained weights\n",
        "usually need to be restored prior to the inferences.\n",
        "\n",
        "```\n",
        "try (Interpreter another_interpreter = new Interpreter(modelBuffer)) {\n",
        "    // Load the trained weights from the checkpoint file.\n",
        "    File outputFile = new File(getFilesDir(), \"checkpoint.ckpt\");\n",
        "    Map<String, Object> inputs = new HashMap<>();\n",
        "    inputs.put(\"checkpoint_path\", outputFile.getAbsolutePath());\n",
        "    Map<String, Object> outputs = new HashMap<>();\n",
        "    another_interpreter.runSignature(inputs, outputs, \"restore\");\n",
        "}\n",
        "```"
      ],
      "id": "v9I_-gjdSnGn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcrv57DSkz2"
      },
      "source": [
        "### Run the inference through the trained weights\n",
        "\n",
        "Developers can use the trained model to run inference through the `infer` signature method."
      ],
      "id": "zjcrv57DSkz2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ROmlpHWS0nX"
      },
      "source": [
        "# Run the inference\n",
        "result = infer(\n",
        "    x=tf.constant(test_images, shape=(len(test_images), IMG_SIZE, IMG_SIZE), dtype=tf.float32))\n",
        "\n",
        "test_labels = np.argmax(result[\"output\"], axis=1)"
      ],
      "id": "_ROmlpHWS0nX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHbRasdfasd4"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[test_labels[i]])\n",
        "plt.show()"
      ],
      "id": "GHbRasdfasd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eijDL3jNS6WI"
      },
      "source": [
        "In Java, after restoring the trained weights, developers can run the inferences based on the loaded data.\n",
        "\n",
        "```\n",
        "try (Interpreter another_interpreter = new Interpreter(modelBuffer)) {\n",
        "    // Restore the weights from the checkpoint file.\n",
        "\n",
        "    int NUM_TESTS = 10;\n",
        "    float[][][] testImages = new float[NUM_TESTS][28][28];\n",
        "    float[][] output = new float[NUM_TESTS][10];\n",
        "\n",
        "    // Fill the test data.\n",
        "\n",
        "    // Run the inference.\n",
        "    inputs = new HashMap<>();\n",
        "    inputs.put(\"x\", testImages);\n",
        "    outputs = new HashMap<>();\n",
        "    outputs.put(\"output\", output);\n",
        "    another_interpreter.runSignature(inputs, outputs, \"infer\");\n",
        "\n",
        "    // Process the result to get the final category values.\n",
        "    int[] testLabels = new int[NUM_TESTS];\n",
        "    for (int i = 0; i < NUM_TESTS; ++i) {\n",
        "        int index = 0;\n",
        "        for (int j = 1; j < 10; ++j) {\n",
        "            if (output[i][index] < output[i][j]) index = testLabels[j];\n",
        "        }\n",
        "        testLabels[i] = index;\n",
        "    }\n",
        "}\n",
        "```"
      ],
      "id": "eijDL3jNS6WI"
    }
  ]
}
